name: "euglena"
services:
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=guest
      - RABBITMQ_DEFAULT_PASS=guest
    networks:
      - enet

#  inference:
#    image: ghcr.io/ggml-org/llama.cpp:server-cuda
#    volumes:
#      - ./inference/models:/models
#    environment:
#      - LLAMA_ARG_MODEL=/models/Phi-3-mini-4k-instruct-q4.gguf
#      - LLAMA_ARG_N_GPU_LAYERS=999
#      - LLAMA_ARG_PORT=8000
#      - LLAMA_ARG_HOST=0.0.0.0
#      - LLAMA_API=true
#    ports:
#      - "8000:8000"
#    entrypoint: ["/app/llama-server"]
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [gpu]
#    networks:
#      - enet

  chroma:
    image: chromadb/chroma:latest
    container_name: euglena-chroma
    ports:
      - "8001:8000"
    volumes:
      - ./chroma:/chroma-cache_retrieved
      - chroma_cache:/root/.cache
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma-data
    networks:
      - enet

  agent:
    build:
      context: .
      dockerfile: agent/.dockerfile
    depends_on:
      - chroma
      - rabbitmq
      - redis
    command: ["python", "-m", "app.main"]
    env_file:
      - .env
      - keys.env
    environment:
      - AGENT_STATUS_TIME=0.2
    networks:
      - enet

  agent-cli:
    profiles: ["cli"]
    build:
      context: .
      dockerfile: agent/.dockerfile
    stdin_open: true
    tty: true
    depends_on:
      - chroma
      - redis
      - rabbitmq
    command: ["python", "-m", "app.basic_cli"]
    env_file:
      - .env
      - keys.env
    networks:
      - enet

  agent-benchmark:
    profiles: ["bench"]
    build:
      context: .
      dockerfile: agent/.dockerfile
    depends_on:
      - chroma
      - redis
      - rabbitmq
    command: ["python", "-m", "app.benchmark_cli"]
    env_file:
      - .env
      - keys.env
    volumes:
      - ./agent/app/benchmark_results:/app/agent/app/benchmark_results
    networks:
      - enet

  agent-test:
    profiles: ["test"]
    build:
      context: .
      dockerfile: agent/.dockerfile
    depends_on:
      - chroma
      - redis
      - rabbitmq
    command: ["pytest", "-v", "tests"]
    env_file:
      - .env
      - keys.env
    environment:
      - AGENT_STATUS_TIME=0.2
    networks:
      - enet

  shared-test:
    profiles: ["test"]
    build:
      context: .
      dockerfile: tests/.dockerfile
    depends_on:
      - rabbitmq
    command: ["pytest", "-v", "shared/tests"]
    environment:
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
    networks:
      - enet

  gateway-test:
    profiles: ["test"]
    build:
      context: .
      dockerfile: gateway/.dockerfile
    depends_on:
      - rabbitmq
      - redis
      - agent
    command: ["pytest", "-vv", "tests"]
    env_file:
      - .env
      - keys.env
    environment:
      - GATEWAY_TEST_MODE=1
    networks:
      - enet

  gateway:
    build:
      context: .
      dockerfile: gateway/.dockerfile
    ports:
      - "8080:8080"
    depends_on:
      - rabbitmq
      - redis
      - agent
    command: ["python", "-m", "app.main"]
    env_file:
      - .env
      - keys.env
    networks:
      - enet

  api-cli:
    profiles: ["cli"]
    build:
      context: .
      dockerfile: apicli/.dockerfile
    stdin_open: true
    tty: true
    depends_on:
      - gateway
    command: [ "python", "-m", "app.apicli" ]
    env_file:
      - .env
      - keys.env
    networks:
      - enet

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: ["redis-server", "--appendonly", "yes"]
    networks:
      - enet

volumes:
  redis_data:
  chroma_cache:

networks:
  enet:
    driver: bridge
