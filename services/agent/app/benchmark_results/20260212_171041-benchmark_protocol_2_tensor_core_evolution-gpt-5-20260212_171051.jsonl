{"ts": 1770916251.7205102, "event": "init", "payload": {"mandate": "Benchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present", "max_ticks": 100, "model": "gpt-5", "blocked_limit": 3}}
{"ts": 1770916252.7274032, "event": "tick_start", "payload": {"tick": 1, "max_ticks": 100}}
{"ts": 1770916252.7291481, "event": "llm_input", "payload": {"tick": 1, "messages": [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n1 / 100"}], "system": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished.", "user": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n1 / 100"}}
{"ts": 1770916296.8411806, "event": "llm_output", "payload": {"tick": 1, "content": "{\n  \"history_update\": \"Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\",\n  \"note_update\": \"Next: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).\",\n  \"cache_update\": [],\n  \"next_action\": \"search, PTX wmma.mma.sync syntax site:nvidia.com PDF\",\n  \"cache_retrieve\": [\n    \"Volta whitepaper Tensor Core FP16 NVIDIA PDF\",\n    \"Hopper whitepaper WGMMA FP8 NVIDIA PDF\",\n    \"PTX ISA WMMA instruction syntax example\",\n    \"NVIDIA CUDA sample or cuBLAS page mentioning Tensor Cores\"\n  ],\n  \"deliverable\": \"\"\n}"}}
{"ts": 1770916296.8420513, "event": "llm_usage", "payload": {"tick": 1, "usage": {"prompt_tokens": 755, "completion_tokens": 1593, "total_tokens": 2348}}}
{"ts": 1770916297.8664687, "event": "search_observation", "payload": {"query": "PTX wmma.mma.sync syntax site:nvidia.com PDF", "results_count": 10}}
{"ts": 1770916318.857958, "event": "cache_retrieve", "payload": {"topics": ["Volta whitepaper Tensor Core FP16 NVIDIA PDF", "Hopper whitepaper WGMMA FP8 NVIDIA PDF", "PTX ISA WMMA instruction syntax example", "NVIDIA CUDA sample or cuBLAS page mentioning Tensor Cores"], "count": 12}}
{"ts": 1770916318.859182, "event": "tick_end", "payload": {"tick": 1, "history_update": "Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.", "note_update": "Next: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).", "deliverable": "", "next_action": [1, "PTX wmma.mma.sync syntax site:nvidia.com PDF"], "cache_retrieve": ["Volta whitepaper Tensor Core FP16 NVIDIA PDF", "Hopper whitepaper WGMMA FP8 NVIDIA PDF", "PTX ISA WMMA instruction syntax example", "NVIDIA CUDA sample or cuBLAS page mentioning Tensor Cores"], "cache_update_count": 0}}
{"ts": 1770916319.8613455, "event": "tick_start", "payload": {"tick": 2, "max_ticks": 100}}
{"ts": 1770916319.863108, "event": "llm_input", "payload": {"tick": 2, "messages": [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n2 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n\nNOTES:\nNext: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[2] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[3] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[4] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[5] NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides official description, references to Intel advisory INTEL-SA-00828 and the researcher site downfall.page, and shows the public publication date as 2023-08-11.\n[6] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[7] Researcher site (downfall.page) by Daniel Moghimi explains that Downfall exploits Intel’s vector Gather instruction (AVX2/AVX-512) during speculative execution to leak stale data from internal vector register files across privilege boundaries. It includes paper links, timelines, and PoC references.\n[8] Public proof-of-concept code for Downfall is hosted on GitHub in the flowyroll/downfall repository; the POC directory contains demos that exploit the Gather instruction to demonstrate transient leakage from vector registers.\n[9] Downfall (CVE-2022-40982) exploits Intel’s Gather instruction during speculative execution to leak data from the internal vector register file, enabling cross-privilege and cross-process data exfiltration (e.g., AES keys, kernel data). The researcher provides PoC code and demos, and reports initial disclosure to Intel on 2022-08-24; public presentations in Aug 2023.\n[10] NVD entry for CVE-2022-40982 (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state in Intel vector execution units. NVD published date is 2023-08-10. References include Intel advisory INTEL-SA-00828 and the researcher site downfall.page, plus various vendor advisories. CVSS v3.1 (CNA) base score 6.5. Use this as primary CVE citation.\n[11] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[12] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n\nOBSERVATIONS:\nWeb search for 'PTX wmma.mma.sync syntax site:nvidia.com PDF':\n- https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html (https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html)\n9.7.14.3. Block Scaling for mma.sync · 9.7.14.4. Matrix multiply-accumulate operation using wmma instructions · 9.7.14.4.1. Matrix Fragments for WMMA · 9.7.14.4.2. Matrix Storage for WMMA · 9.7.14.4.3. Warp-level Matrix Load Instruction: wmma.load · 9.7.14.4.4. Warp-level Matrix Store Instruction: wmma.store ·\n- https://docs.nvidia.com/cuda/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://forums.developer.nvidia.com/t/understand-the-mma-instruction-in-ptx/294471 (https://forums.developer.nvidia.com/t/understand-the-mma-instruction-in-ptx/294471)\nHowever I can not understand why. ... {%4,%5,%6,%7}, {%8,%9}, {\u0010,\u0011,\u0012,\u0013};\\n&quot; : <strong>&quot;=f&quot;(C[cd[0]]), // D[0] 32bit &quot;=f&quot;(C[cd[1]]), // D[1] &quot;=f&quot;(C[cd[2]]), // D[2] &quot;=f&quot;(C[cd[3]])</strong> ......\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf)\n9.7.12.15 Parallel Synchronization and Communication Instructions: mbarrier ... Warp Level Matrix Multiply-Accumulate Instructions . . . . . . . . . . . . . . . . . . . . 315 ... Matrix Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 ... Matrix Data-types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318 ... Matrix multiply-accumulate operation using wmma instructions .\n- https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf (https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf)\nVOLTA MMA.SYNC · <strong>D = A * B + C</strong> · PTX Syntax · mma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype · d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .ctype · = {.f16, .f32}; .dtype · = {.f16, .f32}; d: 8 x .dtype · a: 4 x .f16 · b: 4 x .f16 ·\n- https://forums.developer.nvidia.com/t/complete-minimal-ptx-example-for-mma-sync-aligned-m16n8k16-row-col-f32-f16-f16-f32/311164 (https://forums.developer.nvidia.com/t/complete-minimal-ptx-example-for-mma-sync-aligned-m16n8k16-row-col-f32-f16-f16-f32/311164)\nHere is my minimal example: (doesn’t work) // tensor_mma_kernel.ptx .version 8.4 .target sm_89 .address_size 64 // Entry point for the kernel .entry tensor_mma_kernel( .param.u64.ptr.global.align 16 A_ptr, // Pointer to matrix A in global memory .param.u64.ptr.global.align 16 B_ptr, // Pointer to matrix B in global memory .param.u64.ptr.global.align 16 C_ptr // Pointer to matrix C in global memory ) { // Declare registers for pointers .reg .u64 %ra, %rb, %rc; ...\n- https://forums.developer.nvidia.com/t/why-does-wmma-and-mma-support-different-matrix-tile-size/271067 (https://forums.developer.nvidia.com/t/why-does-wmma-and-mma-support-different-matrix-tile-size/271067)\nI want to do 8-bit matrix multiplications with tensor cores on A6000. According to the PTX guide, <strong>the WMMA instructions support m16n16k16, m8n32k16, and m32n8k16 for 8-bit matrix multiplication</strong>.\n- https://forums.developer.nvidia.com/t/inline-ptx-and-wmma-instructions/285763 (https://forums.developer.nvidia.com/t/inline-ptx-and-wmma-instructions/285763)\nSome WMMA instructions are supposed to take a sequence of 8 values together, like so: <strong>wmma.load.a.sync.aligned.row.m16n16k16.global.f16 {%r26, %r27, %r28, %r29, %r30, %r31, %r32, %r33}, [%rd4], %r25;</strong> Now, here is…\n- https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://forums.developer.nvidia.com/t/confused-about-a-b-layout-on-mma-sync-instructions/286776 (https://forums.developer.nvidia.com/t/confused-about-a-b-layout-on-mma-sync-instructions/286776)\nHello, I was wondering about the .alayout and .blayout modifiers on mma instructions. The layout seems to be very strict and described in sections 9.7.13.4.[1-13] of the PTX ISA specification. Could I get some informati…"}], "system": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished.", "user": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n2 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n\nNOTES:\nNext: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[2] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[3] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[4] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[5] NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides official description, references to Intel advisory INTEL-SA-00828 and the researcher site downfall.page, and shows the public publication date as 2023-08-11.\n[6] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[7] Researcher site (downfall.page) by Daniel Moghimi explains that Downfall exploits Intel’s vector Gather instruction (AVX2/AVX-512) during speculative execution to leak stale data from internal vector register files across privilege boundaries. It includes paper links, timelines, and PoC references.\n[8] Public proof-of-concept code for Downfall is hosted on GitHub in the flowyroll/downfall repository; the POC directory contains demos that exploit the Gather instruction to demonstrate transient leakage from vector registers.\n[9] Downfall (CVE-2022-40982) exploits Intel’s Gather instruction during speculative execution to leak data from the internal vector register file, enabling cross-privilege and cross-process data exfiltration (e.g., AES keys, kernel data). The researcher provides PoC code and demos, and reports initial disclosure to Intel on 2022-08-24; public presentations in Aug 2023.\n[10] NVD entry for CVE-2022-40982 (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state in Intel vector execution units. NVD published date is 2023-08-10. References include Intel advisory INTEL-SA-00828 and the researcher site downfall.page, plus various vendor advisories. CVSS v3.1 (CNA) base score 6.5. Use this as primary CVE citation.\n[11] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[12] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n\nOBSERVATIONS:\nWeb search for 'PTX wmma.mma.sync syntax site:nvidia.com PDF':\n- https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html (https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html)\n9.7.14.3. Block Scaling for mma.sync · 9.7.14.4. Matrix multiply-accumulate operation using wmma instructions · 9.7.14.4.1. Matrix Fragments for WMMA · 9.7.14.4.2. Matrix Storage for WMMA · 9.7.14.4.3. Warp-level Matrix Load Instruction: wmma.load · 9.7.14.4.4. Warp-level Matrix Store Instruction: wmma.store ·\n- https://docs.nvidia.com/cuda/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://forums.developer.nvidia.com/t/understand-the-mma-instruction-in-ptx/294471 (https://forums.developer.nvidia.com/t/understand-the-mma-instruction-in-ptx/294471)\nHowever I can not understand why. ... {%4,%5,%6,%7}, {%8,%9}, {\u0010,\u0011,\u0012,\u0013};\\n&quot; : <strong>&quot;=f&quot;(C[cd[0]]), // D[0] 32bit &quot;=f&quot;(C[cd[1]]), // D[1] &quot;=f&quot;(C[cd[2]]), // D[2] &quot;=f&quot;(C[cd[3]])</strong> ......\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf)\n9.7.12.15 Parallel Synchronization and Communication Instructions: mbarrier ... Warp Level Matrix Multiply-Accumulate Instructions . . . . . . . . . . . . . . . . . . . . 315 ... Matrix Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 ... Matrix Data-types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318 ... Matrix multiply-accumulate operation using wmma instructions .\n- https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf (https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf)\nVOLTA MMA.SYNC · <strong>D = A * B + C</strong> · PTX Syntax · mma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype · d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .ctype · = {.f16, .f32}; .dtype · = {.f16, .f32}; d: 8 x .dtype · a: 4 x .f16 · b: 4 x .f16 ·\n- https://forums.developer.nvidia.com/t/complete-minimal-ptx-example-for-mma-sync-aligned-m16n8k16-row-col-f32-f16-f16-f32/311164 (https://forums.developer.nvidia.com/t/complete-minimal-ptx-example-for-mma-sync-aligned-m16n8k16-row-col-f32-f16-f16-f32/311164)\nHere is my minimal example: (doesn’t work) // tensor_mma_kernel.ptx .version 8.4 .target sm_89 .address_size 64 // Entry point for the kernel .entry tensor_mma_kernel( .param.u64.ptr.global.align 16 A_ptr, // Pointer to matrix A in global memory .param.u64.ptr.global.align 16 B_ptr, // Pointer to matrix B in global memory .param.u64.ptr.global.align 16 C_ptr // Pointer to matrix C in global memory ) { // Declare registers for pointers .reg .u64 %ra, %rb, %rc; ...\n- https://forums.developer.nvidia.com/t/why-does-wmma-and-mma-support-different-matrix-tile-size/271067 (https://forums.developer.nvidia.com/t/why-does-wmma-and-mma-support-different-matrix-tile-size/271067)\nI want to do 8-bit matrix multiplications with tensor cores on A6000. According to the PTX guide, <strong>the WMMA instructions support m16n16k16, m8n32k16, and m32n8k16 for 8-bit matrix multiplication</strong>.\n- https://forums.developer.nvidia.com/t/inline-ptx-and-wmma-instructions/285763 (https://forums.developer.nvidia.com/t/inline-ptx-and-wmma-instructions/285763)\nSome WMMA instructions are supposed to take a sequence of 8 values together, like so: <strong>wmma.load.a.sync.aligned.row.m16n16k16.global.f16 {%r26, %r27, %r28, %r29, %r30, %r31, %r32, %r33}, [%rd4], %r25;</strong> Now, here is…\n- https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://forums.developer.nvidia.com/t/confused-about-a-b-layout-on-mma-sync-instructions/286776 (https://forums.developer.nvidia.com/t/confused-about-a-b-layout-on-mma-sync-instructions/286776)\nHello, I was wondering about the .alayout and .blayout modifiers on mma instructions. The layout seems to be very strict and described in sections 9.7.13.4.[1-13] of the PTX ISA specification. Could I get some informati…"}}
{"ts": 1770916368.541472, "event": "llm_output", "payload": {"tick": 2, "content": "{\n  \"history_update\": \"Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\",\n  \"note_update\": \"Next: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.\",\n  \"cache_update\": [\n    {\n      \"document\": \"PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\",\n      \"metadata\": {\n        \"title\": \"PTX tensor instruction families (WMMA, MMA, WGMMA)\",\n        \"source\": \"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\",\n        \"topics\": \"PTX, tensor cores, wmma, mma, wgmma\"\n      }\n    }\n  ],\n  \"next_action\": \"visit, https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf\",\n  \"cache_retrieve\": [\n    \"NVIDIA Volta whitepaper Tensor Cores FP16 accumulate FP32 citation\",\n    \"NVIDIA Hopper H100 whitepaper WGMMA FP8 BF16 TF32 citation\",\n    \"PTX WMMA exact syntax line wmma.mma.sync.aligned example\"\n  ],\n  \"deliverable\": \"\"\n}"}}
{"ts": 1770916368.5421135, "event": "llm_usage", "payload": {"tick": 2, "usage": {"prompt_tokens": 3126, "completion_tokens": 2030, "total_tokens": 5156}}}
{"ts": 1770916377.1809473, "event": "action_error", "payload": {"action": "ActionType.VISIT", "param": "https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf", "error": "HTTP visit failed: https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf status=None"}}
{"ts": 1770916378.0325866, "event": "cache_retrieve", "payload": {"topics": ["NVIDIA Volta whitepaper Tensor Cores FP16 accumulate FP32 citation", "NVIDIA Hopper H100 whitepaper WGMMA FP8 BF16 TF32 citation", "PTX WMMA exact syntax line wmma.mma.sync.aligned example"], "count": 9}}
{"ts": 1770916378.5898676, "event": "cache_store", "payload": {"count": 1, "metadatas": [{"title": "PTX tensor instruction families (WMMA, MMA, WGMMA)", "source": "https://docs.nvidia.com/cuda/parallel-thread-execution/index.html", "topics": "PTX, tensor cores, wmma, mma, wgmma"}]}}
{"ts": 1770916378.5905116, "event": "tick_end", "payload": {"tick": 2, "history_update": "Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.", "note_update": "Next: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.", "deliverable": "", "next_action": [2, "https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf"], "cache_retrieve": ["NVIDIA Volta whitepaper Tensor Cores FP16 accumulate FP32 citation", "NVIDIA Hopper H100 whitepaper WGMMA FP8 BF16 TF32 citation", "PTX WMMA exact syntax line wmma.mma.sync.aligned example"], "cache_update_count": 1}}
{"ts": 1770916379.559014, "event": "tick_start", "payload": {"tick": 3, "max_ticks": 100}}
{"ts": 1770916379.5598595, "event": "llm_input", "payload": {"tick": 3, "messages": [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n3 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n\nNOTES:\nNext: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[2] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[3] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[4] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[5] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state after transient execution in Intel vector execution units. It lists an NVD published date of 2023-08-10 and references Intel’s INTEL-SA-00828 advisory and the researcher site (downfall.page), providing authoritative citations and a starting point for source tracing.\n[6] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[7] Public proof-of-concept code for Downfall is hosted on GitHub in the flowyroll/downfall repository; the POC directory contains demos that exploit the Gather instruction to demonstrate transient leakage from vector registers.\n[8] Downfall (Gather Data Sampling, CVE-2022-40982) exploits Intel's AVX gather instructions (VGATHER family in AVX2/AVX-512). The transient-execution primitive leaks vector-register or microarchitectural state populated by gather operations, enabling cross-boundary data exposure during speculative execution.\n[9] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n\nOBSERVATIONS:\n[Action error: ActionType.VISIT] HTTP visit failed: https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf status=None"}], "system": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished.", "user": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n3 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n\nNOTES:\nNext: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[2] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[3] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[4] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[5] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state after transient execution in Intel vector execution units. It lists an NVD published date of 2023-08-10 and references Intel’s INTEL-SA-00828 advisory and the researcher site (downfall.page), providing authoritative citations and a starting point for source tracing.\n[6] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[7] Public proof-of-concept code for Downfall is hosted on GitHub in the flowyroll/downfall repository; the POC directory contains demos that exploit the Gather instruction to demonstrate transient leakage from vector registers.\n[8] Downfall (Gather Data Sampling, CVE-2022-40982) exploits Intel's AVX gather instructions (VGATHER family in AVX2/AVX-512). The transient-execution primitive leaks vector-register or microarchitectural state populated by gather operations, enabling cross-boundary data exposure during speculative execution.\n[9] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n\nOBSERVATIONS:\n[Action error: ActionType.VISIT] HTTP visit failed: https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf status=None"}}
{"ts": 1770916412.7456489, "event": "llm_output", "payload": {"tick": 3, "content": "{\n  \"history_update\": \"PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\",\n  \"note_update\": \"Search PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.\",\n  \"cache_update\": [],\n  \"next_action\": \"search, NVIDIA PTX ISA wmma.mma.sync.aligned site:docs.nvidia.com\",\n  \"cache_retrieve\": [\n    \"NVIDIA Volta whitepaper Tensor Cores data types\",\n    \"NVIDIA Hopper whitepaper wgmma warp-group MMA\",\n    \"PTX ISA wgmma.mma_async syntax\",\n    \"PTX ISA wmma.mma.sync.aligned syntax\",\n    \"CUDA cuBLASLt Tensor Cores documentation page\"\n  ],\n  \"deliverable\": \"\"\n}"}}
{"ts": 1770916412.7462733, "event": "llm_usage", "payload": {"tick": 3, "usage": {"prompt_tokens": 1489, "completion_tokens": 1113, "total_tokens": 2602}}}
{"ts": 1770916413.720276, "event": "search_observation", "payload": {"query": "NVIDIA PTX ISA wmma.mma.sync.aligned site:docs.nvidia.com", "results_count": 10}}
{"ts": 1770916414.5856636, "event": "cache_retrieve", "payload": {"topics": ["NVIDIA Volta whitepaper Tensor Cores data types", "NVIDIA Hopper whitepaper wgmma warp-group MMA", "PTX ISA wgmma.mma_async syntax", "PTX ISA wmma.mma.sync.aligned syntax", "CUDA cuBLASLt Tensor Cores documentation page"], "count": 15}}
{"ts": 1770916414.5864098, "event": "tick_end", "payload": {"tick": 3, "history_update": "PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.", "note_update": "Search PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.", "deliverable": "", "next_action": [1, "NVIDIA PTX ISA wmma.mma.sync.aligned site:docs.nvidia.com"], "cache_retrieve": ["NVIDIA Volta whitepaper Tensor Cores data types", "NVIDIA Hopper whitepaper wgmma warp-group MMA", "PTX ISA wgmma.mma_async syntax", "PTX ISA wmma.mma.sync.aligned syntax", "CUDA cuBLASLt Tensor Cores documentation page"], "cache_update_count": 0}}
{"ts": 1770916415.5884714, "event": "tick_start", "payload": {"tick": 4, "max_ticks": 100}}
{"ts": 1770916415.5892284, "event": "llm_input", "payload": {"tick": 4, "messages": [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n4 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n\nNOTES:\nSearch PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[3] NVD entry for CVE-2022-40982 (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state in Intel vector execution units. NVD published date is 2023-08-10. References include Intel advisory INTEL-SA-00828 and the researcher site downfall.page, plus various vendor advisories. CVSS v3.1 (CNA) base score 6.5. Use this as primary CVE citation.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[6] Downfall (CVE-2022-40982), aka Gather Data Sampling (GDS), exploits transient execution in Intel vector 'gather' instructions, enabling leakage from vector registers and microarchitectural buffers. Researcher Daniel Moghimi published technical writeups and a PoC at the flowyroll/downfall GitHub repository; Intel published INTEL-SA-00828 with mitigation guidance.\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] Researcher site (downfall.page) by Daniel Moghimi explains that Downfall exploits Intel’s vector Gather instruction (AVX2/AVX-512) during speculative execution to leak stale data from internal vector register files across privilege boundaries. It includes paper links, timelines, and PoC references.\n[9] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] Downfall (Gather Data Sampling, CVE-2022-40982) exploits Intel's AVX gather instructions (VGATHER family in AVX2/AVX-512). The transient-execution primitive leaks vector-register or microarchitectural state populated by gather operations, enabling cross-boundary data exposure during speculative execution.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n[13] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[14] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[15] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n\nOBSERVATIONS:\nWeb search for 'NVIDIA PTX ISA wmma.mma.sync.aligned site:docs.nvidia.com':\n- https://docs.nvidia.com/cuda/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html (https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html)\n9.7.13.18. Parallel Synchronization and Communication Instructions: clusterlaunchcontrol.query_cancel · 9.7.14. Warp Level Matrix Multiply-Accumulate Instructions ... 9.7.15.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf)\n9.7.12.15 Parallel Synchronization and Communication Instructions: mbarrier ... Warp Level Matrix Multiply-Accumulate Instructions . . . . . . . . . . . . . . . . . . . . 315 ... Matrix Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 ... Matrix Data-types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318 ... Matrix multiply-accumulate operation using wmma instructions .\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.7.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.7.pdf)\nThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).\n- https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/12.9.0/hopper-tuning-guide/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.9.0/hopper-tuning-guide/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/10.1/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/10.1/parallel-thread-execution/index.html)\nWhen compiling to use the Application Binary Interface (ABI), register variables are restricted to function scope and may not be declared at module scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped .reg variables, the compiler silently disables use of the ABI. Registers may have alignment boundaries required by multi-word loads and stores.\n- https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/10.2/pdf/ptx_isa_6.5.pdf (https://docs.nvidia.com/cuda/archive/10.2/pdf/ptx_isa_6.5.pdf)\nThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA)."}], "system": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished.", "user": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n4 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n\nNOTES:\nSearch PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[3] NVD entry for CVE-2022-40982 (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state in Intel vector execution units. NVD published date is 2023-08-10. References include Intel advisory INTEL-SA-00828 and the researcher site downfall.page, plus various vendor advisories. CVSS v3.1 (CNA) base score 6.5. Use this as primary CVE citation.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[6] Downfall (CVE-2022-40982), aka Gather Data Sampling (GDS), exploits transient execution in Intel vector 'gather' instructions, enabling leakage from vector registers and microarchitectural buffers. Researcher Daniel Moghimi published technical writeups and a PoC at the flowyroll/downfall GitHub repository; Intel published INTEL-SA-00828 with mitigation guidance.\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] Researcher site (downfall.page) by Daniel Moghimi explains that Downfall exploits Intel’s vector Gather instruction (AVX2/AVX-512) during speculative execution to leak stale data from internal vector register files across privilege boundaries. It includes paper links, timelines, and PoC references.\n[9] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] Downfall (Gather Data Sampling, CVE-2022-40982) exploits Intel's AVX gather instructions (VGATHER family in AVX2/AVX-512). The transient-execution primitive leaks vector-register or microarchitectural state populated by gather operations, enabling cross-boundary data exposure during speculative execution.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n[13] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[14] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[15] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n\nOBSERVATIONS:\nWeb search for 'NVIDIA PTX ISA wmma.mma.sync.aligned site:docs.nvidia.com':\n- https://docs.nvidia.com/cuda/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html (https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html)\n9.7.13.18. Parallel Synchronization and Communication Instructions: clusterlaunchcontrol.query_cancel · 9.7.14. Warp Level Matrix Multiply-Accumulate Instructions ... 9.7.15.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf)\n9.7.12.15 Parallel Synchronization and Communication Instructions: mbarrier ... Warp Level Matrix Multiply-Accumulate Instructions . . . . . . . . . . . . . . . . . . . . 315 ... Matrix Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 ... Matrix Data-types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318 ... Matrix multiply-accumulate operation using wmma instructions .\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.7.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.7.pdf)\nThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).\n- https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/12.9.0/hopper-tuning-guide/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.9.0/hopper-tuning-guide/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/10.1/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/10.1/parallel-thread-execution/index.html)\nWhen compiling to use the Application Binary Interface (ABI), register variables are restricted to function scope and may not be declared at module scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped .reg variables, the compiler silently disables use of the ABI. Registers may have alignment boundaries required by multi-word loads and stores.\n- https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/10.2/pdf/ptx_isa_6.5.pdf (https://docs.nvidia.com/cuda/archive/10.2/pdf/ptx_isa_6.5.pdf)\nThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA)."}}
{"ts": 1770916491.2022805, "event": "llm_output", "payload": {"tick": 4, "content": "{\n  \"history_update\": \"Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.\",\n  \"note_update\": \"Next: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.\",\n  \"cache_update\": [\n    {\n      \"document\": \"PTX tensor instructions evolved from WMMA (warp-level) introduced with Volta to WGMMA (warp-group, async) in Hopper. Volta used wmma.mma for FP16 matrix multiply with FP32 accumulation. Hopper added wgmma.mma_async supporting FP8 (E4M3/E5M2), BF16, and more, enabling larger tile sizes and warp-group execution.\",\n      \"metadata\": {\n        \"title\": \"PTX tensor instruction evolution: WMMA to WGMMA\",\n        \"topics\": \"PTX,WMMA,WGMMA,Volta,Hopper,Tensor Cores,data types\"\n      }\n    }\n  ],\n  \"next_action\": \"visit, https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\",\n  \"cache_retrieve\": [\n    \"PTX WMMA exact syntax line for Volta wmma.mma\",\n    \"PTX WGMMA exact syntax line for Hopper wgmma.mma_async\",\n    \"Volta whitepaper FP16 Tensor Core description\",\n    \"Hopper PTX doc showing FP8 e4m3,e5m2 support\"\n  ],\n  \"deliverable\": \"\"\n}"}}
{"ts": 1770916491.2029297, "event": "llm_usage", "payload": {"tick": 4, "usage": {"prompt_tokens": 3090, "completion_tokens": 2914, "total_tokens": 6004}}}
{"ts": 1770916497.5927188, "event": "visit_observation", "payload": {"url": "https://docs.nvidia.com/cuda/parallel-thread-execution/index.html"}}
{"ts": 1770916498.4664688, "event": "cache_retrieve", "payload": {"topics": ["PTX WMMA exact syntax line for Volta wmma.mma", "PTX WGMMA exact syntax line for Hopper wgmma.mma_async", "Volta whitepaper FP16 Tensor Core description", "Hopper PTX doc showing FP8 e4m3,e5m2 support"], "count": 12}}
{"ts": 1770916498.86876, "event": "cache_store", "payload": {"count": 1, "metadatas": [{"title": "PTX tensor instruction evolution: WMMA to WGMMA", "topics": "PTX,WMMA,WGMMA,Volta,Hopper,Tensor Cores,data types"}]}}
{"ts": 1770916498.8693187, "event": "tick_end", "payload": {"tick": 4, "history_update": "Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.", "note_update": "Next: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.", "deliverable": "", "next_action": [2, "https://docs.nvidia.com/cuda/parallel-thread-execution/index.html"], "cache_retrieve": ["PTX WMMA exact syntax line for Volta wmma.mma", "PTX WGMMA exact syntax line for Hopper wgmma.mma_async", "Volta whitepaper FP16 Tensor Core description", "Hopper PTX doc showing FP8 e4m3,e5m2 support"], "cache_update_count": 1}}
{"ts": 1770916499.8711877, "event": "tick_start", "payload": {"tick": 5, "max_ticks": 100}}
{"ts": 1770916499.8812053, "event": "llm_input", "payload": {"tick": 5, "messages": [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n5 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n4. [Tick 4] Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.\n\nNOTES:\nNext: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] CVE-2022-40982 public disclosure was August 10, 2023 per NVD. Research by Daniel Moghimi (downfall.page) with PoC code linked from the project site to GitHub. Intel advisory is INTEL-SA-00828 naming it Gather Data Sampling (GDS).\n[3] Downfall (CVE-2022-40982) exploits Intel's vector 'Gather' instruction (vgather* variants) allowing transient/execution leakage of the internal vector register file. Intel advisory INTEL-SA-00828 names this Gather Data Sampling (GDS) issue and credits researcher Daniel Moghimi.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] Linux kernel added documentation and mitigation support for Gather Data Sampling under Documentation/admin-guide/hw-vuln/gather_data_sampling.rst and related x86 mitigation code; distributions offer kernel/microcode updates and a gather_data_sampling= kernel parameter to control mitigation behavior.\n[6] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[9] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] OpenSSH release-9.8 / 9.8p1 advisory notes the regreSSHion (CVE-2024-6387) fix: Portable OpenSSH 8.5p1–9.7p1 were affected and the 9.8/9.8p1 release contains the vendor fix; advisory lists affected versions and mitigation details.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n\nOBSERVATIONS:\nVisited https://docs.nvidia.com/cuda/parallel-thread-execution/index.html:\n1. Introduction — PTX ISA 9.1 documentation\n1. Introduction\n1.1. Scalable Data-Parallel Computing using GPUs\n1.2. Goals of PTX\n1.3. PTX ISA Version 9.1\n1.4. Document Structure\n2. Programming Model\n2.1. A Highly Multithreaded Coprocessor\n2.2. Thread Hierarchy\n2.2.1. Cooperative Thread Arrays\n2.2.2. Cluster of Cooperative Thread Arrays\n2.2.3. Grid of Clusters\n2.3. Memory Hierarchy\n3. PTX Machine Model\n3.1. A Set of SIMT Multiprocessors\n3.2. Independent Thread Scheduling\n3.3. On-chip Shared Memory\n4. Syntax\n4.1. Source Format\n4.2. Comments\n4.3. Statements\n4.3.1. Directive Statements\n4.3.2. Instruction Statements\n4.4. Identifiers\n4.5. Constants\n4.5.1. Integer Constants\n4.5.2. Floating-Point Constants\n4.5.3. Predicate Constants\n4.5.4. Constant Expressions\n4.5.5. Integer Constant Expression Evaluation\n4.5.6. Summary of Constant Expression Evaluation Rules\n5. State Spaces, Types, and Variables\n5.1. State Spaces\n5.1.1. Register State Space\n5.1.2. Special Register State Space\n5.1.3. Constant State Space\n5.1.3.1. Banked Constant State Space (deprecated)\n5.1.4. Global State Space\n5.1.5. Local State Space\n5.1.6. Parameter State Space\n5.1.6.1. Kernel Function Parameters\n5.1.6.2. Kernel Function Parameter Attributes\n5.1.6.3. Kernel Parameter Attribute:\n.ptr\n5.1.6.4. Device Function Parameters\n5.1.7. Shared State Space\n5.1.8. Texture State Space (deprecated)\n5.2. Types\n5.2.1. Fundamental Types\n5.2.2. Restricted Use of Sub-Word Sizes\n5.2.3. Alternate Floating-Point Data Formats\n5.2.4. Fixed-point Data format\n5.2.5. Packed Data Types\n5.2.5.1. Packed Floating Point Data Types\n5.2.5.2. Packed Integer Data Types\n5.2.5.3. Packed Fixed-Point Data Types\n5.3. Texture Sampler and Surface Types\n5.3.1. Texture and Surface Properties\n5.3.2. Sampler Properties\n5.3.3. Channel Data Type and Channel Order Fields\n5.4. Variables\n5.4.1. Variable Declarations\n5.4.2. Vectors\n5.4.3. Array Declarations\n5.4.4. Initializers\n5.4.5. Alignment\n5.4.6. Parameterized Variable Names\n5.4.7. Variable Attributes\n5.4.8. Variable and Function Attribute Directive:\n.attribute\n5.5. Tensors\n5.5.1. Tensor Dimension, size and format\n5.5.1.1. Sub-byte Types\n5.5.1.1.1. Padding and alignment of the sub-byte types\n5.5.2. Tensor Access Modes\n5.5.3. Tiled Mode\n5.5.3.1. Bounding Box\n5.5.3.2. Traversal-Stride\n5.5.3.3. Out of Boundary Access\n5.5.3.4.\n.tile::scatter4\nand\n.tile::gather4\nmodes\n5.5.3.4.1. Bounding Box\n5.5.4.\nim2col\nmode\n5.5.4.1. Bounding Box\n5.5.4.2. Traversal Stride\n5.5.4.3. Out of Boundary Access\n5.5.5.\nim2col::w\nand\nim2col::w::128\nmodes\n5.5.5.1. Bounding Box\n5.5.5.2. Traversal Stride\n5.5.5.3.\nwHalo\n5.5.5.4.\nwOffset\n5.5.6. Interleave layout\n5.5.7. Swizzling Modes\n5.5.8. Tensor-map\n6. Instruction Operands\n6.1. Operand Type Information\n6.2. Source Operands\n6.3. Destination Operands\n6.4. Using Addresses, Arrays, and Vectors\n6.4.1. Addresses as Operands\n6.4.1.1. Generic Addressing\n6.4.2. Arrays as Operands\n6.4.3. Vectors as Operands\n6.4.4. Labels and Function Names as Operands\n6.5. Type Conversion\n6.5.1. Scalar Conversions\n6.5.2. Rounding Modifiers\n6.6. Operand Costs\n7. Abstracting the ABI\n7.1. Function Declarations and Definitions\n7.1.1. Changes from PTX ISA Version 1.x\n7.2. Variadic Functions\n7.3. Alloca\n8. Memory Consistency Model\n8.1. Scope and applicability of the model\n8.1.1. Limitations on atomicity at system scope\n8.2. Memory operations\n8.2.1. Overlap\n8.2.2. Aliases\n8.2.3. Multimem Addresses\n8.2.4. Memory Operations on Vector Data Types\n8.2.5. Memory Operations on Packed Data Types\n8.2.6. Initialization\n8.3. State spaces\n8.4. Operation types\n8.4.1. mmio Operation\n8.4.2. volatile Operation\n8.5. Scope\n8.6. Proxies\n8.7. Morally strong operations\n8.7.1. Conflict and Data-races\n8.7.2. Limitations on Mixed-size Data-races\n8.8. Release and Acquire Patterns\n8.9. Ordering of memory operations\n8.9.1. Program Order\n8.9.1.1. Asynchronous Operations\n8.9.2. Observation Order\n8.9.3. Fence-SC Order\n8.9.4. Memory synchronization\n8.9.5. Causality Order\n8.9.6. Coherence Order\n8.9.7. Communication Order\n8.10. Axioms\n8.10.1. Coherence\n8.10.2. Fence-SC\n8.10.3. Atomicity\n8.10.4. No Thin Air\n8.10.5. Sequential Consistency Per Location\n8.10.6. Causality\n8.11. Special Cases\n8.11.1. Reductions do not form Acquire Patterns\n9. Instruction Set\n9.1. Format and Semantics of Instruction Descriptions\n9.2. PTX Instructions\n9.3. Predicated Execution\n9.3.1. Comparisons\n9.3.1.1. Integer and Bit-Size Comparisons\n9.3.1.2. Floating Point Comparisons\n9.3.2. Manipulating Predicates\n9.4. Type Information for Instructions and Operands\n9.4.1. Operand Size Exceeding Instruction-Type Size\n9.5. Divergence of Threads in Control Constructs\n9.6. Semantics\n9.6.1. Machine-Specific Semantics of 16-bit Code\n9.7. Instructions\n9.7.1. Integer Arithmetic Instructions\n9.7.1.1. Integer Arithmetic Instructions:\nadd\n9.7.1.2. Integer Arithmetic Instructions:\nsub\n9.7.1.3. Integer Arithmetic Instructions:\nmul\n9.7.1.4. Integer Arithmetic Instructions:\nmad\n9.7.1.5. Integer Arithmetic Instructions:\nmul24\n9.7.1.6. Integer Arithmetic Instructions:\nmad24\n9.7.1.7. Integer Arithmetic Instructions:\nsad\n9.7.1.8. Integer Arithmetic Instructions:\ndiv\n9.7.1.9. Integer Arithmetic Instructions:\nrem\n9.7.1.10. Integer Arithmetic Instructions:\nabs\n9.7.1.11. Integer Arithmetic Instructions:\nneg\n9.7.1.12. Integer Arithmetic Instructions:\nmin\n9.7.1.13. Integer Arithmetic Instructions:\nmax\n9.7.1.14. Integer Arithmetic Instructions:\npopc\n9.7.1.15. Integer Arithmetic Instructions:\nclz\n9.7.1.16. Integer Arithmetic Instructions:\nbfind\n9.7.1.17. Integer Arithmetic Instructions:\nfns\n9.7.1.18. Integer Arithmetic Instructions:\nbrev\n9.7.1.19. Integer Arithmetic Instructions:\nbfe\n9.7.1.20. Integer Arithmetic Instructions:\nbfi\n9.7.1.21. Integer Arithmetic Instructions:\nszext\n9.7.1.22. Integer Arithmetic Instructions:\nbmsk\n9.7.1.23. Integer Arithmetic Instructions:\ndp4a\n9.7.1.24. Integer Arithmetic Instructions:\ndp2a\n9.7.2. Extended-Precision Integer Arithmetic Instructions\n9.7.2.1. Extended-Precision Arithmetic Instructions:\nadd.cc\n9.7.2.2. Extended-Precision Arithmetic Instructions:\naddc\n9.7.2.3. Extended-Precision Arithmetic Instructions:\nsub.cc\n9.7.2.4. Extended-Precision Arithmetic Instructions:\nsubc\n9.7.2.5. Extended-Precision Arithmetic Instructions:\nmad.cc\n9.7.2.6. Extended-Precision Arithmetic Instructions:\nmadc\n9.7.3. Floating-Point Instructions\n9.7.3.1. Floating Point Instructions:\ntestp\n9.7.3.2. Floating Point Instructions:\ncopysign\n9.7.3.3. Floating Point Instructions:\nadd\n9.7.3.4. Floating Point Instructions:\nsub\n9.7.3.5. Floating Point Instructions:\nmul\n9.7.3.6. Floating Point Instructions:\nfma\n9.7.3.7. Floating Point Instructions:\nmad\n9.7.3.8. Floating Point Instructions:\ndiv\n9.7.3.9. Floating Point Instructions:\nabs\n9.7.3.10. Floating Point Instructions:\nneg\n9.7.3.11. Floating Point Instructions:\nmin\n9.7.3.12. Floating Point Instructions:\nmax\n9.7.3.13. Floating Point Instructions:\nrcp\n9.7.3.14. Floating Point Instructions:\nrcp.approx.ftz.f64\n9.7.3.15. Floating Point Instructions:\nsqrt\n9.7.3.16. Floating Point Instructions:\nrsqrt\n9.7.3.17. Floating Point Instructions:\nrsqrt.approx.ftz.f64\n9.7.3.18. Floating Point Instructions:\nsin\n9.7.3.19. Floating Point Instructions:\ncos\n9.7.3.20. Floating Point Instructions:\nlg2\n9.7.3.21. Floating Point Instructions:\nex2\n9.7.3.22. Floating Point Instructions:\ntanh\n9.7.4. Half Precision Floating-Point Instructions\n9.7.4.1. Half Precision Floating Point Instructions:\nadd\n9.7.4.2. Half Precision Floating Point Instructions:\nsub\n9.7.4.3. Half Precision Floating Point Instructions:\nmul\n9.7.4.4. Half Precision Floating Point Instructions:\nfma\n9.7.4.5. Half Precision Floating Point Instructions:\nneg\n9.7.4.6. Half Precision Floating Point Instructions:\nabs\n9.7.4.7. Half Precision Floating Point Instructions:\nmin\n9.7.4.8. Half Precision Floating Point Instructions:\nmax\n9.7.4.9. Half Precision Floating Point Instructions:\ntanh\n9.7.4.10. Half Precision Floating Point Instructions:\nex2\n9.7.5. Mixed Precision Floating-Point Instructions\n9.7.5.1. Mixed Precision Floating Point Instructions:\nadd\n9.7.5.2. Mixed Precision Floating Point Instructions:\nsub\n9.7.5.3. Mixed Precision Floating Point Instructions:\nfma\n9.7.6. Comparison and Selection Instructions\n9.7.6.1. Comparison and Selection Instructions:\nset\n9.7.6.2. Comparison and Selection Instructions:\nsetp\n9.7.6.3. Comparison and Selection Instructions:\nselp\n9.7.6.4. Comparison and Selection Instructions:\nslct\n9.7.7. Half Precision Comparison Instructions\n9.7.7.1. Half Precision Comparison Instructions:\nset\n9.7.7.2. Half Precision Comparison Instructions:\nsetp\n9.7.8. Logic and Shift Instructions\n9.7.8.1. Logic and Shift Instructions:\nand\n9.7.8.2. Logic and Shift Instructions:\nor\n9.7.8.3. Logic and Shift Instructions:\nxor\n9.7.8.4. Logic and Shift Instructions:\nnot\n9.7.8.5. Logic and Shift Instructions:\ncnot\n9.7.8.6. Logic and Shift Instructions:\nlop3\n9.7.8.7. Logic and Shift Instructions:\nshf\n9.7.8.8. Logic and Shift Instructions:\nshl\n9.7.8.9. Logic and Shift Instructions:\nshr\n9.7.9. Data Movement and Conversion Instructions\n9.7.9.1. Cache Operators\n9.7.9.2. Cache Eviction Priority Hints\n9.7.9.3. Data Movement and Conversion Instructions:\nmov\n9.7.9.4. Data Movement and Conversion Instructions:\nmov\n9.7.9.5. Data Movement and Conversion Instructions:\nshfl\n(deprecated)\n9.7.9.6. Data Movement and Conversion Instructions:\nshfl.sync\n9.7.9.7. Data Movement and Conversion Instructions:\nprmt\n9.7.9.8. Data Movement and Conversion Instructions:\nld\n9.7.9.9. Data Movement and Conversion Instructions:\nld.global.nc\n9.7.9.10. Data Movement and Conversion Instructions:\nldu\n9.7.9.11. Data Movement and Conversion Instructions:\nst\n9.7.9.12. Data Movement and Conversion Instructions:\nst.async\n9.7.9.13. Data Movement and Conversion Instructions:\nst.bulk\n9.7.9.14. Data Movement and Conversion Instructions:\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\n9.7.9.15. Data Movement and Conversion Instructions:\nprefetch\n,\nprefetchu\n9.7.9.16. Data Movement and Conversion Instructions:\napplypriority\n9.7.9.17. Data Movement and Conversion Instructions:\ndiscard\n9.7.9.18. Data Movement and Conversion Instructions:\ncreatepolicy\n9.7.9.19. Data Movement and Conversion Instructions:\nisspacep\n9.7.9.20. Data Movement and Conversion Instructions:\ncvta\n9.7.9.21. Data Movement and Conversion Instructions:\ncvt\n9.7.9.22. Data Movement and Conversion Instructions:\ncvt.pack\n9.7.9.23. Data Movement and Conversion Instructions:\nmapa\n9.7.9.24. Data Movement and Conversion Instructions:\ngetctarank\n9.7.9.25. Data Movement and Conversion Instructions: Asynchronous copy\n9.7.9.25.1. Completion Mechanisms for Asynchronous Copy Operations\n9.7.9.25.1.1. Async-group mechanism\n9.7.9.25.1.2. Mbarrier-based mechanism\n9.7.9.25.2. Async Proxy\n9.7.9.25.3. Data Movement and Conversion Instructions: Non-bulk copy\n9.7.9.25.3.1. Data Movement and Conversion Instructions:\ncp.async\n9.7.9.25.3.2. Data Movement and Conversion Instructions:\ncp.async.commit_group\n9.7.9.25.3.3. Data Movement and Conversion Instructions:\ncp.async.wait_group\n/\ncp.async.wait_all\n9.7.9.25.4. Data Movement and Conversion Instructions: Bulk copy\n9.7.9.25.4.1. Data Movement and Conversion Instructions:\ncp.async.bulk\n9.7.9.25.4.2. Data Movement and Conversion Instructions:\ncp.reduce.async.bulk\n9.7.9.25.4.3. Data Movement and Conversion Instructions:\ncp.async.bulk.prefetch\n9.7.9.26. Data Movement and Conversion Instructions:\nmultimem.cp.async.bulk\n9.7.9.27. Data Movement and Conversion Instructions:\nmultimem.cp.reduce.async.bulk\n9.7.9.27.1. Data Movement and Conversion Instructions: Tensor copy\n9.7.9.27.1.1. Restriction on Tensor Copy instructions\n9.7.9.27.1.2. Data Movement and Conversion Instructions:\ncp.async.bulk.tensor\n9.7.9.27.1.3. Data Movement and Conversion Instructions:\ncp.reduce.async.bulk.tensor\n9.7.9.27.1.4. Data Movement and Conversion Instructions:\ncp.async.bulk.prefetch.tensor\n9.7.9.27.2. Data Movement and Conversion Instructions: Bulk and Tensor copy completion instructions\n9.7.9.27.2.1. Data Movement and Conversion Instructions:\ncp.async.bulk.commit_group\n9.7.9.27.2.2. Data Movement and Conversion Instructions:\ncp.async.bulk.wait_group\n9.7.9.28. Data Movement and Conversion Instructions:\ntensormap.replace\n9.7.10. Texture Instructions\n9.7.10.1. Texturing Modes\n9.7.10.2. Mipmaps\n9.7.10.3. Texture Instructions:\ntex\n9.7.10.4. Texture Instructions:\ntld4\n9.7.10.5. Texture Instructions:\ntxq\n9.7.10.6. Texture Instructions:\nistypep\n9.7.11. Surface Instructions\n9.7.11.1. Surface Instructions:\nsuld\n9.7.11.2. Surface Instructions:\nsust\n9.7.11.3. Surface Instructions:\nsured\n9.7.11.4. Surface Instructions:\nsuq\n9.7.12. Control Flow Instructions\n9.7.12.1. Control Flow Instructions:\n{}\n9.7.12.2. Control Flow Instructions:\n@\n9.7.12.3. Control Flow Instructions:\nbra\n9.7.12.4. Control Flow Instructions:\nbrx.idx\n9.7.12.5. Control Flow Instructions:\ncall\n9.7.12.6. Control Flow Instructions:\nret\n9.7.12.7. Control Flow Instructions:\nexit\n9.7.13. Parallel Synchronization and Communication Instructions\n9.7.13.1. Parallel Synchronization and Communication Instructions:\nbar\n,\nbarrier\n9.7.13.2. Parallel Synchronization and Communication Instructions:\nbar.warp.sync\n9.7.13.3. Parallel Synchronization and Communication Instructions:\nbarrier.cluster\n9.7.13.4. Parallel Synchronization and Communication Instructions:\nmembar\n/\nfence\n9.7.13.5. Parallel Synchronization and Communication Instructions:\natom\n9.7.13.6. Parallel Synchronization and Communication Instructions:\nred\n9.7.13.7. Parallel Synchronization and Communication Instructions:\nred.async\n9.7.13.8. Parallel Synchronization and Communication Instructions:\nvote\n(deprecated)\n9.7.13.9. Parallel Synchronization and Communication Instructions:\nvote.sync\n9.7.13.10. Parallel Synchronization and Communication Instructions:\nmatch.sync\n9.7.13.11. Parallel Synchronization and Communication Instructions:\nactivemask\n9.7.13.12. Parallel Synchronization and Communication Instructions:\nredux.sync\n9.7.13.13. Parallel Synchronization and Communication Instructions:\ngriddepcontrol\n9.7.13.14. Parallel Synchronization and Communication Instructions:\nelect.sync\n9.7.13.15. Parallel Synchronization and Communication Instructions:\nmbarrier\n9.7.13.15.1. Size and alignment of mbarrier object\n9.7.13.15.2. Contents of the mbarrier object\n9.7.13.15.3. Lifecycle of the mbarrier object\n9.7.13.15.4. Phase of the mbarrier object\n9.7.13.15.5. Tracking asynchronous operations by the mbarrier object\n9.7.13.15.5.1. expect-tx operation\n9.7.13.15.5.2. complete-tx operation\n9.7.13.15.6. Phase Completion of the mbarrier object\n9.7.13.15.7. Arrive-on operation on mbarrier object\n9.7.13.15.8. mbarrier support with shared memory\n9.7.13.15.9. Parallel Synchronization and Communication Instructions:\nmbarrier.init\n9.7.13.15.10. Parallel Synchronization and Communication Instructions:\nmbarrier.inval\n9.7.13.15.11. Parallel Synchronization and Communication Instructions:\nmbarrier.expect_tx\n9.7.13.15.12. Parallel Synchronization and Communication Instructions:\nmbarrier.complete_tx\n9.7.13.15.13. Parallel Synchronization and Communication Instructions:\nmbarrier.arrive\n9.7.13.15.14. Parallel Synchronization and Communication Instructions:\nmbarrier.arrive_drop\n9.7.13.15.15. Parallel Synchronization and Communication Instructions:\ncp.async.mbarrier.arrive\n9.7.13.15.16. Parallel Synchronization and Communication Instructions:\nmbarrier.test_wait\n/\nmbarrier.try_wait\n9.7.13.15.17. Parallel Synchronization and Communication Instructions:\nmbarrier.pending_count\n9.7.13.16. Parallel Synchronization and Communication Instructions:\ntensormap.cp_fenceproxy\n9.7.13.17. Parallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.try_cancel\n9.7.13.18. Parallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.query_cancel\n9.7.14. Warp Level Matrix Multiply-Accumulate Instructions\n9.7.14.1. Matrix Shape\n9.7.14.2. Matrix Data-types\n9.7.14.3. Block Scaling for\nmma.sync\n9.7.14.4. Matrix multiply-accumulate operation using\nwmma\ninstructions\n9.7.14.4.1. Matrix Fragments for WMMA\n9.7.14.4.2. Matrix Storage for WMMA\n9.7.14.4.3. Warp-level Matrix Load Instruction:\nwmma.load\n9.7.14.4.4. Warp-level Matrix Store Instruction:\nwmma.store\n9.7.14.4.5. Warp-level Matrix Multiply-and-Accumulate Instruction:\nwmma.mma\n9.7.14.5. Matrix multiply-accumulate operation using\nmma\ninstruction\n9.7.14.5.1. Matrix Fragments for\nmma.m8n8k4\nwith\n.f16\nfloating point type\n9.7.14.5.2. Matrix Fragments for\nmma.m8n8k4\nwith\n.f64\nfloating point type\n9.7.14.5.3. Matrix Fragments for\nmma.m8n8k16\n9.7.14.5.4. Matrix Fragments for\nmma.m8n8k32\n9.7.14.5.5. Matrix Fragments for\nmma.m8n8k128\n9.7.14.5.6. Matrix Fragments for\nmma.m16n8k4\n9.7.14.5.7. Matrix Fragments for\nmma.m16n8k8\n9.7.14.5.8. Matrix Fragments for\nmma.m16n8k16\nwith floating point type\n9.7.14.5.9. Matrix Fragments for\nmma.m16n8k16\nwith integer type\n9.7.14.5.10. Matrix Fragments for\nmma.m16n8k32\n9.7.14.5.11. Matrix Fragments for\nmma.m16n8k64\n9.7.14.5.12. Matrix Fragments for\nmma.m16n8k128\n9.7.14.5.13. Matrix Fragments for\nmma.m16n8k256\n9.7.14.5.14. Multiply-and-Accumulate Instruction:\nmma\n9.7.14.5.15. Warp-level matrix load instruction:\nldmatrix\n9.7.14.5.16. Warp-level matrix store instruction:\nstmatrix\n9.7.14.5.17. Warp-level matrix transpose instruction:\nmovmatrix\n9.7.14.6. Matrix multiply-accumulate operation using\nmma.sp\ninstruction with sparse matrix A\n9.7.14.6.1. Sparse matrix storage\n9.7.14.6.2. Matrix fragments for multiply-accumulate operation with sparse matrix A\n9.7.14.6.2.1. Matrix Fragments for sparse\nmma.m16n8k16\nwith\n.f16\nand\n.bf16\ntypes\n9.7.14.6.2.2. Matrix Fragments for sparse\nmma.m16n8k32\nwith\n.f16\nand\n.bf16\ntypes\n9.7.14.6.2.3. Matrix Fragments for sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type\n9.7.14.6.2.4. Matrix Fragments for sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type\n9.7.14.6.2.5. Matrix Fragments for sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type\n9.7.14.6.2.6. Matrix Fragments for sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\ntype\n9.7.14.6.2.7. Matrix Fragments for sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type\n9.7.14.6.2.8. Matrix Fragments for sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\ninteger type\n9.7.14.6.3. Multiply-and-Accumulate Instruction:\nmma.sp\n/\nmma.sp::ordered_metadata\n9.7.15. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions\n9.7.15.1. Warpgroup\n9.7.15.2. Matrix Shape\n9.7.15.3. Matrix Data-types\n9.7.15.4. Async Proxy\n9.7.15.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using\nwgmma.mma_async\ninstruction\n9.7.15.5.1. Register Fragments and Shared Memory Matrix Layouts\n9.7.15.5.1.1. Register Fragments\n9.7.15.5.1.1.1. Matrix Fragments for\nwgmma.mma_async.m64nNk16\n9.7.15.5.1.1.2. Matrix Fragments for\nwgmma.mma_async.m64nNk8\n9.7.15.5.1.1.3. Matrix Fragments for\nwgmma.mma_async.m64nNk32\n9.7.15.5.1.1.4. Matrix Fragments for\nwgmma.mma_async.m64nNk256\n9.7.15.5.1.2. Shared Memory Matrix Layout\n9.7.15.5.1.2.1. Major-ness supported by Strides\n9.7.15.5.1.2.1.1. Leading Dimension Byte Offset\n9.7.15.5.1.2.1.2. Stride Dimension Byte Offset\n9.7.15.5.1.2.1.3. Canonical Layouts\n9.7.15.5.1.2.2. Matrix Descriptor Format\n9.7.15.5.2. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async\n9.7.15.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using\nwgmma.mma_async.sp\ninstruction\n9.7.15.6.1. Sparse matrix storage\n9.7.15.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A\n9.7.15.6.2.1. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk32\n9.7.15.6.2.2. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk16\n9.7.15.6.2.3. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk64\n9.7.15.6.3. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async.sp\n9.7.15.7. Asynchronous\nwgmma\nProxy Operations\n9.7.15.7.1. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.fence\n9.7.15.7.2. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.commit_group\n9.7.15.7.3. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.wait_group\n9.7.16. TensorCore 5th Generation Family Instructions\n9.7.16.1. Tensor Memory\n9.7.16.1.1. Tensor Memory Addressing\n9.7.16.1.2. Tensor Memory Allocation\n9.7.16.2. Matrix and Data Movement Shape\n9.7.16.2.1. Matrix Shape\n9.7.16.2.1.1. Target ISA Note\n9.7.16.2.2. Specifying Matrix Shape\n9.7.16.2.3. Data Movement Shape\n9.7.16.2.3.1. Memory Layout\n9.7.16.2.3.1.1. Matrix fragments for shape .32x32b\n9.7.16.2.3.1.2. Matrix fragments for shape .16x64b\n9.7.16.2.3.1.3. Matrix fragments for shape .16x128b\n9.7.16.2.3.1.4. Matrix fragments for shape .16x256b\n9.7.16.2.3.1.5. Matrix fragments for shape .16x32bx2\n9.7.16.3. Major-ness supported by Strides\n9.7.16.3.1. Leading Dimension Stride: relative offset or absolute address\n9.7.16.3.1.1. Relative offset mode\n9.7.16.3.1.2. Absolute address mode for K dimension being 48B\n9.7.16.3.1.2.1. Restrictions on the Leading Dimension Absolute Address Stride\n9.7.16.3.2. Stride Dimension Byte Offset\n9.7.16.3.3. Canonical Layouts\n9.7.16.4. Matrix Descriptors\n9.7.16.4.1. Shared memory descriptor\n9.7.16.4.1.1. Target ISA Note\n9.7.16.4.2. Instruction descriptor\n9.7.16.4.3. Zero-Column Mask Descriptor\n9.7.16.5. Issue Granularity\n9.7.16.5.1. CTA Pair\n9.7.16.5.2. Peer CTA\n9.7.16.6. Memory Consistency Model for 5th generation of TensorCore operations\n9.7.16.6.1. Asynchronous Operations\n9.7.16.6.2. Pipelined tcgen05 Instructions\n9.7.16.6.2.1. Implicitly pipelined tcgen05 Instructions\n9.7.16.6.2.1.1. mbarrier based completion mechanism\n9.7.16.6.2.1.2.\ntcgen05.wait\ninstruction based completion mechanism\n9.7.16.6.3. Specialized Inter-thread Synchronization for tcgen05 instructions\n9.7.16.6.4. Canonical synchronization patterns\n9.7.16.6.4.1. Pipelined instructions, same thread\n9.7.16.6.4.2. Non-pipelined instructions, same thread\n9.7.16.6.4.3. Pipelined instructions, different thread\n9.7.16.6.4.4. Non-pipelined instructions, different thread\n9.7.16.6.4.5. Register dependencies, same thread\n9.7.16.6.5. Shared Memory Accesses\n9.7.16.7. Tensor Memory Allocation and Management Instructions\n9.7.16.7.1. Tensorcore 5th Generation Instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n9.7.16.8. Tensor Memory and Register Load/Store Instructions\n9.7.16.8.1. Access restrictions\n9.7.16.8.2. Packing and Unpacking\n9.7.16.8.3. Tensorcore 5th Generation Instructions:\ntcgen05.ld\n9.7.16.8.4. Tensorcore 5th Generation Instructions:\ntcgen05.st\n9.7.16.8.5. Tensorcore 5th Generation Instructions:\ntcgen05.wait\n9.7.16.9. Tensor Memory Data Movement Instructions\n9.7.16.9.1. Optional Decompression\n9.7.16.9.1.1. Decompression of 4-bit floating point to 8-bit type\n9.7.16.9.1.2. Decompression of 6-bit floating point to 8-bit type\n9.7.16.9.2. Tensorcore 5th Generation Instructions:\ntcgen05.cp\n9.7.16.9.3. Tensorcore 5th Generation Instructions:\ntcgen05.shift\n9.7.16.10. TensorCore 5th Generation Matrix Multiply and accumulate Operations\n9.7.16.10.1. Transpose and Negate operations\n9.7.16.10.2. Matrix Layout Organization\n9.7.16.10.3. Valid Combinations of Type-Size, Major-ness and Swizzling\n9.7.16.10.4. Packing formats of elements in Tensor and Shared memory\n9.7.16.10.4.1. Packing format for matrix D in Tensor Memory\n9.7.16.10.4.2. Packing format for matrix A and B\n9.7.16.10.4.3. Packing format used for matrix A by\n.kind::mxf8f6f4\nin Tensor Memory\n9.7.16.10.4.4. Packing format used for matrix A and B by\n.kind::mxf8f6f4\nin Shared Memory\n9.7.16.10.4.5. Packing format used for matrix A by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Tensor Memory\n9.7.16.10.4.6. Packing format used for matrix A and B by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Shared Memory\n9.7.16.10.5. Data Path Layout Organization\n9.7.16.10.5.1. Layout A (M = 256)\n9.7.16.10.5.2. Layout B (M = 128 + cta-group::2 + Dense A matrix)\n9.7.16.10.5.3. Layout C (M = 128 + cta-group::2 + Sparse A matrix)\n9.7.16.10.5.4. Layout D (M = 128 + cta-group::1)\n9.7.16.10.5.5. Layout E (M = 64 + .ws mode)\n9.7.16.10.5.6. Layout F (M = 64 + non .ws mode)\n9.7.16.10.5.7. Layout G (M = 32)\n9.7.16.10.6. Shared Memory Layout and Swizzling\n9.7.16.10.7. Block Scaling for\ntcgen05.mma.sync\n9.7.16.10.7.1. Valid combinations of scale_vectorsize with types and MMA-Kind\n9.7.16.10.7.2. Scale Factor A ID\n9.7.16.10.7.2.1. Layout of the Scale Factor A Matrix for scale_vec::1X/block32 with K=32/K=64\n9.7.16.10.7.2.2. Layout of the Scale Factor A Matrix for scale_vec::2X/block32 with K=64/K=128\n9.7.16.10.7.2.3. Layout of the Scale Factor A Matrix for scale_vec::4X/block16 with K=64/K=128\n9.7.16.10.7.2.4. Layout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n9.7.16.10.7.2.5. Layout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n9.7.16.10.7.3. Scale Factor B ID\n9.7.16.10.7.3.1. Layout of the Scale Factor B Matrix for scale_vec::1X/block32 with K=32/K=64\n9.7.16.10.7.3.2. Layout of the Scale Factor B Matrix for scale_vec::2X/block32 with K=64/K=128\n9.7.16.10.7.3.3. Layout of the Scale Factor B Matrix for scale_vec::4X/block16 with K=64/K=128\n9.7.16.10.7.3.4. Layout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n9.7.16.10.7.3.5. Layout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n9.7.16.10.8. Sparse Matrices\n9.7.16.10.8.1. Sparse\ntcgen05.mma.sp\nwith\n.kind::tf32\n9.7.16.10.8.2. Sparse\ntcgen05.mma.sp\nwith\n.kind::f16\n,\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n9.7.16.10.8.3. Sparse\ntcgen05.mma.sp\nwith\n.kind::mxf4\nand\n.kind::mxf4nvf4\n9.7.16.10.8.4. Sparsity selector\n9.7.16.10.8.4.1. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f16\n9.7.16.10.8.4.2. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f16\n9.7.16.10.8.4.3. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::tf32\n9.7.16.10.8.4.4. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::tf32\n9.7.16.10.8.4.5. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n9.7.16.10.8.4.6. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n9.7.16.10.8.5. Alignment restriction\n9.7.16.10.9. TensorCore 5th Generation of MMA Instructions\n9.7.16.10.9.1. TensorCore 5th Generation Instructions:\ntcgen05.mma\n9.7.16.10.9.2. TensorCore 5th Generation Instructions:\ntcgen05.mma.sp\n9.7.16.10.9.3. TensorCore 5th Generation Instructions:\ntcgen05.mma.ws\n9.7.16.10.9.4. TensorCore 5th Generation Instructions:\ntcgen05.mma.ws.sp\n9.7.16.11. TensorCore 5th Generation Specialized Synchronization Operations\n9.7.16.11.1. TensorCore 5th Generation Instructions:\ntcgen05.fence\n9.7.16.12. TensorCore 5th Generation Async Synchronization Operations\n9.7.16.12.1. TensorCore 5th Generation Instructions:\ntcgen05.commit\n9.7.17. Stack Manipulation Instructions\n9.7.17.1. Stack Manipulation Instructions:\nstacksave\n9.7.17.2. Stack Manipulation Instructions:\nstackrestore\n9.7.17.3. Stack Manipulation Instructions:\nalloca\n9.7.18. Video Instructions\n9.7.18.1. Scalar Video Instructions\n9.7.18.1.1. Scalar Video Instructions:\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\n9.7.18.1.2. Scalar Video Instructions:\nvshl\n,\nvshr\n9.7.18.1.3. Scalar Video Instructions:\nvmad\n9.7.18.1.4. Scalar Video Instructions:\nvset\n9.7.18.2. SIMD Video Instructions\n9.7.18.2.1. SIMD Video Instructions:\nvadd2\n,\nvsub2\n,\nvavrg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\n9.7.18.2.2. SIMD Video Instructions:\nvset2\n9.7.18.2.3. SIMD Video Instructions:\nvadd4\n,\nvsub4\n,\nvavrg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\n9.7.18.2.4. SIMD Video Instructions:\nvset4\n9.7.19. Miscellaneous Instructions\n9.7.19.1. Miscellaneous Instructions:\nbrkpt\n9.7.19.2. Miscellaneous Instructions:\nnanosleep\n9.7.19.3. Miscellaneous Instructions:\npmevent\n9.7.19.4. Miscellaneous Instructions:\ntrap\n9.7.19.5. Miscellaneous Instructions:\nsetmaxnreg\n10. Special Registers\n10.1. Special Registers:\n%tid\n10.2. Special Registers:\n%ntid\n10.3. Special Registers:\n%laneid\n10.4. Special Registers:\n%warpid\n10.5. Special Registers:\n%nwarpid\n10.6. Special Registers:\n%ctaid\n10.7. Special Registers:\n%nctaid\n10.8. Special Registers:\n%smid\n10.9. Special Registers:\n%nsmid\n10.10. Special Registers:\n%gridid\n10.11. Special Registers:\n%is_explicit_cluster\n10.12. Special Registers:\n%clusterid\n10.13. Special Registers:\n%nclusterid\n10.14. Special Registers:\n%cluster_ctaid\n10.15. Special Registers:\n%cluster_nctaid\n10.16. Special Registers:\n%cluster_ctarank\n10.17. Special Registers:\n%cluster_nctarank\n10.18. Special Registers:\n%lanemask_eq\n10.19. Special Registers:\n%lanemask_le\n10.20. Special Registers:\n%lanemask_lt\n10.21. Special Registers:\n%lanemask_ge\n10.22. Special Registers:\n%lanemask_gt\n10.23. Special Registers:\n%clock\n,\n%clock_hi\n10.24. Special Registers:\n%clock64\n10.25. Special Registers:\n%pm0\n…\n%pm7\n10.26. Special Registers:\n%pm0_64\n…\n%pm7_64\n10.27. Special Registers:\n%envreg<32>\n10.28. Special Registers:\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n10.29. Special Registers:\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n10.30. Special Registers:\n%total_smem_size\n10.31. Special Registers:\n%aggr_smem_size\n10.32. Special Registers:\n%dynamic_smem_size\n10.33. Special Registers:\n%current_graph_exec\n11. Directives\n11.1. PTX Module Directives\n11.1.1. PTX Module Directives:\n.version\n11.1.2. PTX Module Directives:\n.target\n11.1.3. PTX Module Directives:\n.address_size\n11.2. Specifying Kernel Entry Points and Functions\n11.2.1. Kernel and Function Directives:\n.entry\n11.2.2. Kernel and Function Directives:\n.func\n11.2.3. Kernel and Function Directives:\n.alias\n11.3. Control Flow Directives\n11.3.1. Control Flow Directives:\n.branchtargets\n11.3.2. Control Flow Directives:\n.calltargets\n11.3.3. Control Flow Directives:\n.callprototype\n11.4. Performance-Tuning Directives\n11.4.1. Performance-Tuning Directives:\n.maxnreg\n11.4.2. Performance-Tuning Directives:\n.maxntid\n11.4.3. Performance-Tuning Directives:\n.reqntid\n11.4.4. Performance-Tuning Directives:\n.minnctapersm\n11.4.5. Performance-Tuning Directives:\n.maxnctapersm\n(deprecated)\n11.4.6. Performance-Tuning Directives:\n.noreturn\n11.4.7. Performance-Tuning Directives:\n.pragma\n11.4.8. Performance-Tuning Directives:\n.abi_preserve\n11.4.9. Performance-Tuning Directives:\n.abi_preserve_control\n11.5. Debugging Directives\n11.5.1. Debugging Directives:\n@@dwarf\n11.5.2. Debugging Directives:\n.section\n11.5.3. Debugging Directives:\n.file\n11.5.4. Debugging Directives:\n.loc\n11.6. Linking Directives\n11.6.1. Linking Directives:\n.extern\n11.6.2. Linking Directives:\n.visible\n11.6.3. Linking Directives:\n.weak\n11.6.4. Linking Directives:\n.common\n11.7. Cluster Dimension Directives\n11.7.1. Cluster Dimension Directives:\n.reqnctapercluster\n11.7.2. Cluster Dimension Directives:\n.explicitcluster\n11.7.3. Cluster Dimension Directives:\n.maxclusterrank\n11.8. Miscellaneous Directives\n11.8.1. Miscellaneous Directives:\n.blocksareclusters\n12. Descriptions of\n.pragma\nStrings\n12.1. Pragma Strings:\n\"nounroll\"\n12.2. Pragma Strings:\n\"used_bytes_mask\"\n12.3. Pragma Strings:\n\"enable_smem_spilling\"\n12.4. Pragma Strings:\n\"frequency\"\n13. Release Notes\n13.1. Changes in PTX ISA Version 9.1\n13.2. Changes in PTX ISA Version 9.0\n13.3. Changes in PTX ISA Version 8.8\n13.4. Changes in PTX ISA Version 8.7\n13.5. Changes in PTX ISA Version 8.6\n13.6. Changes in PTX ISA Version 8.5\n13.7. Changes in PTX ISA Version 8.4\n13.8. Changes in PTX ISA Version 8.3\n13.9. Changes in PTX ISA Version 8.2\n13.10. Changes in PTX ISA Version 8.1\n13.11. Changes in PTX ISA Version 8.0\n13.12. Changes in PTX ISA Version 7.8\n13.13. Changes in PTX ISA Version 7.7\n13.14. Changes in PTX ISA Version 7.6\n13.15. Changes in PTX ISA Version 7.5\n13.16. Changes in PTX ISA Version 7.4\n13.17. Changes in PTX ISA Version 7.3\n13.18. Changes in PTX ISA Version 7.2\n13.19. Changes in PTX ISA Version 7.1\n13.20. Changes in PTX ISA Version 7.0\n13.21. Changes in PTX ISA Version 6.5\n13.22. Changes in PTX ISA Version 6.4\n13.23. Changes in PTX ISA Version 6.3\n13.24. Changes in PTX ISA Version 6.2\n13.25. Changes in PTX ISA Version 6.1\n13.26. Changes in PTX ISA Version 6.0\n13.27. Changes in PTX ISA Version 5.0\n13.28. Changes in PTX ISA Version 4.3\n13.29. Changes in PTX ISA Version 4.2\n13.30. Changes in PTX ISA Version 4.1\n13.31. Changes in PTX ISA Version 4.0\n13.32. Changes in PTX ISA Version 3.2\n13.33. Changes in PTX ISA Version 3.1\n13.34. Changes in PTX ISA Version 3.0\n13.35. Changes in PTX ISA Version 2.3\n13.36. Changes in PTX ISA Version 2.2\n13.37. Changes in PTX ISA Version 2.1\n13.38. Changes in PTX ISA Version 2.0\n14. Notices\n14.1. Notice\n14.2. OpenCL\n14.3. Trademarks\nPTX ISA\n»\n1.\nIntroduction\nv9.1 |\nPDF\n|\nArchive\nParallel Thread Execution ISA Version 9.1\nThe programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture).\n1.\nIntroduction\n\nThis document describes PTX, a low-level\nparallel thread execution\nvirtual machine and instruction\nset architecture (ISA). PTX exposes the GPU as a data-parallel computing\ndevice\n.\n1.1.\nScalable Data-Parallel Computing using GPUs\n\nDriven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable\nGPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous\ncomputational horsepower and very high memory bandwidth. The GPU is especially well-suited to\naddress problems that can be expressed as data-parallel computations - the same program is executed\non many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic\noperations to memory operations. Because the same program is executed for each data element, there\nis a lower requirement for sophisticated flow control; and because it is executed on many data\nelements and has high arithmetic intensity, the memory access latency can be hidden with\ncalculations instead of big data caches.\nData-parallel processing maps data elements to parallel processing threads. Many applications that\nprocess large data sets can use a data-parallel programming model to speed up the computations. In\n3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and\nmedia processing applications such as post-processing of rendered images, video encoding and\ndecoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to\nparallel processing threads. In fact, many algorithms outside the field of image rendering and\nprocessing are accelerated by data-parallel processing, from general signal processing or physics\nsimulation to computational finance or computational biology.\nPTX\ndefines a virtual machine and ISA for general purpose parallel thread execution. PTX programs\nare translated at install time to the target hardware instruction set. The PTX-to-GPU translator\nand driver enable NVIDIA GPUs to be used as programmable parallel computers.\n1.2.\nGoals of PTX\n\nPTX\nprovides a stable programming model and instruction set for general purpose parallel\nprogramming. It is designed to be efficient on NVIDIA GPUs supporting the computation features\ndefined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA\nand C/C++ generate PTX instructions, which are optimized for and translated to native\ntarget-architecture instructions.\nThe goals for PTX include the following:\nProvide a stable ISA that spans multiple GPU generations.\nAchieve performance in compiled applications comparable to native GPU performance.\nProvide a machine-independent ISA for C/C++ and other compilers to target.\nProvide a code distribution ISA for application and middleware developers.\nProvide a common source-level ISA for optimizing code generators and translators, which map PTX to\nspecific target machines.\nFacilitate hand-coding of libraries, performance kernels, and architecture tests.\nProvide a scalable programming model that spans GPU sizes from a single unit to many parallel units.\n1.3.\nPTX ISA Version 9.1\n\nPTX ISA version 9.1 introduces the following new features:\nAdds support for\n.volatile\nqualifier with\n.local\nstate space for\nld\nand\nst\ninstructions.\nAdds support for\n.f16x2\nand\n.bf16x2\nsource types for\ncvt\ninstruction\nwith destination types\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x2\n,\n.e5m2x2\n.\nAdds support for\n.scale_vec::4X\nwith\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nfor\nmma\n/\nmma.sp\ninstructions.\nAdds support for\n.s2f6x2\ninstruction type for\ncvt\ninstruction.\nAdds support for\nmultimem.cp.async.bulk\nand\nmultimem.cp.reduce.async.bulk\ninstructions.\n1.4.\nDocument Structure\n\nThe information in this document is organized into the following Chapters:\nProgramming Model\noutlines the programming model.\nPTX Machine Model\ngives an overview of the PTX virtual machine model.\nSyntax\ndescribes the basic syntax of the PTX language.\nState Spaces, Types, and Variables\ndescribes\nstate spaces, types, and variable declarations.\nInstruction Operands\ndescribes instruction operands.\nAbstracting the ABI\ndescribes the function and call syntax,\ncalling convention, and PTX support for abstracting the\nApplication Binary Interface (ABI)\n.\nInstruction Set\ndescribes the instruction set.\nSpecial Registers\nlists special registers.\nDirectives\nlists the assembly directives supported in PTX.\nRelease Notes\nprovides release notes for PTX ISA versions 2.x and\nbeyond.\nReferences\n754-2008 IEEE Standard for Floating-Point Arithmetic. ISBN 978-0-7381-5752-8, 2008.\nhttp://ieeexplore.ieee.org/servlet/opac?punumber=4610933\nThe OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011.\nhttp://www.khronos.org/registry/cl/specs/opencl-1.1.pdf\nCUDA Programming Guide.\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\nCUDA Dynamic Parallelism Programming Guide.\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism\nCUDA Atomicity Requirements.\nhttps://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity\nPTX Writers Guide to Interoperability.\nhttps://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html\n2.\nProgramming Model\n\n2.1.\nA Highly Multithreaded Coprocessor\n\nThe GPU is a compute device capable of executing a very large number of threads in parallel. It\noperates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive\nportions of applications running on the host are off-loaded onto the device.\nMore precisely, a portion of an application that is executed many times, but independently on\ndifferent data, can be isolated into a kernel function that is executed on the GPU as many different\nthreads. To that effect, such a function is compiled to the PTX instruction set and the resulting\nkernel is translated at install time to the target GPU instruction set.\n2.2.\nThread Hierarchy\n\nThe batch of threads that executes a kernel is organized as a grid. A grid consists of either\ncooperative thread arrays or clusters of cooperative thread arrays as described in this section and\nillustrated in\nFigure 1\nand\nFigure 2\n.\nCooperative thread arrays (CTAs)\nimplement CUDA\nthread blocks and clusters implement CUDA thread block clusters.\n2.2.1.\nCooperative Thread Arrays\n\nThe\nParallel Thread Execution (PTX)\nprogramming model is explicitly parallel: a PTX program\nspecifies the execution of a given thread of a parallel thread array. A\ncooperative thread array\n,\nor CTA, is an array of threads that execute a kernel concurrently or in parallel.\nThreads within a CTA can communicate with each other. To coordinate the communication of the threads\nwithin the CTA, one can specify synchronization points where threads wait until all threads in the\nCTA have arrived.\nEach thread has a unique thread identifier within the CTA. Programs use a data parallel\ndecomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread\nuses its thread identifier to determine its assigned role, assign specific input and output\npositions, compute addresses, and select work to perform. The thread identifier is a three-element\nvector\ntid\n, (with elements\ntid.x\n,\ntid.y\n, and\ntid.z\n) that specifies the thread’s\nposition within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the\nnumber of thread ids in that CTA dimension.\nEach CTA has a 1D, 2D, or 3D shape specified by a three-element vector\nntid\n(with elements\nntid.x\n,\nntid.y\n, and\nntid.z\n). The vector\nntid\nspecifies the number of threads in each\nCTA dimension.\nThreads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called\nwarps\n. A\nwarp\nis a maximal subset of threads from a single CTA, such that the threads execute\nthe same instructions at the same time. Threads within a warp are sequentially numbered. The warp\nsize is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be\nable to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate\nconstant,\nWARP_SZ\n, which may be used in any instruction where an immediate operand is allowed.\n2.2.2.\nCluster of Cooperative Thread Arrays\n\nCluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate\nwith each other via shared memory. The executing CTA has to make sure that the shared memory of the\npeer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before\ncompleting the shared memory operation.\nThreads within the different CTAs in a cluster can synchronize and communicate with each other via\nshared memory. Cluster-wide barriers can be used to synchronize all the threads within the\ncluster. Each CTA in a cluster has a unique CTA identifier within its cluster\n(\ncluster_ctaid\n). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter\ncluster_nctaid\n. Each CTA in the cluster also has a unique CTA identifier (\ncluster_ctarank\n)\nacross all dimensions. The total number of CTAs across all the dimensions in the cluster is\nspecified by\ncluster_nctarank\n. Threads may read and use these values through predefined, read-only\nspecial registers\n%cluster_ctaid\n,\n%cluster_nctaid\n,\n%cluster_ctarank\n,\n%cluster_nctarank\n.\nCluster level is applicable only on target architecture\nsm_90\nor higher. Specifying cluster\nlevel during launch time is optional. If the user specifies the cluster dimensions at launch time\nthen it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster\nlaunch with default dimension 1x1x1. PTX provides read-only special register\n%is_explicit_cluster\nto differentiate between explicit and implicit cluster launch.\n2.2.3.\nGrid of Clusters\n\nThere is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a\ncluster can contain. However, clusters with CTAs that execute the same kernel can be batched\ntogether into a grid of clusters, so that the total number of threads that can be launched in a\nsingle kernel invocation is very large. This comes at the expense of reduced thread communication\nand synchronization, because threads in different clusters cannot communicate and synchronize with\neach other.\nEach cluster has a unique cluster identifier (\nclusterid\n) within a grid of clusters. Each grid of\nclusters has a 1D, 2D , or 3D shape specified by the parameter\nnclusterid\n. Each grid also has a\nunique temporal grid identifier (\ngridid\n). Threads may read and use these values through\npredefined, read-only special registers\n%tid\n,\n%ntid\n,\n%clusterid\n,\n%nclusterid\n, and\n%gridid\n.\nEach CTA has a unique identifier (\nctaid\n) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape\nspecified by the parameter\nnctaid\n. Thread may use and read these values through predefined,\nread-only special registers\n%ctaid\nand\n%nctaid\n.\nEach kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs\nwhere cluster is optional level and is applicable only for target architectures\nsm_90\nand\nhigher.\nFigure 1\nshows a grid consisting of CTAs and\nFigure 2\nshows a grid consisting of clusters.\nGrids may be launched with dependencies between one another - a grid may be a dependent grid and/or\na prerequisite grid. To understand how grid dependencies may be defined, refer to the section on\nCUDA Graphs\nin the\nCuda Programming Guide\n.\nFigure 1\nGrid with CTAs\n\nFigure 2\nGrid with clusters\n\nA cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads\nthat execute the same kernel program. A grid is a set of clusters consisting of CTAs that\nexecute independently.\n2.3.\nMemory Hierarchy\n\nPTX threads may access data from multiple state spaces during their execution as illustrated by\nFigure 3\nwhere cluster level is introduced from\ntarget architecture\nsm_90\nonwards. Each thread has a private local memory. Each thread block\n(CTA) has a shared memory visible to all threads of the block and to all active blocks in the\ncluster and with the same lifetime as the block. Finally, all threads have access to the same global\nmemory.\nThere are additional state spaces accessible by all threads: the constant, param, texture, and\nsurface state spaces.  Constant and texture memory are read-only; surface memory is readable and\nwritable. The global, constant, param, texture, and surface state spaces are optimized for different\nmemory usages. For example, texture memory offers different addressing modes as well as data\nfiltering for specific data formats. Note that texture and surface memory is cached, and within the\nsame kernel call, the cache is not kept coherent with respect to global memory writes and surface\nmemory writes, so any texture fetch or surface read to an address that has been written to via a\nglobal or a surface write in the same kernel call returns undefined data. In other words, a thread\ncan safely read some texture or surface memory location only if this memory location has been\nupdated by a previous kernel call or memory copy, but not if it has been previously updated by the\nsame thread or another thread from the same kernel call.\nThe global, constant, and texture state spaces are persistent across kernel launches by the same\napplication.\nBoth the host and the device maintain their own local memory, referred to as\nhost memory\nand\ndevice memory\n, respectively. The device memory may be mapped and read or written by the host, or,\nfor more efficient transfer, copied from the host memory through optimized API calls that utilize\nthe device’s high-performance\nDirect Memory Access (DMA)\nengine.\nFigure 3\nMemory Hierarchy\n\n3.\nPTX Machine Model\n\n3.1.\nA Set of SIMT Multiprocessors\n\nThe NVIDIA GPU architecture is built around a scalable array of multithreaded\nStreaming\nMultiprocessors (SMs)\n. When a host program invokes a kernel grid, the blocks of the grid are\nenumerated and distributed to multiprocessors with available execution capacity. The threads of a\nthread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are\nlaunched on the vacated multiprocessors.\nA multiprocessor consists of multiple\nScalar Processor (SP)\ncores, a multithreaded instruction\nunit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent\nthreads in hardware with zero scheduling overhead. It implements a single-instruction barrier\nsynchronization. Fast barrier synchronization together with lightweight thread creation and\nzero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for\nexample, a low granularity decomposition of problems by assigning one thread to each data element\n(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation).\nTo manage hundreds of threads running several different programs, the multiprocessor employs an\narchitecture we call\nSIMT (single-instruction, multiple-thread)\n. The multiprocessor maps each\nthread to one scalar processor core, and each scalar thread executes independently with its own\ninstruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,\nand executes threads in groups of parallel threads called\nwarps\n. (This term originates from\nweaving, the first parallel thread technology.) Individual threads composing a SIMT warp start\ntogether at the same program address but are otherwise free to branch and execute independently.\nWhen a multiprocessor is given one or more thread blocks to execute, it splits them into warps that\nget scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp\ncontains threads of consecutive, increasing thread IDs with the first warp containing thread 0.\nAt every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues\nthe next instruction to the active threads of the warp. A warp executes one common instruction at a\ntime, so full efficiency is realized when all threads of a warp agree on their execution path. If\nthreads of a warp diverge via a data-dependent conditional branch, the warp serially executes each\nbranch path taken, disabling threads that are not on that path, and when all paths complete, the\nthreads converge back to the same execution path. Branch divergence occurs only within a warp;\ndifferent warps execute independently regardless of whether they are executing common or disjointed\ncode paths.\nSIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a\nsingle instruction controls multiple processing elements. A key difference is that SIMD vector\norganizations expose the SIMD width to the software, whereas SIMT instructions specify the execution\nand branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables\nprogrammers to write thread-level parallel code for independent, scalar threads, as well as\ndata-parallel code for coordinated threads. For the purposes of correctness, the programmer can\nessentially ignore the SIMT behavior; however, substantial performance improvements can be realized\nby taking care that the code seldom requires threads in a warp to diverge. In practice, this is\nanalogous to the role of cache lines in traditional code: Cache line size can be safely ignored when\ndesigning for correctness but must be considered in the code structure when designing for peak\nperformance. Vector architectures, on the other hand, require the software to coalesce loads into\nvectors and manage divergence manually.\nHow many blocks a multiprocessor can process at once depends on how many registers per thread and\nhow much shared memory per block are required for a given kernel since the multiprocessor’s\nregisters and shared memory are split among all the threads of the batch of blocks. If there are not\nenough registers or shared memory available per multiprocessor to process at least one block, the\nkernel will fail to launch.\nFigure 4\nHardware Model\n\nA set of SIMT multiprocessors with on-chip shared memory.\n3.2.\nIndependent Thread Scheduling\n\nOn architectures prior to Volta, warps used a single program counter shared amongst all 32 threads\nin the warp together with an active mask specifying the active threads of the warp. As a result,\nthreads from the same warp in divergent regions or different states of execution cannot signal each\nother or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or\nmutexes can easily lead to deadlock, depending on which warp the contending threads come from.\nStarting with the Volta architecture,\nIndependent Thread Scheduling\nallows full concurrency\nbetween threads, regardless of warp. With\nIndependent Thread Scheduling\n, the GPU maintains\nexecution state per thread, including a program counter and call stack, and can yield execution at a\nper-thread granularity, either to make better use of execution resources or to allow one thread to\nwait for data to be produced by another. A schedule optimizer determines how to group active threads\nfrom the same warp together into SIMT units. This retains the high throughput of SIMT execution as\nin prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at\nsub-warp granularity.\nIndependent Thread Scheduling\ncan lead to a rather different set of threads participating in the\nexecuted code than intended if the developer made assumptions about warp-synchronicity of previous\nhardware architectures. In particular, any warp-synchronous code (such as synchronization-free,\nintra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the\nsection on Compute Capability 7.x in the\nCuda Programming Guide\nfor further details.\n3.3.\nOn-chip Shared Memory\n\nAs illustrated by\nFigure 4\n, each multiprocessor has\non-chip memory of the four following types:\nOne set of local 32-bit\nregisters\nper processor,\nA parallel data cache or\nshared memory\nthat is shared by all scalar processor cores and is where\nthe shared memory space resides,\nA read-only\nconstant cache\nthat is shared by all scalar processor cores and speeds up reads from\nthe constant memory space, which is a read-only region of device memory,\nA read-only\ntexture cache\nthat is shared by all scalar processor cores and speeds up reads from\nthe texture memory space, which is a read-only region of device memory; each multiprocessor\naccesses the texture cache via a\ntexture unit\nthat implements the various addressing modes and\ndata filtering.\nThe local and global memory spaces are read-write regions of device memory.\n4.\nSyntax\n\nPTX programs are a collection of text source modules (files). PTX source modules have an\nassembly-language style syntax with instruction operation codes and operands. Pseudo-operations\nspecify symbol and addressing management. The ptxas optimizing backend compiler optimizes and\nassembles PTX source modules to produce corresponding binary object files.\n4.1.\nSource Format\n\nSource modules are ASCII text. Lines are separated by the newline character (\n\\n\n).\nAll whitespace characters are equivalent; whitespace is ignored except for its use in separating\ntokens in the language.\nThe C preprocessor cpp may be used to process PTX source modules. Lines beginning with\n#\nare\npreprocessor directives. The following are common preprocessor directives:\n#include\n,\n#define\n,\n#if\n,\n#ifdef\n,\n#else\n,\n#endif\n,\n#line\n,\n#file\nC: A Reference Manual\nby Harbison and Steele provides a good description of the C preprocessor.\nPTX is case sensitive and uses lowercase for keywords.\nEach PTX module must begin with a\n.version\ndirective specifying the PTX language version,\nfollowed by a\n.target\ndirective specifying the target architecture assumed. See\nPTX Module Directives\nfor a more information on these directives.\n4.2.\nComments\n\nComments in PTX follow C/C++ syntax, using non-nested\n/*\nand\n*/\nfor comments that may span\nmultiple lines, and using\n//\nto begin a comment that extends up to the next newline character,\nwhich terminates the current line. Comments cannot occur within character constants, string\nliterals, or within other comments.\nComments in PTX are treated as whitespace.\n4.3.\nStatements\n\nA PTX statement is either a directive or an instruction. Statements begin with an optional label and\nend with a semicolon.\nExamples\n.reg     .b32 r1, r2;\n        .global  .f32  array[N];\n\nstart:  mov.b32   r1, %tid.x;\n        shl.b32   r1, r1, 2;          // shift thread id by 2 bits\n        ld.global.b32 r2, array[r1];  // thread[tid] gets array[tid]\n        add.f32   r2, r2, 0.5;        // add 1/2\n4.3.1.\nDirective Statements\n\nDirective keywords begin with a dot, so no conflict is possible with user-defined identifiers. The\ndirectives in PTX are listed in\nTable 1\nand\ndescribed in\nState Spaces, Types, and Variables\nand\nDirectives\n.\nTable 1\nPTX Directives\n\n.address_size\n.explicitcluster\n.maxnreg\n.section\n.alias\n.extern\n.maxntid\n.shared\n.align\n.file\n.minnctapersm\n.sreg\n.branchtargets\n.func\n.noreturn\n.target\n.callprototype\n.global\n.param\n.tex\n.calltargets\n.loc\n.pragma\n.version\n.common\n.local\n.reg\n.visible\n.const\n.maxclusterrank\n.reqnctapercluster\n.weak\n.entry\n.maxnctapersm\n.reqntid\n4.3.2.\nInstruction Statements\n\nInstructions are formed from an instruction opcode followed by a comma-separated list of zero or\nmore operands, and terminated with a semicolon. Operands may be register variables, constant\nexpressions, address expressions, or label names. Instructions have an optional guard predicate\nwhich controls conditional execution. The guard predicate follows the optional label and precedes\nthe opcode, and is written as\n@p\n, where\np\nis a predicate register. The guard predicate may\nbe optionally negated, written as\n@!p\n.\nThe destination operand is first, followed by source operands.\nInstruction keywords are listed in\nTable 2\n. All instruction keywords are\nreserved tokens in PTX.\nTable 2\nReserved Instruction Keywords\n\nabs\ncvta\nmembar\nsetp\nvabsdiff\nactivemask\ndiscard\nmin\nshf\nvabsdiff2\nadd\ndiv\nmma\nshfl\nvabsdiff4\naddc\ndp2a\nmov\nshl\nvadd\nalloca\ndp4a\nmovmatrix\nshr\nvadd2\nand\nelect\nmul\nsin\nvadd4\napplypriority\nex2\nmul24\nslct\nvavrg2\natom\nexit\nmultimem\nsqrt\nvavrg4\nbar\nfence\nnanosleep\nst\nvmad\nbarrier\nfma\nneg\nstackrestore\nvmax\nbfe\nfns\nnot\nstacksave\nvmax2\nbfi\ngetctarank\nor\nstmatrix\nvmax4\nbfind\ngriddepcontrol\npmevent\nsub\nvmin\nbmsk\nisspacep\npopc\nsubc\nvmin2\nbra\nistypep\nprefetch\nsuld\nvmin4\nbrev\nld\nprefetchu\nsuq\nvote\nbrkpt\nldmatrix\nprmt\nsured\nvset\nbrx\nldu\nrcp\nsust\nvset2\ncall\nlg2\nred\nszext\nvset4\nclz\nlop3\nredux\ntanh\nvshl\ncnot\nmad\nrem\ntcgen05\nvshr\ncopysign\nmad24\nret\ntensormap\nvsub\ncos\nmadc\nrsqrt\ntestp\nvsub2\nclusterlaunchcontrol\nmapa\nsad\ntex\nvsub4\ncp\nmatch\nselp\ntld4\nwgmma\ncreatepolicy\nmax\nset\ntrap\nwmma\ncvt\nmbarrier\nsetmaxnreg\ntxq\nxor\n4.4.\nIdentifiers\n\nUser-defined identifiers follow extended C++ rules: they either start with a letter followed by zero\nor more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar,\nor percentage character followed by one or more letters, digits, underscore, or dollar characters:\nfollowsym:   [a-zA-Z0-9_$]\nidentifier:  [a-zA-Z]{followsym}* | {[_$%]{followsym}+\nPTX does not specify a maximum length for identifiers and suggests that all implementations support\na minimum length of at least 1024 characters.\nMany high-level languages such as C and C++ follow similar rules for identifier names, except that\nthe percentage sign is not allowed. PTX allows the percentage sign as the first character of an\nidentifier. The percentage sign can be used to avoid name conflicts, e.g., between user-defined\nvariable names and compiler-generated names.\nPTX predefines one constant and a small number of special registers that begin with the percentage\nsign, listed in\nTable 3\n.\nTable 3\nPredefined Identifiers\n\n%aggr_smem_size\n%dynamic_smem_size\n%lanemask_gt\n%reserved_smem_offset_begin\n%clock\n%envreg<32>\n%lanemask_le\n%reserved_smem_offset_cap\n%clock64\n%globaltimer\n%lanemask_lt\n%reserved_smem_offset_end\n%cluster_ctaid\n%globaltimer_hi\n%nclusterid\n%smid\n%cluster_ctarank\n%globaltimer_lo\n%nctaid\n%tid\n%cluster_nctaid\n%gridid\n%nsmid\n%total_smem_size\n%cluster_nctarank\n%is_explicit_cluster\n%ntid\n%warpid\n%clusterid\n%laneid\n%nwarpid\nWARP_SZ\n%ctaid\n%lanemask_eq\n%pm0,\n...,\n%pm7\n%current_graph_exec\n%lanemask_ge\n%reserved_smem_offset_<2>\n4.5.\nConstants\n\nPTX supports integer and floating-point constants and constant expressions. These constants may be\nused in data initialization and as operands to instructions. Type checking rules remain the same for\ninteger, floating-point, and bit-size types. For predicate-type data and instructions, integer\nconstants are allowed and are interpreted as in C, i.e., zero values are\nFalse\nand non-zero\nvalues are\nTrue\n.\n4.5.1.\nInteger Constants\n\nInteger constants are 64-bits in size and are either signed or unsigned, i.e., every integer\nconstant has type\n.s64\nor\n.u64\n. The signed/unsigned nature of an integer constant is needed\nto correctly evaluate constant expressions containing operations such as division and ordered\ncomparisons, where the behavior of the operation depends on the operand types. When used in an\ninstruction or data initialization, each integer constant is converted to the appropriate size based\non the data or instruction type at its use.\nInteger literals may be written in decimal, hexadecimal, octal, or binary notation. The syntax\nfollows that of C. Integer literals may be followed immediately by the letter\nU\nto indicate that\nthe literal is unsigned.\nhexadecimal literal:  0[xX]{hexdigit}+U?\noctal literal:        0{octal digit}+U?\nbinary literal:       0[bB]{bit}+U?\ndecimal literal       {nonzero-digit}{digit}*U?\nInteger literals are non-negative and have a type determined by their magnitude and optional type\nsuffix as follows: literals are signed (\n.s64\n) unless the value cannot be fully represented in\n.s64\nor the unsigned suffix is specified, in which case the literal is unsigned (\n.u64\n).\nThe predefined integer constant\nWARP_SZ\nspecifies the number of threads per warp for the target\nplatform; to date, all target architectures have a\nWARP_SZ\nvalue of 32.\n4.5.2.\nFloating-Point Constants\n\nFloating-point constants are represented as 64-bit double-precision values, and all floating-point\nconstant expressions are evaluated using 64-bit double precision arithmetic. The only exception is\nthe 32-bit hex notation for expressing an exact single-precision floating-point value; such values\nretain their exact 32-bit single-precision value and may not be used in constant expressions. Each\n64-bit floating-point constant is converted to the appropriate floating-point size based on the data\nor instruction type at its use.\nFloating-point literals may be written with an optional decimal point and an optional signed\nexponent. Unlike C and C++, there is no suffix letter to specify size; literals are always\nrepresented in 64-bit double-precision format.\nPTX includes a second representation of floating-point constants for specifying the exact machine\nrepresentation using a hexadecimal constant. To specify IEEE 754 double-precision floating point\nvalues, the constant begins with\n0d\nor\n0D\nfollowed by 16 hex digits. To specify IEEE 754\nsingle-precision floating point values, the constant begins with\n0f\nor\n0F\nfollowed by 8 hex\ndigits.\n0[fF]{hexdigit}{8}      // single-precision floating point\n0[dD]{hexdigit}{16}     // double-precision floating point\nExample\nmov.f32  $f3, 0F3f800000;       //  1.0\n4.5.3.\nPredicate Constants\n\nIn PTX, integer constants may be used as predicates. For predicate-type data initializers and\ninstruction operands, integer constants are interpreted as in C, i.e., zero values are\nFalse\nand\nnon-zero values are\nTrue\n.\n4.5.4.\nConstant Expressions\n\nIn PTX, constant expressions are formed using operators as in C and are evaluated using rules\nsimilar to those in C, but simplified by restricting types and sizes, removing most casts, and\ndefining full semantics to eliminate cases where expression evaluation in C is implementation\ndependent.\nConstant expressions are formed from constant literals, unary plus and minus, basic arithmetic\noperators (addition, subtraction, multiplication, division), comparison operators, the conditional\nternary operator (\n?:\n), and parentheses. Integer constant expressions also allow unary logical\nnegation (\n!\n), bitwise complement (\n~\n), remainder (\n%\n), shift operators (\n<<\nand\n>>\n), bit-type operators (\n&\n,\n|\n, and\n^\n), and logical operators (\n&&\n,\n||\n).\nConstant expressions in PTX do not support casts between integer and floating-point.\nConstant expressions are evaluated using the same operator precedence as\nin C.\nTable 4\ngives operator precedence and\nassociativity. Operator precedence is highest for unary operators and decreases with each line in\nthe chart. Operators on the same line have the same precedence and are evaluated right-to-left for\nunary operators and left-to-right for binary operators.\nTable 4\nOperator Precedence\n\nKind\nOperator Symbols\nOperator Names\nAssociates\nPrimary\n()\nparenthesis\nn/a\nUnary\n+-\n!\n~\nplus, minus, negation, complement\nright\n(.s64)\n(.u64)\ncasts\nright\nBinary\n*/\n%\nmultiplication, division, remainder\nleft\n+-\naddition, subtraction\n>>\n<<\nshifts\n<\n>\n<=\n>=\nordered comparisons\n==\n!=\nequal, not equal\n&\nbitwise AND\n^\nbitwise XOR\n|\nbitwise OR\n&&\nlogical AND\n||\nlogical OR\nTernary\n?:\nconditional\nright\n4.5.5.\nInteger Constant Expression Evaluation\n\nInteger constant expressions are evaluated at compile time according to a set of rules that\ndetermine the type (signed\n.s64\nversus unsigned\n.u64\n) of each sub-expression. These rules\nare based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and\nbehavior is fully defined in all cases (specifically, for remainder and shift operators).\nLiterals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a\nU\nsuffix. For example:\n42\n,\n0x1234\n,\n0123\nare signed.\n0xfabc123400000000\n,\n42U\n,\n0x1234U\nare unsigned.\nUnary plus and minus preserve the type of the input operand. For example:\n+123\n,\n-1\n,\n-(-42)\nare signed.\n-1U\n,\n-0xfabc123400000000\nare unsigned.\nUnary logical negation (\n!\n) produces a signed result with value\n0\nor\n1\n.\nUnary bitwise complement (\n~\n) interprets the source operand as unsigned and produces an\nunsigned result.\nSome binary operators require normalization of source operands. This normalization is known as\nthe usual arithmetic conversions\nand simply converts both operands to unsigned type if either\noperand is unsigned.\nAddition, subtraction, multiplication, and division perform the usual arithmetic conversions and\nproduce a result with the same type as the converted operands. That is, the operands and result\nare unsigned if either source operand is unsigned, and is otherwise signed.\nRemainder (\n%\n) interprets the operands as unsigned. Note that this differs from C, which allows\na negative divisor but defines the behavior to be implementation dependent.\nLeft and right shift interpret the second operand as unsigned and produce a result with the same\ntype as the first operand. Note that the behavior of right-shift is determined by the type of the\nfirst operand: right shift of a signed value is arithmetic and preserves the sign, and right shift\nof an unsigned value is logical and shifts in a zero bit.\nAND (\n&\n), OR (\n|\n), and XOR (\n^\n) perform the usual arithmetic conversions and produce a\nresult with the same type as the converted operands.\nAND_OP (\n&&\n), OR_OP (\n||\n), Equal (\n==\n), and Not_Equal (\n!=\n) produce a signed\nresult. The result value is 0 or 1.\nOrdered comparisons (\n<\n,\n<=\n,\n>\n,\n>=\n) perform the usual arithmetic conversions on\nsource operands and produce a signed result. The result value is\n0\nor\n1\n.\nCasting of expressions to signed or unsigned is supported using (\n.s64\n) and (\n.u64\n) casts.\nFor the conditional operator (\n?\n:\n) , the first operand must be an integer, and the second\nand third operands are either both integers or both floating-point. The usual arithmetic\nconversions are performed on the second and third operands, and the result type is the same as the\nconverted type.\n4.5.6.\nSummary of Constant Expression Evaluation Rules\n\nTable 5\ncontains a summary of the constant expression evaluation rules.\nTable 5\nConstant Expression Evaluation Rules\n\nKind\nOperator\nOperand Types\nOperand Interpretation\nResult Type\nPrimary\n()\nany type\nsame as source\nsame as source\nconstant literal\nn/a\nn/a\n.u64\n,\n.s64\n, or\n.f64\nUnary\n+-\nany type\nsame as source\nsame as source\n!\ninteger\nzero or non-zero\n.s64\n~\ninteger\n.u64\n.u64\nCast\n(.u64)\ninteger\n.u64\n.u64\n(.s64)\ninteger\n.s64\n.s64\nBinary\n+-\n*\n/\n.f64\n.f64\n.f64\ninteger\nuse usual conversions\nconverted type\n<\n>\n<=\n>=\n.f64\n.f64\n.s64\ninteger\nuse usual conversions\n.s64\n==\n!=\n.f64\n.f64\n.s64\ninteger\nuse usual conversions\n.s64\n%\ninteger\n.u64\n.s64\n>>\n<<\ninteger\n1st unchanged, 2nd is\n.u64\nsame as 1st operand\n&\n|\n^\ninteger\n.u64\n.u64\n&&\n||\ninteger\nzero or non-zero\n.s64\nTernary\n?:\nint\n?\n.f64\n:\n.f64\nsame as sources\n.f64\nint\n?\nint\n:\nint\nuse usual conversions\nconverted type\n5.\nState Spaces, Types, and Variables\n\nWhile the specific resources available in a given target GPU will vary, the kinds of resources will\nbe common across platforms, and these resources are abstracted in PTX through state spaces and data\ntypes.\n5.1.\nState Spaces\n\nA state space is a storage area with particular characteristics. All variables reside in some state\nspace. The characteristics of a state space include its size, addressability, access speed, access\nrights, and level of sharing between threads.\nThe state spaces defined in PTX are a byproduct of parallel programming and graphics\nprogramming. The list of state spaces is shown in\nTable 6\n,and\nproperties of state spaces are shown in\nTable 7\n.\nTable 6\nState Spaces\n\nName\nDescription\n.reg\nRegisters, fast.\n.sreg\nSpecial registers. Read-only; pre-defined; platform-specific.\n.const\nShared, read-only memory.\n.global\nGlobal memory, shared by all threads.\n.local\nLocal memory, private to each thread.\n.param\nKernel parameters, defined per-grid; or\nFunction or local parameters, defined per-thread.\n.shared\nAddressable memory, defined per CTA, accessible to all threads in the cluster\nthroughout the lifetime of the CTA that defines it.\n.tex\nGlobal texture memory (deprecated).\nTable 7\nProperties of State Spaces\n\nName\nAddressable\nInitializable\nAccess\nSharing\n.reg\nNo\nNo\nR/W\nper-thread\n.sreg\nNo\nNo\nRO\nper-CTA\n.const\nYes\nYes\n1\nRO\nper-grid\n.global\nYes\nYes\n1\nR/W\nContext\n.local\nYes\nNo\nR/W\nper-thread\n.param\n(as input to kernel)\nYes\n2\nNo\nRO\nper-grid\n.param\n(used in functions)\nRestricted\n3\nNo\nR/W\nper-thread\n.shared\nYes\nNo\nR/W\nper-cluster\n5\n.tex\nNo\n4\nYes, via driver\nRO\nContext\nNotes:\n1\nVariables in\n.const\nand\n.global\nstate spaces are initialized to zero by default.\n2\nAccessible only via the\nld.param{::entry}\ninstruction. Address may be taken via\nmov\ninstruction.\n3\nAccessible via\nld.param{::func}\nand\nst.param{::func}\ninstructions. Device function\ninput and return parameters may have their address taken via\nmov\n; the parameter is then located\non the stack frame and its address is in the\n.local\nstate space.\n4\nAccessible only via the\ntex\ninstruction.\n5\nVisible to the owning CTA and other active CTAs in the cluster.\n5.1.1.\nRegister State Space\n\nRegisters (\n.reg\nstate space) are fast storage locations. The number of registers is limited, and\nwill vary from platform to platform. When the limit is exceeded, register variables will be spilled\nto memory, causing changes in performance. For each architecture, there is a recommended maximum\nnumber of registers to use (see the\nCUDA Programming Guide\nfor details).\nRegisters may be typed (signed integer, unsigned integer, floating point, predicate) or\nuntyped. Register size is restricted; aside from predicate registers which are 1-bit, scalar\nregisters have a width of 8-, 16-, 32-, 64-, or 128-bits, and vector registers have a width of\n16-, 32-, 64-, or 128-bits. The most common use of 8-bit registers is with\nld\n,\nst\n, and\ncvt\ninstructions, or as elements of vector tuples.\nRegisters differ from the other state spaces in that they are not fully addressable, i.e., it is not\npossible to refer to the address of a register. When compiling to use the Application Binary\nInterface (ABI), register variables are restricted to function scope and may not be declared at\nmodule scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped\n.reg\nvariables, the compiler silently disables use of the ABI. Registers may have alignment\nboundaries required by multi-word loads and stores.\n5.1.2.\nSpecial Register State Space\n\nThe special register (\n.sreg\n) state space holds predefined, platform-specific registers, such as\ngrid, cluster, CTA, and thread parameters, clock counters, and performance monitoring registers. All\nspecial registers are predefined.\n5.1.3.\nConstant State Space\n\nThe constant (\n.const\n) state space is a read-only memory initialized by the host. Constant memory\nis accessed with a\nld.const\ninstruction. Constant memory is restricted in size, currently\nlimited to 64 KB which can be used to hold statically-sized constant variables. There is an\nadditional 640 KB of constant memory, organized as ten independent 64 KB regions. The driver may\nallocate and initialize constant buffers in these regions and pass pointers to the buffers as kernel\nfunction parameters. Since the ten regions are not contiguous, the driver must ensure that constant\nbuffers are allocated so that each buffer fits entirely within a 64 KB region and does not span a\nregion boundary.\nStatically-sized constant variables have an optional variable initializer; constant variables with\nno explicit initializer are initialized to zero by default. Constant buffers allocated by the driver\nare initialized by the host, and pointers to such buffers are passed to the kernel as\nparameters. See the description of kernel parameter attributes in\nKernel Function Parameter Attributes\nfor more details on passing pointers\nto constant buffers as kernel parameters.\n5.1.3.1.\nBanked Constant State Space (deprecated)\n\nPrevious versions of PTX exposed constant memory as a set of eleven 64 KB banks, with explicit bank\nnumbers required for variable declaration and during access.\nPrior to PTX ISA version 2.2, the constant memory was organized into fixed size banks. There were\neleven 64 KB banks, and banks were specified using the\n.const[bank]\nmodifier, where\nbank\nranged from 0 to 10. If no bank number was given, bank zero was assumed.\nBy convention, bank zero was used for all statically-sized constant variables. The remaining banks\nwere used to declare\nincomplete\nconstant arrays (as in C, for example), where the size is not\nknown at compile time. For example, the declaration\n.extern .const[2] .b32 const_buffer[];\nresulted in\nconst_buffer\npointing to the start of constant bank two. This pointer could then be\nused to access the entire 64 KB constant bank. Multiple incomplete array variables declared in the\nsame bank were aliased, with each pointing to the start address of the specified constant bank.\nTo access data in contant banks 1 through 10, the bank number was required in the state space of the\nload instruction. For example, an incomplete array in bank 2 was accessed as follows:\n.extern .const[2] .b32 const_buffer[];\nld.const[2].b32  %r1, [const_buffer+4]; // load second word\nIn PTX ISA version 2.2, we eliminated explicit banks and replaced the incomplete array\nrepresentation of driver-allocated constant buffers with kernel parameter attributes that allow\npointers to constant buffers to be passed as kernel parameters.\n5.1.4.\nGlobal State Space\n\nThe global (\n.global\n) state space is memory that is accessible by all threads in a context. It is\nthe mechanism by which threads in different CTAs, clusters, and grids can communicate. Use\nld.global\n,\nst.global\n, and\natom.global\nto access global variables.\nGlobal variables have an optional variable initializer; global variables with no explicit\ninitializer are initialized to zero by default.\n5.1.5.\nLocal State Space\n\nThe local state space (\n.local\n) is private memory for each thread to keep its own data. It is\ntypically standard memory with cache. The size is limited, as it must be allocated on a per-thread\nbasis. Use\nld.local\nand\nst.local\nto access local variables.\nWhen compiling to use the\nApplication Binary Interface (ABI)\n,\n.local\nstate-space variables\nmust be declared within function scope and are allocated on the stack. In implementations that do\nnot support a stack, all local memory variables are stored at fixed addresses, recursive function\ncalls are not supported, and\n.local\nvariables may be declared at module scope. When compiling\nlegacy PTX code (ISA versions prior to 3.0) containing module-scoped\n.local\nvariables, the\ncompiler silently disables use of the ABI.\n5.1.6.\nParameter State Space\n\nThe parameter (\n.param\n) state space is used (1) to pass input arguments from the host to the\nkernel, (2a) to declare formal input and return parameters for device functions called from within\nkernel execution, and (2b) to declare locally-scoped byte array variables that serve as function\ncall arguments, typically for passing large structures by value to a function. Kernel function\nparameters differ from device function parameters in terms of access and sharing (read-only versus\nread-write, per-kernel versus per-thread). Note that PTX ISA versions 1.x supports only kernel\nfunction parameters in .param space; device function parameters were previously restricted to the\nregister state space. The use of parameter state space for device function parameters was introduced\nin PTX ISA version 2.0 and requires target architecture\nsm_20\nor higher. Additional sub-qualifiers\n::entry\nor\n::func\ncan be specified on instructions with\n.param\nstate space to indicate\nwhether the address refers to kernel function parameter or device function parameter. If no\nsub-qualifier is specified with the\n.param\nstate space, then the default sub-qualifier is specific\nto and dependent on the exact instruction. For example,\nst.param\nis equivalent to\nst.param::func\nwhereas\nisspacep.param\nis equivalent to\nisspacep.param::entry\n. Refer to the instruction\ndescription for more details on default sub-qualifier assumption.\nNote\nThe location of parameter space is implementation specific. For example, in some implementations\nkernel parameters reside in global memory. No access protection is provided between parameter and\nglobal space in this case. Though the exact location of the kernel parameter space is\nimplementation specific, the kernel parameter space window is always contained within the global\nspace window. Similarly, function parameters are mapped to parameter passing registers and/or\nstack locations based on the function calling conventions of the\nApplication Binary Interface\n(ABI)\n. Therefore, PTX code should make no assumptions about the relative locations or ordering\nof\n.param\nspace variables.\n5.1.6.1.\nKernel Function Parameters\n\nEach kernel function definition includes an optional list of parameters. These parameters are\naddressable, read-only variables declared in the\n.param\nstate space. Values passed from the host\nto the kernel are accessed through these parameter variables using\nld.param\ninstructions. The\nkernel parameter variables are shared across all CTAs from all clusters within a grid.\nThe address of a kernel parameter may be moved into a register using the\nmov\ninstruction. The\nresulting address is in the\n.param\nstate space and is accessed using\nld.param\ninstructions.\nExample\n.entry foo ( .param .b32 N, .param .align 8 .b8 buffer[64] )\n{\n    .reg .u32 %n;\n    .reg .f64 %d;\n\n    ld.param.u32 %n, [N];\n    ld.param.f64 %d, [buffer];\n    ...\nExample\n.entry bar ( .param .b32 len )\n{\n    .reg .u32 %ptr, %n;\n\n    mov.u32      %ptr, len;\n    ld.param.u32 %n, [%ptr];\n    ...\nKernel function parameters may represent normal data values, or they may hold addresses to objects\nin constant, global, local, or shared state spaces. In the case of pointers, the compiler and\nruntime system need information about which parameters are pointers, and to which state space they\npoint. Kernel parameter attribute directives are used to provide this information at the PTX\nlevel. See\nKernel Function Parameter Attributes\nfor a description of kernel parameter attribute\ndirectives.\nNote\nThe current implementation does not allow creation of generic pointers to constant variables\n(\ncvta.const\n) in programs that have pointers to constant buffers passed as kernel parameters.\n5.1.6.2.\nKernel Function Parameter Attributes\n\nKernel function parameters may be declared with an optional .ptr attribute to indicate that a\nparameter is a pointer to memory, and also indicate the state space and alignment of the memory\nbeing pointed to.\nKernel Parameter Attribute: .ptr\ndescribes the\n.ptr\nkernel parameter attribute.\n5.1.6.3.\nKernel Parameter Attribute:\n.ptr\n\n.ptr\nKernel parameter alignment attribute.\nSyntax\n.param .type .ptr .space .align N  varname\n.param .type .ptr        .align N  varname\n\n.space = { .const, .global, .local, .shared };\nDescription\nUsed to specify the state space and, optionally, the alignment of memory pointed to by a pointer\ntype kernel parameter. The alignment value\nN\n, if present, must be a power of two. If no state\nspace is specified, the pointer is assumed to be a generic address pointing to one of const, global,\nlocal, or shared memory. If no alignment is specified, the memory pointed to is assumed to be\naligned to a 4 byte boundary.\nSpaces between\n.ptr\n,\n.space\n, and\n.align\nmay be eliminated to improve readability.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.2.\nSupport for generic addressing of .const space added in PTX ISA version 3.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo ( .param .u32 param1,\n             .param .u32 .ptr.global.align 16 param2,\n             .param .u32 .ptr.const.align 8 param3,\n             .param .u32 .ptr.align 16 param4  // generic address\n                                               // pointer\n) { .. }\n5.1.6.4.\nDevice Function Parameters\n\nPTX ISA version 2.0 extended the use of parameter space to device function parameters. The most\ncommon use is for passing objects by value that do not fit within a PTX register, such as C\nstructures larger than 8 bytes. In this case, a byte array in parameter space is used. Typically,\nthe caller will declare a locally-scoped\n.param\nbyte array variable that represents a flattened\nC structure or union. This will be passed by value to a callee, which declares a\n.param\nformal\nparameter having the same size and alignment as the passed argument.\nExample\n// pass object of type struct { double d; int y; };\n.func foo ( .reg .b32 N, .param .align 8 .b8 buffer[12] )\n{\n    .reg .f64 %d;\n    .reg .s32 %y;\n\n    ld.param.f64 %d, [buffer];\n    ld.param.s32 %y, [buffer+8];\n    ...\n}\n\n// code snippet from the caller\n// struct { double d; int y; } mystruct; is flattened, passed to foo\n    ...\n    .reg .f64 dbl;\n    .reg .s32 x;\n    .param .align 8 .b8 mystruct;\n    ...\n    st.param.f64 [mystruct+0], dbl;\n    st.param.s32 [mystruct+8], x;\n    call foo, (4, mystruct);\n    ...\nSee the section on function call syntax for more details.\nFunction input parameters may be read via\nld.param\nand function return parameters may be written\nusing\nst.param\n; it is illegal to write to an input parameter or read from a return parameter.\nAside from passing structures by value,\n.param\nspace is also required whenever a formal\nparameter has its address taken within the called function. In PTX, the address of a function input\nparameter may be moved into a register using the\nmov\ninstruction. Note that the parameter will\nbe copied to the stack if necessary, and so the address will be in the\n.local\nstate space and is\naccessed via\nld.local\nand\nst.local\ninstructions. It is not possible to use\nmov\nto get\nthe address of or a locally-scoped\n.param\nspace variable. Starting PTX ISA version 6.0, it is\npossible to use\nmov\ninstruction to get address of return parameter of device function.\nExample\n// pass array of up to eight floating-point values in buffer\n.func foo ( .param .b32 N, .param .b32 buffer[32] )\n{\n    .reg .u32  %n, %r;\n    .reg .f32  %f;\n    .reg .pred %p;\n\n    ld.param.u32 %n, [N];\n    mov.u32      %r, buffer;  // forces buffer to .local state space\nLoop:\n    setp.eq.u32  %p, %n, 0;\n@%p bra         Done;\n    ld.local.f32 %f, [%r];\n    ...\n    add.u32      %r, %r, 4;\n    sub.u32      %n, %n, 1;\n    bra          Loop;\nDone:\n    ...\n}\n5.1.7.\nShared State Space\n\nThe shared (\n.shared\n) state space is a memory that is owned by an executing CTA and is accessible\nto the threads of all the CTAs within a cluster. An address in shared memory can be read and written\nby any thread in a CTA cluster.\nAdditional sub-qualifiers\n::cta\nor\n::cluster\ncan be specified on instructions with\n.shared\nstate space to indicate whether the address belongs to the shared memory window of the\nexecuting CTA or of any CTA in the cluster respectively. The addresses in the\n.shared::cta\nwindow also fall within the\n.shared::cluster\nwindow. If no sub-qualifier is specified with the\n.shared\nstate space, then it defaults to\n::cta\n. For example,\nld.shared\nis equivalent to\nld.shared::cta\n.\nVariables declared in\n.shared\nstate space refer to the memory addresses in the current\nCTA. Instruction\nmapa\ngives the\n.shared::cluster\naddress of the corresponding variable in\nanother CTA in the cluster.\nShared memory typically has some optimizations to support the sharing. One example is broadcast;\nwhere all threads read from the same address. Another is sequential access from sequential threads.\n5.1.8.\nTexture State Space (deprecated)\n\nThe texture (\n.tex\n) state space is global memory accessed via the texture instruction. It is\nshared by all threads in a context. Texture memory is read-only and cached, so accesses to texture\nmemory are not coherent with global memory stores to the texture image.\nThe GPU hardware has a fixed number of texture bindings that can be accessed within a single kernel\n(typically 128). The .tex directive will bind the named texture memory variable to a hardware\ntexture identifier, where texture identifiers are allocated sequentially beginning with\nzero. Multiple names may be bound to the same physical texture identifier. An error is generated if\nthe maximum number of physical resources is exceeded. The texture name must be of type\n.u32\nor\n.u64\n.\nPhysical texture resources are allocated on a per-kernel granularity, and\n.tex\nvariables are\nrequired to be defined in the global scope.\nTexture memory is read-only. A texture’s base address is assumed to be aligned to a 16 byte\nboundary.\nExample\n.tex .u32 tex_a;         // bound to physical texture 0\n.tex .u32 tex_c, tex_d;  // both bound to physical texture 1\n.tex .u32 tex_d;         // bound to physical texture 2\n.tex .u32 tex_f;         // bound to physical texture 3\nNote\nExplicit declarations of variables in the texture state space is deprecated, and programs should\ninstead reference texture memory through variables of type\n.texref\n. The\n.tex\ndirective is\nretained for backward compatibility, and variables declared in the\n.tex\nstate space are\nequivalent to module-scoped\n.texref\nvariables in the\n.global\nstate space.\nFor example, a legacy PTX definitions such as\n.tex .u32 tex_a;\nis equivalent to:\n.global .texref tex_a;\nSee\nTexture Sampler and Surface Types\nfor the\ndescription of the\n.texref\ntype and\nTexture Instructions\nfor its use in texture instructions.\n5.2.\nTypes\n\n5.2.1.\nFundamental Types\n\nIn PTX, the fundamental types reflect the native data types supported by the target architectures. A\nfundamental type specifies both a basic type and a size. Register variables are always of a\nfundamental type, and instructions operate on these types. The same type-size specifiers are used\nfor both variable definitions and for typing instructions, so their names are intentionally short.\nTable 8\nlists the fundamental type specifiers for\neach basic type:\nTable 8\nFundamental Type Specifiers\n\nBasic Type\nFundamental Type Specifiers\nSigned integer\n.s8\n,\n.s16\n,\n.s32\n,\n.s64\nUnsigned integer\n.u8\n,\n.u16\n,\n.u32\n,\n.u64\nFloating-point\n.f16\n,\n.f16x2\n,\n.f32\n,\n.f64\nBits (untyped)\n.b8\n,\n.b16\n,\n.b32\n,\n.b64\n,\n.b128\nPredicate\n.pred\nMost instructions have one or more type specifiers, needed to fully specify instruction\nbehavior. Operand types and sizes are checked against instruction types for compatibility.\nTwo fundamental types are compatible if they have the same basic type and are the same size. Signed\nand unsigned integer types are compatible if they have the same size. The bit-size type is\ncompatible with any fundamental type having the same size.\nIn principle, all variables (aside from predicates) could be declared using only bit-size types, but\ntyped variables enhance program readability and allow for better operand type checking.\n5.2.2.\nRestricted Use of Sub-Word Sizes\n\nThe\n.u8\n,\n.s8\n, and\n.b8\ninstruction types are restricted to\nld\n,\nst\n, and\ncvt\ninstructions. The\n.f16\nfloating-point type is allowed only in conversions to and from\n.f32\n,\n.f64\ntypes, in half precision floating point instructions and texture fetch instructions. The\n.f16x2\nfloating point type is allowed only in half precision floating point arithmetic\ninstructions and texture fetch instructions.\nFor convenience,\nld\n,\nst\n, and\ncvt\ninstructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes.\n5.2.3.\nAlternate Floating-Point Data Formats\n\nThe fundamental floating-point types supported in PTX have implicit bit representations that\nindicate the number of bits used to store exponent and mantissa. For example, the\n.f16\ntype\nindicates 5 bits reserved for exponent and 10 bits reserved for mantissa. In addition to the\nfloating-point representations assumed by the fundamental types, PTX allows the following alternate\nfloating-point data formats:\nbf16\ndata format:\nThis data format is a 16-bit floating point format with 8 bits for exponent and 7 bits for\nmantissa. A register variable containing\nbf16\ndata must be declared with\n.b16\ntype.\ne4m3\ndata format:\nThis data format is an 8-bit floating point format with 4 bits for exponent and 3 bits for\nmantissa. The\ne4m3\nencoding does not support infinity and\nNaN\nvalues are limited to\n0x7f\nand\n0xff\n. A register variable containing\ne4m3\nvalue must be declared using\nbit-size type.\ne5m2\ndata format:\nThis data format is an 8-bit floating point format with 5 bits for exponent and 2 bits for\nmantissa. A register variable containing\ne5m2\nvalue must be declared using bit-size type.\ntf32\ndata format:\nThis data format is a special 32-bit floating point format supported by the matrix\nmultiply-and-accumulate instructions, with the same range as\n.f32\nand reduced precision (>=10\nbits). The internal layout of\ntf32\nformat is implementation defined. PTX facilitates\nconversion from single precision\n.f32\ntype to\ntf32\nformat. A register variable containing\ntf32\ndata must be declared with\n.b32\ntype.\ne2m1\ndata format:\nThis data format is a 4-bit floating point format with 2 bits for exponent and 1 bit for mantissa.\nThe\ne2m1\nencoding does not support infinity and\nNaN\n.\ne2m1\nvalues must be used in a\npacked format specified as\ne2m1x2\n. A register variable containing two\ne2m1\nvalues must be\ndeclared with\n.b8\ntype.\ne2m3\ndata format:\nThis data format is a 6-bit floating point format with 2 bits for exponent and 3 bits for mantissa.\nThe\ne2m3\nencoding does not support infinity and\nNaN\n.\ne2m3\nvalues must be used in a\npacked format specified as\ne2m3x2\n. A register variable containing two\ne2m3\nvalues must be\ndeclared with\n.b16\ntype where each\n.b8\nelement has 6-bit floating point value and 2 MSB\nbits padded with zeros.\ne3m2\ndata format:\nThis data format is a 6-bit floating point format with 3 bits for exponent and 2 bits for mantissa.\nThe\ne3m2\nencoding does not support infinity and\nNaN\n.\ne3m2\nvalues must be used in a\npacked format specified as\ne3m2x2\n. A register variable containing two\ne3m2\nvalues must be\ndeclared with\n.b16\ntype where each\n.b8\nelement has 6-bit floating point value and 2 MSB\nbits padded with zeros.\nue8m0\ndata format:\nThis data format is an 8-bit unsigned floating-point format with 8 bits for exponent and 0 bits for\nmantissa. The\nue8m0\nencoding does not support infinity.\nNaN\nvalue is limited to\n0xff\n.\nue8m0\nvalues must be used in a packed format specified as\nue8m0x2\n. A register variable\ncontaining two\nue8m0\nvalues must be declared with\n.b16\ntype.\nue4m3\ndata format:\nThis data format is a 7-bit unsigned floating-point format with 4 bits for exponent and 3 bits for\nmantissa. The\nue4m3\nencoding does not support infinity.\nNaN\nvalue is limited to\n0x7f\n.\nA register variable containing single\nue4m3\nvalue must be declared with\n.b8\ntype having\nMSB bit padded with zero.\nAlternate data formats cannot be used as fundamental types. They are supported as source or\ndestination formats by certain instructions.\n5.2.4.\nFixed-point Data format\n\nPTX supports following fixed-point data formats:\ns2f6\ndata format:\nThis data format is 8-bit signed 2’s complement integer with 2 sign-integer bits and\n6 fractional bits with form\nxx.xxxxxx\n. The\ns2f6\nencoding does not support infinity\nand\nNaN\n.\ns2f6\nvalue = s8 value * 2^(-6)\nPositive max representation = 01.111111 = 127 * 2^(-6) = 1.984375\nNegative max representation = 10.000000 = -128 * 2^(-6) = -2.0\n5.2.5.\nPacked Data Types\n\nCertain PTX instructions operate on two or more sets of inputs in parallel, and produce two or more\noutputs. Such instructions can use the data stored in a packed format. PTX supports packing two or\nfour values of the same scalar data type into a single, larger value. The packed value is considered\nas a value of a\npacked data type\n. In this section we describe the packed data types supported in PTX.\n5.2.5.1.\nPacked Floating Point Data Types\n\nPTX supports various variants of packed floating point data types. Out of them, only\n.f16x2\nis\nsupported as a fundamental type, while others cannot be used as fundamental types - they are\nsupported as instruction types on certain instructions. When using an instruction with such\nnon-fundamental types, the operand data variables must be of bit type of appropriate size.\nFor example, all of the operand variables must be of type\n.b32\nfor an instruction with\ninstruction type as\n.bf16x2\n.\nTable 9\ndescribed various variants\nof packed floating point data types in PTX.\nTable 9\nOperand types for packed floating point instruction type.\n\nPacked floating\npoint type\nNumber of elements\ncontained in a\npacked format\nType of each\nelement\nRegister variable type\nto be used in the\ndeclaration\n.f16x2\nTwo\n.f16\n.f16x2\nor\n.b32\n.f32x2\n.f32\n.b64\n.bf16x2\n.bf16\n.b32\n.e4m3x2\n.e4m3\n.b16\n.e5m2x2\n.e5m2\n.e2m3x2\n.e2m3\n.e3m2x2\n.e3m2\n.ue8m0x2\n.ue8m0\n.s2f6x2\n.s2f6\n.e2m1x2\n.e2m1\n.b8\n.e4m3x4\nFour\n.e4m3\n.b32\n.e5m2x4\n.e5m2\n.e2m3x4\n.e2m3\n.e3m2x4\n.e3m2\n.e2m1x4\n.e2m1\n.b16\n5.2.5.2.\nPacked Integer Data Types\n\nPTX supports two variants of packed integer data types:\n.u16x2\nand\n.s16x2\n. The packed data\ntype consists of two\n.u16\nor\n.s16\nvalues. A register variable containing\n.u16x2\nor\n.s16x2\ndata must be declared with\n.b32\ntype. Packed integer data types cannot be used as\nfundamental types. They are supported as instruction types on certain instructions.\n5.2.5.3.\nPacked Fixed-Point Data Types\n\nPTX supports\n.s2f6x2\npacked fixed-point data type consisting of two\n.s2f6\npacked\nfixed-point values. A register variable containing\n.s2f6x2\nvalue must be declared with\n.b16\ntype. Packed fixed-point data type cannot be used as fundamental type and is only\nsupported as instruction type.\n5.3.\nTexture Sampler and Surface Types\n\nPTX includes built-in\nopaque\ntypes for defining texture, sampler, and surface descriptor\nvariables. These types have named fields similar to structures, but all information about layout,\nfield ordering, base address, and overall size is hidden to a PTX program, hence the term\nopaque\n. The use of these opaque types is limited to:\nVariable definition within global (module) scope and in kernel entry parameter lists.\nStatic initialization of module-scope variables using comma-delimited static assignment\nexpressions for the named members of the type.\nReferencing textures, samplers, or surfaces via texture and surface load/store instructions\n(\ntex\n,\nsuld\n,\nsust\n,\nsured\n).\nRetrieving the value of a named member via query instructions (\ntxq\n,\nsuq\n).\nCreating pointers to opaque variables using\nmov\n, e.g.,\nmov.u64\nreg,\nopaque_var;\n. The\nresulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and\nde-referenced by texture and surface load, store, and query instructions, but the pointer cannot\notherwise be treated as an address, i.e., accessing the pointer with\nld\nand\nst\ninstructions, or performing pointer arithmetic will result in undefined results.\nOpaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque\nvariable.\nNote\nIndirect access to textures and surfaces using pointers to opaque variables is supported\nbeginning with PTX ISA version 3.1 and requires target\nsm_20\nor later.\nIndirect access to textures is supported only in unified texture mode (see below).\nThe three built-in types are\n.texref\n,\n.samplerref\n, and\n.surfref\n. For working with\ntextures and samplers, PTX has two modes of operation. In the\nunified mode,\ntexture and sampler\ninformation is accessed through a single\n.texref\nhandle. In the\nindependent mode\n, texture and\nsampler information each have their own handle, allowing them to be defined separately and combined\nat the site of usage in the program. In independent mode, the fields of the\n.texref\ntype that\ndescribe sampler properties are ignored, since these properties are defined by\n.samplerref\nvariables.\nTable 10\nand\nTable 11\nlist the named members\nof each type for unified and independent texture modes. These members and their values have\nprecise mappings to methods and values defined in the texture\nHW\nclass as well as\nexposed values via the API.\nTable 10\nOpaque Type Fields in Unified Texture Mode\n\nMember\n.texref values\n.surfref values\nwidth\nin elements\nheight\nin elements\ndepth\nin elements\nchannel_data_type\nenum\ntype corresponding to source language API\nchannel_order\nenum\ntype corresponding to source language API\nnormalized_coords\n0\n,\n1\nN/A\nfilter_mode\nnearest\n,\nlinear\nN/A\naddr_mode_0\n,\naddr_mode_1\n,\naddr_mode_2\nwrap\n,\nmirror\n,\nclamp_ogl\n,\nclamp_to_edge\n,\nclamp_to_border\nN/A\narray_size\nas number of textures in a texture\narray\nas number of surfaces in a surface array\nnum_mipmap_levels\nas number of levels in a mipmapped\ntexture\nN/A\nnum_samples\nas number of samples in a multi-sample\ntexture\nN/A\nmemory_layout\nN/A\n1\nfor linear memory layout;\n0\notherwise\n5.3.1.\nTexture and Surface Properties\n\nFields\nwidth\n,\nheight\n, and\ndepth\nspecify the size of the texture or surface in number of\nelements in each dimension.\nThe\nchannel_data_type\nand\nchannel_order\nfields specify these properties of the texture or\nsurface using enumeration types corresponding to the source language API. For example, see\nChannel Data Type and Channel Order Fields\nfor\nthe OpenCL enumeration types currently supported in PTX.\n5.3.2.\nSampler Properties\n\nThe\nnormalized_coords\nfield indicates whether the texture or surface uses normalized coordinates\nin the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N). If no value is\nspecified, the default is set by the runtime system based on the source language.\nThe\nfilter_mode\nfield specifies how the values returned by texture reads are computed based on\nthe input texture coordinates.\nThe\naddr_mode_{0,1,2}\nfields define the addressing mode in each dimension, which determine how\nout-of-range coordinates are handled.\nSee the\nCUDA C++ Programming Guide\nfor more details of these properties.\nTable 11\nOpaque Type Fields in Independent Texture Mode\n\nMember\n.samplerref values\n.texref values\n.surfref values\nwidth\nN/A\nin elements\nheight\nN/A\nin elements\ndepth\nN/A\nin elements\nchannel_data_type\nN/A\nenum\ntype corresponding to source\nlanguage API\nchannel_order\nN/A\nenum\ntype corresponding to source\nlanguage AP\nnormalized_coords\nN/A\n0\n,\n1\nN/A\nforce_unnormalized_coords\n0\n,\n1\nN/A\nN/A\nfilter_mode\nnearest\n,\nlinear\nignored\nN/A\naddr_mode_0\n,\naddr_mode_1\n,\naddr_mode_2\nwrap\n,\nmirror\n,\nclamp_ogl\n,\nclamp_to_edge\n,\nclamp_to_border\nN/A\nN/A\narray_size\nN/A\nas number of textures\nin a texture array\nas number of surfaces in\na surface array\nnum_mipmap_levels\nN/A\nas number of levels\nin a mipmapped\ntexture\nN/A\nnum_samples\nN/A\nas number of samples\nin a multi-sample\ntexture\nN/A\nmemory_layout\nN/A\nN/A\n1\nfor linear memory\nlayout;\n0\notherwise\nIn independent texture mode, the sampler properties are carried in an independent\n.samplerref\nvariable, and these fields are disabled in the\n.texref\nvariables. One additional sampler\nproperty,\nforce_unnormalized_coords\n, is available in independent texture mode.\nThe\nforce_unnormalized_coords\nfield is a property of\n.samplerref\nvariables that allows the\nsampler to override the texture header\nnormalized_coords\nproperty. This field is defined only in\nindependent texture mode. When\nTrue\n, the texture header setting is overridden and unnormalized\ncoordinates are used; when\nFalse\n, the texture header setting is used.\nThe\nforce_unnormalized_coords\nproperty is used in compiling OpenCL; in OpenCL, the property of\nnormalized coordinates is carried in sampler headers. To compile OpenCL to PTX, texture headers are\nalways initialized with\nnormalized_coords\nset to True, and the OpenCL sampler-based\nnormalized_coords\nflag maps (negated) to the PTX-level\nforce_unnormalized_coords\nflag.\nVariables using these types may be declared at module scope or within kernel entry parameter\nlists. At module scope, these variables must be in the\n.global\nstate space. As kernel\nparameters, these variables are declared in the\n.param\nstate space.\nExample\n.global .texref     my_texture_name;\n.global .samplerref my_sampler_name;\n.global .surfref    my_surface_name;\nWhen declared at module scope, the types may be initialized using a list of static expressions\nassigning values to the named members.\nExample\n.global .texref tex1;\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n5.3.3.\nChannel Data Type and Channel Order Fields\n\nThe\nchannel_data_type\nand\nchannel_order\nfields have enumeration types corresponding to the\nsource language API. Currently, OpenCL is the only source language that defines these\nfields.\nTable 13\nand\nTable 12\nshow the\nenumeration values defined in OpenCL version 1.0 for channel data type and channel order.\nTable 12\nOpenCL 1.0 Channel Data Type Definition\n\nCL_SNORM_INT8\n0x10D0\nCL_SNORM_INT16\n0x10D1\nCL_UNORM_INT8\n0x10D2\nCL_UNORM_INT16\n0x10D3\nCL_UNORM_SHORT_565\n0x10D4\nCL_UNORM_SHORT_555\n0x10D5\nCL_UNORM_INT_101010\n0x10D6\nCL_SIGNED_INT8\n0x10D7\nCL_SIGNED_INT16\n0x10D8\nCL_SIGNED_INT32\n0x10D9\nCL_UNSIGNED_INT8\n0x10DA\nCL_UNSIGNED_INT16\n0x10DB\nCL_UNSIGNED_INT32\n0x10DC\nCL_HALF_FLOAT\n0x10DD\nCL_FLOAT\n0x10DE\nTable 13\nOpenCL 1.0 Channel Order Definition\n\nCL_R\n0x10B0\nCL_A\n0x10B1\nCL_RG\n0x10B2\nCL_RA\n0x10B3\nCL_RGB\n0x10B4\nCL_RGBA\n0x10B5\nCL_BGRA\n0x10B6\nCL_ARGB\n0x10B7\nCL_INTENSITY\n0x10B8\nCL_LUMINANCE\n0x10B9\n5.4.\nVariables\n\nIn PTX, a variable declaration describes both the variable’s type and its state space. In addition\nto fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays.\n5.4.1.\nVariable Declarations\n\nAll storage for data is specified with variable declarations. Every variable must reside in one of\nthe state spaces enumerated in the previous section.\nA variable declaration names the space in which the variable resides, its type and size, its name,\nan optional array size, an optional initializer, and an optional fixed address for the variable.\nPredicate variables may only be declared in the register state space.\nExamples\n.global .u32 loc;\n.reg    .s32 i;\n.const  .f32 bias[] = {-1.0, 1.0};\n.global .u8  bg[4] = {0, 0, 0, 0};\n.reg    .v4 .f32 accel;\n.reg    .pred p, q, r;\n5.4.2.\nVectors\n\nLimited-length vector types are supported. Vectors of length 2 and 4 of any non-predicate\nfundamental type can be declared by prefixing the type with\n.v2\nor\n.v4\n. Vectors must be\nbased on a fundamental type, and they may reside in the register space. Vectors cannot exceed\n128-bits in length; for example,\n.v4\n.f64\nis not allowed. Three-element vectors may be\nhandled by using a\n.v4\nvector, where the fourth element provides padding. This is a common case\nfor three-dimensional grids, textures, etc.\nExamples\n.global .v4 .f32 V;   // a length-4 vector of floats\n.shared .v2 .u16 uv;  // a length-2 vector of unsigned ints\n.global .v4 .b8  v;   // a length-4 vector of bytes\nBy default, vector variables are aligned to a multiple of their overall size (vector length times\nbase-type size), to enable vector load and store instructions which require addresses aligned to a\nmultiple of the access size.\n5.4.3.\nArray Declarations\n\nArray declarations are provided to allow the programmer to reserve space. To declare an array, the\nvariable name is followed with dimensional declarations similar to fixed-size array declarations\nin C. The size of each dimension is a constant expression.\nExamples\n.local  .u16 kernel[19][19];\n.shared .u8  mailbox[128];\nThe size of the array specifies how many elements should be reserved. For the declaration of array\nkernel\nabove, 19*19 = 361 halfwords are reserved, for a total of 722 bytes.\nWhen declared with an initializer, the first dimension of the array may be omitted. The size of the\nfirst array dimension is determined by the number of elements in the array initializer.\nExamples\n.global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n.global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} };\nArray\nindex\nhas eight elements, and array\noffset\nis a 4x2 array.\n5.4.4.\nInitializers\n\nDeclared variables may specify an initial value using a syntax similar to C/C++, where the variable\nname is followed by an equals sign and the initial value or values for the variable. A scalar takes\na single value, while vectors and arrays take nested lists of values inside of curly braces (the\nnesting matches the dimensionality of the declaration).\nAs in C, array initializers may be incomplete, i.e., the number of initializer elements may be less\nthan the extent of the corresponding array dimension, with remaining array locations initialized to\nthe default value for the specified array type.\nExamples\n.const  .f32 vals[8] = { 0.33, 0.25, 0.125 };\n.global .s32 x[3][2] = { {1,2}, {3} };\nis equivalent to\n.const  .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 };\n.global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} };\nCurrently, variable initialization is supported only for constant and global state spaces. Variables\nin constant and global state spaces with no explicit initializer are initialized to zero by\ndefault. Initializers are not allowed in external variable declarations.\nVariable names appearing in initializers represent the address of the variable; this can be used to\nstatically initialize a pointer to a variable. Initializers may also contain\nvar+offset\nexpressions, where\noffset\nis a byte offset added to the address of\nvar\n. Only variables in\n.global\nor\n.const\nstate spaces may be used in initializers. By default, the resulting\naddress is the offset in the variable’s state space (as is the case when taking the address of a\nvariable with a\nmov\ninstruction). An operator,\ngeneric()\n, is provided to create a generic\naddress for variables used in initializers.\nStarting PTX ISA version 7.1, an operator\nmask()\nis provided, where\nmask\nis an integer\nimmediate. The only allowed expressions in the\nmask()\noperator are integer constant expression\nand symbol expression representing address of variable. The\nmask()\noperator extracts\nn\nconsecutive bits from the expression used in initializers and inserts these bits at the lowest\nposition of the initialized variable. The number\nn\nand the starting position of the bits to be\nextracted is specified by the integer immediate\nmask\n. PTX ISA version 7.1 only supports\nextracting a single byte starting at byte boundary from the address of the variable. PTX ISA version\n7.3 supports Integer constant expression as an operand in the\nmask()\noperator.\nSupported values for\nmask\nare: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000,\n0xFF000000000000, 0xFF00000000000000.\nExamples\n.const  .u32 foo = 42;\n.global .u32 bar[] = { 2, 3, 5 };\n.global .u32 p1 = foo;          // offset of foo in .const space\n.global .u32 p2 = generic(foo); // generic address of foo\n\n// array of generic-address pointers to elements of bar\n.global .u32 parr[] = { generic(bar), generic(bar)+4,\ngeneric(bar)+8 };\n\n// examples using mask() operator are pruned for brevity\n.global .u8 addr[] = {0xff(foo), 0xff00(foo), 0xff0000(foo), ...};\n\n.global .u8 addr2[] = {0xff(foo+4), 0xff00(foo+4), 0xff0000(foo+4),...}\n\n.global .u8 addr3[] = {0xff(generic(foo)), 0xff00(generic(foo)),...}\n\n.global .u8 addr4[] = {0xff(generic(foo)+4), 0xff00(generic(foo)+4),...}\n\n// mask() operator with integer const expression\n.global .u8 addr5[] = { 0xFF(1000 + 546), 0xFF00(131187), ...};\nNote\nPTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit\ngeneric()\noperator for each global variable used in an initializer. PTX 3.1 code should\neither include explicit\ngeneric()\noperators in initializers, use\ncvta.global\nto form\ngeneric addresses at runtime, or load from the non-generic address using\nld.global\n.\nDevice function names appearing in initializers represent the address of the first instruction in\nthe function; this can be used to initialize a table of function pointers to be used with indirect\ncalls. Beginning in PTX ISA version 3.1, kernel function names can be used as initializers e.g. to\ninitialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch\nkernels from GPU. See the\nCUDA Dynamic Parallelism Programming Guide\nfor details.\nLabels cannot be used in initializers.\nVariables that hold addresses of variables or functions should be of type\n.u8\nor\n.u32\nor\n.u64\n.\nType\n.u8\nis allowed only if the\nmask()\noperator is used.\nInitializers are allowed for all types except\n.f16\n,\n.f16x2\nand\n.pred\n.\nExamples\n.global .s32 n = 10;\n.global .f32 blur_kernel[][3]\n               = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}};\n\n.global .u32 foo[] = { 2, 3, 5, 7, 9, 11 };\n.global .u64 ptr = generic(foo);   // generic address of foo[0]\n.global .u64 ptr = generic(foo)+8; // generic address of foo[2]\n5.4.5.\nAlignment\n\nByte alignment of storage for all addressable variables can be specified in the variable\ndeclaration. Alignment is specified using an optional\n.align\nbyte-count\nspecifier immediately\nfollowing the state-space specifier. The variable will be aligned to an address which is an integer\nmultiple of byte-count. The alignment value byte-count must be a power of two. For arrays, alignment\nspecifies the address alignment for the starting address of the entire array, not for individual\nelements.\nThe default alignment for scalar and array variables is to a multiple of the base-type size. The\ndefault alignment for vector variables is to a multiple of the overall vector size.\nExamples\n// allocate array at 4-byte aligned address.  Elements are bytes.\n.const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0};\nNote that all PTX instructions that access memory require that the address be aligned to a multiple\nof the access size. The access size of a memory instruction is the total number of bytes accessed in\nmemory. For example, the access size of\nld.v4.b32\nis 16 bytes, while the access size of\natom.f16x2\nis 4 bytes.\n5.4.6.\nParameterized Variable Names\n\nSince PTX supports virtual registers, it is quite common for a compiler frontend to generate a large\nnumber of register names. Rather than require explicit declaration of every name, PTX supports a\nsyntax for creating a set of variables having a common prefix string appended with integer suffixes.\nFor example, suppose a program uses a large number, say one hundred, of\n.b32\nvariables, named\n%r0\n,\n%r1\n, …,\n%r99\n. These 100 register variables can be declared as follows:\n.reg .b32 %r<100>;    // declare %r0, %r1, ..., %r99\nThis shorthand syntax may be used with any of the fundamental types and with any state space, and\nmay be preceded by an alignment specifier. Array variables cannot be declared this way, nor are\ninitializers permitted.\n5.4.7.\nVariable Attributes\n\nVariables may be declared with an optional\n.attribute\ndirective which allows specifying special\nattributes of variables. Keyword\n.attribute\nis followed by attribute specification inside\nparenthesis. Multiple attributes are separated by comma.\nVariable and Function Attribute Directive: .attribute\ndescribes the\n.attribute\ndirective.\n5.4.8.\nVariable and Function Attribute Directive:\n.attribute\n\n.attribute\nVariable and function attributes\nDescription\nUsed to specify special attributes of a variable or a function.\nThe following attributes are supported.\n.managed\n.managed\nattribute specifies that variable will be allocated at a location in unified virtual\nmemory environment where host and other devices in the system can reference the variable\ndirectly. This attribute can only be used with variables in .global state space. See the\nCUDA\nUVM-Lite Programming Guide\nfor details.\n.unified\n.unified\nattribute specifies that function has the same memory address on the host and on\nother devices in the system. Integer constants\nuuid1\nand\nuuid2\nrespectively specify upper\nand lower 64 bits of the unique identifier associated with the function or the variable. This\nattribute can only be used on device functions or on variables in the\n.global\nstate\nspace. Variables with\n.unified\nattribute are read-only and must be loaded by specifying\n.unified\nqualifier on the address operand of\nld\ninstruction, otherwise the behavior is\nundefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.0.\nSupport for function attributes introduced in PTX ISA version 8.0.\nTarget ISA Notes\n.managed\nattribute requires\nsm_30\nor higher.\n.unified\nattribute requires\nsm_90\nor higher.\nExamples\n.global .attribute(.managed) .s32 g;\n.global .attribute(.managed) .u64 x;\n\n.global .attribute(.unified(19,95)) .f32 f;\n\n.func .attribute(.unified(0xAB, 0xCD)) bar() { ... }\n5.5.\nTensors\n\nA tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following\nproperties:\nDimensionality\nDimension sizes across each dimension\nIndividual element types\nTensor stride across each dimension\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include:\nCopying data between global and shared memories\nReducing the destination tensor data with the source.\nThe Tensor data can be operated on by various\nwmma.mma\n,\nmma\nand\nwgmma.mma_async\ninstructions.\nPTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure\nand treat the data in the shared memory as a linear data.\n5.5.1.\nTensor Dimension, size and format\n\nTensors can have dimensions: 1D, 2D, 3D, 4D or 5D.\nEach dimension has a size which represents the number of elements along the dimension. The elements\ncan have one the following types:\nBit-sized type:\n.b32\n,\n.b64\nSub-byte types:\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\n,\n.b6p2x16\nInteger:\n.u8\n,\n.u16\n,\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\nFloating point and alternate floating point:\n.f16\n,\n.bf16\n,\n.tf32\n,\n.f32\n,\n.f64\n(rounded to nearest even).\nTensor can have padding at the end in each of the dimensions to provide alignment for the data in\nthe subsequent dimensions. Tensor stride can be used to specify the amount of padding in each\ndimension.\n5.5.1.1.\nSub-byte Types\n\n5.5.1.1.1.\nPadding and alignment of the sub-byte types\n\nThe sub-byte types are expected to packed contiguously in the global memory and\nthe Tensor copy instruction will expand them by appending empty spaces as shown below:\nType\n.b4x16\n:\nWith this type, there is no padding involved and the packed sixteen\n.b4\nelements\nin a 64-bits container is copied as is between the shared memory and the global memory.\nType\n.b4x16_p64\n:\nWith this type, sixteen contiguous 4-bits of data is copied from global memory to the\nshared memory with the append of 64-bits of padding as shown in\nFigure 5\nFigure 5\nLayout for .b4x16_p64\n\nThe padded region that gets added is un-initialized.\nType\n.b6x16_p32\n:\nWith this type, sixteen 6-bits of data is copied from global memory to the shared memory\nwith an append of 32-bits of padding as shown in\nFigure 6\nFigure 6\nLayout for .b6x16_p32\n\nThe padded region that gets added is un-initialized.\nType\n.b6p2x16\n:\nWith this type, sixteen elements, each containing 6-bits of data at the LSB and 2-bits\nof padding at the MSB, are copied from shared memory into the global memory by discarding\nthe 2-bits of padding data and packing the 6-bits data contiguously as shown in\nFigure 7\nFigure 7\nLayout for .b6p2x16\n\nIn case of\n.b6x16_p32\nand\n.b4x16_p64\n, the padded region that gets added is\nun-initialized.\nThe types\n.b6x16_p32\nand\n.b6p2x16\nshare the same encoding value in the\ndescriptor (value 15) as the two types are applicable for different types of\ntensor copy operations:\nType\nValid Tensor Copy Direction\n.b6x16_p32\n.shared::cluster.global\n,\n.shared::cta.global\n.b6p2x16\n.global.shared::cta\n5.5.2.\nTensor Access Modes\n\nTensor data can be accessed in two modes:\nTiled mode:\nIn tiled mode, the source multi-dimensional tensor layout is preserved at the destination.\nIm2col mode:\nIn im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns\nat the destination. Refer\nhere\nfor more details.\n5.5.3.\nTiled Mode\n\nThis section talks about how Tensor and Tensor access work in tiled mode.\n5.5.3.1.\nBounding Box\n\nA tensor can be accessed in chunks known as\nBounding Box\n. The Bounding Box has the same\ndimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple\nof 16 bytes. The address of the bounding Box must also be aligned to 16 bytes.\nBounding Box has the following access properties:\nBounding Box dimension sizes\nOut of boundary access mode\nTraversal strides\nThe tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the\nbounding box. Starting offset of the bounding box along with the rest of the bounding box\ninformation together are used to determine the elements which are to be accessed.\n5.5.3.2.\nTraversal-Stride\n\nWhile the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies\nthe exact number of elements to be skipped. If no jump over is required, default value of 1 must be\nspecified.\nThe traversal stride in dimension 0 can be used for the\nInterleave layout\n.\nFor non-interleaved layout, the traversal stride in\ndimension 0 must always be 1.\nFigure 8\nillustrates tensor, tensor size, tensor stride,\nBounding Box size and traversal stride.\nFigure 8\nTiled mode bounding box, tensor size and traversal stride\n\n5.5.3.3.\nOut of Boundary Access\n\nPTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor\nboundary in any dimension. There are 2 modes:\nZero fill mode:\nElements in the Bounding Box which fall outside of the tensor boundary are set to 0.\nOOB-NaN\nfill mode:\nElements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN\ncalled\nOOB-NaN\n.\nFigure 9\nshows an example of the out of boundary access.\nFigure 9\nOut of boundary access\n\n5.5.3.4.\n.tile::scatter4\nand\n.tile::gather4\nmodes\n\nThese modes are similar to the tiled mode with restriction that these modes work only on 2D tensor data.\nTile::scatter4\nand\nTile::gather4\nmodes are used to access multiple non-contiguous rows of tensor data.\nIn\nTile::scatter4\nmode single 2D source tensor is divided into four rows in the 2D destination tensor.\nIn\nTile::gather4\nmode four rows in the source 2D tensor are combined to form single 2D destination tensor.\nThese modes work on four rows and hence the instruction will take:\nfour tensor coordinates across the dimension 0\none tensor coordinate across the dimension 1\nThe interleave layout is not supported for\n.tile::scatter4\nand\n.tile::gather4\nmodes.\nAll other constraints and rules of the tile mode apply to these modes as well.\n5.5.3.4.1.\nBounding Box\n\nFor\nTile::scatter4\nand\nTile::gather4\nmodes, four request coordinates will form four Bounding\nBoxes in the tensor space.\nFigure 10\nshows an example of the same with start\ncoordinates (1, 2), (1, 5), (1, 0) and (1, 9).\nThe size of the bounding box in the dimension 0 represents the length of the rows.\nThe size of the bounding box in the dimension 1 must be one.\nFigure 10\ntiled::scatter4/tiled::gather4 mode bounding box example\n\n5.5.4.\nim2col\nmode\n\nIm2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data\nis treated as a batch of images with the following properties:\nN : number of images in the batch\nD, H, W : size of a 3D image (depth, height and width)\nC: channels per image element\nThe above properties are associated with 3D, 4D and 5D tensors as follows:\nDimension\nN/D/H/W/C applicability\n3D\nNWC\n4D\nNHWC\n5D\nNDHWC\n5.5.4.1.\nBounding Box\n\nIn im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are\nspecified by Pixels-per-Column and Channels-per-Pixel parameters as described below.\nThe dimensionality of the Bounding Box is two less than the tensor dimensionality.\nThe following properties describe how to access of the elements in im2col mode:\nBounding-Box Lower-Corner\nBounding-Box Upper-Corner\nPixels-per-Column\nChannels-per-Pixel\nBounding-box Lower-Corner\nand\nBounding-box Upper-Corner\nspecify the two opposite corners of the\nBounding Box in the DHW space.\nBounding-box Lower-Corner\nspecifies the corner with the smallest\ncoordinate and\nBounding-box Upper-Corner\nspecifies the corner with the largest coordinate.\nBounding-box Upper-\nand\nLower-Corners\nare 16-bit signed values whose limits varies across the\ndimensions and are as shown below:\n3D\n4D\n5D\nUpper- / Lower- Corner sizes\n[-2\n15\n, 2\n15\n-1]\n[-2\n7\n, 2\n7\n-1]\n[-2\n4\n, 2\n4\n-1]\nFigure 11\nand\nFigure 12\nshow the Upper-Corners and Lower-Corners.\nFigure 11\nim2col mode bounding box example 1\n\nFigure 12\nim2col mode bounding box example 2\n\nThe\nBounding-box Upper-\nand\nLower- Corners\nspecify only the boundaries and not the number of\nelements to be accessed.\nPixels-per-Column\nspecifies the number of elements to be accessed in the\nNDHW space.\nChannels-per-Pixel\nspecifies the number of elements to access across the C dimension.\nThe tensor coordinates, specified in the PTX tensor instructions, behaves differently in different\ndimensions:\nAcross N and C dimensions: specify the starting offsets along the dimension, similar to the tiled\nmode.\nAcross DHW dimensions: specify the location of the convolution filter base in the tensor\nspace. The filter corner location must be within the bounding box.\nThe im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter\nbase coordinates to determine the starting location in the tensor space from where the elements are\naccessed.\nThe size of the im2col offsets varies across the dimensions and their valid ranges are as shown\nbelow:\n3D\n4D\n5D\nim2col offsets range\n[0, 2\n16\n-1]\n[0, 2\n8\n-1]\n[0, 2\n5\n-1]\nFollowing are some examples of the im2col mode accesses:\nExample 1 (\nFigure 13\n):\nTensor\nSize\n[\n0\n]\n=\n64\nTensor\nSize\n[\n1\n]\n=\n9\nTensor\nSize\n[\n2\n]\n=\n14\nTensor\nSize\n[\n3\n]\n=\n64\nPixels\n-\nper\n-\nColumn\n=\n64\nchannels\n-\nper\n-\npixel\n=\n8\nBounding\n-\nBox\nLower\n-\nCorner\nW\n=\n-1\nBounding\n-\nBox\nLower\n-\nCorner\nH\n=\n-1\nBounding\n-\nBox\nUpper\n-\nCorner\nW\n=\n-1\nBounding\n-\nBox\nUpper\n-\nCorner\nH\n=\n-1.\ntensor\ncoordinates\n=\n(\n7\n,\n7\n,\n4\n,\n0\n)\nim2col\noffsets\n:\n(\n0\n,\n0\n)\nFigure 13\nim2col mode example 1\n\nExample 2 (\nFigure 14\n):\nTensor\nSize\n[\n0\n]\n=\n64\nTensor\nSize\n[\n1\n]\n=\n9\nTensor\nSize\n[\n2\n]\n=\n14\nTensor\nSize\n[\n3\n]\n=\n64\nPixels\n-\nper\n-\nColumn\n=\n64\nchannels\n-\nper\n-\npixel\n=\n8\nBounding\n-\nBox\nLower\n-\nCorner\nW\n=\n0\nBounding\n-\nBox\nLower\n-\nCorner\nH\n=\n0\nBounding\n-\nBox\nUpper\n-\nCorner\nW\n=\n-2\nBounding\n-\nBox\nUpper\n-\nCorner\nH\n=\n-2\ntensor\ncoordinates\n=\n(\n7\n,\n7\n,\n4\n,\n0\n)\nim2col\noffsets\n:\n(\n2\n,\n2\n)\nFigure 14\nim2col mode example 2\n\n5.5.4.2.\nTraversal Stride\n\nThe traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being\naccessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being\naccessed, in im2col mode.\nThe number of elements traversed along the D, H and W dimensions is strided by the traversal stride\nfor that dimension.\nThe following example with\nFigure 15\nillustrates accesse with traversal-strides:\nTensor Size[0] = 64\nTensor Size[1] = 8\nTensor Size[2] = 14\nTensor Size[3] = 64\nTraversal Stride = 2\nPixels-per-Column = 32\nchannels-per-pixel = 16\nBounding-Box Lower-Corner W = -1\nBounding-Box Lower-Corner H = -1\nBounding-Box Upper-Corner W = -1\nBounding-Box Upper-Corner H = -1.\nTensor coordinates in the instruction = (7, 7, 5, 0)\nIm2col offsets in the instruction : (1, 1)\nFigure 15\nim2col mode traversal stride example\n\n5.5.4.3.\nOut of Boundary Access\n\nIn im2col mode, when the number of requested pixels in NDHW space specified by\nPixels-per-Column\nexceeds the number of available pixels in the image batch then out-of-bounds access is performed.\nSimilar to tiled mode, zero fill or\nOOB-NaN\nfill can be performed based on the Fill-Mode\nspecified.\n5.5.5.\nim2col::w\nand\nim2col::w::128\nmodes\n\nThese modes are similar to the im2col mode with the restriction that elements are accessed across\nthe\nW\ndimension only while keeping the\nH\nand\nD\ndimension constant.\nAll the constraints and rules of the im2col mode apply to these modes as well.\nThe number of elements accessed in the\nim2col::w::128\nmode is fixed and is equal to 128.\nThe number of elements accessed in the\nim2col::w\nmode depends on the field Pixels-per-Column\nfield in the TensorMap.\n5.5.5.1.\nBounding Box\n\nIn these modes, the size of the bounding box in\nD\nand\nH\ndimensions are 1.\nThe\nD\nand\nH\ndimensions in the tensor coordinates argument in the PTX instruction specify\nthe position of the bounding box in the tensor space.\nThe Bounding-Box\nLower-Corner-W\nand Bounding-Box\nUpper-Corner-W\nspecify the two opposite\ncorners of the Bounding Box in the\nW\ndimension.\nThe\nW\ndimension in the tensor coordinates argument in the PTX instruction specify the location\nof the first element that is to be accessed in the bounding box.\nNumber of pixels loaded in\nim2col::w\nmode is as specified by Pixels-per-Column in the TensorMap.\nNumber of pixels loaded in\nim2col::w::128\nmode is always 128. So, Pixels-per-Column is ignored\nin\nim2col::w::128\nmode.\nFigure 16\nshows an example of the\nim2col::w\nand\nim2col::w:128\nmodes.\nFigure 16\nim2col::w and im2col::w::128 modes example\n\nThe first element can lie outside of the Bounding Box in the W-dimension only and only on the left\nside of the Bounding Box.\nFigure 17\nshows of an example of this.\nFigure 17\nim2col::w and im2col::w::128 modes first element outside Bounding Box example\n\n5.5.5.2.\nTraversal Stride\n\nThis is similar to im2col mode with the exception of that the number of elements traversed\nalong only the\nW\ndimension is strided by the traversal stride as specified in the TensorMap.\n5.5.5.3.\nwHalo\n\nIn\nim2col::w\nmode, the\nwHalo\nargument in the PTX instruction specifies how many filter\nhalo elements must be loaded at the end of the image.\nIn\nim2col::w::128\nmode, the halo elements are loaded after every 32 elements in the bounding\nbox along the\nW\ndimension. The\nwHalo\nargument in the PTX instruction specifies how many\nhalo elements must be loaded after every 32 elements.\nFollowing is an example of\n.im2col::w\nmode access:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 7\nTensor Size [3] = 64\nPixels-per-column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = 0\nBounding Box Upper Corner W = 0\n\nTensor Coordinates in the instruction = (7, 2, 3, 0)\nwHalo in the instruction = 2 (as 3x3 convolution filter is used)\nA tensor copy operation with the above parameters loads 128 pixels and the two halo pixels as shown in\nFigure 18\n.\nFigure 18\ntensor copy operation with im2col::w mode example\n\nThe halo pixels are always loaded in the shared memory next to the main row pixels as shown in\nFigure 18\n.\nFollowing is an example of\n.im2col::w::128\nmode access:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 7\nTensor Size [3] = 64\nChannels-per-pixel = 64\nBounding Box Lower Corner W = 0\nBounding Box Upper Corner W = 0\n\nTensor Coordinates in the instruction = (7, 2, 3, 0)\nwHalo in the instruction = 2 (as 3x3 convolution filter is used)\nA tensor copy operation with the above parameters loads 128 elements such that after every 32 elements,\nwHalo number of elements are loaded as shown in\nFigure 19\n.\nFigure 19\ntensor copy operation with im2col::w::128 mode example\n\n5.5.5.4.\nwOffset\n\nIn the convolution calculations, the same elements along the\nW\ndimension are reused for different\nlocations within the convolution filter footprint. Based on the number of times a pixel is used, the\npixels may be loaded into different shared memory buffers. Each buffer can be loaded by a separate\ntensor copy operation.\nThe\nwOffset\nargument in the tensor copy and prefetch instruction adjusts the source pixel location\nfor each buffer. The exact position of the buffer is adjusted along the\nW\ndimension using the\nfollowing formula:\nBounding Box Lower Corner W += wOffset\nBounding Box Upper Corner W += wOffset\nW += wOffset\nFollowing are examples of tensor copy to multiple buffers with various\nwHalo\nand\nwOffset\nvalues:\nExample 1:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 67\nTensor Size [3] = 64\nPixels-per-Column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = -1\nBounding Box Upper Corner W = 0\nTraversal Stride = 2\n\nTensor Coordinates in the instruction = (7, 2, -1, 0)\n\nShared memory buffer 1:\n   wHalo = 1\n   wOffset = 0\n\nShared memory buffer 2:\n   wHalo = 0\n   wOffset = 1\nFigure 20\ntensor copy operation to buffer 1 of Example 1\n\nFigure 21\ntensor copy operation to buffer 2 of Example 1\n\nExample 2:\nTensor Size [0] = 128\nTensor Size [1] = 7\nTensor Size [2] = 7\nTensor Size [3] = 64\nPixels-per-Column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = -1\nBounding Box Upper Corner W = -1\nTraversal Stride = 3\n\nTensor Coordinates in the instruction = (7, 2, -1, 0)\n\nShared memory buffer 1:\n   wHalo = 0\n   wOffset = 0\n\nShared memory buffer 2:\n   wHalo = 0\n   wOffset = 1\n\nShared memory buffer 3:\n   wHalo = 0\n   wOffset = 2\nFigure 22\ntensor copy operation to buffer 1 of Example 2\n\nFigure 23\ntensor copy operation to buffer 2 of Example 2\n\nFigure 24\ntensor copy operation to buffer 3 of Example 2\n\n5.5.6.\nInterleave layout\n\nTensor can be interleaved and the following interleave layouts are supported:\nNo interleave (NDHWC)\n8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel.\n16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel.\nThe\nC\ninformation is organized in slices where sequential C elements are grouped in 16 byte or 32\nbyte quantities.\nIf the total number of channels is not a multiple of the number of channels per slice, then the last\nslice must be padded with zeros to make it complete 16B or 32B slice.\nInterleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D.\nThe interleave layout is not supported for\n.im2col::w\nand\n.im2col::w::128\nmodes.\n5.5.7.\nSwizzling Modes\n\nThe layout of the data in the shared memory can be different to that of global memory, for access\nperformance reasons. The following describes various swizzling modes:\nNo swizzle mode:\nThere is no swizzling in this mode and the destination data layout is exactly similar to the\nsource data layout.\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n… Pattern repeats …\n32 byte swizzle mode:\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n256 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n… Pattern repeats …\nAn example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension,\nwith the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in\nFigure 25\n.\nFigure 25\n32-byte swizzle mode example\n\nFigure 26\nshows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1.\nFigure 26\n32-byte swizzle mode fragments\n\nFigure 27\nshows the destination data layout with 32 byte swizzling.\nFigure 27\n32-byte swizzle mode destination data layout\n\n64 byte swizzle mode:\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n512 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n2\n3\n0\n1\n6\n7\n4\n5\n3\n2\n1\n0\n7\n6\n5\n4\n… Pattern repeats …\nAn example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 32 channels, is shown in\nFigure 28\n.\nFigure 28\n64-byte swizzle mode example\n\nEach colored cell represents 8 channels.\nFigure 29\nshows the source data layout.\nFigure 29\n64-byte swizzle mode source data layout\n\nFigure 30\nshows the destination data layout with 64 byte swizzling.\nFigure 30\n64-byte swizzle mode destination data layout\n\n96 byte swizzle mode:\nThe following table where each element (numbered cell) is 16 byte shows the swizzling pattern at the destination\ndata layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n… Pattern repeats …\nAn example of the data layout in global memory and its swizzled data layout in shared memory where each element\n(colored cell) is 16 bytes and the starting address is 256 bytes aligned is shown in\nFigure 31\n.\nFigure 31\n96-byte swizzle mode example\n\n128 byte swizzle mode:\nThe 128-byte swizzling mode supports the following sub-modes:\n16-byte atomicity sub-mode:\nIn this sub-mode, the 16-byte of data is kept intact while swizzling.\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n1024 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n2\n3\n0\n1\n6\n7\n4\n5\n3\n2\n1\n0\n7\n6\n5\n4\n4\n5\n6\n7\n0\n1\n2\n3\n5\n4\n7\n6\n1\n0\n3\n2\n6\n7\n4\n5\n2\n3\n0\n1\n7\n6\n5\n4\n3\n2\n1\n0\n… Pattern repeats …\nAn example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 64 channels, is shown in\nFigure 32\n.\nFigure 32\n128-byte swizzle mode example\n\nEach colored cell represents 8 channels.\nFigure 33\nshows the source data layout.\nFigure 33\n128-byte swizzle mode source data layout\n\nFigure 34\nshows the destination data layout with 128 byte swizzling.\nFigure 34\n128-byte swizzle mode destination data layout\n\n32-byte atomicity sub-mode:\nIn this sub-mode, the 32-byte of data is kept intact while swizzling.\nThe following table where each element (numbered cell) is 16 byte shows the\nswizzling pattern at the destination data layout:\n0   1\n2   3\n4   5\n6   7\n2   3\n0   1\n6   7\n4   5\n4   5\n6   7\n0   1\n2   3\n6   7\n4   5\n2   3\n0   1\n… Pattern repeats …\nThis sub-mode requires 32 byte alignment at shared memory.\nAn example of the data layout in global memory and its swizzled data layout in shared memory\nwhere each element (colored cell) is 16 bytes is shown in\nFigure 35\nFigure 35\n128-byte swizzle mode example with 32-byte atomicity\n\n32-byte atomicity with 8-byte flip sub-mode:\nThe swizzling pattern for this sub-mode is similar to the 32-byte atomicity sub-mode except that\nthere is a flip of adjacent 8-bytes within the 16-byte data at every alternate shared memory line.\nAn example of the data layout in global memory and its swizzled data layout in shared memory where\neach element (colored cell) is 16 bytes (two 8-byte sub-elements for each 16-byte colored cell are\nshown to show the flip) is shown in\nFigure 36\nFigure 36\n128-byte swizzle mode example with 32-byte atomicity with 8-byte flip\n\n64-byte atomicity sub-mode:\nIn this sub-mode, the 64-byte of data is kept intact while swizzling.\nThe following table where each element (numbered cell) is 16 byte shows the swizzling\npattern at the destination data layout:\n0   1   2   3\n4   5   6   7\n4   5   6   7\n0   1   2   3\n… Pattern repeats …\nThis sub-mode requires 64-byte alignment at shared memory.\nAn example of the data layout in global memory and its swizzled data layout\nin shared memory where each element (colored cell) is 16 bytes is shown\nin\nFigure 37\nFigure 37\n128-byte swizzle mode example with 64-byte atomicity\n\nTable 14\nlists the valid combination of swizzle-atomicity with the swizzling-mode.\nTable 14\nValid combination of swizzle-atomicity with swizzling-mode\n\nSwizzling Mode\nSwizzle-Atomicity\nNo Swizzling\n–\n32B Swizzling Mode\n16B\n64B Swizzling Mode\n16B\n96B Swizzling Mode\n16B\n128B Swizzling Mode\n16B\n32B\n32B + 8B-flip\n64B\nThe value of swizzle base offset is 0 when the\ndstMem\nshared memory address is located\nat the following boundary:\nSwizzling Mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n96-Byte swizzle\n256-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the swizzle base offset is a non-zero value, computed using following formula:\nSwizzling Mode\nFormula\n128-Byte swizzle\nbase offset = (dstMem / 128) % 8\n96-Byte swizzle\nbase offset = (dstMem / 128) % 2\n64-Byte swizzle\nbase offset = (dstMem / 128) % 4\n32-Byte swizzle\nbase offset = (dstMem / 128) % 2\n5.5.8.\nTensor-map\n\nThe tensor-map is a 128-byte opaque object either in\n.const\nspace or\n.param\n(kernel function\nparameter) space or\n.global\nspace which describes the tensor properties and the access properties\nof the tensor data described in previous sections.\nTensor-Map can be created using CUDA APIs. Refer to\nCUDA programming guide\nfor more details.\n6.\nInstruction Operands\n\n6.1.\nOperand Type Information\n\nAll operands in instructions have a known type from their declarations. Each operand type must be\ncompatible with the type determined by the instruction template and instruction type. There is no\nautomatic conversion between types.\nThe bit-size type is compatible with every type having the same size. Integer types of a common size\nare compatible with each other. Operands having type different from but compatible with the\ninstruction type are silently cast to the instruction type.\n6.2.\nSource Operands\n\nThe source operands are denoted in the instruction descriptions by the names\na\n,\nb\n, and\nc\n. PTX describes a load-store machine, so operands for ALU instructions must all be in variables\ndeclared in the\n.reg\nregister state space. For most operations, the sizes of the operands must\nbe consistent.\nThe\ncvt\n(convert) instruction takes a variety of operand types and sizes, as its job is to\nconvert from nearly any data type to any other data type (and size).\nThe\nld\n,\nst\n,\nmov\n, and\ncvt\ninstructions copy data from one location to\nanother. Instructions\nld\nand\nst\nmove data from/to addressable state spaces to/from\nregisters. The\nmov\ninstruction copies data between registers.\nMost instructions have an optional predicate guard that controls conditional execution, and a few\ninstructions have additional predicate source operands. Predicate operands are denoted by the names\np\n,\nq\n,\nr\n,\ns\n.\n6.3.\nDestination Operands\n\nPTX instructions that produce a single result store the result in the field denoted by\nd\n(for\ndestination) in the instruction descriptions. The result operand is a scalar or vector variable in\nthe register state space.\n6.4.\nUsing Addresses, Arrays, and Vectors\n\nUsing scalar variables as operands is straightforward. The interesting capabilities begin with\naddresses, arrays, and vectors.\n6.4.1.\nAddresses as Operands\n\nAll the memory instructions take an address operand that specifies the memory location being\naccessed. This addressable operand is one of:\n[var]\nthe name of an addressable variable\nvar\n.\n[reg]\nan integer or bit-size type register\nreg\ncontaining a byte address.\n[reg+immOff]\na sum of register\nreg\ncontaining a byte address plus a constant integer byte offset (signed, 32-bit).\n[var+immOff]\na sum of address of addressable variable\nvar\ncontaining a byte address plus a constant integer\nbyte offset (signed, 32-bit).\n[immAddr]\nan immediate absolute byte address (unsigned, 32-bit).\nvar[immOff]\nan array element as described in\nArrays as Operands\n.\nThe register containing an address may be declared as a bit-size type or integer type.\nThe access size of a memory instruction is the total number of bytes accessed in memory. For\nexample, the access size of\nld.v4.b32\nis 16 bytes, while the access size of\natom.f16x2\nis 4\nbytes.\nThe address must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined. For example, among other things, the access\nmay proceed by silently masking off low-order address bits to achieve proper rounding, or the\ninstruction may fault.\nThe address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. Addresses are\nzero-extended to the specified width as needed, and truncated if the register width exceeds the\nstate space address width for the target architecture.\nAddress arithmetic is performed using integer arithmetic and logical instructions. Examples include\npointer arithmetic and pointer comparisons. All addresses and address computations are byte-based;\nthere is no support for C-style pointer arithmetic.\nThe\nmov\ninstruction can be used to move the address of a variable into a pointer. The address is\nan offset in the state space in which the variable is declared. Load and store operations move data\nbetween registers and locations in addressable state spaces. The syntax is similar to that used in\nmany assembly languages, where scalar variables are simply named and addresses are de-referenced by\nenclosing the address expression in square brackets. Address expressions include variable names,\naddress registers, address register plus byte offset, and immediate address expressions which\nevaluate at compile-time to a constant address.\nHere are a few examples:\n.shared .u16 x;\n.reg    .u16 r0;\n.global .v4 .f32 V;\n.reg    .v4 .f32 W;\n.const  .s32 tbl[256];\n.reg    .b32 p;\n.reg    .s32 q;\n\nld.shared.u16   r0,[x];\nld.global.v4.f32 W, [V];\nld.const.s32    q, [tbl+12];\nmov.u32         p, tbl;\n6.4.1.1.\nGeneric Addressing\n\nIf a memory instruction does not specify a state space, the operation is performed using generic\naddressing. The state spaces\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.local\nand\n.shared\nare modeled as\nwindows within the generic address space. Each window is defined by a window base and a window size\nthat is equal to the size of the corresponding state space. A generic address maps to\nglobal\nmemory unless it falls within the window for\nconst\n,\nlocal\n, or\nshared\nmemory. The\nKernel Function Parameters\n(\n.param\n) window is contained\nwithin the\n.global\nwindow. Within each window, a generic address maps to an address in the\nunderlying state space by subtracting the window base from the generic address.\n6.4.2.\nArrays as Operands\n\nArrays of all types can be declared, and the identifier becomes an address constant in the space\nwhere the array is declared. The size of the array is a constant in the program.\nArray elements can be accessed using an explicitly calculated byte address, or by indexing into the\narray using square-bracket notation. The expression within square brackets is either a constant\ninteger, a register variable, or a simple\nregister with constant offset\nexpression, where the\noffset is a constant expression that is either added or subtracted from a register variable. If more\ncomplicated indexing is desired, it must be written as an address calculation prior to use. Examples\nare:\nld.global.u32  s, a[0];\nld.global.u32  s, a[N-1];\nmov.u32        s, a[1];  // move address of a[1] into s\n6.4.3.\nVectors as Operands\n\nVector operands can be specified as source and destination operands for instructions. However, when\nspecified as destination operand, all elements in vector expression must be unique, otherwise behavior\nis undefined.\nVectors may also be passed as arguments to called functions.\nVector elements can be extracted from the vector with the suffixes\n.x\n,\n.y\n,\n.z\nand\n.w\n, as well as the typical color fields\n.r\n,\n.g\n,\n.b\nand\n.a\n.\nA brace-enclosed list is used for pattern matching to pull apart vectors.\n.reg .v4 .f32 V;\n.reg .f32     a, b, c, d;\n\nmov.v4.f32 {a,b,c,d}, V;\nVector loads and stores can be used to implement wide loads and stores, which may improve memory\nperformance. The registers in the load/store operations can be a vector, or a brace-enclosed list of\nsimilarly typed scalars. Here are examples:\nld.global.v4.f32  {a,b,c,d}, [addr+16];\nld.global.v2.u32  V2, [addr+8];\nElements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows:\nRa = V.x = V.r\nRb = V.y = V.g\nRc = V.z = V.b\nRd = V.w = V.a\n6.4.4.\nLabels and Function Names as Operands\n\nLabels and function names can be used only in\nbra\n/\nbrx.idx\nand\ncall\ninstructions\nrespectively. Function names can be used in\nmov\ninstruction to get the address of the function\ninto a register, for use in an indirect call.\nBeginning in PTX ISA version 3.1, the\nmov\ninstruction may be used to take the address of kernel\nfunctions, to be passed to a system call that initiates a kernel launch from the GPU. This feature\nis part of the support for CUDA Dynamic Parallelism. See the\nCUDA Dynamic Parallelism Programming\nGuide\nfor details.\n6.5.\nType Conversion\n\nAll operands to all arithmetic, logic, and data movement instruction must be of the same type and\nsize, except for operations where changing the size and/or type is part of the definition of the\ninstruction. Operands of different sizes or types must be converted prior to the operation.\n6.5.1.\nScalar Conversions\n\nTable 15\nand\nTable 16\nshow what\nprecision and format the cvt instruction uses given operands of differing types. For example, if a\ncvt.s32.u16\ninstruction is given a\nu16\nsource operand and\ns32\nas a destination operand,\nthe\nu16\nis zero-extended to\ns32\n.\nConversions to floating-point that are beyond the range of floating-point numbers are represented\nwith the maximum floating-point value (IEEE 754 Inf for\nf32\nand\nf64\n, and ~131,000 for\nf16\n).\nTable 15\nConvert Instruction Precision and Format Table 1\n\nDestination Format\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nbf16\ntf32\nSource\nFormat\ns8\n–\nsext\nsext\nsext\n–\nsext\nsext\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns16\nchop\n1\n–\nsext\nsext\nchop\n1\n–\nsext\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns32\nchop\n1\nchop\n1\n–\nsext\nchop\n1\nchop\n1\n–\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns64\nchop\n1\nchop\n1\nchop\n1\n–\nchop\n1\nchop\n1\nchop\n1\n–\ns2f\ns2f\ns2f\ns2f\n–\nu8\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu16\nchop\n1\n–\nzext\nzext\nchop\n1\n–\nzext\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu32\nchop\n1\nchop\n1\n–\nzext\nchop\n1\nchop\n1\n–\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu64\nchop\n1\nchop\n1\nchop\n1\n–\nchop\n1\nchop\n1\nchop\n1\n–\nu2f\nu2f\nu2f\nu2f\n–\nf16\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\n–\nf2f\nf2f\nf2f\n–\nf32\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\n–\nf2f\nf2f\nf2f\nf64\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\nf2f\n–\nf2f\n–\nbf16\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\nf2f\nf2f\nf2f\n–\ntf32\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\nTable 16\nConvert Instruction Precision and Format Table 2\n\nDestination Format\nf16\nf32\nbf16\ne4m3\ne5m2\ne2m3\ne3m2\ne2m1\nue8m0\ns2f6\nSource\nFormat\nf16\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\n–\n–\nf32\nf2f\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nbf16\nf2f\nf2f\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\ne4m3\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne5m2\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne2m3\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne3m2\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne2m1\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\nue8m0\n–\n–\nf2f\n–\n–\n–\n–\n–\n–\n–\ns2f6\n–\n–\nf2f\n–\n–\n–\n–\n–\n–\n–\nNotes\nsext = sign-extend; zext = zero-extend; chop = keep only low bits that fit;\ns2f = signed-to-float; f2s = float-to-signed; u2f = unsigned-to-float;\nf2u = float-to-unsigned; f2f = float-to-float.\n1\nIf the destination register is wider than the destination format, the result is extended to the\ndestination register width after chopping. The type of extension (sign or zero) is based on the\ndestination format. For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then\nsign-extends to 32-bit.\n6.5.2.\nRounding Modifiers\n\nConversion instructions may specify a rounding modifier. In PTX, there are four integer rounding\nmodifiers and six floating-point rounding\nmodifiers.\nTable 17\nand\nTable 18\nsummarize the rounding modifiers.\nTable 17\nFloating-Point Rounding Modifiers\n\nModifier\nDescription\n.rn\nrounds to nearest even\n.rna\nrounds to nearest, ties away from zero\n.rz\nrounds towards zero\n.rm\nrounds towards negative infinity\n.rp\nrounds towards positive infinity\n.rs\nrounds either towards zero or away from zero based\non the carry out of the integer addition of random\nbits and the discarded bits of mantissa\nTable 18\nInteger Rounding Modifiers\n\nModifier\nDescription\n.rni\nround to nearest integer, choosing even integer if source is equidistant between two integers.\n.rzi\nround to nearest integer in the direction of zero\n.rmi\nround to nearest integer in direction of negative infinity\n.rpi\nround to nearest integer in direction of positive infinity\n6.6.\nOperand Costs\n\nOperands from different state spaces affect the speed of an operation. Registers are fastest, while\nglobal memory is slowest. Much of the delay to memory can be hidden in a number of ways. The first\nis to have multiple threads of execution so that the hardware can issue a memory operation and then\nswitch to other execution. Another way to hide latency is to issue the load instructions as early as\npossible, as execution is not blocked until the desired result is used in a subsequent (in time)\ninstruction. The register in a store operation is available much more\nquickly.\nTable 19\ngives estimates of the\ncosts of using different kinds of memory.\nTable 19\nCost Estimates for Accessing State-Spaces\n\nSpace\nTime\nNotes\nRegister\n0\nShared\n0\nConstant\n0\nAmortized cost is low, first access is high\nLocal\n> 100 clocks\nParameter\n0\nImmediate\n0\nGlobal\n> 100 clocks\nTexture\n> 100 clocks\nSurface\n> 100 clocks\n7.\nAbstracting the ABI\n\nRather than expose details of a particular calling convention, stack layout, and Application Binary\nInterface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI\nimplementations. In this section, we describe the features of PTX needed to achieve this hiding of\nthe ABI. These include syntax for function definitions, function calls, parameter passing, and\nmemory allocated on the stack (\nalloca\n).\nRefer to\nPTX Writers Guide to Interoperability\nfor details on generating PTX compliant with\nApplication Binary Interface (ABI) for the CUDA\n®\narchitecture.\n7.1.\nFunction Declarations and Definitions\n\nIn PTX, functions are declared and defined using the\n.func\ndirective. A function\ndeclaration\nspecifies an optional list of return parameters, the function name, and an optional list of input\nparameters; together these specify the function’s interface, or prototype. A function\ndefinition\nspecifies both the interface and the body of the function. A function must be declared or defined\nprior to being called.\nThe simplest function has no parameters or return values, and is represented in PTX as follows:\n.func foo\n{\n    ...\n    ret;\n}\n\n    ...\n    call foo;\n    ...\nHere, execution of the\ncall\ninstruction transfers control to\nfoo\n, implicitly saving the\nreturn address. Execution of the\nret\ninstruction within\nfoo\ntransfers control to the\ninstruction following the call.\nScalar and vector base-type input and return parameters may be represented simply as register\nvariables. At the call, arguments may be register variables or constants, and return values may be\nplaced directly into register variables. The arguments and return variables at the call must have\ntype and size that match the callee’s corresponding formal parameters.\nExample\n.func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc )\n{\n    add.u32 %res, %ptr, %inc;\n    ret;\n}\n\n    ...\n    call (%r1), inc_ptr, (%r1,4);\n    ...\nWhen using the ABI,\n.reg\nstate space parameters must be at least 32-bits in size. Subword scalar\nobjects in the source language should be promoted to 32-bit registers in PTX, or use\n.param\nstate space byte arrays described next.\nObjects such as C structures and unions are flattened into registers or byte arrays in PTX and are\nrepresented using\n.param\nspace memory. For example, consider the following C structure, passed\nby value to a function:\nstruct {\n    double dbl;\n    char   c[4];\n};\nIn PTX, this structure will be flattened into a byte array. Since memory accesses are required to be\naligned to a multiple of the access size, the structure in this example will be a 12 byte array with\n8 byte alignment so that accesses to the\n.f64\nfield are aligned. The\n.param\nstate space is\nused to pass the structure by value:\nExample\n.func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12])\n{\n    .reg .f64 f1;\n    .reg .b32 c1, c2, c3, c4;\n    ...\n    ld.param.f64 f1, [y+0];\n    ld.param.b8  c1, [y+8];\n    ld.param.b8  c2, [y+9];\n    ld.param.b8  c3, [y+10];\n    ld.param.b8  c4, [y+11];\n    ...\n    ... // computation using x,f1,c1,c2,c3,c4;\n}\n\n{\n     .param .b8 .align 8 py[12];\n     ...\n     st.param.b64 [py+ 0], %rd;\n     st.param.b8  [py+ 8], %rc1;\n     st.param.b8  [py+ 9], %rc2;\n     st.param.b8  [py+10], %rc1;\n     st.param.b8  [py+11], %rc2;\n     // scalar args in .reg space, byte array in .param space\n     call (%out), bar, (%x, py);\n     ...\nIn this example, note that\n.param\nspace variables are used in two ways. First, a\n.param\nvariable\ny\nis used in function definition bar to represent a formal parameter. Second, a\n.param\nvariable\npy\nis declared in the body of the calling function and used to set up the\nstructure being passed to bar.\nThe following is a conceptual way to think about the\n.param\nstate space use in device functions.\nFor a caller,\nThe\n.param\nstate space is used to set values that will be passed to a called function and/or\nto receive return values from a called function. Typically, a\n.param\nbyte array is used to\ncollect together fields of a structure being passed by value.\nFor a callee,\nThe\n.param\nstate space is used to receive parameter values and/or pass return values back to\nthe caller.\nThe following restrictions apply to parameter passing.\nFor a caller,\nArguments may be\n.param\nvariables,\n.reg\nvariables, or constants.\nIn the case of\n.param\nspace formal parameters that are byte arrays, the argument must also be\na\n.param\nspace byte array with matching type, size, and alignment. A\n.param\nargument must\nbe declared within the local scope of the caller.\nIn the case of\n.param\nspace formal parameters that are base-type scalar or vector variables,\nthe corresponding argument may be either a\n.param\nor\n.reg\nspace variable with matching\ntype and size, or a constant that can be represented in the type of the formal parameter.\nIn the case of\n.reg\nspace formal parameters, the corresponding argument may be either a\n.param\nor\n.reg\nspace variable of matching type and size, or a constant that can be\nrepresented in the type of the formal parameter.\nIn the case of\n.reg\nspace formal parameters, the register must be at least 32-bits in size.\nAll\nst.param\ninstructions used for passing arguments to function call must immediately precede\nthe corresponding\ncall\ninstruction and\nld.param\ninstruction used for collecting return\nvalue must immediately follow the\ncall\ninstruction without any control flow\nalteration.\nst.param\nand\nld.param\ninstructions used for argument passing cannot be\npredicated. This enables compiler optimization and ensures that the\n.param\nvariable does not\nconsume extra space in the caller’s frame beyond that needed by the ABI. The\n.param\nvariable\nsimply allows a mapping to be made at the call site between data that may be in multiple\nlocations (e.g., structure being manipulated by caller is located in registers and memory) to\nsomething that can be passed as a parameter or return value to the callee.\nFor a callee,\nInput and return parameters may be\n.param\nvariables or\n.reg\nvariables.\nParameters in\n.param\nmemory must be aligned to a multiple of 1, 2, 4, 8, or 16 bytes.\nParameters in the\n.reg\nstate space must be at least 32-bits in size.\nThe\n.reg\nstate space can be used to receive and return base-type scalar and vector values,\nincluding sub-word size objects when compiling in non-ABI mode. Supporting the\n.reg\nstate\nspace provides legacy support.\nNote that the choice of\n.reg\nor\n.param\nstate space for parameter passing has no impact on\nwhether the parameter is ultimately passed in physical registers or on the stack. The mapping of\nparameters to physical registers and stack locations depends on the ABI definition and the order,\nsize, and alignment of parameters.\n7.1.1.\nChanges from PTX ISA Version 1.x\n\nIn PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no\nsupport for array parameters. Objects such as C structures were flattened and passed or returned\nusing multiple registers. PTX ISA version 1.x supports multiple return values for this purpose.\nBeginning with PTX ISA version 2.0, formal parameters may be in either\n.reg\nor\n.param\nstate\nspace, and\n.param\nspace parameters support arrays. For targets\nsm_20\nor higher, PTX\nrestricts functions to a single return value, and a\n.param\nbyte array should be used to return\nobjects that do not fit into a register. PTX continues to support multiple return registers for\nsm_1x\ntargets.\nNote\nPTX implements a stack-based ABI only for targets\nsm_20\nor higher.\nPTX ISA versions prior to 3.0 permitted variables in\n.reg\nand\n.local\nstate spaces to be\ndefined at module scope. When compiling to use the ABI, PTX ISA version 3.0 and later disallows\nmodule-scoped\n.reg\nand\n.local\nvariables and restricts their use to within function\nscope. When compiling without use of the ABI, module-scoped\n.reg\nand\n.local\nvariables are\nsupported as before. When compiling legacy PTX code (ISA versions prior to 3.0) containing\nmodule-scoped\n.reg\nor\n.local\nvariables, the compiler silently disables use of the ABI.\n7.2.\nVariadic Functions\n\nNote\nSupport for variadic functions which was unimplemented has been removed from the spec.\nPTX version 6.0 supports passing unsized array parameter to a function which can be used to\nimplement variadic functions.\nRefer to\nKernel and Function Directives: .func\nfor details\n7.3.\nAlloca\n\nPTX provides\nalloca\ninstruction for allocating storage at runtime on the per-thread local memory\nstack. The allocated stack memory can be accessed with\nld.local\nand\nst.local\ninstructions\nusing the pointer returned by\nalloca\n.\nIn order to facilitate deallocation of memory allocated with\nalloca\n, PTX provides two additional\ninstructions:\nstacksave\nwhich allows reading the value of stack pointer in a local variable, and\nstackrestore\nwhich can restore the stack pointer with the saved value.\nalloca\n,\nstacksave\n, and\nstackrestore\ninstructions are described in\nStack Manipulation Instructions\n.\n8.\nMemory Consistency Model\n\nIn multi-threaded executions, the side-effects of memory operations performed by each thread become\nvisible to other threads in a partial and non-identical order. This means that any two operations\nmay appear to happen in no order, or in different orders, to different threads. The axioms\nintroduced by the memory consistency model specify exactly which contradictions are forbidden\nbetween the orders observed by different threads.\nIn the absence of any constraint, each read operation returns the value committed by some write\noperation to the same memory location, including the initial write to that memory location. The\nmemory consistency model effectively constrains the set of such candidate writes from which a read\noperation can return a value.\n8.1.\nScope and applicability of the model\n\nThe constraints specified under this model apply to PTX programs with any PTX ISA version number,\nrunning on\nsm_70\nor later architectures.\nThe memory consistency model does not apply to texture (including\nld.global.nc\n) and surface\naccesses.\n8.1.1.\nLimitations on atomicity at system scope\n\nWhen communicating with the host CPU, certain strong operations with system scope may not be\nperformed atomically on some systems. For more details on atomicity guarantees to host memory, see\nthe\nCUDA Atomicity Requirements\n.\n8.2.\nMemory operations\n\nThe fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits. Each state\nspace available to a PTX program is a sequence of contiguous bytes in memory. Every byte in a PTX\nstate space has a unique address relative to all threads that have access to the same state space.\nEach PTX memory instruction specifies an address operand and a data type. The address operand\ncontains a virtual address that gets converted to a physical address during memory access. The\nphysical address and the size of the data type together define a physical memory location, which is\nthe range of bytes starting from the physical address and extending up to the size of the data type\nin bytes.\nThe memory consistency model specification uses the terms “address” or “memory address” to indicate\na virtual address, and the term “memory location” to indicate a physical memory location.\nEach PTX memory instruction also specifies the operation — either a read, a write or an atomic\nread-modify-write — to be performed on all the bytes in the corresponding memory location.\n8.2.1.\nOverlap\n\nTwo memory locations are said to overlap when the starting address of one location is within the\nrange of bytes constituting the other location. Two memory operations are said to overlap when they\nspecify the same virtual address and the corresponding memory locations overlap. The overlap is said\nto be complete when both memory locations are identical, and it is said to be partial otherwise.\n8.2.2.\nAliases\n\nTwo distinct virtual addresses are said to be aliases if they map to the same memory location.\n8.2.3.\nMultimem Addresses\n\nA multimem address is a virtual address which points to multiple distinct memory locations across\ndevices.\nOnly\nmultimem.\n* operations are valid on multimem addresses. That is, the behavior of accessing\na multimem address in any other memory operation is undefined.\n8.2.4.\nMemory Operations on Vector Data Types\n\nThe memory consistency model relates operations executed on memory locations with scalar data types,\nwhich have a maximum size and alignment of 64 bits. Memory operations with a vector data type are\nmodelled as a set of equivalent memory operations with a scalar data type, executed in an\nunspecified order on the elements in the vector.\n8.2.5.\nMemory Operations on Packed Data Types\n\nA packed data type consists of two values of the same scalar data type, as described in\nPacked Data Types\n. These values are accessed in adjacent memory locations. A\nmemory operation on a packed data type is modelled as a pair of equivalent memory operations on the\nscalar data type, executed in an unspecified order on each element of the packed data.\n8.2.6.\nInitialization\n\nEach byte in memory is initialized by a hypothetical write\nW0\nexecuted before starting any thread\nin the program. If the byte is included in a program variable, and that variable has an initial\nvalue, then\nW0\nwrites the corresponding initial value for that byte; else\nW0\nis assumed to have\nwritten an unknown but constant value to the byte.\n8.3.\nState spaces\n\nThe relations defined in the memory consistency model are independent of state spaces. In\nparticular, causality order closes over all memory operations across all the state spaces. But the\nside-effect of a memory operation in one state space can be observed directly only by operations\nthat also have access to the same state space. This further constrains the synchronizing effect of a\nmemory operation in addition to scope. For example, the synchronizing effect of the PTX instruction\nld.relaxed.shared.sys\nis identical to that of\nld.relaxed.shared.cluster\n, since no thread\noutside the same cluster can execute an operation that accesses the same memory location.\n8.4.\nOperation types\n\nFor simplicity, the rest of the document refers to the following operation types, instead of\nmentioning specific instructions that give rise to them.\nTable 20\nOperation Types\n\nOperation Type\nInstruction/Operation\natomic operation\natom\nor\nred\ninstruction.\nread operation\nAll variants of\nld\ninstruction and\natom\ninstruction (but not\nred\ninstruction).\nwrite operation\nAll variants of\nst\ninstruction, and\natomic\noperations if they result\nin a write.\nmemory operation\nA\nread\nor\nwrite\noperation.\nvolatile operation\nAn instruction with\n.volatile\nqualifier.\nacquire operation\nA\nmemory\noperation with\n.acquire\nor\n.acq_rel\nqualifier.\nrelease operation\nA\nmemory\noperation with\n.release\nor\n.acq_rel\nqualifier.\nmmio operation\nAn\nld\nor\nst\ninstruction with\n.mmio\nqualifier.\nmemory fence operation\nA\nmembar\n,\nfence.sc\nor\nfence.acq_rel\ninstruction.\nproxy fence operation\nA\nfence.proxy\nor a\nmembar.proxy\ninstruction.\nstrong operation\nA\nmemory fence\noperation, or a\nmemory\noperation with a\n.relaxed\n,\n.acquire\n,\n.release\n,\n.acq_rel\n,\n.volatile\n, or\n.mmio\nqualifier.\nweak operation\nAn\nld\nor\nst\ninstruction with a\n.weak\nqualifier.\nsynchronizing operation\nA\nbarrier\ninstruction,\nfence\noperation,\nrelease\noperation or\nacquire\noperation.\n8.4.1.\nmmio Operation\n\nAn\nmmio\noperation is a memory operation with\n.mmio\nqualifier specified. It is usually performed\non a memory location which is mapped to the control registers of peer I/O devices. It can also be\nused for communication between threads but has poor performance relative to non-\nmmio\noperations.\nThe semantic meaning of\nmmio\noperations cannot be defined precisely as it is defined by the\nunderlying I/O device. For formal specification of semantics of\nmmio\noperation from Memory\nConsistency Model perspective, it is equivalent to the semantics of a\nstrong\noperation. But it\nfollows a few implementation-specific properties, if it meets the\nCUDA atomicity requirements\nat\nthe specified scope:\nWrites are always performed and are never combined within the scope specified.\nReads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any\ncache within the scope specified.\nAs an exception, in some implementations, the surrounding locations may also be loaded. In such\ncases the amount of data loaded is implementation specific and varies between 32 and 128 bytes\nin size.\n8.4.2.\nvolatile Operation\n\nA\nvolatile\noperation is a memory operation with\n.volatile\nqualifier specified.\nThe semantics of volatile operations are equivalent to a relaxed memory operation with system-scope\nbut with the following extra implementation-specific constraints:\nThe number of volatile\ninstructions\n(not operations) executed by a program is preserved.\nHardware may combine and merge volatile\noperations\nissued by multiple different volatile\ninstructions\n, that is, the number of volatile\noperations\nin the program is not preserved.\nVolatile\ninstructions\nare not re-ordered around other volatile\ninstructions\n, but the memory\noperations\nperformed by those\ninstructions\nmay be re-ordered around each other.\nNote\nPTX volatile operations are intended for compilers to lower volatile read and write operations from\nCUDA C++, and other programming languages sharing CUDA C++ volatile semantics, to PTX.\nSince volatile operations are relaxed at system-scope with extra constraints, prefer using other\nstrong\nread or write operations (e.g.\nld.relaxed.sys\nor\nst.relaxed.sys\n) for\nInter-Thread Synchronization\ninstead, which may deliver better performance.\nPTX volatile operations are not suited for\nMemory Mapped IO (MMIO)\nbecause volatile operations\ndo not preserve the number of memory operations performed, and may perform more or less operations\nthan requested in a non-deterministic way.\nUse\n.mmio operations\ninstead, which strictly preserve the number of operations\nperformed.\n8.5.\nScope\n\nEach\nstrong\noperation must specify a\nscope\n, which is the set of threads that may interact\ndirectly with that operation and establish any of the relations described in the memory consistency\nmodel. There are four scopes:\nTable 21\nScopes\n\nScope\nDescription\n.cta\nThe set of all threads executing in the same CTA as the current thread.\n.cluster\nThe set of all threads executing in the same cluster as the current thread.\n.gpu\nThe set of all threads in the current program executing on the same compute\ndevice as the current thread. This also includes other kernel grids invoked by\nthe host program on the same compute device.\n.sys\nThe set of all threads in the current program, including all kernel grids\ninvoked by the host program on all compute devices, and all threads\nconstituting the host program itself.\nNote that the warp is not a\nscope\n; the CTA is the smallest collection of threads that qualifies as\na\nscope\nin the memory consistency model.\n8.6.\nProxies\n\nA\nmemory proxy\n, or a\nproxy\nis an abstract label applied to a method of memory access. When two\nmemory operations use distinct methods of memory access, they are said to be different\nproxies\n.\nMemory operations as defined in\nOperation types\nuse\ngeneric\nmethod of memory access, i.e. a\ngeneric proxy\n. Other operations such as textures and surfaces all\nuse distinct methods of memory access, also distinct from the\ngeneric\nmethod.\nA\nproxy fence\nis required to synchronize memory operations across different\nproxies\n. Although\nvirtual aliases use the\ngeneric\nmethod of memory access, since using distinct virtual addresses\nbehaves as if using different\nproxies\n, they require a\nproxy fence\nto establish memory ordering.\n8.7.\nMorally strong operations\n\nTwo operations are said to be\nmorally strong\nrelative to each other if they satisfy all of the\nfollowing conditions:\nThe operations are related in\nprogram order\n(i.e, they are both executed by the same thread),\nor each operation is\nstrong\nand specifies a\nscope\nthat includes the thread executing the\nother operation.\nBoth operations are performed via the same\nproxy\n.\nIf both are memory operations, then they overlap completely.\nMost (but not all) of the axioms in the memory consistency model depend on relations between\nmorally strong\noperations.\n8.7.1.\nConflict and Data-races\n\nTwo\noverlapping\nmemory operations are said to\nconflict\nwhen at least one of them is a\nwrite\n.\nTwo\nconflicting\nmemory operations are said to be in a\ndata-race\nif they are not related in\ncausality order\nand they are not\nmorally strong\n.\n8.7.2.\nLimitations on Mixed-size Data-races\n\nA\ndata-race\nbetween operations that\noverlap\ncompletely is called a\nuniform-size data-race\n,\nwhile a\ndata-race\nbetween operations that\noverlap\npartially is called a\nmixed-size data-race\n.\nThe axioms in the memory consistency model do not apply if a PTX program contains one or more\nmixed-size data-races\n. But these axioms are sufficient to describe the behavior of a PTX program\nwith only\nuniform-size data-races\n.\nAtomicity of mixed-size RMW operations\nIn any program with or without\nmixed-size data-races\n, the following property holds for every pair\nof\noverlapping atomic\noperations A1 and A2 such that each specifies a\nscope\nthat includes the\nother: Either the\nread-modify-write\noperation specified by A1 is performed completely before A2 is\ninitiated, or vice versa. This property holds irrespective of whether the two operations A1 and A2\noverlap partially or completely.\n8.8.\nRelease and Acquire Patterns\n\nSome sequences of instructions give rise to patterns that participate in memory synchronization as\ndescribed later. The\nrelease\npattern makes prior operations from the current thread\n1\nvisible to some operations from other threads. The\nacquire\npattern makes some operations from\nother threads visible to later operations from the current thread.\nA\nrelease\npattern on a location M consists of one of the following:\nA\nrelease\noperation on M\nE.g.:\nst.release\n[M];\nor\natom.release\n[M];\nor\nmbarrier.arrive.release\n[M];\nOr a\nrelease\nor\nacquire-release\noperation on M followed by a\nstrong\nwrite on M in\nprogram order\nE.g.:\nst.release\n[M]\n;\nst.relaxed\n[M];\nOr a\nrelease\nor\nacquire-release\nmemory fence\nfollowed by a\nstrong\nwrite on M in\nprogram order\nE.g.:\nfence.release;\nst.relaxed\n[M];\nor\nfence.release;\natom.relaxed\n[M];\nAny\nmemory synchronization\nestablished by a\nrelease\npattern only affects operations occurring in\nprogram order\nbefore the first instruction in that pattern.\nAn\nacquire\npattern on a location M consists of one of the following:\nAn\nacquire\noperation on M\nE.g.:\nld.acquire\n[M];\nor\natom.acquire\n[M];\nor\nmbarrier.test_wait.acquire\n[M];\nOr a\nstrong\nread on M followed by an\nacquire\noperation on M in\nprogram order\nE.g.:\nld.relaxed\n[M];\nld.acquire\n[M];\nOr a\nstrong\nread on M followed by an acquire\nmemory fence\nin\nprogram order\nE.g.:\nld.relaxed\n[M];\nfence.acquire;\nor\natom.relaxed\n[M];\nfence.acquire;\nAny\nmemory synchronization\nestablished by an\nacquire\npattern only affects operations occurring\nin\nprogram order\nafter the last instruction in that pattern.\nNote that while atomic reductions conceptually perform a strong read as part of its\nread-modify-write sequence, this strong read does not form an acquire pattern.\nE.g.:\nred.add\n[M],\n1;\nfence.acquire;\nis not an acquire pattern.\n1\nFor both\nrelease\nand\nacquire\npatterns, this effect is further extended to operations in\nother threads through the transitive nature of\ncausality order\n.\n8.9.\nOrdering of memory operations\n\nThe sequence of operations performed by each thread is captured as\nprogram order\nwhile\nmemory\nsynchronization\nacross threads is captured as\ncausality order\n. The visibility of the side-effects\nof memory operations to other memory operations is captured as\ncommunication order\n. The memory\nconsistency model defines contradictions that are disallowed between communication order on the one\nhand, and\ncausality order\nand\nprogram order\non the other.\n8.9.1.\nProgram Order\n\nThe\nprogram order\nrelates all operations performed by a thread to the order in which a sequential\nprocessor will execute instructions in the corresponding PTX source. It is a transitive relation\nthat forms a total order over the operations performed by the thread, but does not relate operations\nfrom different threads.\n8.9.1.1.\nAsynchronous Operations\n\nSome PTX instructions (all variants of\ncp.async\n,\ncp.async.bulk\n,\ncp.reduce.async.bulk\n,\nwgmma.mma_async\n) perform operations that are asynchronous to the thread that executed the\ninstruction. These asynchronous operations are ordered after prior instructions in the same thread\n(except in the case of\nwgmma.mma_async\n), but they are not part of the program order for that\nthread. Instead, they provide weaker ordering guarantees as documented in the instruction\ndescription.\nFor example, the loads and stores performed as part of a\ncp.async\nare ordered with respect to\neach other, but not to those of any other\ncp.async\ninstructions initiated by the same thread,\nnor any other instruction subsequently issued by the thread with the exception of\ncp.async.commit_group\nor\ncp.async.mbarrier.arrive\n. The asynchronous mbarrier\narrive-on\noperation\nperformed by a\ncp.async.mbarrier.arrive\ninstruction is ordered with respect to the memory\noperations performed by all prior\ncp.async\noperations initiated by the same thread, but not to\nthose of any other instruction issued by the thread. The implicit mbarrier\ncomplete-tx\noperation that is part of all variants of\ncp.async.bulk\nand\ncp.reduce.async.bulk\ninstructions is ordered only with respect to the memory operations performed by the same\nasynchronous instruction, and in particular it does not transitively establish ordering with respect\nto prior instructions from the issuing thread.\n8.9.2.\nObservation Order\n\nObservation order\nrelates a write W to a read R through an optional sequence of atomic\nread-modify-write operations.\nA write W precedes a read R in\nobservation order\nif:\nR and W are\nmorally strong\nand R reads the value written by W, or\nFor some atomic operation Z, W precedes Z and Z precedes R in\nobservation order\n.\n8.9.3.\nFence-SC Order\n\nThe\nFence-SC\norder is an acyclic partial order, determined at runtime, that relates every pair of\nmorally strong fence.sc\noperations.\n8.9.4.\nMemory synchronization\n\nSynchronizing operations performed by different threads synchronize with each other at runtime as\ndescribed here. The effect of such synchronization is to establish\ncausality order\nacross threads.\nA\nfence.sc\noperation X\nsynchronizes\nwith a\nfence.sc\noperation Y if X precedes Y in the\nFence-SC\norder.\nA\nbar{.cta}.sync\nor\nbar{.cta}.red\nor\nbar{.cta}.arrive\noperation\nsynchronizes\nwith a\nbar{.cta}.sync\nor\nbar{.cta}.red\noperation executed on the same barrier.\nA\nbarrier.cluster.arrive\noperation synchronizes with a\nbarrier.cluster.wait\noperation.\nA\nrelease\npattern X\nsynchronizes\nwith an\nacquire\npattern Y, if a\nwrite\noperation in X\nprecedes a\nread\noperation in Y in\nobservation order\n, and the first operation in X and the\nlast operation in Y are\nmorally strong\n.\nAPI synchronization\nA\nsynchronizes\nrelation can also be established by certain CUDA APIs.\nCompletion of a task enqueued in a CUDA stream\nsynchronizes\nwith the start of the following\ntask in the same stream, if any.\nFor purposes of the above, recording or waiting on a CUDA event in a stream, or causing a\ncross-stream barrier to be inserted due to\ncudaStreamLegacy\n, enqueues tasks in the associated\nstreams even if there are no direct side effects. An event record task\nsynchronizes\nwith\nmatching event wait tasks, and a barrier arrival task\nsynchronizes\nwith matching barrier wait\ntasks.\nStart of a CUDA kernel\nsynchronizes\nwith start of all threads in the kernel. End of all threads\nin a kernel\nsynchronize\nwith end of the kernel.\nStart of a CUDA graph\nsynchronizes\nwith start of all source nodes in the graph. Completion of\nall sink nodes in a CUDA graph\nsynchronizes\nwith completion of the graph. Completion of a graph\nnode\nsynchronizes\nwith start of all nodes with a direct dependency.\nStart of a CUDA API call to enqueue a task\nsynchronizes\nwith start of the task.\nCompletion of the last task queued to a stream, if any,\nsynchronizes\nwith return from\ncudaStreamSynchronize\n. Completion of the most recently queued matching event record task, if\nany,\nsynchronizes\nwith return from\ncudaEventSynchronize\n. Synchronizing a CUDA device or\ncontext behaves as if synchronizing all streams in the context, including ones that have been\ndestroyed.\nReturning\ncudaSuccess\nfrom an API to query a CUDA handle, such as a stream or event, behaves\nthe same as return from the matching synchronization API.\nIn addition to establishing a\nsynchronizes\nrelation, the CUDA API synchronization mechanisms above\nalso participate in\nproxy-preserved base causality order\n.\n8.9.5.\nCausality Order\n\nCausality order\ncaptures how memory operations become visible across threads through synchronizing\noperations. The axiom “Causality” uses this order to constrain the set of write operations from\nwhich a read operation may read a value.\nRelations in the\ncausality order\nprimarily consist of relations in\nBase causality order\n1\n, which is a transitive order, determined at runtime.\nBase causality order\nAn operation X precedes an operation Y in\nbase causality order\nif:\nX precedes Y in\nprogram order\n, or\nX\nsynchronizes\nwith Y, or\nFor some operation Z,\nX precedes Z in\nprogram order\nand Z precedes Y in\nbase causality order\n, or\nX precedes Z in\nbase causality order\nand Z precedes Y in\nprogram order\n, or\nX precedes Z in\nbase causality order\nand Z precedes Y in\nbase causality order\n.\nProxy-preserved base causality order\nA memory operation X precedes a memory operation Y in\nproxy-preserved base causality order\nif X\nprecedes Y in\nbase causality order\n, and:\nX and Y are performed to the same address, using the\ngeneric proxy\n, or\nX and Y are performed to the same address, using the same\nproxy\n, and by the same thread block,\nor\nX and Y are aliases and there is an alias\nproxy fence\nalong the base causality path from X\nto Y.\nCausality order\nCausality order\ncombines\nbase causality order\nwith some non-transitive relations as follows:\nAn operation X precedes an operation Y in\ncausality order\nif:\nX precedes Y in\nproxy-preserved base causality order\n, or\nFor some operation Z, X precedes Z in observation order, and Z precedes Y in\nproxy-preserved\nbase causality order\n.\n1\nThe transitivity of\nbase causality order\naccounts for the “cumulativity” of synchronizing\noperations.\n8.9.6.\nCoherence Order\n\nThere exists a partial transitive order that relates\noverlapping\nwrite operations, determined at\nruntime, called the\ncoherence order\n1\n. Two\noverlapping\nwrite operations are related in\ncoherence order\nif they are\nmorally strong\nor if they are related in\ncausality order\n. Two\noverlapping\nwrites are unrelated in\ncoherence order\nif they are in a\ndata-race\n, which gives\nrise to the partial nature of\ncoherence order\n.\n1\nCoherence order\ncannot be observed directly since it consists entirely of write\noperations. It may be observed indirectly by its use in constraining the set of candidate\nwrites that a read operation may read from.\n8.9.7.\nCommunication Order\n\nThe\ncommunication order\nis a non-transitive order, determined at runtime, that relates write\noperations to other\noverlapping\nmemory operations.\nA write W precedes an\noverlapping\nread R in\ncommunication order\nif R returns the value of any\nbyte that was written by W.\nA write W precedes a write W’ in\ncommunication order\nif W precedes W’ in\ncoherence order\n.\nA read R precedes an\noverlapping\nwrite W in\ncommunication order\nif, for any byte accessed by\nboth R and W, R returns the value written by a write W’ that precedes W in\ncoherence order\n.\nCommunication order\ncaptures the visibility of memory operations — when a memory operation X1\nprecedes a memory operation X2 in\ncommunication order\n, X1 is said to be visible to X2.\n8.10.\nAxioms\n\n8.10.1.\nCoherence\n\nIf a write W precedes an\noverlapping\nwrite W’ in\ncausality order\n, then W must precede W’ in\ncoherence order\n.\n8.10.2.\nFence-SC\n\nFence-SC\norder cannot contradict\ncausality order\n. For a pair of\nmorally strong\nfence.sc\noperations F1 and F2, if F1 precedes F2 in\ncausality order\n, then F1 must precede F2 in\nFence-SC\norder.\n8.10.3.\nAtomicity\n\nSingle-Copy Atomicity\nConflicting\nmorally strong\noperations are performed with\nsingle-copy atomicity\n. When a read R\nand a write W are\nmorally strong\n, then the following two communications cannot both exist in the\nsame execution, for the set of bytes accessed by both R and W:\nR reads any byte from W.\nR reads any byte from any write W’ which precedes W in\ncoherence order\n.\nAtomicity of read-modify-write (RMW) operations\nWhen an\natomic\noperation A and a write W\noverlap\nand are\nmorally strong\n, then the following\ntwo communications cannot both exist in the same execution, for the set of bytes accessed by both A\nand W:\nA reads any byte from a write W’ that precedes W in\ncoherence order\n.\nA follows W in\ncoherence order\n.\nLitmus Test 1\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2\nA1\n:\natom\n.\nsys\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nA2\n:\natom\n.\nsys\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nFINAL\nSTATE\n:\nx\n==\n2\nAtomicity is guaranteed when the operations are\nmorally strong\n.\nLitmus Test 2\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2 (In a different CTA)\nA1\n:\natom\n.\ncta\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nA2\n:\natom\n.\ngpu\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nFINAL\nSTATE\n:\nx\n==\n1\nOR\nx\n==\n2\nAtomicity is not guaranteed if the operations are not\nmorally strong\n.\n8.10.4.\nNo Thin Air\n\nValues may not appear “out of thin air”: an execution cannot speculatively produce a value in such a\nway that the speculation becomes self-satisfying through chains of instruction dependencies and\ninter-thread communication. This matches both programmer intuition and hardware reality, but is\nnecessary to state explicitly when performing formal analysis.\nLitmus Test: Load Buffering\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\ny\n=\n0\n;\nT1\nT2\nA1\n:\nld\n.\nglobal\n.\nu32\n%\nr0\n,\n[\nx\n];\nB1\n:\nst\n.\nglobal\n.\nu32\n[\ny\n],\n%\nr0\n;\nA2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ny\n];\nB2\n:\nst\n.\nglobal\n.\nu32\n[\nx\n],\n%\nr1\n;\nFINAL\nSTATE\n:\nx\n==\n0\nAND\ny\n==\n0\nThe litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of\nthin air. Two threads T1 and T2 each read from a first variable and copy the observed result into a\nsecond variable, with the first and second variable exchanged between the threads. If each variable\nis initially zero, the final result shall also be zero. If A1 reads from B2 and A2 reads from B1,\nthen values passing through the memory operations in this example form a cycle:\nA1->B1->A2->B2->A1. Only the values x == 0 and y == 0 are allowed to satisfy this cycle. If any of\nthe memory operations in this example were to speculatively associate a different value with the\ncorresponding memory location, then such a speculation would become self-fulfilling, and hence\nforbidden.\n8.10.5.\nSequential Consistency Per Location\n\nWithin any set of\noverlapping\nmemory operations that are pairwise\nmorally strong\n,\ncommunication\norder\ncannot contradict\nprogram order\n, i.e., a concatenation of\nprogram order\nbetween\noverlapping\noperations and\nmorally strong\nrelations in\ncommunication order\ncannot result in a\ncycle. This ensures that each program slice of\noverlapping\npairwise morally\nstrong operations\nis\nstrictly\nsequentially-consistent\n.\nLitmus Test: CoRR\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n[\nx\n],\n1\n;\nR1\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr0\n,\n[\nx\n];\nR2\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr1\n,\n[\nx\n];\nIF\n%\nr0\n==\n1\nTHEN\n%\nr1\n==\n1\nThe litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee. A\nthread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence\nof) reads R1 and R2 on the same location x. No other writes are executed on x, except the one\nmodelling the initial value. The operations W1, R1 and R2 are pairwise\nmorally strong\n. If R1 reads\nfrom W1, then the subsequent read R2 must also observe the same value. If R2 observed the initial\nvalue of x instead, then this would form a sequence of\nmorally-strong\nrelations R2->W1->R1 in\ncommunication order\nthat contradicts the\nprogram order\nR1->R2 in thread T2. Hence R2 cannot read\nthe initial value of x in such an execution.\n8.10.6.\nCausality\n\nRelations in\ncommunication order\ncannot contradict\ncausality order\n. This constrains the set of\ncandidate write operations that a read operation may read from:\nIf a read R precedes an\noverlapping\nwrite W in\ncausality order\n, then R cannot read from W.\nIf a write W precedes an\noverlapping\nread R in\ncausality order\n, then for any byte accessed by\nboth R and W, R cannot read from any write W’ that precedes W in\ncoherence order\n.\nLitmus Test: Message Passing\n.\nglobal\n.\nu32\ndata\n=\n0\n;\n.\nglobal\n.\nu32\nflag\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nu32\n[\ndata\n],\n1\n;\nF1\n:\nfence\n.\nsys\n;\nW2\n:\nst\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n[\nflag\n],\n1\n;\nR1\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr0\n,\n[\nflag\n];\nF2\n:\nfence\n.\nsys\n;\nR2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ndata\n];\nIF\n%\nr0\n==\n1\nTHEN\n%\nr1\n==\n1\nThe litmus test known as “MP” (Message Passing) represents the essence of typical synchronization\nalgorithms. A vast majority of useful programs can be reduced to sequenced applications of this\npattern.\nThread T1 first writes to a data variable and then to a flag variable while a second thread T2 first\nreads from the flag variable and then from the data variable. The operations on the flag are\nmorally strong\nand the memory operations in each thread are separated by a\nfence\n, and these\nfences\nare\nmorally strong\n.\nIf R1 observes W2, then the release pattern “F1; W2”\nsynchronizes\nwith the\nacquire pattern\n“R1;\nF2”. This establishes the\ncausality order\nW1 -> F1 -> W2 -> R1 -> F2 -> R2. Then axiom\ncausality\nguarantees that R2 cannot read from any write that precedes W1 in\ncoherence order\n. In the absence\nof any other writes in this example, R2 must read from W1.\nLitmus Test: CoWR\n// These addresses are aliases\n.\nglobal\n.\nu32\ndata_alias_1\n;\n.\nglobal\n.\nu32\ndata_alias_2\n;\nT1\nW1\n:\nst\n.\nglobal\n.\nu32\n[\ndata_alias_1\n],\n1\n;\nF1\n:\nfence\n.\nproxy\n.\nalias\n;\nR1\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ndata_alias_2\n];\n%\nr1\n==\n1\nVirtual aliases require an alias\nproxy fence\nalong the synchronization path.\nLitmus Test: Store Buffering\nThe litmus test known as “SB” (Store Buffering) demonstrates the\nsequential consistency\nenforced\nby the\nfence.sc\n. A thread T1 writes to a first variable, and then reads the value of a second\nvariable, while a second thread T2 writes to the second variable and then reads the value of the\nfirst variable. The memory operations in each thread are separated by\nfence.\nsc instructions,\nand these\nfences\nare\nmorally strong\n.\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\ny\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nu32\n[\nx\n],\n1\n;\nF1\n:\nfence\n.\nsc\n.\nsys\n;\nR1\n:\nld\n.\nglobal\n.\nu32\n%\nr0\n,\n[\ny\n];\nW2\n:\nst\n.\nglobal\n.\nu32\n[\ny\n],\n1\n;\nF2\n:\nfence\n.\nsc\n.\nsys\n;\nR2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\nx\n];\n%\nr0\n==\n1\nOR\n%\nr1\n==\n1\nIn any execution, either F1 precedes F2 in\nFence-SC\norder, or vice versa. If F1 precedes F2 in\nFence-SC\norder, then F1\nsynchronizes\nwith F2. This establishes the\ncausality order\nin W1 -> F1\n-> F2 -> R2. Axiom\ncausality\nensures that R2 cannot read from any write that precedes W1 in\ncoherence order\n. In the absence of any other write to that variable, R2 must read from\nW1. Similarly, in the case where F2 precedes F1 in\nFence-SC\norder, R1 must read from W2. If each\nfence.sc\nin this example were replaced by a\nfence.acq_rel\ninstruction, then this outcome is\nnot guaranteed. There may be an execution where the write from each thread remains unobserved from\nthe other thread, i.e., an execution is possible, where both R1 and R2 return the initial value “0”\nfor variables y and x respectively.\n8.11.\nSpecial Cases\n\n8.11.1.\nReductions do not form Acquire Patterns\n\nAtomic reduction operations like\nred\ndo not form acquire patterns with acquire fences.\nLitmus Test: Message Passing with a Red Instruction\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\nflag\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nu32\n[\nx\n],\n42\n;\nW2\n:\nst\n.\nrelease\n.\ngpu\n.\nu32\n[\nflag\n],\n1\n;\nRMW1\n:\nred\n.\nsys\n.\nglobal\n.\nadd\n.\nu32\n[\nflag\n],\n1\n;\nF2\n:\nfence\n.\nacquire\n.\ngpu\n;\nR2\n:\nld\n.\nweak\n.\nu32\n%\nr1\n,\n[\nx\n];\n%\nr1\n==\n0\nAND\nflag\n==\n2\nThe litmus test known as “MP” (Message Passing) demonstrates the consequence\nof reductions being excluded from acquire patterns.\nIt is possible to observe the outcome where\nR2\nreads the value\n0\nfrom\nx\nand\nflag\nhas the final value of\n2\n.\nThis outcome is possible since the release pattern in\nT1\ndoes not synchronize\nwith any acquire pattern in\nT2\n.\nUsing the\natom\ninstruction instead of\nred\nforbids this outcome.\n9.\nInstruction Set\n\n9.1.\nFormat and Semantics of Instruction Descriptions\n\nThis section describes each PTX instruction. In addition to the name and the format of the\ninstruction, the semantics are described, followed by some examples that attempt to show several\npossible instantiations of the instruction.\n9.2.\nPTX Instructions\n\nPTX instructions generally have from zero to four operands, plus an optional guard predicate\nappearing after an\n@\nsymbol to the left of the\nopcode\n:\n@p\nopcode;\n@p\nopcode\na;\n@p\nopcode\nd,\na;\n@p\nopcode\nd,\na,\nb;\n@p\nopcode\nd,\na,\nb,\nc;\nFor instructions that create a result value, the\nd\noperand is the destination operand, while\na\n,\nb\n, and\nc\nare source operands.\nThe\nsetp\ninstruction writes two destination registers. We use a\n|\nsymbol to separate\nmultiple destination registers.\nsetp.lt.s32  p|q, a, b;  // p = (a < b); q = !(a < b);\nFor some instructions the destination operand is optional. A\nbit bucket\noperand denoted with an\nunderscore (\n_\n) may be used in place of a destination register.\n9.3.\nPredicated Execution\n\nIn PTX, predicate registers are virtual and have\n.pred\nas the type specifier. So, predicate\nregisters can be declared as\n.reg .pred p, q, r;\nAll instructions have an optional\nguard predicate\nwhich controls conditional execution of the\ninstruction. The syntax to specify conditional execution is to prefix an instruction with\n@{!}p\n,\nwhere\np\nis a predicate variable, optionally negated. Instructions without a guard predicate are\nexecuted unconditionally.\nPredicates are most commonly set as the result of a comparison performed by the\nsetp\ninstruction.\nAs an example, consider the high-level code\nif (i < n)\n    j = j + 1;\nThis can be written in PTX as\nsetp.lt.s32  p, i, n;    // p = (i < n)\n@p    add.s32      j, j, 1;    // if i < n, add 1 to j\nTo get a conditional branch or conditional function call, use a predicate to control the execution\nof the branch or call instructions. To implement the above example as a true conditional branch, the\nfollowing PTX instruction sequence might be used:\nsetp.lt.s32  p, i, n;    // compare i to n\n@!p   bra  L1;                 // if False, branch over\n      add.s32      j, j, 1;\nL1:     ...\n9.3.1.\nComparisons\n\n9.3.1.1.\nInteger and Bit-Size Comparisons\n\nThe signed integer comparisons are the traditional\neq\n(equal),\nne\n(not-equal),\nlt\n(less-than),\nle\n(less-than-or-equal),\ngt\n(greater-than), and\nge\n(greater-than-or-equal). The unsigned comparisons are\neq\n,\nne\n,\nlo\n(lower),\nls\n(lower-or-same),\nhi\n(higher), and\nhs\n(higher-or-same). The bit-size comparisons are\neq\nand\nne\n; ordering comparisons are not defined for bit-size types.\nTable 22\nshows the operators for signed integer, unsigned integer, and bit-size types.\nTable 22\nOperators for Signed Integer, Unsigned Integer, and Bit-Size Types\n\nMeaning\nSigned Operator\nUnsigned Operator\nBit-Size Operator\na\n==\nb\neq\neq\neq\na\n!=\nb\nne\nne\nne\na\n<\nb\nlt\nlo\nn/a\na\n<=\nb\nle\nls\nn/a\na\n>\nb\ngt\nhi\nn/a\na\n>=\nb\nge\nhs\nn/a\n9.3.1.2.\nFloating Point Comparisons\n\nThe ordered floating-point comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n, and\nge\n. If\neither operand is\nNaN\n, the result is\nFalse\n.\nTable 23\nlists the floating-point\ncomparison operators.\nTable 23\nFloating-Point Comparison Operators\n\nMeaning\nFloating-Point Operator\na\n==\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\neq\na\n!=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nne\na\n<\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nlt\na\n<=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nle\na\n>\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\ngt\na\n>=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nge\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered floating-point comparisons\nare provided:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n, and\ngeu\n. If both operands are numeric\nvalues (not\nNaN\n), then the comparison has the same result as its ordered counterpart. If either\noperand is\nNaN\n, then the result of the comparison is\nTrue\n.\nTable 24\nlists the floating-point\ncomparison operators accepting\nNaN\nvalues.\nTable 24\nFloating-Point Comparison Operators Accepting NaN\n\nMeaning\nFloating-Point Operator\na\n==\nb\n||\nisNaN(a)\n||\nisNaN(b)\nequ\na\n!=\nb\n||\nisNaN(a)\n||\nisNaN(b)\nneu\na\n<\nb\n||\nisNaN(a)\n||\nisNaN(b)\nltu\na\n<=\nb\n||\nisNaN(a)\n||\nisNaN(b)\nleu\na\n>\nb\n||\nisNaN(a)\n||\nisNaN(b)\ngtu\na\n>=\nb\n||\nisNaN(a)\n||\nisNaN(b)\ngeu\nTo test for\nNaN\nvalues, two operators\nnum\n(\nnumeric\n) and\nnan\n(\nisNaN\n) are\nprovided.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nTable 25\nlists the\nfloating-point comparison operators testing for\nNaN\nvalues.\nTable 25\nFloating-Point Comparison Operators Testing for NaN\n\nMeaning\nFloating-Point Operator\n!isNaN(a)\n&&\n!isNaN(b)\nnum\nisNaN(a)\n||\nisNaN(b)\nnan\n9.3.2.\nManipulating Predicates\n\nPredicate values may be computed and manipulated using the following instructions:\nand\n,\nor\n,\nxor\n,\nnot\n, and\nmov\n.\nThere is no direct conversion between predicates and integer values, and no direct way to load or\nstore predicate register values. However,\nsetp\ncan be used to generate a predicate from an\ninteger, and the predicate-based select (\nselp\n) instruction can be used to generate an integer\nvalue based on the value of a predicate; for example:\nselp.u32 %r1,1,0,%p;    // convert predicate to 32-bit value\n9.4.\nType Information for Instructions and Operands\n\nTyped instructions must have a type-size modifier. For example, the\nadd\ninstruction requires\ntype and size information to properly perform the addition operation (signed, unsigned, float,\ndifferent sizes), and this information must be specified as a suffix to the opcode.\nExample\n.reg .u16 d, a, b;\n\nadd.u16 d, a, b;    // perform a 16-bit unsigned add\nSome instructions require multiple type-size modifiers, most notably the data conversion instruction\ncvt\n. It requires separate type-size modifiers for the result and source, and these are placed in\nthe same order as the operands. For example:\n.reg .u16 a;\n.reg .f32 d;\n\ncvt.f32.u16 d, a;   // convert 16-bit unsigned to 32-bit float\nIn general, an operand’s type must agree with the corresponding instruction-type modifier. The rules\nfor operand and instruction type conformance are as follows:\nBit-size types agree with any type of the same size.\nSigned and unsigned integer types agree provided they have the same size, and integer operands are\nsilently cast to the instruction type if needed. For example, an unsigned integer operand used in\na signed integer instruction will be treated as a signed integer by the instruction.\nFloating-point types agree only if they have the same size; i.e., they must match exactly.\nTable 26\nsummarizes these type\nchecking rules.\nTable 26\nType Checking Rules\n\nOperand Type\n.bX\n.sX\n.uX\n.fX\nInstruction Type\n.bX\nokay\nokay\nokay\nokay\n.sX\nokay\nokay\nokay\ninvalid\n.uX\nokay\nokay\nokay\ninvalid\n.fX\nokay\ninvalid\ninvalid\nokay\nNote\nSome operands have their type and size defined independently from the instruction type-size. For\nexample, the shift amount operand for left and right shift instructions always has type\n.u32\n,\nwhile the remaining operands have their type and size determined by the instruction type.\nExample\n// 64-bit arithmetic right shift; shift amount 'b' is .u32\n    shr.s64 d,a,b;\n9.4.1.\nOperand Size Exceeding Instruction-Type Size\n\nFor convenience,\nld\n,\nst\n, and\ncvt\ninstructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes. The operand type checking rules are relaxed for bit-size and integer (signed and unsigned)\ninstruction types; floating-point instruction types still require that the operand type-size matches\nexactly, unless the operand is of bit-size type.\nWhen a source operand has a size that exceeds the instruction-type size, the source data is\ntruncated (chopped) to the appropriate number of bits specified by the instruction type-size.\nTable 27\nsummarizes the relaxed type-checking rules for source operands. Note that some combinations may\nstill be invalid for a particular instruction; for example, the\ncvt\ninstruction does not support\n.bX\ninstruction types, so those rows are invalid for\ncvt\n.\nTable 27\nRelaxed Type-checking Rules for Source Operands\n\nSource Operand Type\nb8\nb16\nb32\nb64\nb128\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nInstruction Type\nb8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\nchop\nchop\nchop\nb16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\n–\nchop\nchop\nb32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\n–\nchop\nb64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\n–\nb128\ninv\ninv\ninv\ninv\n–\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ns8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\ninv\ninv\ninv\ns16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\ninv\ninv\ninv\ns32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\ninv\ninv\ns64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nu8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\ninv\ninv\ninv\nu16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\ninv\ninv\ninv\nu32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\ninv\ninv\nu64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nf16\ninv\n–\nchop\nchop\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\ninv\nf32\ninv\ninv\n–\nchop\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\nf64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\nNotes\nchop = keep only low bits that fit; “–” = allowed, but no conversion needed;\ninv = invalid, parse error.\nSource register size must be of equal or greater size than the instruction-type size.\nBit-size source registers may be used with any appropriately-sized instruction type. The data are\ntruncated (“chopped”) to the instruction-type size and interpreted according to the instruction\ntype.\nInteger source registers may be used with any appropriately-sized bit-size or integer instruction\ntype. The data are truncated to the instruction-type size and interpreted according to the\ninstruction type.\nFloating-point source registers can only be used with bit-size or floating-point instruction types.\nWhen used with a narrower bit-size instruction type, the data are truncated. When used with a\nfloating-point instruction type, the size must match exactly.\nWhen a destination operand has a size that exceeds the instruction-type size, the destination data\nis zero- or sign-extended to the size of the destination register. If the corresponding instruction\ntype is signed integer, the data is sign-extended; otherwise, the data is zero-extended.\nTable 28\nsummarizes the relaxed type-checking rules for destination operands.\nTable 28\nRelaxed Type-checking Rules for Destination Operands\n\nDestination Operand Type\nb8\nb16\nb32\nb64\nb128\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nInstruction Type\nb8\n–\nzext\nzext\nzext\nzext\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\nzext\nzext\nzext\nb16\ninv\n–\nzext\nzext\nzext\ninv\n–\nzext\nzext\ninv\n–\nzext\nzext\n–\nzext\nzext\nb32\ninv\ninv\n–\nzext\nzext\ninv\ninv\n–\nzext\ninv\ninv\n–\nzext\ninv\n–\nzext\nb64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\n–\nb128\ninv\ninv\ninv\ninv\n–\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ns8\n–\nsext\nsext\nsext\nsext\n–\nsext\nsext\nsext\n–\nsext\nsext\nsext\ninv\ninv\ninv\ns16\ninv\n–\nsext\nsext\nsext\ninv\n–\nsext\nsext\ninv\n–\nsext\nsext\ninv\ninv\ninv\ns32\ninv\ninv\n–\nsext\nsext\ninv\ninv\n–\nsext\ninv\ninv\n–\nsext\ninv\ninv\ninv\ns64\ninv\ninv\ninv\n–\nsext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nu8\n–\nzext\nzext\nzext\nzext\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\ninv\ninv\ninv\nu16\ninv\n–\nzext\nzext\nzext\ninv\n–\nzext\nzext\ninv\n–\nzext\nzext\ninv\ninv\ninv\nu32\ninv\ninv\n–\nzext\nzext\ninv\ninv\n–\nzext\ninv\ninv\n–\nzext\ninv\ninv\ninv\nu64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nf16\ninv\n–\nzext\nzext\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\ninv\nf32\ninv\ninv\n–\nzext\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\nf64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\nNotes\nsext = sign-extend; zext = zero-extend; “–” = allowed, but no conversion needed;\ninv = invalid, parse error.\nDestination register size must be of equal or greater size than the instruction-type size.\nBit-size destination registers may be used with any appropriately-sized instruction type. The data\nare sign-extended to the destination register width for signed integer instruction types, and are\nzero-extended to the destination register width otherwise.\nInteger destination registers may be used with any appropriately-sized bit-size or integer\ninstruction type. The data are sign-extended to the destination register width for signed integer\ninstruction types, and are zero-extended to the destination register width for bit-size an d\nunsigned integer instruction types.\nFloating-point destination registers can only be used with bit-size or floating-point instruction\ntypes. When used with a narrower bit-size instruction type, the data are zero-extended. When used\nwith a floating-point instruction type, the size must match exactly.\n9.5.\nDivergence of Threads in Control Constructs\n\nThreads in a CTA execute together, at least in appearance, until they come to a conditional control\nconstruct such as a conditional branch, conditional function call, or conditional return. If threads\nexecute down different control flow paths, the threads are called\ndivergent\n. If all of the threads\nact in unison and follow a single control flow path, the threads are called\nuniform\n. Both\nsituations occur often in programs.\nA CTA with divergent threads may have lower performance than a CTA with uniformly executing threads,\nso it is important to have divergent threads re-converge as soon as possible. All control constructs\nare assumed to be divergent points unless the control-flow instruction is marked as uniform, using\nthe\n.uni\nsuffix. For divergent control flow, the optimizing code generator automatically\ndetermines points of re-convergence. Therefore, a compiler or code author targeting PTX can ignore\nthe issue of divergent threads, but has the opportunity to improve performance by marking branch\npoints as uniform when the compiler or author can guarantee that the branch point is non-divergent.\n9.6.\nSemantics\n\nThe goal of the semantic description of an instruction is to describe the results in all cases in as\nsimple language as possible. The semantics are described using C, until C is not expressive enough.\n9.6.1.\nMachine-Specific Semantics of 16-bit Code\n\nA PTX program may execute on a GPU with either a 16-bit or a 32-bit data path. When executing on a\n32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit\ncomputations are\npromoted\nto 32-bit computations. This can lead to computational differences\nbetween code run on a 16-bit machine versus the same code run on a 32-bit machine, since the\npromoted computation may have bits in the high-order half-word of registers that are not present in\n16-bit physical registers. These extra precision bits can become visible at the application level,\nfor example, by a right-shift instruction.\nAt the PTX language level, one solution would be to define semantics for 16-bit code that is\nconsistent with execution on a 16-bit data path. This approach introduces a performance penalty for\n16-bit code executing on a 32-bit data path, since the translated code would require many additional\nmasking instructions to suppress extra precision bits in the high-order half-word of 32-bit\nregisters.\nRather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of\n16-bit instructions in PTX is machine-specific. A compiler or programmer may chose to enforce\nportable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at\nappropriate points in the program to guarantee portability of the code. However, for many\nperformance-critical applications, this is not desirable, and for many applications the difference\nin execution is preferable to limiting performance.\n9.7.\nInstructions\n\nAll PTX instructions may be predicated. In the following descriptions, the optional guard predicate\nis omitted from the syntax.\n9.7.1.\nInteger Arithmetic Instructions\n\nInteger arithmetic instructions operate on the integer types in register and constant immediate\nforms. The integer arithmetic instructions are:\nadd\nsub\nmul\nmad\nmul24\nmad24\nsad\ndiv\nrem\nabs\nneg\nmin\nmax\npopc\nclz\nbfind\nfns\nbrev\nbfe\nbfi\nbmsk\nszext\ndp4a\ndp2a\n9.7.1.1.\nInteger Arithmetic Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd.type       d, a, b;\nadd{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64,\n          .u16x2, .s16x2 };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce\n.u16x2\n,\n.s16x2\nresult in\ndestination.\nOperands\nd\n,\na\nand\nb\nhave type\n.type\n. For instruction types\n.u16x2\n,\n.s16x2\n,\noperands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = iA[i] + iB[i];\n    }\n} else {\n    d = a + b;\n}\nNotes\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation. Applies only to\n.s32\ntype.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nadd.u16x2\nand\nadd.s16x2\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nadd.u16x2\nand\nadd.s16x2\nrequire\nsm_90\nor higher.\nExamples\n@p  add.u32     x,y,z;\n    add.sat.s32 c,c,1;\n    add.u16x2   u,v,w;\n9.7.1.2.\nInteger Arithmetic Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub.type       d, a, b;\nsub{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nSemantics\nd = a - b;\nNotes\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation. Applies only to\n.s32\ntype.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsub.s32 c,a,b;\n9.7.1.3.\nInteger Arithmetic Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul.mode.type  d, a, b;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nCompute the product of two values.\nSemantics\nt = a * b;\nn = bitwidth of type;\nd = t;            // for .wide\nd = t<2n-1..n>;   // for .hi variant\nd = t<n-1..0>;    // for .lo variant\nNotes\nThe type of the operation represents the types of the\na\nand\nb\noperands. If\n.hi\nor\n.lo\nis specified, then\nd\nis the same size as\na\nand\nb\n, and either the upper or lower\nhalf of the result is written to the destination register. If\n.wide\nis specified, then\nd\nis\ntwice as wide as\na\nand\nb\nto receive the full result of the multiplication.\nThe\n.wide\nsuffix is supported only for 16- and 32-bit integer types.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmul.wide.s16 fa,fxs,fys;   // 16*16 bits yields 32 bits\nmul.lo.s16 fa,fxs,fys;     // 16*16 bits, save only the low 16 bits\nmul.wide.s32 z,x,y;        // 32*32 bits, creates 64 bit result\n9.7.1.4.\nInteger Arithmetic Instructions:\nmad\n\nmad\nMultiply two values, optionally extract the high or low half of the intermediate result, and add a third value.\nSyntax\nmad.mode.type  d, a, b, c;\nmad.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nMultiplies two values, optionally extracts the high or low half of the intermediate result, and adds\na third value. Writes the result into a destination register.\nSemantics\nt = a * b;\nn = bitwidth of type;\nd = t + c;           // for .wide\nd = t<2n-1..n> + c;  // for .hi variant\nd = t<n-1..0> + c;   // for .lo variant\nNotes\nThe type of the operation represents the types of the\na\nand\nb\noperands. If .hi or .lo is\nspecified, then\nd\nand\nc\nare the same size as\na\nand\nb\n, and either the upper or lower\nhalf of the result is written to the destination register. If\n.wide\nis specified, then\nd\nand\nc\nare twice as wide as\na\nand\nb\nto receive the result of the multiplication.\nThe\n.wide\nsuffix is supported only for 16-bit and 32-bit integer types.\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation.\nApplies only to\n.s32\ntype in\n.hi\nmode.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n@p  mad.lo.s32 d,a,b,c;\n    mad.lo.s32 r,p,q,r;\n9.7.1.5.\nInteger Arithmetic Instructions:\nmul24\n\nmul24\nMultiply two 24-bit integer values.\nSyntax\nmul24.mode.type  d, a, b;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 };\nDescription\nCompute the product of two 24-bit integer values held in 32-bit source registers, and return either\nthe high or low 32-bits of the 48-bit result.\nSemantics\nt = a * b;\nd = t<47..16>;    // for .hi variant\nd = t<31..0>;     // for .lo variant\nNotes\nInteger multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.\nmul24.hi\nperforms a 24x24-bit multiply and returns the high 32 bits of the 48-bit result.\nmul24.lo\nperforms a 24x24-bit multiply and returns the low 32 bits of the 48-bit result.\nAll operands are of the same type and size.\nmul24.hi\nmay be less efficient on machines without hardware support for 24-bit multiply.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmul24.lo.s32 d,a,b;   // low 32-bits of 24x24-bit signed multiply.\n9.7.1.6.\nInteger Arithmetic Instructions:\nmad24\n\nmad24\nMultiply two 24-bit integer values and add a third value.\nSyntax\nmad24.mode.type  d, a, b, c;\nmad24.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 };\nDescription\nCompute the product of two 24-bit integer values held in 32-bit source registers, and add a third,\n32-bit value to either the high or low 32-bits of the 48-bit result. Return either the high or low\n32-bits of the 48-bit result.\nSemantics\nt = a * b;\nd = t<47..16> + c;   // for .hi variant\nd = t<31..0> + c;    // for .lo variant\nNotes\nInteger multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.\nmad24.hi\nperforms a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third\nvalue.\nmad24.lo\nperforms a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third\nvalue.\nAll operands are of the same type and size.\nSaturation modifier:\n.sat\nlimits result of 32-bit signed addition to\nMININT..MAXINT\n(no overflow). Applies only to\n.s32\ntype in .hi mode.\nmad24.hi\nmay be less efficient on machines without hardware support for 24-bit multiply.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmad24.lo.s32 d,a,b,c;   // low 32-bits of 24x24-bit signed multiply.\n9.7.1.7.\nInteger Arithmetic Instructions:\nsad\n\nsad\nSum of absolute differences.\nSyntax\nsad.type  d, a, b, c;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nAdds the absolute value of\na-b\nto\nc\nand writes the resulting value into\nd\n.\nSemantics\nd = c + ((a<b) ? b-a : a-b);\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsad.s32  d,a,b,c;\nsad.u32  d,a,b,d;  // running sum\n9.7.1.8.\nInteger Arithmetic Instructions:\ndiv\n\ndiv\nDivide one value by another.\nSyntax\ndiv.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nDivides\na\nby\nb\n, stores result in\nd\n.\nSemantics\nd = a / b;\nNotes\nDivision by zero yields an unspecified, machine-specific value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ndiv.s32  b,n,i;\n9.7.1.9.\nInteger Arithmetic Instructions:\nrem\n\nrem\nThe remainder of integer division.\nSyntax\nrem.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nDivides\na\nby\nb\n, store the remainder in\nd\n.\nSemantics\nd = a % b;\nNotes\nThe behavior for negative numbers is machine-dependent and depends on whether divide rounds towards\nzero or negative infinity.\nDivision by zero yields an unspecified, machine-specific value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nrem.s32  x,x,8;    // x = x%8;\n9.7.1.10.\nInteger Arithmetic Instructions:\nabs\n\nabs\nAbsolute value.\nSyntax\nabs.type  d, a;\n\n.type = { .s16, .s32, .s64 };\nDescription\nTake the absolute value of\na\nand store it in\nd\n.\nSemantics\nd = |a|;\nNotes\nOnly for signed integers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nabs.s32  r0,a;\n9.7.1.11.\nInteger Arithmetic Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg.type  d, a;\n\n.type = { .s16, .s32, .s64 };\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nSemantics\nd = -a;\nNotes\nOnly for signed integers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nneg.s32  r0,a;\n9.7.1.12.\nInteger Arithmetic Instructions:\nmin\n\nmin\nFind the minimum of two values.\nSyntax\nmin.atype         d, a, b;\nmin{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 };\nDescription\nStore the minimum of\na\nand\nb\nin\nd\n.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce\n.u16x2\n,\n.s16x2\nresult\nin destination.\nOperands\nd\n,\na\nand\nb\nhave the same type as the instruction type. For instruction types\n.u16x2\n,\n.s16x2\n, operands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] < iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a < b) ? a : b; // Integer (signed and unsigned)\n}\nNotes\nSigned and unsigned differ.\nSaturation modifier:\nmin.relu.{s16x2,\ns32}\nclamps the result to 0 if negative.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmin.u16x2\n,\nmin{.relu}.s16x2\nand\nmin.relu.s32\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nmin.u16x2\n,\nmin{.relu}.s16x2\nand\nmin.relu.s32\nrequire\nsm_90\nor higher.\nExamples\nmin.s32  r0,a,b;\n@p  min.u16  h,i,j;\n    min.s16x2.relu u,v,w;\n9.7.1.13.\nInteger Arithmetic Instructions:\nmax\n\nmax\nFind the maximum of two values.\nSyntax\nmax.atype         d, a, b;\nmax{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 };\nDescription\nStore the maximum of\na\nand\nb\nin\nd\n.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce\n.u16x2\n,\n.s16x2\nresult\nin destination.\nOperands\nd\n,\na\nand\nb\nhave the same type as the instruction type. For instruction types\n.u16x2\n,\n.s16x2\n, operands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] > iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a > b) ? a : b; // Integer (signed and unsigned)\n}\nNotes\nSigned and unsigned differ.\nSaturation modifier:\nmax.relu.{s16x2,\ns32}\nclamps the result to 0 if negative.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmax.u16x2\n,\nmax{.relu}.s16x2\nand\nmax.relu.s32\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nmax.u16x2\n,\nmax{.relu}.s16x2\nand\nmax.relu.s32\nrequire\nsm_90\nor higher.\nExamples\nmax.u32  d,a,b;\nmax.s32  q,q,0;\nmax.relu.s16x2 t,t,u;\n9.7.1.14.\nInteger Arithmetic Instructions:\npopc\n\npopc\nPopulation count.\nSyntax\npopc.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nCount the number of one bits in\na\nand place the resulting\npopulation count\nin 32-bit\ndestination register\nd\n. Operand\na\nhas the instruction type and destination\nd\nhas type\n.u32\n.\nSemantics\n.u32  d = 0;\nwhile (a != 0) {\n   if (a & 0x1)  d++;\n   a = a >> 1;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\npopc\nrequires\nsm_20\nor higher.\nExamples\npopc.b32  d, a;\npopc.b64  cnt, X;  // cnt is .u32\n9.7.1.15.\nInteger Arithmetic Instructions:\nclz\n\nclz\nCount leading zeros.\nSyntax\nclz.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nCount the number of leading zeros in\na\nstarting with the most-significant bit and place the\nresult in 32-bit destination register\nd\n. Operand\na\nhas the instruction type, and destination\nd\nhas type\n.u32\n. For\n.b32\ntype, the number of leading zeros is between 0 and 32,\ninclusively. For\n.b64\ntype, the number of leading zeros is between 0 and 64, inclusively.\nSemantics\n.u32  d = 0;\nif (.type == .b32)   { max = 32; mask = 0x80000000; }\nelse                 { max = 64; mask = 0x8000000000000000; }\n\nwhile (d < max && (a&mask == 0) ) {\n    d++;\n    a = a << 1;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nclz\nrequires\nsm_20\nor higher.\nExamples\nclz.b32  d, a;\nclz.b64  cnt, X;  // cnt is .u32\n9.7.1.16.\nInteger Arithmetic Instructions:\nbfind\n\nbfind\nFind most significant non-sign bit.\nSyntax\nbfind.type           d, a;\nbfind.shiftamt.type  d, a;\n\n.type = { .u32, .u64,\n          .s32, .s64 };\nDescription\nFind the bit position of the most significant non-sign bit in\na\nand place the result in\nd\n. Operand\na\nhas the instruction type, and destination\nd\nhas type\n.u32\n. For unsigned\nintegers,\nbfind\nreturns the bit position of the most significant\n1\n. For signed integers,\nbfind\nreturns the bit position of the most significant\n0\nfor negative inputs and the most\nsignificant\n1\nfor non-negative inputs.\nIf\n.shiftamt\nis specified,\nbfind\nreturns the shift amount needed to left-shift the found bit\ninto the most-significant bit position.\nbfind\nreturns\n0xffffffff\nif no non-sign bit is found.\nSemantics\nmsb = (.type==.u32 || .type==.s32) ? 31 : 63;\n// negate negative signed inputs\nif ( (.type==.s32 || .type==.s64) && (a & (1<<msb)) ) {\n    a = ~a;\n}\n.u32  d = 0xffffffff;\nfor (.s32 i=msb; i>=0; i--) {\n    if (a & (1<<i))  { d = i; break; }\n}\nif (.shiftamt && d != 0xffffffff)  { d = msb - d; }\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfind\nrequires\nsm_20\nor higher.\nExamples\nbfind.u32  d, a;\nbfind.shiftamt.s64  cnt, X;  // cnt is .u32\n9.7.1.17.\nInteger Arithmetic Instructions:\nfns\n\nfns\nFind the n-th set bit\nSyntax\nfns.b32 d, mask, base, offset;\nDescription\nGiven a 32-bit value\nmask\nand an integer value\nbase\n(between 0 and 31), find the n-th (given\nby offset) set bit in\nmask\nfrom the\nbase\nbit, and store the bit position in\nd\n. If not\nfound, store 0xffffffff in\nd\n.\nOperand\nmask\nhas a 32-bit type. Operand\nbase\nhas\n.b32\n,\n.u32\nor\n.s32\ntype. Operand offset has\n.s32\ntype. Destination\nd\nhas type\n.b32.\nOperand\nbase\nmust be <= 31, otherwise behavior is undefined.\nSemantics\nd = 0xffffffff;\nif (offset == 0) {\n    if (mask[base] == 1) {\n        d = base;\n    }\n} else {\n    pos = base;\n    count = |offset| - 1;\n    inc = (offset > 0) ? 1 : -1;\n\n    while ((pos >= 0) && (pos < 32)) {\n        if (mask[pos] == 1) {\n            if (count == 0) {\n              d = pos;\n              break;\n           } else {\n               count = count - 1;\n           }\n        }\n        pos = pos + inc;\n    }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nfns\nrequires\nsm_30\nor higher.\nExamples\nfns.b32 d, 0xaaaaaaaa, 3, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 3, -1;  // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, -1;  // d = 1\n9.7.1.18.\nInteger Arithmetic Instructions:\nbrev\n\nbrev\nBit reverse.\nSyntax\nbrev.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nPerform bitwise reversal of input.\nSemantics\nmsb = (.type==.b32) ? 31 : 63;\n\nfor (i=0; i<=msb; i++) {\n    d[i] = a[msb-i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbrev\nrequires\nsm_20\nor higher.\nExamples\nbrev.b32  d, a;\n9.7.1.19.\nInteger Arithmetic Instructions:\nbfe\n\nbfe\nBit Field Extract.\nSyntax\nbfe.type  d, a, b, c;\n\n.type = { .u32, .u64,\n          .s32, .s64 };\nDescription\nExtract bit field from\na\nand place the zero or sign-extended result in\nd\n. Source\nb\ngives\nthe bit field starting bit position, and source\nc\ngives the bit field length in bits.\nOperands\na\nand\nd\nhave the same type as the instruction type. Operands\nb\nand\nc\nare\ntype\n.u32\n, but are restricted to the 8-bit value range\n0..255\n.\nThe sign bit of the extracted field is defined as:\n.u32\n,\n.u64\n:\nzero\n.s32\n,\n.s64\n:\nmsb\nof input a if the extracted field extends beyond the\nmsb\nof a\nmsb\nof extracted\nfield, otherwise\nIf the bit field length is zero, the result is zero.\nThe destination\nd\nis padded with the sign bit of the extracted field. If the start position is\nbeyond the\nmsb\nof the input, the destination\nd\nis filled with the replicated sign bit of the\nextracted field.\nSemantics\nmsb = (.type==.u32 || .type==.s32) ? 31 : 63;\npos = b & 0xff;  // pos restricted to 0..255 range\nlen = c & 0xff;  // len restricted to 0..255 range\n\nif (.type==.u32 || .type==.u64 || len==0)\n    sbit = 0;\nelse\n    sbit = a[min(pos+len-1,msb)];\n\nd = 0;\nfor (i=0; i<=msb; i++) {\n    d[i] = (i<len && pos+i<=msb) ? a[pos+i] : sbit;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfe\nrequires\nsm_20\nor higher.\nExamples\nbfe.b32  d,a,start,len;\n9.7.1.20.\nInteger Arithmetic Instructions:\nbfi\n\nbfi\nBit Field Insert.\nSyntax\nbfi.type  f, a, b, c, d;\n\n.type = { .b32, .b64 };\nDescription\nAlign and insert a bit field from\na\ninto\nb\n, and place the result in\nf\n. Source\nc\ngives the starting bit position for the insertion, and source\nd\ngives the bit field length in\nbits.\nOperands\na\n,\nb\n, and\nf\nhave the same type as the instruction type. Operands\nc\nand\nd\nare type\n.u32\n, but are restricted to the 8-bit value range\n0..255\n.\nIf the bit field length is zero, the result is\nb\n.\nIf the start position is beyond the msb of the input, the result is\nb\n.\nSemantics\nmsb = (.type==.b32) ? 31 : 63;\npos = c & 0xff;  // pos restricted to 0..255 range\nlen = d & 0xff;  // len restricted to 0..255 range\n\nf = b;\nfor (i=0; i<len && pos+i<=msb; i++) {\n    f[pos+i] = a[i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfi\nrequires\nsm_20\nor higher.\nExamples\nbfi.b32  d,a,b,start,len;\n9.7.1.21.\nInteger Arithmetic Instructions:\nszext\n\nszext\nSign-extend or Zero-extend.\nSyntax\nszext.mode.type  d, a, b;\n\n.mode = { .clamp, .wrap };\n.type = { .u32, .s32 };\nDescription\nSign-extends or zero-extends an N-bit value from operand\na\nwhere N is specified in operand\nb\n. The resulting value is stored in the destination operand\nd\n.\nFor the\n.s32\ninstruction type, the value in\na\nis treated as an N-bit signed value and the\nmost significant bit of this N-bit value is replicated up to bit 31. For the\n.u32\ninstruction\ntype, the value in\na\nis treated as an N-bit unsigned number and is zero-extended to 32\nbits. Operand\nb\nis an unsigned 32-bit value.\nIf the value of N is 0, then the result of\nszext\nis 0. If the value of N is 32 or higher, then\nthe result of\nszext\ndepends upon the value of the\n.mode\nqualifier as follows:\nIf\n.mode\nis\n.clamp\n, then the result is the same as the source operand\na\n.\nIf\n.mode\nis\n.wrap\n, then the result is computed using the wrapped value of N.\nSemantics\nb1        = b & 0x1f;\ntoo_large = (b >= 32 && .mode == .clamp) ? true : false;\nmask      = too_large ? 0 : (~0) << b1;\nsign_pos  = (b1 - 1) & 0x1f;\n\nif (b1 == 0 || too_large || .type != .s32) {\n    sign_bit = false;\n} else {\n    sign_bit = (a >> sign_pos) & 1;\n}\nd = (a & ~mask) | (sign_bit ? mask | 0);\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nszext\nrequires\nsm_70\nor higher.\nExamples\nszext.clamp.s32 rd, ra, rb;\nszext.wrap.u32  rd, 0xffffffff, 0; // Result is 0.\n9.7.1.22.\nInteger Arithmetic Instructions:\nbmsk\n\nbmsk\nBit Field Mask.\nSyntax\nbmsk.mode.b32  d, a, b;\n\n.mode = { .clamp, .wrap };\nDescription\nGenerates a 32-bit mask starting from the bit position specified in operand\na\n, and of the width\nspecified in operand\nb\n. The generated bitmask is stored in the destination operand\nd\n.\nThe resulting bitmask is 0 in the following cases:\nWhen the value of\na\nis 32 or higher and\n.mode\nis\n.clamp\n.\nWhen either the specified value of\nb\nor the wrapped value of\nb\n(when\n.mode\nis\nspecified as\n.wrap\n) is 0.\nSemantics\na1    = a & 0x1f;\nmask0 = (~0) << a1;\nb1    = b & 0x1f;\nsum   = a1 + b1;\nmask1 = (~0) << sum;\n\nsum-overflow          = sum >= 32 ? true : false;\nbit-position-overflow = false;\nbit-width-overflow    = false;\n\nif (.mode == .clamp) {\n    if (a >= 32) {\n        bit-position-overflow = true;\n        mask0 = 0;\n    }\n    if (b >= 32) {\n        bit-width-overflow = true;\n    }\n}\n\nif (sum-overflow || bit-position-overflow || bit-width-overflow) {\n    mask1 = 0;\n} else if (b1 == 0) {\n    mask1 = ~0;\n}\nd = mask0 & ~mask1;\nNotes\nThe bitmask width specified by operand\nb\nis limited to range\n0..32\nin\n.clamp\nmode and to\nrange\n0..31\nin\n.wrap\nmode.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nbmsk\nrequires\nsm_70\nor higher.\nExamples\nbmsk.clamp.b32  rd, ra, rb;\nbmsk.wrap.b32   rd, 1, 2; // Creates a bitmask of 0x00000006.\n9.7.1.23.\nInteger Arithmetic Instructions:\ndp4a\n\ndp4a\nFour-way byte dot product-accumulate.\nSyntax\ndp4a.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\nDescription\nFour-way byte dot product which is accumulated in 32-bit result.\nOperand\na\nand\nb\nare 32-bit inputs which hold 4 byte inputs in packed form for dot product.\nOperand\nc\nhas type\n.u32\nif both\n.atype\nand\n.btype\nare\n.u32\nelse operand\nc\nhas type\n.s32\n.\nSemantics\nd = c;\n\n// Extract 4 bytes from a 32bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_4(a, .atype);\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nfor (i = 0; i < 4; ++i) {\n    d += Va[i] * Vb[i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\nRequires\nsm_61\nor higher.\nExamples\ndp4a.u32.u32           d0, a0, b0, c0;\ndp4a.u32.s32           d1, a1, b1, c1;\n9.7.1.24.\nInteger Arithmetic Instructions:\ndp2a\n\ndp2a\nTwo-way dot product-accumulate.\nSyntax\ndp2a.mode.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\n.mode = { .lo, .hi };\nDescription\nTwo-way 16-bit to 8-bit dot product which is accumulated in 32-bit result.\nOperand\na\nand\nb\nare 32-bit inputs. Operand\na\nholds two 16-bits inputs in packed form and\noperand\nb\nholds 4 byte inputs in packed form for dot product.\nDepending on the\n.mode\nspecified, either lower half or upper half of operand\nb\nwill be used\nfor dot product.\nOperand\nc\nhas type\n.u32\nif both\n.atype\nand\n.btype\nare\n.u32\nelse operand\nc\nhas type\n.s32\n.\nSemantics\nd = c;\n// Extract two 16-bit values from a 32-bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_2(a, .atype);\n\n// Extract four 8-bit values from a 32-bit input and sign or zer extend\n// based on input type.\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nb_select = (.mode == .lo) ? 0 : 2;\n\nfor (i = 0; i < 2; ++i) {\n    d += Va[i] * Vb[b_select + i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\nRequires\nsm_61\nor higher.\nExamples\ndp2a.lo.u32.u32           d0, a0, b0, c0;\ndp2a.hi.u32.s32           d1, a1, b1, c1;\n9.7.2.\nExtended-Precision Integer Arithmetic Instructions\n\nInstructions\nadd.cc\n,\naddc\n,\nsub.cc\n,\nsubc\n,\nmad.cc\nand\nmadc\nreference an\nimplicitly specified condition code register (\nCC\n) having a single carry flag bit (\nCC.CF\n)\nholding carry-in/carry-out or borrow-in/borrow-out. These instructions support extended-precision\ninteger addition, subtraction, and multiplication. No other instructions access the condition code,\nand there is no support for setting, clearing, or testing the condition code. The condition code\nregister is not preserved across calls and is mainly intended for use in straight-line code\nsequences for computing extended-precision integer addition, subtraction, and multiplication.\nThe extended-precision arithmetic instructions are:\nadd.cc\n,\naddc\nsub.cc\n,\nsubc\nmad.cc\n,\nmadc\n9.7.2.1.\nExtended-Precision Arithmetic Instructions:\nadd.cc\n\nadd.cc\nAdd two values with carry-out.\nSyntax\nadd.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer addition and writes the carry-out value into the condition code register.\nSemantics\nd = a + b;\ncarry-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nadd.cc\nintroduced in PTX ISA version 1.2.\n64-bit\nadd.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nadd.cc\nis supported on all target architectures.\n64-bit\nadd.cc\nrequires\nsm_20\nor higher.\nExamples\n@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4;\n9.7.2.2.\nExtended-Precision Arithmetic Instructions:\naddc\n\naddc\nAdd two values with carry-in and optional carry-out.\nSyntax\naddc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer addition with carry-in and optionally writes the carry-out value into the condition\ncode register.\nSemantics\nd = a + b + CC.CF;\nif\n.cc\nspecified, carry-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\naddc\nintroduced in PTX ISA version 1.2.\n64-bit\naddc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\naddc\nis supported on all target architectures.\n64-bit\naddc\nrequires\nsm_20\nor higher.\nExamples\n@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4;\n9.7.2.3.\nExtended-Precision Arithmetic Instructions:\nsub.cc\n\nsub.cc\nSubtract one value from another, with borrow-out.\nSyntax\nsub.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer subtraction and writes the borrow-out value into the condition code register.\nSemantics\nd = a - b;\nborrow-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nsub.cc\nintroduced in PTX ISA version 1.2.\n64-bit\nsub.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nsub.cc\nis supported on all target architectures.\n64-bit\nsub.cc\nrequires\nsm_20\nor higher.\nExamples\n@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4;\n9.7.2.4.\nExtended-Precision Arithmetic Instructions:\nsubc\n\nsubc\nSubtract one value from another, with borrow-in and optional borrow-out.\nSyntax\nsubc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer subtraction with borrow-in and optionally writes the borrow-out value into the\ncondition code register.\nSemantics\nd = a  - (b + CC.CF);\nif\n.cc\nspecified, borrow-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nsubc\nintroduced in PTX ISA version 1.2.\n64-bit\nsubc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nsubc\nis supported on all target architectures.\n64-bit\nsubc\nrequires\nsm_20\nor higher.\nExamples\n@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4;\n9.7.2.5.\nExtended-Precision Arithmetic Instructions:\nmad.cc\n\nmad.cc\nMultiply two values, extract high or low half of result, and add a third value with carry-out.\nSyntax\nmad{.hi,.lo}.cc.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nMultiplies two values, extracts either the high or low part of the result, and adds a third\nvalue. Writes the result to the destination register and the carry-out from the addition into the\ncondition code register.\nSemantics\nt = a * b;\nd = t<63..32> + c;    // for .hi variant\nd = t<31..0> + c;     // for .lo variant\ncarry-out from addition is written to\nCC.CF\nNotes\nGenerally used in combination with\nmadc\nand\naddc\nto implement extended-precision multi-word\nmultiplication. See\nmadc\nfor an example.\nPTX ISA Notes\n32-bit\nmad.cc\nintroduced in PTX ISA version 3.0.\n64-bit\nmad.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nRequires target\nsm_20\nor higher.\nExamples\n@p  mad.lo.cc.u32 d,a,b,c;\n    mad.lo.cc.u32 r,p,q,r;\n9.7.2.6.\nExtended-Precision Arithmetic Instructions:\nmadc\n\nmadc\nMultiply two values, extract high or low half of result, and add a third value with carry-in and\noptional carry-out.\nSyntax\nmadc{.hi,.lo}{.cc}.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nMultiplies two values, extracts either the high or low part of the result, and adds a third value\nalong with carry-in. Writes the result to the destination register and optionally writes the\ncarry-out from the addition into the condition code register.\nSemantics\nt = a * b;\nd = t<63..32> + c + CC.CF;     // for .hi variant\nd = t<31..0> + c + CC.CF;      // for .lo variant\nif\n.cc\nspecified, carry-out from addition is written to\nCC.CF\nNotes\nGenerally used in combination with\nmad.cc\nand\naddc\nto implement extended-precision\nmulti-word multiplication. See example below.\nPTX ISA Notes\n32-bit\nmadc\nintroduced in PTX ISA version 3.0.\n64-bit\nmadc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nRequires target\nsm_20\nor higher.\nExamples\n// extended-precision multiply:  [r3,r2,r1,r0] = [r5,r4] * [r7,r6]\nmul.lo.u32     r0,r4,r6;      // r0=(r4*r6).[31:0], no carry-out\nmul.hi.u32     r1,r4,r6;      // r1=(r4*r6).[63:32], no carry-out\nmad.lo.cc.u32  r1,r5,r6,r1;   // r1+=(r5*r6).[31:0], may carry-out\nmadc.hi.u32    r2,r5,r6,0;    // r2 =(r5*r6).[63:32]+carry-in,\n                              // no carry-out\nmad.lo.cc.u32   r1,r4,r7,r1;  // r1+=(r4*r7).[31:0], may carry-out\nmadc.hi.cc.u32  r2,r4,r7,r2;  // r2+=(r4*r7).[63:32]+carry-in,\n                              // may carry-out\naddc.u32        r3,0,0;       // r3 = carry-in, no carry-out\nmad.lo.cc.u32   r2,r5,r7,r2;  // r2+=(r5*r7).[31:0], may carry-out\nmadc.hi.u32     r3,r5,r7,r3;  // r3+=(r5*r7).[63:32]+carry-in\n9.7.3.\nFloating-Point Instructions\n\nFloating-point instructions operate on\n.f32\nand\n.f64\nregister operands and constant\nimmediate values. The floating-point instructions are:\ntestp\ncopysign\nadd\nsub\nmul\nfma\nmad\ndiv\nabs\nneg\nmin\nmax\nrcp\nsqrt\nrsqrt\nsin\ncos\nlg2\nex2\ntanh\nInstructions that support rounding modifiers are IEEE-754 compliant. Double-precision instructions\nsupport subnormal inputs and results. Single-precision instructions support subnormal inputs and\nresults by default for\nsm_20\nand subsequent targets, and flush subnormal inputs and results to\nsign-preserving zero for\nsm_1x\ntargets. The optional\n.ftz\nmodifier on single-precision\ninstructions provides backward compatibility with\nsm_1x\ntargets by flushing subnormal inputs and\nresults to sign-preserving zero regardless of the target architecture.\nSingle-precision\nadd\n,\nsub\n,\nmul\n, and\nmad\nsupport saturation of results to the range\n[0.0, 1.0], with\nNaN\ns being flushed to positive zero.\nNaN\npayloads are supported for\ndouble-precision instructions (except for\nrcp.approx.ftz.f64\nand\nrsqrt.approx.ftz.f64\n, which\nmaps input\nNaN\ns to a canonical\nNaN\n). Single-precision instructions return an unspecified\nNaN\n. Note that future implementations may support\nNaN\npayloads for single-precision\ninstructions, so PTX programs should not rely on the specific single-precision\nNaN\ns being\ngenerated.\nTable 29\nsummarizes\nfloating-point instructions in PTX.\nTable 29\nSummary of Floating-Point Instructions\n\nInstruction\n.rn\n.rz\n.rm\n.rp\n.ftz\n.sat\nNotes\n{add,sub,mul}.rnd.f32\nx\nx\nx\nx\nx\nx\nIf no rounding modifier is specified,\ndefault is\n.rn\nand instructions may\nbe folded into a multiply-add.\n{add,sub,mul}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\nIf no rounding modifier is specified,\ndefault is\n.rn\nand instructions may\nbe folded into a multiply-add.\nmad.f32\nn/a\nn/a\nn/a\nn/a\nx\nx\n.target\nsm_1x\nNo rounding modifier.\n{mad,fma}.rnd.f32\nx\nx\nx\nx\nx\nx\n.target\nsm_20\nor higher\nmad.f32\nand\nfma.f32\nare the same.\n{mad,fma}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\nmad.f64\nand\nfma.f64\nare the same.\ndiv.full.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nNo rounding modifier.\n{div,rcp,sqrt}.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nn/a\nrcp.approx.ftz.f64\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n.target\nsm_20\nor higher\n{div,rcp,sqrt}.rnd.f32\nx\nx\nx\nx\nx\nn/a\n.target\nsm_20\nor higher\n{div,rcp,sqrt}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\n.target\nsm_20\nor higher\n{abs,neg,min,max}.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n{abs,neg,min,max}.f64\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nrsqrt.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nrsqrt.approx.f64\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nrsqrt.approx.ftz.f64\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n.target\nsm_20\nor higher\n{sin,cos,lg2,ex2}.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\ntanh.approx.f32\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\n.target\nsm_75\nor higher\n9.7.3.1.\nFloating Point Instructions:\ntestp\n\ntestp\nTest floating-point property.\nSyntax\ntestp.op.type  p, a;  // result is .pred\n\n.op   = { .finite, .infinite,\n          .number, .notanumber,\n          .normal, .subnormal };\n.type = { .f32, .f64 };\nDescription\ntestp\ntests common properties of floating-point numbers and returns a predicate value of\n1\nif\nTrue\nand\n0\nif\nFalse\n.\ntestp.finite\nTrue\nif the input is not infinite or\nNaN\ntestp.infinite\nTrue\nif the input is positive or negative infinity\ntestp.number\nTrue\nif the input is not\nNaN\ntestp.notanumber\nTrue\nif the input is\nNaN\ntestp.normal\nTrue\nif the input is a normal number (not\nNaN\n, not infinity)\ntestp.subnormal\nTrue\nif the input is a subnormal number (not\nNaN\n, not infinity)\nAs a special case, positive and negative zero are considered normal numbers.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ntestp.notanumber.f32  isnan, f0;\ntestp.infinite.f64    p, X;\n9.7.3.2.\nFloating Point Instructions:\ncopysign\n\ncopysign\nCopy sign of one input to another.\nSyntax\ncopysign.type  d, a, b;\n\n.type = { .f32, .f64 };\nDescription\nCopy sign bit of\na\ninto value of\nb\n, and return the result as\nd\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ncopysign.f32  x, y, z;\ncopysign.f64  A, B, C;\n9.7.3.3.\nFloating Point Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd{.rnd}{.ftz}{.sat}.f32  d, a, b;\nadd{.rnd}{.ftz}.f32x2      d, a, b;\nadd{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values from\nsource operands. Single precision (\n.f32\n) operands are then added in parallel to produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a + b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] + fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nadd.ftz.f32\n,\nadd.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nadd.f64\nsupports subnormal numbers.\nadd.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nadd.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nadd.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nadd.f32\nsupported on all target architectures.\nadd.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nadd.f64\n, requires\nsm_13\nor higher.\nfor\nadd.f32\n, requires\nsm_20\nor higher.\nadd.f32x2\nrequires\nsm_100\nor higher.\nExamples\n@p  add.rz.ftz.f32  f1,f2,f3;\nadd.rp.ftz.f32x2    d, a, b;\n9.7.3.4.\nFloating Point Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub{.rnd}{.ftz}{.sat}.f32  d, a, b;\nsub{.rnd}{.ftz}.f32x2      d, a, b;\nsub{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values\nfrom source operands. Single precision (\n.f32\n) operands are then subtracted in parallel\nto produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a - b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] - fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that a\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsub.ftz.f32\n,\nsub.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nsub.f64\nsupports subnormal numbers.\nsub.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nsub.sat.f32\nclamps the result to [0.0, 1.0]. NaN results are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nsub.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nsub.f32\nsupported on all target architectures.\nsub.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nsub.f64\n, requires\nsm_13\nor higher.\nfor\nsub.f32\n, requires\nsm_20\nor higher.\nsub.f32x2\nrequires\nsm_100\nor higher.\nExamples\nsub.f32 c,a,b;\nsub.rn.ftz.f32  f1,f2,f3;\n9.7.3.5.\nFloating Point Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul{.rnd}{.ftz}{.sat}.f32  d, a, b;\nmul{.rnd}{.ftz}.f32x2      d, a, b;\nmul{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute the product of two values.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values\nfrom source operands. Single precision (\n.f32\n) operands are then multiplied in parallel\nto produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a * b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] * fB[i];\n    }\n}\nNotes\nFor floating-point multiplication, all operands must be the same size.\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that a\nmul\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nmul\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul/add\nand\nmul/sub\nsequences with no rounding modifiers may be\noptimized to use fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmul.ftz.f32\n,\nmul.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmul.f64\nsupports subnormal numbers.\nmul.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmul.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmul.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nmul.f32\nsupported on all target architectures.\nmul.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nmul.f64\n, requires\nsm_13\nor higher.\nfor\nmul.f32\n, requires\nsm_20\nor higher.\nmul.f32x2\nrequires\nsm_100\nor higher.\nExamples\nmul.ftz.f32 circumf,radius,pi  // a single-precision multiply\n9.7.3.6.\nFloating Point Instructions:\nfma\n\nfma\nFused multiply-add.\nSyntax\nfma.rnd{.ftz}{.sat}.f32  d, a, b, c;\nfma.rnd{.ftz}.f32x2      d, a, b, c;\nfma.rnd.f64              d, a, b, c;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms a fused multiply-add with no loss of precision in the intermediate product and addition.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values from\nsource operands. Single precision (\n.f32\n) operands are then operated in parallel to produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a * b + c;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    fC[0] = c[0:31];\n    fC[1] = c[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] * fB[i] + fC[i];\n    }\n}\nNotes\nfma.f32\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by\n.rnd\n.\nfma.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n.\nfma.f64\nis the same as\nmad.f64\n.\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nfma.ftz.f32\n,\nfma.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nfma.f64\nsupports subnormal numbers.\nfma.f32\nis unimplemented for\nsm_1x\ntargets.\nSaturation:\nfma.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nfma.f64\nintroduced in PTX ISA version 1.4.\nfma.f32\nintroduced in PTX ISA version 2.0.\nfma.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nfma.f32\nrequires\nsm_20\nor higher.\nfma.f64\nrequires\nsm_13\nor higher.\nfma.f32x2\nrequires\nsm_100\nor higher.\nExamples\nfma.rn.ftz.f32  w,x,y,z;\n@p  fma.rn.f64      d,a,b,c;\n    fma.rp.ftz.f32x2 p,q,r,s;\n9.7.3.7.\nFloating Point Instructions:\nmad\n\nmad\nMultiply two values and add a third value.\nSyntax\nmad{.ftz}{.sat}.f32      d, a, b, c;    // .target sm_1x\nmad.rnd{.ftz}{.sat}.f32  d, a, b, c;    // .target sm_20\nmad.rnd.f64              d, a, b, c;    // .target sm_13 and higher\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nMultiplies two values and adds a third, and then writes the resulting value into a destination\nregister.\nSemantics\nd = a*b + c;\nNotes\nFor\n.target\nsm_20\nand higher:\nmad.f32\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by\n.rnd\n.\nmad.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n.\nmad.{f32,f64}\nis the same as\nfma.{f32,f64}\n.\nFor\n.target\nsm_1x\n:\nmad.f32\ncomputes the product of\na\nand\nb\nat double precision, and then the mantissa is\ntruncated to 23 bits, but the exponent is preserved. Note that this is different from computing\nthe product with\nmul\n, where the mantissa can be rounded and the exponent will be clamped. The\nexception for\nmad.f32\nis when\nc\n=\n+/-0.0\n,\nmad.f32\nis identical to the result computed\nusing separate mul and add instructions. When JIT-compiled for SM 2.0 devices,\nmad.f32\nis\nimplemented as a fused multiply-add (i.e.,\nfma.rn.ftz.f32\n). In this case,\nmad.f32\ncan\nproduce slightly different numeric results and backward compatibility is not guaranteed in this\ncase.\nmad.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n. Unlike\nmad.f32\n, the treatment of subnormal\ninputs and output follows IEEE 754 standard.\nmad.f64\nis the same as\nfma.f64\n.\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmad.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmad.f64\nsupports subnormal numbers.\nmad.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmad.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nIn PTX ISA versions 1.4 and later, a rounding modifier is required for\nmad.f64\n.\nLegacy\nmad.f64\ninstructions having no rounding modifier will map to\nmad.rn.f64\n.\nIn PTX ISA versions 2.0 and later, a rounding modifier is required for\nmad.f32\nfor\nsm_20\nand higher targets.\nErrata\nmad.f32\nrequires a rounding modifier for\nsm_20\nand higher targets. However for PTX ISA\nversion 3.0 and earlier, ptxas does not enforce this requirement and\nmad.f32\nsilently defaults\nto\nmad.rn.f32\n. For PTX ISA version 3.1, ptxas generates a warning and defaults to\nmad.rn.f32\n, and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later.\nTarget ISA Notes\nmad.f32\nsupported on all target architectures.\nmad.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\n,\n.rm\n,\n.rp\nfor\nmad.f64\n, requires\nsm_13\nor higher.\n.rn\n,\n.rz\n,\n.rm\n,\n.rp\nfor\nmad.f32\n, requires\nsm_20\nor higher.\nExamples\n@p  mad.f32  d,a,b,c;\n9.7.3.8.\nFloating Point Instructions:\ndiv\n\ndiv\nDivide one value by another.\nSyntax\ndiv.approx{.ftz}.f32  d, a, b;  // fast, approximate divide\ndiv.full{.ftz}.f32    d, a, b;  // full-range approximate divide\ndiv.rnd{.ftz}.f32     d, a, b;  // IEEE 754 compliant rounding\ndiv.rnd.f64           d, a, b;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nDivides\na\nby\nb\n, stores result in\nd\n.\nSemantics\nd = a / b;\nNotes\nFast, approximate single-precision divides:\ndiv.approx.f32\nimplements a fast approximation to divide, computed as\nd\n=\na\n*\n(1/b)\n. For\n|b|\nin [2\n-126\n, 2\n126\n], the maximum\nulp\nerror is 2. For 2\n126\n<\n|b|\n< 2\n128\n, if\na\nis infinity,\ndiv.approx.f32\nreturns\nNaN\n, otherwise it\nreturns a sign-preserving zero.\ndiv.full.f32\nimplements a relatively fast, full-range approximation that scales operands to\nachieve better accuracy, but is not fully IEEE 754 compliant and does not support rounding\nmodifiers. The maximum\nulp\nerror is 2 across the full range of inputs.\nDivide with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\ndiv.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\ndiv.f64\nsupports subnormal numbers.\ndiv.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\ndiv.f32\nand\ndiv.f64\nintroduced in PTX ISA version 1.0.\nExplicit modifiers\n.approx\n,\n.full\n,\n.ftz\n, and rounding introduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, one of\n.approx\n,\n.full\n, or\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\ndiv.f32\ndefaults to\ndiv.approx.ftz.f32\n, and\ndiv.f64\ndefaults to\ndiv.rn.f64\n.\nTarget ISA Notes\ndiv.approx.f32\nand\ndiv.full.f32\nsupported on all target architectures.\ndiv.rnd.f32\nrequires\nsm_20\nor higher.\ndiv.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32\n.\ndiv.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\ndiv.approx.ftz.f32  diam,circum,3.14159;\ndiv.full.ftz.f32    x, y, z;\ndiv.rn.f64          xd, yd, zd;\n9.7.3.9.\nFloating Point Instructions:\nabs\n\nabs\nAbsolute value.\nSyntax\nabs{.ftz}.f32  d, a;\nabs.f64        d, a;\nDescription\nTake the absolute value of\na\nand store the result in\nd\n.\nSemantics\nd = |a|;\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nabs.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nabs.f64\nsupports subnormal numbers.\nabs.f32\nflushes subnormal inputs and results to sign-preserving zero.\nFor\nabs.f32\n,\nNaN\ninput yields unspecified\nNaN\n. For\nabs.f64\n,\nNaN\ninput is passed\nthrough unchanged. Future implementations may comply with the IEEE 754 standard by preserving\npayload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nabs.f32\nsupported on all target architectures.\nabs.f64\nrequires\nsm_13\nor higher.\nExamples\nabs.ftz.f32  x,f0;\n9.7.3.10.\nFloating Point Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg{.ftz}.f32  d, a;\nneg.f64        d, a;\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nSemantics\nd = -a;\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nneg.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nneg.f64\nsupports subnormal numbers.\nneg.f32\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nneg.f32\nsupported on all target architectures.\nneg.f64\nrequires\nsm_13\nor higher.\nExamples\nneg.ftz.f32  x,f0;\n9.7.3.11.\nFloating Point Instructions:\nmin\n\nmin\nFind the minimum of given values.\nSyntax\nmin{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmin{.ftz}{.NaN}{.abs}.f32          d, a, b, c;\nmin.f64                            d, a, b;\nDescription\nStore the minimum of\na\n,\nb\nand optionally\nc\nin\nd\n.\nIf\n.NaN\nmodifier is specified, then the result is canonical\nNaN\nif any of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the minimum of\nabsolute values of both input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both inputs\na\nand\nb\n. The\n.xorsign\nqualifier cannot be specified for three\ninputs operation.\nQualifier\n.xorsign\nrequires qualifier\n.abs\nto be specified. In such cases,\n.xorsign\nconsiders the sign bit of both inputs before applying\n.abs\noperation.\nIf the result of\nmin\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\ndef min_num (z, x, y) {\n    if (isNaN(x) && isNaN(y))\n        z = NaN;\n    else if (isNaN(x))\n        z = y;\n    else if (isNaN(y))\n        z = x;\n    else\n        // note: -0.0 < +0.0 here\n        z = (x < y) ? x : y;\n    return z;\n}\n\ndef min_nan (z, x, y) {\n    if (isNaN(x) || isNaN(y))\n        z = NaN;\n    else\n        // note: -0.0 < +0.0 here\n        z = (x < y) ? x : y;\n    return z;\n}\n\ndef two_inputs_min (z, x, y) {\n    if (.NaN)\n        z = min_nan(z, x, y);\n    else\n        z = min_num(z, x, y);\n    return z;\n}\n\nif (.xorsign && !isPresent(c)) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n}\nif (.abs) {\n    a = |a|;\n    b = |b|;\n    if (isPresent(c)) {\n        c = |c|;\n    }\n}\n\nd = two_inputs_min(d, a, b)\nif (isPresent(c)) {\n    d = two_inputs_min(d, d, c)\n}\nif (.xorsign && !isPresent(c) && !isNaN(d)) {\n    setSignBit(d, xorsign);\n}\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmin.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmin.f64\nsupports subnormal numbers.\nmin.f32\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmin.NaN\nintroduced in PTX ISA version 7.0.\nmin.xorsign.abs\nintroduced in PTX ISA version 7.2.\nmin\nwith three input arguments introduced in PTX ISA version 8.8.\nTarget ISA Notes\nmin.f32\nsupported on all target architectures.\nmin.f64\nrequires\nsm_13\nor higher.\nmin.NaN\nrequires\nsm_80\nor higher.\nmin.xorsign.abs\nrequires\nsm_86\nor higher.\nmin\nwith three input arguments requires\nsm_100\nor higher.\nExamples\n@p  min.ftz.f32  z,z,x;\n    min.f64      a,b,c;\n    // fp32 min with .NaN\n    min.NaN.f32  f0,f1,f2;\n    // fp32 min with .xorsign.abs\n    min.xorsign.abs.f32 Rd, Ra, Rb;\n9.7.3.12.\nFloating Point Instructions:\nmax\n\nmax\nFind the maximum of given values.\nSyntax\nmax{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmax{.ftz}{.NaN}{.abs}.f32          d, a, b, c;\nmax.f64                            d, a, b;\nDescription\nStore the maximum of\na\n,\nb\nand optionally\nc\nin\nd\n.\nIf\n.NaN\nmodifier is specified, the result is canonical\nNaN\nif any of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the maximum of\nabsolute values of the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of the inputs:\na\nand\nb\n. The\n.xorsign\nqualifier cannot be specified for three\ninputs operation.\nQualifier\n.xorsign\nrequires qualifier\n.abs\nto be specified. In such cases,\n.xorsign\nconsiders the sign bit of both inputs before applying\n.abs\noperation.\nIf the result of\nmax\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\ndef max_num (z, x, y) {\n    if (isNaN(x) && isNaN(y))\n        z = NaN;\n    else if (isNaN(x))\n        z = y;\n    else if (isNaN(y))\n        z = x;\n    else\n        // note: +0.0 > -0.0 here\n        z = (x > y) ? x : y;\n    return z;\n}\n\ndef max_nan (z, x, y) {\n    if (isNaN(x) || isNaN(y))\n        z = NaN;\n    else\n        // note: +0.0 > -0.0 here\n        z = (x > y) ? x : y;\n    return z;\n}\n\ndef two_inputs_max (z, x, y) {\n    if (.NaN)\n        z = max_nan(z, x, y);\n    else\n        z = max_num(z, x, y);\n    return z;\n}\n\nif (.xorsign && !isPresent(c)) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n}\nif (.abs) {\n    a = |a|;\n    b = |b|;\n    if (isPresent(c)) {\n        c = |c|;\n    }\n}\n\nd = two_inputs_max (d, a, b)\nif (isPresent(c)) {\n    d = two_inputs_max (d, d, c)\n}\n\nif (.xorsign && !isPresent(c) !isNaN(d)) {\n    setSignBit(d, xorsign);\n}\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmax.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmax.f64\nsupports subnormal numbers.\nmax.f32\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmax.NaN\nintroduced in PTX ISA version 7.0.\nmax.xorsign.abs\nintroduced in PTX ISA version 7.2.\nmax\nwith three input arguments introduced in PTX ISA version 8.8.\nTarget ISA Notes\nmax.f32\nsupported on all target architectures.\nmax.f64\nrequires\nsm_13\nor higher.\nmax.NaN\nrequires\nsm_80\nor higher.\nmax.xorsign.abs\nrequires\nsm_86\nor higher.\nmax\nwith three input arguments requires\nsm_100\nor higher.\nExamples\nmax.ftz.f32  f0,f1,f2;\nmax.f64      a,b,c;\n// fp32 max with .NaN\nmax.NaN.f32  f0,f1,f2;\n// fp32 max with .xorsign.abs\nmax.xorsign.abs.f32 Rd, Ra, Rb;\n9.7.3.13.\nFloating Point Instructions:\nrcp\n\nrcp\nTake the reciprocal of a value.\nSyntax\nrcp.approx{.ftz}.f32  d, a;  // fast, approximate reciprocal\nrcp.rnd{.ftz}.f32     d, a;  // IEEE 754 compliant rounding\nrcp.rnd.f64           d, a;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute\n1/a\n, store result in\nd\n.\nSemantics\nd = 1 / a;\nNotes\nFast, approximate single-precision reciprocal:\nrcp.approx.f32\nimplements a fast approximation to reciprocal.\nThe maximum ulp error is 1 across the full range of inputs.\nInput\nResult\n-Inf\n-0.0\n-0.0\n-Inf\n+0.0\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nReciprocal with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nrcp.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nrcp.f64\nsupports subnormal numbers.\nrcp.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nrcp.f32\nand\nrcp.f64\nintroduced in PTX ISA version 1.0.\nrcp.rn.f64\nand explicit modifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4. General rounding modifiers were\nadded in PTX ISA version 2.0.\nFor PTX ISA version 1.4 and later, one of\n.approx\nor\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\nrcp.f32\ndefaults to\nrcp.approx.ftz.f32\n, and\nrcp.f64\ndefaults to\nrcp.rn.f64\n.\nTarget ISA Notes\nrcp.approx.f32\nsupported on all target architectures.\nrcp.rnd.f32\nrequires\nsm_20\nor higher.\nrcp.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32.\nrcp.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\nrcp.approx.ftz.f32  ri,r;\nrcp.rn.ftz.f32      xi,x;\nrcp.rn.f64          xi,x;\n9.7.3.14.\nFloating Point Instructions:\nrcp.approx.ftz.f64\n\nrcp.approx.ftz.f64\nCompute a fast, gross approximation to the reciprocal of a value.\nSyntax\nrcp.approx.ftz.f64  d, a;\nDescription\nCompute a fast, gross approximation to the reciprocal as follows:\nextract the most-significant 32 bits of\n.f64\noperand\na\nin 1.11.20 IEEE floating-point\nformat (i.e., ignore the least-significant 32 bits of\na\n),\ncompute an approximate\n.f64\nreciprocal of this value using the most-significant 20 bits of\nthe mantissa of operand\na\n,\nplace the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits\nof destination\nd\n,and\nzero the least significant 32 mantissa bits of\n.f64\ndestination\nd\n.\nSemantics\ntmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / tmp;\nd[31:0] = 0x00000000;\nNotes\nrcp.approx.ftz.f64\nimplements a fast, gross approximation to reciprocal.\nInput a[63:32]\nResult d[63:32]\n-Inf\n-0.0\n-subnormal\n-Inf\n-0.0\n-Inf\n+0.0\n+Inf\n+subnormal\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nInput\nNaN\ns map to a canonical\nNaN\nwith encoding\n0x7fffffff00000000\n.\nSubnormal inputs and results are flushed to sign-preserving zero.\nPTX ISA Notes\nrcp.approx.ftz.f64\nintroduced in PTX ISA version 2.1.\nTarget ISA Notes\nrcp.approx.ftz.f64\nrequires\nsm_20\nor higher.\nExamples\nrcp.approx.ftz.f64  xi,x;\n9.7.3.15.\nFloating Point Instructions:\nsqrt\n\nsqrt\nTake the square root of a value.\nSyntax\nsqrt.approx{.ftz}.f32  d, a; // fast, approximate square root\nsqrt.rnd{.ftz}.f32     d, a; // IEEE 754 compliant rounding\nsqrt.rnd.f64           d, a; // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute sqrt(\na\n) and store the result in\nd\n.\nSemantics\nd = sqrt(a);\nNotes\nsqrt.approx.f32\nimplements a fast approximation to square root.\nThe maximum relative error over the entire positive finite floating-point\nrange is 2\n-23\n.\nFor various corner-case inputs, results of\nsqrt\ninstruction are shown\nin below table:\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n+Inf\nNaN\nNaN\nSquare root with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsqrt.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nsqrt.f64\nsupports subnormal numbers.\nsqrt.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nsqrt.f32\nand\nsqrt.f64\nintroduced in PTX ISA version 1.0.\nsqrt.rn.f64\nand explicit\nmodifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4. General rounding\nmodifiers were added in PTX ISA version 2.0.\nFor PTX ISA version 1.4 and later, one of\n.approx\nor\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\nsqrt.f32\ndefaults to\nsqrt.approx.ftz.f32\n, and\nsqrt.f64\ndefaults to\nsqrt.rn.f64\n.\nTarget ISA Notes\nsqrt.approx.f32\nsupported on all target architectures.\nsqrt.rnd.f32\nrequires\nsm_20\nor higher.\nsqrt.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32\n.\nsqrt.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\nsqrt.approx.ftz.f32  r,x;\nsqrt.rn.ftz.f32      r,x;\nsqrt.rn.f64          r,x;\n9.7.3.16.\nFloating Point Instructions:\nrsqrt\n\nrsqrt\nTake the reciprocal of the square root of a value.\nSyntax\nrsqrt.approx{.ftz}.f32  d, a;\nrsqrt.approx.f64        d, a;\nDescription\nCompute\n1/sqrt(a)\nand store the result in\nd\n.\nSemantics\nd = 1/sqrt(a);\nNotes\nrsqrt.approx\nimplements an approximation to the reciprocal square root.\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-Inf\n+0.0\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nThe maximum relative error for\nrsqrt.f32\nover the entire positive\nfinite floating-point range is 2\n-22.9\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nrsqrt.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nrsqrt.f64\nsupports subnormal numbers.\nrsqrt.f32\nflushes subnormal inputs and results to sign-preserving zero.\nNote that\nrsqrt.approx.f64\nis emulated in software and are relatively slow.\nPTX ISA Notes\nrsqrt.f32\nand\nrsqrt.f64\nwere introduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nrsqrt.f32\ndefaults to\nrsqrt.approx.ftz.f32\n, and\nrsqrt.f64\ndefaults to\nrsqrt.approx.f64\n.\nTarget ISA Notes\nrsqrt.f32\nsupported on all target architectures.\nrsqrt.f64\nrequires\nsm_13\nor higher.\nExamples\nrsqrt.approx.ftz.f32  isr, x;\nrsqrt.approx.f64      ISR, X;\n9.7.3.17.\nFloating Point Instructions:\nrsqrt.approx.ftz.f64\n\nrsqrt.approx.ftz.f64\nCompute an approximation of the square root reciprocal of a value.\nSyntax\nrsqrt.approx.ftz.f64 d, a;\nDescription\nCompute a double-precision (\n.f64\n) approximation of the square root reciprocal of a value. The\nleast significant 32 bits of the double-precision (\n.f64\n) destination\nd\nare all zeros.\nSemantics\ntmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / sqrt(tmp);\nd[31:0] = 0x00000000;\nNotes\nrsqrt.approx.ftz.f64\nimplements a fast approximation of the square root reciprocal of a value.\nInput\nResult\n-Inf\nNaN\n-subnormal\n-Inf\n-0.0\n-Inf\n+0.0\n+Inf\n+subnormal\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nInput\nNaN\ns map to a canonical\nNaN\nwith encoding\n0x7fffffff00000000\n.\nSubnormal inputs and results are flushed to sign-preserving zero.\nPTX ISA Notes\nrsqrt.approx.ftz.f64\nintroduced in PTX ISA version 4.0.\nTarget ISA Notes\nrsqrt.approx.ftz.f64\nrequires\nsm_20\nor higher.\nExamples\nrsqrt.approx.ftz.f64 xi,x;\n9.7.3.18.\nFloating Point Instructions:\nsin\n\nsin\nFind the sine of a value.\nSyntax\nsin.approx{.ftz}.f32  d, a;\nDescription\nFind the sine of the angle\na\n(in radians).\nSemantics\nd = sin(a);\nNotes\nsin.approx.f32\nimplements a fast approximation to sine.\nInput\nResult\n-Inf\nNaN\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\nNaN\nNaN\nNaN\nThe maximum absolute error over input range is as follows:\nRange\n[-2pi .. 2pi]\n[-100pi .. +100pi]\nError\n2\n-20.5\n2\n-14.7\nOutside of the range [-100pi .. +100pi], only best effort\nis provided. There are no defined error guarantees.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsin.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nsin.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the .approx modifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nsin.f32\ndefaults to\nsin.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsin.approx.ftz.f32  sa, a;\n9.7.3.19.\nFloating Point Instructions:\ncos\n\ncos\nFind the cosine of a value.\nSyntax\ncos.approx{.ftz}.f32  d, a;\nDescription\nFind the cosine of the angle\na\n(in radians).\nSemantics\nd = cos(a);\nNotes\ncos.approx.f32\nimplements a fast approximation to cosine.\nInput\nResult\n-Inf\nNaN\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\nNaN\nNaN\nNaN\nThe maximum absolute error over input range is as follows:\nRange\n[-2pi .. 2pi]\n[-100pi .. +100pi]\nError\n2\n-20.5\n2\n-14.7\nOutside of the range [-100pi .. +100pi], only best effort\nis provided. There are no defined error guarantees.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\ncos.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\ncos.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\ncos.f32\ndefaults to\ncos.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ncos.approx.ftz.f32  ca, a;\n9.7.3.20.\nFloating Point Instructions:\nlg2\n\nlg2\nFind the base-2 logarithm of a value.\nSyntax\nlg2.approx{.ftz}.f32  d, a;\nDescription\nDetermine the log\n2\nof\na\n.\nSemantics\nd = log(a) / log(2);\nNotes\nlg2.approx.f32\nimplements a fast approximation to log\n2\n(a).\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-Inf\n+0.0\n-Inf\n+Inf\n+Inf\nNaN\nNaN\nThe maximum absolute error is 2\n-22\nwhen the input operand is in the\nrange (0.5, 2). For positive finite inputs outside of this interval, maximum\nrelative error is 2\n-22\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nlg2.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nlg2.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nlg2.f32\ndefaults to\nlg2.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nlg2.approx.ftz.f32  la, a;\n9.7.3.21.\nFloating Point Instructions:\nex2\n\nex2\nFind the base-2 exponential of a value.\nSyntax\nex2.approx{.ftz}.f32  d, a;\nDescription\nRaise 2 to the power\na\n.\nSemantics\nd = 2 ^ a;\nNotes\nex2.approx.f32\nimplements a fast approximation to 2\na\n.\nInput\nResult\n-Inf\n+0.0\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nThe maximum ulp error is 2 ulp from correctly rounded result across the\nfull range of inputs.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nex2.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nex2.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nex2.f32\ndefaults to\nex2.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nex2.approx.ftz.f32  xa, a;\n9.7.3.22.\nFloating Point Instructions:\ntanh\n\ntanh\nFind the hyperbolic tangent of a value (in radians)\nSyntax\ntanh.approx.f32 d, a;\nDescription\nTake hyperbolic tangent value of\na\n.\nThe operands\nd\nand\na\nare of type\n.f32\n.\nSemantics\nd = tanh(a);\nNotes\ntanh.approx.f32\nimplements a fast approximation to FP32 hyperbolic-tangent.\nResults of\ntanh\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n-1.0\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n1.0\nNaN\nNaN\nThe maximum relative error over the entire floating point\nrange is 2\n-11\n.\nThe subnormal numbers are supported.\nNote\nThe subnormal inputs gets passed through to the output since the value of\ntanh(x)\nfor small\nvalues of\nx\nis approximately the same as\nx\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\ntanh.approx.f32 ta, a;\n9.7.4.\nHalf Precision Floating-Point Instructions\n\nHalf precision floating-point instructions operate on\n.f16\nand\n.f16x2\nregister operands. The\nhalf precision floating-point instructions are:\nadd\nsub\nmul\nfma\nneg\nabs\nmin\nmax\ntanh\nex2\nHalf-precision\nadd\n,\nsub\n,\nmul\n, and\nfma\nsupport saturation of results to the range\n[0.0, 1.0], with\nNaN\ns being flushed to positive zero. Half-precision instructions return an\nunspecified\nNaN\n.\n9.7.4.1.\nHalf Precision Floating Point Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd{.rnd}{.ftz}{.sat}.f16   d, a, b;\nadd{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nadd{.rnd}.bf16   d, a, b;\nadd{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce\n.f16x2\nor\n.bf16x2\nresult\nin destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a + b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] + fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nadd.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nadd.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nadd{.rnd}.bf16\nand\nadd{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nadd{.rnd}.bf16\nand\nadd{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 additions\nadd.f16        d0, a0, b0;\nadd.rn.f16     d1, a1, b1;\nadd.bf16       bd0, ba0, bb0;\nadd.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 addition\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nadd.f16x2  p3, p1, p2;   // SIMD f16x2 addition\n\n// SIMD bf16 addition\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nadd.bf16x2  p6, p4, p5;       // SIMD bf16x2 addition\n\n// SIMD fp16 addition\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nadd.f16x2       f2, f0, f1;     // SIMD f16x2 addition\n\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nadd.bf16x2      f5, f3, f4;      // SIMD bf16x2 addition\n9.7.4.2.\nHalf Precision Floating Point Instructions:\nsub\n\nsub\nSubtract two values.\nSyntax\nsub{.rnd}{.ftz}{.sat}.f16   d, a, b;\nsub{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nsub{.rnd}.bf16   d, a, b;\nsub{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then subtracted in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a - b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] - fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that a\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nsub.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nsub.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nsub{.rnd}.bf16\nand\nsub{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nsub{.rnd}.bf16\nand\nsub{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 subtractions\nsub.f16        d0, a0, b0;\nsub.rn.f16     d1, a1, b1;\nsub.bf16       bd0, ba0, bb0;\nsub.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 subtraction\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nsub.f16x2  p3, p1, p2;   // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nsub.bf16x2  p6, p4, p5;       // SIMD bf16x2 subtraction\n\n// SIMD fp16 subtraction\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nsub.f16x2       f2, f0, f1;     // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nsub.bf16x2      f5, f3, f4;      // SIMD bf16x2 subtraction\n9.7.4.3.\nHalf Precision Floating Point Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul{.rnd}{.ftz}{.sat}.f16   d, a, b;\nmul{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nmul{.rnd}.bf16   d, a, b;\nmul{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms multiplication and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then multiplied in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a * b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that a\nmul\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nmul\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nand\nmul/sub\nsequences with no rounding modifiers may\nbe optimized to use fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmul.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmul.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nmul{.rnd}.bf16\nand\nmul{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nmul{.rnd}.bf16\nand\nmul{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 multiplications\nmul.f16        d0, a0, b0;\nmul.rn.f16     d1, a1, b1;\nmul.bf16       bd0, ba0, bb0;\nmul.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 multiplication\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nmul.f16x2  p3, p1, p2;   // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nmul.bf16x2  p6, p4, p5;       // SIMD bf16x2 multiplication\n\n// SIMD fp16 multiplication\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nmul.f16x2       f2, f0, f1;     // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nmul.bf16x2      f5, f3, f4;      // SIMD bf16x2 multiplication\n9.7.4.4.\nHalf Precision Floating Point Instructions:\nfma\n\nfma\nFused multiply-add\nSyntax\nfma.rnd{.ftz}{.sat}.f16     d, a, b, c;\nfma.rnd{.ftz}{.sat}.f16x2   d, a, b, c;\nfma.rnd{.ftz}.relu.f16      d, a, b, c;\nfma.rnd{.ftz}.relu.f16x2    d, a, b, c;\nfma.rnd{.relu}.bf16         d, a, b, c;\nfma.rnd{.relu}.bf16x2       d, a, b, c;\nfma.rnd.oob.{relu}.type     d, a, b, c;\n\n.rnd = { .rn };\nDescription\nPerforms a fused multiply-add with no loss of precision in the intermediate product and addition.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then operated in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a * b + c;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    fC[0] = c[0:15];\n    fC[1] = c[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i] + fC[i];\n    }\n}\nNotes\nRounding modifiers (default is\n.rn\n):\n.rn\nmantissa LSB rounds to nearest even\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nfma.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nfma.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nfma.relu.{f16,\nf16x2,\nbf16,\nbf16x2}\nclamps the result to 0 if negative.\nNaN\nresult is\nconverted to canonical\nNaN\n.\nOut Of Bounds modifier:\nfma.oob.{f16,\nf16x2,\nbf16,\nbf16x2}\nclamps the result to 0 if either of the operands\nis\nOOB\nNaN\n(defined under\nTensors\n) value. The test for the special\nNaN\nvalue\nand resultant forcing of the result to +0.0 is performed independently for each of the\ntwo SIMD operations.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nfma.relu.{f16,\nf16x2}\nand\nfma{.relu}.{bf16,\nbf16x2}\nintroduced in PTX ISA version 7.0.\nSupport for modifier\n.oob\nintroduced in PTX ISA version 8.1.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nfma.relu.{f16,\nf16x2}\nand\nfma{.relu}.{bf16,\nbf16x2}\nrequire\nsm_80\nor higher.\nfma{.oob}.{f16,\nf16x2,\nbf16,\nbf16x2}\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 fused multiply-add\nfma.rn.f16         d0, a0, b0, c0;\nfma.rn.f16         d1, a1, b1, c1;\nfma.rn.relu.f16    d1, a1, b1, c1;\nfma.rn.oob.f16      d1, a1, b1, c1;\nfma.rn.oob.relu.f16 d1, a1, b1, c1;\n\n// scalar bf16 fused multiply-add\nfma.rn.bf16        d1, a1, b1, c1;\nfma.rn.relu.bf16   d1, a1, b1, c1;\nfma.rn.oob.bf16       d1, a1, b1, c1;\nfma.rn.oob.relu.bf16  d1, a1, b1, c1;\n\n// SIMD f16 fused multiply-add\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1}; // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3}; // pack two f16 to 32bit f16x2\nfma.rn.f16x2  p3, p1, p2, p2;   // SIMD f16x2 fused multiply-add\nfma.rn.relu.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.f16x2 p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier and relu saturation mode\n\n// SIMD fp16 fused multiply-add\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nfma.rn.f16x2    f2, f0, f1, f1; // SIMD f16x2 fused multiply-add\n\n// SIMD bf16 fused multiply-add\nfma.rn.bf16x2       f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add\nfma.rn.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier and relu saturation mode\n9.7.4.5.\nHalf Precision Floating Point Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg{.ftz}.f16    d, a;\nneg{.ftz}.f16x2  d, a;\nneg.bf16         d, a;\nneg.bf16x2       d, a;\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vector by extracting half word values\nfrom the source operand. Half-word operands are then negated in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.b32\ntype. For\n.bf16\ninstruction\ntype, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type, operands\nd\nand\na\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = -a;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = -fA[i];\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nneg.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nneg.bf16\nand\nneg.bf16x2\nintroduced in PTX ISA 7.0.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nneg.bf16\nand\nneg.bf16x2\nrequires architecture\nsm_80\nor higher.\nExamples\nneg.ftz.f16  x,f0;\nneg.bf16     x,b0;\nneg.bf16x2   x1,b1;\n9.7.4.6.\nHalf Precision Floating Point Instructions:\nabs\n\nabs\nAbsolute value\nSyntax\nabs{.ftz}.f16    d, a;\nabs{.ftz}.f16x2  d, a;\nabs.bf16         d, a;\nabs.bf16x2       d, a;\nDescription\nTake absolute value of\na\nand store the result in\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vector by extracting half word values\nfrom the source operand. Absolute values of half-word operands are then computed in parallel to\nproduce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = |a|;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = |fA[i]|;\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nabs.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nabs.bf16\nand\nabs.bf16x2\nintroduced in PTX ISA 7.0.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nabs.bf16\nand\nabs.bf16x2\nrequires architecture\nsm_80\nor higher.\nExamples\nabs.ftz.f16  x,f0;\nabs.bf16     x,b0;\nabs.bf16x2   x1,b1;\n9.7.4.7.\nHalf Precision Floating Point Instructions:\nmin\n\nmin\nFind the minimum of two values.\nSyntax\nmin{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmin{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16x2         d, a, b;\nDescription\nStore the minimum of\na\nand\nb\nin\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nIf\n.NaN\nmodifier is specified, then the result is canonical\nNaN\nif either of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the minimum of\nabsolute values of both the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both the inputs.\nModifiers\n.abs\nand\n.xorsign\nmust be specified together and\n.xorsign\nconsiders the sign\nbit of both inputs before applying\n.abs\noperation.\nIf the result of\nmin\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\nif (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a < b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n               fA[i] = |fA[i]|;\n               fB[i] = |fB[i]|;\n           }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] < fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(d[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmin.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nmin.xorsign\nintroduced in PTX ISA version 7.2.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nmin.xorsign.abs\nsupport requires\nsm_86\nor higher.\nExamples\nmin.ftz.f16       h0,h1,h2;\nmin.f16x2         b0,b1,b2;\n// SIMD fp16 min with .NaN\nmin.NaN.f16x2     b0,b1,b2;\nmin.bf16          h0, h1, h2;\n// SIMD bf16 min with NaN\nmin.NaN.bf16x2    b0, b1, b2;\n// scalar bf16 min with xorsign.abs\nmin.xorsign.abs.bf16 Rd, Ra, Rb\n9.7.4.8.\nHalf Precision Floating Point Instructions:\nmax\n\nmax\nFind the maximum of two values.\nSyntax\nmax{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmax{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16x2         d, a, b;\nDescription\nStore the maximum of\na\nand\nb\nin\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nIf\n.NaN\nmodifier is specified, the result is canonical\nNaN\nif either of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the maximum of\nabsolute values of both the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both the inputs.\nModifiers\n.abs\nand\n.xorsign\nmust be specified together and\n.xorsign\nconsiders the sign\nbit of both inputs before applying\n.abs\noperation.\nIf the result of\nmax\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\nif (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a > b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n                fA[i] = |fA[i]|;\n                fB[i] = |fB[i]|;\n            }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] > fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(fA[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmax.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nmax.xorsign.abs\nintroduced in PTX ISA version 7.2.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nmax.xorsign.abs\nsupport requires\nsm_86\nor higher.\nExamples\nmax.ftz.f16       h0,h1,h2;\nmax.f16x2         b0,b1,b2;\n// SIMD fp16 max with NaN\nmax.NaN.f16x2     b0,b1,b2;\n// scalar f16 max with xorsign.abs\nmax.xorsign.abs.f16 Rd, Ra, Rb;\nmax.bf16          h0, h1, h2;\n// scalar bf16 max and NaN\nmax.NaN.bf16x2    b0, b1, b2;\n// SIMD bf16 max with xorsign.abs\nmax.xorsign.abs.bf16x2 Rd, Ra, Rb;\n9.7.4.9.\nHalf Precision Floating Point Instructions:\ntanh\n\ntanh\nFind the hyperbolic tangent of a value (in radians)\nSyntax\ntanh.approx.type d, a;\n\n.type = {.f16, .f16x2, .bf16, .bf16x2}\nDescription\nTake hyperbolic tangent value of\na\n.\nThe type of operands\nd\nand\na\nare as specified by\n.type\n.\nFor\n.f16x2\nor\n.bf16x2\ninstruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a\n.f16x2\nor\n.bf16x2\n.\nSemantics\nif (.type == .f16 || .type == .bf16) {\n  d = tanh(a)\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = tanh(fA[0])\n  d[1] = tanh(fA[1])\n}\nNotes\ntanh.approx.{f16,\nf16x2,\nbf16,\nbf16x2}\nimplements an approximate hyperbolic tangent in the\ntarget format.\nResults of\ntanh\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n-1.0\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n1.0\nNaN\nNaN\nThe maximum absolute error for\n.f16\ntype is 2-10.987. The maximum absolute error for\n.bf16\ntype is 2-8.\nThe subnormal numbers are supported.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\ntanh.approx.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\ntanh.approx.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\ntanh.approx.f16    h1, h0;\ntanh.approx.f16x2  hd1, hd0;\ntanh.approx.bf16   b1, b0;\ntanh.approx.bf16x2 hb1, hb0;\n9.7.4.10.\nHalf Precision Floating Point Instructions:\nex2\n\nex2\nFind the base-2 exponent of input.\nSyntax\nex2.approx.atype     d, a;\nex2.approx.ftz.btype d, a;\n\n.atype = { .f16,  .f16x2}\n.btype = { .bf16, .bf16x2}\nDescription\nRaise 2 to the power\na\n.\nThe type of operands\nd\nand\na\nare as specified by\n.type\n.\nFor\n.f16x2\nor\n.bf16x2\ninstruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a\n.f16x2\nor\n.bf16x2\n.\nSemantics\nif (.type == .f16 || .type == .bf16) {\n  d = 2 ^ a\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = 2 ^ fA[0]\n  d[1] = 2 ^ fA[1]\n}\nNotes\nex2.approx.{f16,\nf16x2,\nbf16,\nbf16x2}\nimplement a fast approximation to 2\na\n.\nFor the\n.f16\ntype, subnormal inputs are supported.\nex2.approx.ftz.bf16\nflushes subnormal\ninputs and results to sign-preserving zero.\nResults of\nex2.approx.ftz.bf16\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n+0.0\n-subnormal\n+1.0\n-0.0\n+1.0\n+0.0\n+1.0\n+subnormal\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nResults of\nex2.approx.f16\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n+0.0\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nThe maximum relative error for\n.f16\ntype is 2-9.9. The maximum relative error for\n.bf16\ntype\nis 2-7.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nex2.approx.ftz.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nex2.approx.ftz.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\nex2.approx.f16         h1, h0;\nex2.approx.f16x2       hd1, hd0;\nex2.approx.ftz.bf16    b1, b2;\nex2.approx.ftz.bf16x2  hb1, hb2;\n9.7.5.\nMixed Precision Floating-Point Instructions\n\nMixed precision floating-point instructions operate on data with varied floating point precision.\nBefore executing the specified operation, operands with different precision needs to be converted\nsuch that all the instruction operands can be represented with a consistent floating-point precision.\nThe register variable to be used for holding a particular operand depends upon the combination of\nthe instruction types. Refer\nFundamental Types\nand\nAlternate Floating-Point Data Formats\nfor more details\naround exact register operand to be used for a given data type.\nThe mixed precision floating point instructions are:\nadd\nsub\nfma\nMixed precision\nadd\n,\nsub\n,\nfma\nsupport saturation of results to the range [0.0, 1.0],\nwith\nNaN\nbeing flushed to positive zero.\n9.7.5.1.\nMixed Precision Floating Point Instructions:\nadd\n\nadd\nAdd 2 values.\nSyntax\nadd{.rnd}{.sat}.f32.atype  d, a, c;\n\n.atype = { .f16, .bf16};\n.rnd   = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operand\na\nfrom\n.atype\ninto\n.f32\ntype. The converted value is then\nused for the addition. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) + c;\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nadd.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nadd.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nadd.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .b16 ba;\nadd.rz.f32.bf16.sat   fd, fa, fc;\n9.7.5.2.\nMixed Precision Floating Point Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub{.rnd}{.sat}.f32.atype  d, a, c;\n\n.atype = { .f16, .bf16};\n.rnd   = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operand\na\nfrom\n.atype\ninto\n.f32\ntype. The converted value is then\nused for the subtraction. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) - c;\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nsub.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nsub.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nsub.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .f16 ha;\nsub.rz.f32.f16.sat   fd, ha, fc;\n9.7.5.3.\nMixed Precision Floating Point Instructions:\nfma\n\nfma\nFused multiply-add.\nSyntax\nfma.rnd{.sat}.f32.abtype  d, a, b, c;\n\n.abtype = { .f16, .bf16};\n.rnd    = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operands\na\nand\nb\nfrom\n.atype\ninto\n.f32\ntype. The converted values\nare then used to perform fused multiply-add operation with no loss of precision in the intermediate\nproduct and addition. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) * convert(b) + c;\nNotes\nfma.f32.{f16/bf16}\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto this product, again in infinite precision. The resulting value is then rounded to single\nprecision using the rounding mode specified by\n.rnd\n.\nRounding modifiers(no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nfma.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nfma.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nfma.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .f16 ha, hb;\nfma.rz.sat.f32.f16.sat   fd, ha, hb, fc;\n9.7.6.\nComparison and Selection Instructions\n\nThe comparison select instructions are:\nset\nsetp\nselp\nslct\nAs with single-precision floating-point instructions, the\nset\n,\nsetp\n, and\nslct\ninstructions support subnormal numbers for\nsm_20\nand higher targets and flush single-precision\nsubnormal inputs to sign-preserving zero for\nsm_1x\ntargets. The optional\n.ftz\nmodifier\nprovides backward compatibility with\nsm_1x\ntargets by flushing subnormal inputs and results to\nsign-preserving zero regardless of the target architecture.\n9.7.6.1.\nComparison and Selection Instructions:\nset\n\nset\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nset.CmpOp{.ftz}.dtype.stype         d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.stype  d, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.dtype  = { .u32, .s32, .f32 };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 };\nDescription\nCompares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator. If this result is\nTrue\n,\n1.0f\nis written for floating-point\ndestination types, and\n0xffffffff\nis written for integer destination types. Otherwise,\n0x00000000\nis written.\nOperand\nd\nhas type\n.dtype\n; operands\na\nand\nb\nhave type\n.stype\n; operand\nc\nhas\ntype\n.pred\n.\nSemantics\nt = (a CmpOp b) ? 1 : 0;\nif (isFloat(dtype))\n    d = BoolOp(t, c) ? 1.0f : 0x00000000;\nelse\n    d = BoolOp(t, c) ? 0xffffffff : 0x00000000;\nInteger Notes\nThe signed and unsigned comparison operators are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n.\nFor unsigned values, the comparison operators\nlo\n,\nls\n,\nhi\n, and\nhs\nfor lower,\nlower-or-same, higher, and higher-or-same may be used instead of\nlt\n,\nle\n,\ngt\n,\nge\n,\nrespectively.\nThe untyped, bit-size comparisons are\neq\nand\nne\n.\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nset.ftz.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nsm_1x\nset.dtype.f64\nsupports subnormal numbers.\nset.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nset\nwith\n.f64\nsource type requires\nsm_13\nor higher.\nExamples\n@p  set.lt.and.f32.s32  d,a,b,r;\n    set.eq.u32.u32      d,i,n;\n9.7.6.2.\nComparison and Selection Instructions:\nsetp\n\nsetp\nCompare two numeric values with a relational operator, and (optionally) combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nsetp.CmpOp{.ftz}.type         p[|q], a, b;\nsetp.CmpOp.BoolOp{.ftz}.type  p[|q], a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.type   = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 };\nDescription\nCompares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the first destination operand. A related value computed using\nthe complement of the compare result is written to the second destination operand.\nApplies to all numeric types. Operands\na\nand\nb\nhave type\n.type\n; operands\np\n,\nq\n,\nand\nc\nhave type\n.pred\n. The sink symbol ‘_’ may be used in place of any one of the\ndestination operands.\nSemantics\nt = (a CmpOp b) ? 1 : 0;\np = BoolOp(t, c);\nq = BoolOp(!t, c);\nInteger Notes\nThe signed and unsigned comparison operators are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n.\nFor unsigned values, the comparison operators\nlo\n,\nls\n,\nhi\n, and\nhs\nfor lower,\nlower-or-same, higher, and higher-or-same may be used instead of\nlt\n,\nle\n,\ngt\n,\nge\n,\nrespectively.\nThe untyped, bit-size comparisons are\neq\nand\nne\n.\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsetp.ftz.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nsm_1x\nsetp.dtype.f64\nsupports subnormal numbers.\nsetp.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nsetp\nwith\n.f64\nsource type requires\nsm_13\nor higher.\nExamples\nsetp.lt.and.s32  p|q,a,b,r;\n@q  setp.eq.u32      p,i,n;\n9.7.6.3.\nComparison and Selection Instructions:\nselp\n\nselp\nSelect between source operands, based on the value of the predicate source operand.\nSyntax\nselp.type d, a, b, c;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 };\nDescription\nConditional selection. If\nc\nis\nTrue\n,\na\nis stored in\nd\n,\nb\notherwise. Operands\nd\n,\na\n, and\nb\nmust be of the same type. Operand\nc\nis a predicate.\nSemantics\nd = (c == 1) ? a : b;\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nselp.f64\nrequires\nsm_13\nor higher.\nExamples\nselp.s32  r0,r,g,p;\n@q  selp.f32  f0,t,x,xp;\n9.7.6.4.\nComparison and Selection Instructions:\nslct\n\nslct\nSelect one source operand, based on the sign of the third operand.\nSyntax\nslct.dtype.s32        d, a, b, c;\nslct{.ftz}.dtype.f32  d, a, b, c;\n\n.dtype = { .b16, .b32, .b64,\n           .u16, .u32, .u64,\n           .s16, .s32, .s64,\n                 .f32, .f64 };\nDescription\nConditional selection. If\nc\n>= 0,\na\nis stored in\nd\n, otherwise\nb\nis stored in\nd\n. Operands\nd\n,\na\n, and\nb\nare treated as a bitsize type of the same width as the first\ninstruction type; operand\nc\nmust match the second instruction type (\n.s32\nor\n.f32\n). The\nselected input is copied to the output without modification.\nSemantics\nd = (c >= 0) ? a : b;\nFloating Point Notes\nFor\n.f32\ncomparisons, negative zero equals zero.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nslct.ftz.dtype.f32\nflushes subnormal values of operand\nc\nto sign-preserving zero, and\noperand\na\nis selected.\nsm_1x\nslct.dtype.f32\nflushes subnormal values of operand\nc\nto sign-preserving zero, and operand\na\nis selected.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nIf operand\nc\nis\nNaN\n, the comparison is ordered and operand\nb\nis selected.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nslct.f64\nrequires\nsm_13\nor higher.\nExamples\nslct.u32.s32  x, y, z, val;\nslct.ftz.u64.f32  A, B, C, fval;\n9.7.7.\nHalf Precision Comparison Instructions\n\nThe comparison instructions are:\nset\nsetp\n9.7.7.1.\nHalf Precision Comparison Instructions:\nset\n\nset\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nset.CmpOp{.ftz}.f16.stype            d, a, b;\nset.CmpOp.BoolOp{.ftz}.f16.stype     d, a, b, {!}c;\n\nset.CmpOp.bf16.stype                 d, a, b;\nset.CmpOp.BoolOp.bf16.stype          d, a, b, {!}c;\n\nset.CmpOp{.ftz}.dtype.f16            d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16     d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp.dtype.bf16                 d, a, b;\nset.CmpOp.BoolOp.dtype.bf16          d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp{.ftz}.dtype.f16x2          d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16x2   d, a, b, {!}c;\n.dtype  = { .f16x2, .u32, .s32}\n\nset.CmpOp.dtype.bf16x2               d, a, b;\nset.CmpOp.BoolOp.dtype.bf16x2        d, a, b, {!}c;\n.dtype  = { .bf16x2, .u32, .s32}\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n            .f16, .f32, .f64};\nDescription\nCompares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator.\nResult of this computation is written in destination register in the following way:\nIf result is\nTrue\n,\n0xffffffff\nis written for destination types\n.u32\n/\n.s32\n.\n0xffff\nis written for destination types\n.u16\n/\n.s16\n.\n1.0\nin target precision floating point format is written for destination type\n.f16\n,\n.bf16\n.\nIf result is\nFalse\n,\n0x0\nis written for all integer destination types.\n0.0\nin target precision floating point format is written for destination type\n.f16\n,\n.bf16\n.\nIf the source type is\n.f16x2\nor\n.bf16x2\nthen result of individual operations are packed in\nthe 32-bit destination operand.\nOperand\nc\nhas type\n.pred\n.\nSemantics\nif (stype == .f16x2 || stype == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0]   = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1]   = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    if (dtype == .f16x2 || stype == .bf16x2) {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 1.0 : 0.0;\n        }\n    } else {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 0xffff : 0;\n        }\n    }\n} else if (dtype == .f16 || stype == .bf16) {\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? 1.0 : 0.0;\n} else  { // Integer destination type\n    trueVal = (isU16(dtype) || isS16(dtype)) ?  0xffff : 0xffffffff;\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? trueVal : 0;\n}\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nWhen\n.ftz\nmodifier is specified then subnormal inputs and results are flushed to sign\npreserving zero.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nset.{u16,\nu32,\ns16,\ns32}.f16\nand\nset.{u32,\ns32}.f16x2\nare introduced in PTX ISA version 6.5.\nset.{u16,\nu32,\ns16,\ns32}.bf16\n,\nset.{u32,\ns32,\nbf16x2}.bf16x2\n,\nset.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64}\nare introduced in PTX ISA version\n7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nset.{u16,\nu32,\ns16,\ns32}.bf16\n,\nset.{u32,\ns32,\nbf16x2}.bf16x2\n,\nset.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64}\nrequire\nsm_90\nor higher.\nExamples\nset.lt.and.f16.f16  d,a,b,r;\nset.eq.f16x2.f16x2  d,i,n;\nset.eq.u32.f16x2    d,i,n;\nset.lt.and.u16.f16  d,a,b,r;\nset.ltu.or.bf16.f16    d,u,v,s;\nset.equ.bf16x2.bf16x2  d,j,m;\nset.geu.s32.bf16x2     d,j,m;\nset.num.xor.s32.bf16   d,u,v,s;\n9.7.7.2.\nHalf Precision Comparison Instructions:\nsetp\n\nsetp\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nsetp.CmpOp{.ftz}.f16           p, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16    p, a, b, {!}c;\n\nsetp.CmpOp{.ftz}.f16x2         p|q, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16x2  p|q, a, b, {!}c;\n\nsetp.CmpOp.bf16                p, a, b;\nsetp.CmpOp.BoolOp.bf16         p, a, b, {!}c;\n\nsetp.CmpOp.bf16x2              p|q, a, b;\nsetp.CmpOp.BoolOp.bf16x2       p|q, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\nDescription\nCompares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the destination operand.\nOperand\nc\n,\np\nand\nq\nhas type\n.pred\n.\nFor instruction type\n.f16\n, operands\na\nand\nb\nhave type\n.b16\nor\n.f16\n.\nFor instruction type\n.f16x2\n, operands\na\nand\nb\nhave type\n.b32\n.\nFor instruction type\n.bf16\n, operands\na\nand\nb\nhave type\n.b16\n.\nFor instruction type\n.bf16x2\n, operands\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == .f16 || type == .bf16) {\n     t = (a CmpOp b) ? 1 : 0;\n     p = BoolOp(t, c);\n} else if (type == .f16x2 || type == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    p = BoolOp(t[0], c);\n    q = BoolOp(t[1], c);\n}\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nsetp.ftz.{f16,f16x2}\nflushes subnormal inputs to sign-preserving zero.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nsetp.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nsetp.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\nsetp.lt.and.f16x2  p|q,a,b,r;\n@q  setp.eq.f16    p,i,n;\n\nsetp.gt.or.bf16x2  u|v,c,d,s;\n@q  setp.eq.bf16   u,j,m;\n9.7.8.\nLogic and Shift Instructions\n\nThe logic and shift instructions are fundamentally untyped, performing bit-wise operations on\noperands of any type, provided the operands are of the same size. This permits bit-wise operations\non floating point values without having to define a union to access the bits. Instructions\nand\n,\nor\n,\nxor\n, and\nnot\nalso operate on predicates.\nThe logical shift instructions are:\nand\nor\nxor\nnot\ncnot\nlop3\nshf\nshl\nshr\n9.7.8.1.\nLogic and Shift Instructions:\nand\n\nand\nBitwise AND.\nSyntax\nand.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise and operation for the bits in\na\nand\nb\n.\nSemantics\nd = a & b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nand.b32  x,q,r;\nand.b32  sign,fpvalue,0x80000000;\n9.7.8.2.\nLogic and Shift Instructions:\nor\n\nor\nBiwise OR.\nSyntax\nor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise or operation for the bits in\na\nand\nb\n.\nSemantics\nd = a | b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nor.b32  mask mask,0x00010001\nor.pred  p,q,r;\n9.7.8.3.\nLogic and Shift Instructions:\nxor\n\nxor\nBitwise exclusive-OR (inequality).\nSyntax\nxor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise exclusive-or operation for the bits in\na\nand\nb\n.\nSemantics\nd = a ^ b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nxor.b32  d,q,r;\nxor.b16  d,x,0x0001;\n9.7.8.4.\nLogic and Shift Instructions:\nnot\n\nnot\nBitwise negation; one’s complement.\nSyntax\nnot.type d, a;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nInvert the bits in\na\n.\nSemantics\nd = ~a;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicates.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nnot.b32  mask,mask;\nnot.pred  p,q;\n9.7.8.5.\nLogic and Shift Instructions:\ncnot\n\ncnot\nC/C++ style logical negation.\nSyntax\ncnot.type d, a;\n\n.type = { .b16, .b32, .b64 };\nDescription\nCompute the logical negation using C/C++ semantics.\nSemantics\nd = (a==0) ? 1 : 0;\nNotes\nThe size of the operands must match, but not necessarily the type.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ncnot.b32 d,a;\n9.7.8.6.\nLogic and Shift Instructions:\nlop3\n\nlop3\nArbitrary logical operation on 3 inputs.\nSyntax\nlop3.b32 d, a, b, c, immLut;\nlop3.BoolOp.b32 d|p, a, b, c, immLut, q;\n\n.BoolOp   = { .or , .and };\nDescription\nCompute bitwise logical operation on inputs\na\n,\nb\n,\nc\nand store the result in destination\nd\n.\nOptionally,\n.BoolOp\ncan be specified to compute the predicate result\np\nby performing a\nBoolean operation on the destination operand\nd\nwith the predicate\nq\nin the following manner:\np = (d != 0) BoolOp q;\nThe sink symbol ‘_’ may be used in place of the destination operand\nd\nwhen\n.BoolOp\nqualifier\nis specified.\nThe logical operation is defined by a look-up table which, for 3 inputs, can be represented as an\n8-bit value specified by operand\nimmLut\nas described below.\nimmLut\nis an integer constant\nthat can take values from 0 to 255, thereby allowing up to 256 distinct logical operations on inputs\na\n,\nb\n,\nc\n.\nFor a logical operation\nF(a,\nb,\nc)\nthe value of\nimmLut\ncan be computed by applying the same\noperation to three predefined constant values as follows:\nta = 0xF0;\ntb = 0xCC;\ntc = 0xAA;\n\nimmLut = F(ta, tb, tc);\nExamples:\nIf F = (a & b & c);\nimmLut = 0xF0 & 0xCC & 0xAA = 0x80\n\nIf F = (a | b | c);\nimmLut = 0xF0 | 0xCC | 0xAA = 0xFE\n\nIf F = (a & b & ~c);\nimmLut = 0xF0 & 0xCC & (~0xAA) = 0x40\n\nIf F = ((a & b | c) ^ a);\nimmLut = (0xF0 & 0xCC | 0xAA) ^ 0xF0 = 0x1A\nThe following table illustrates computation of\nimmLut\nfor various logical operations:\nta\ntb\ntc\nOper 0 (False)\nOper 1 (ta & tb & tc)\nOper 2 (ta & tb & ~tc)\n…\nOper 254 (ta | tb | tc)\nOper 255 (True)\n0\n0\n0\n0\n0\n0\n…\n0\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n0\n1\n1\nimmLut\n0x0\n0x80\n0x40\n…\n0xFE\n0xFF\nSemantics\nF = GetFunctionFromTable(immLut); // returns the function corresponding to immLut value\nd = F(a, b, c);\nif (BoolOp specified) {\n    p = (d != 0) BoolOp q;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 4.3.\nSupport for\n.BoolOp\nqualifier introduced in PTX ISA version 8.2.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nQualifier\n.BoolOp\nrequires\nsm_70\nor higher.\nExamples\nlop3.b32       d, a, b, c, 0x40;\nlop3.or.b32  d|p, a, b, c, 0x3f, q;\nlop3.and.b32 _|p, a, b, c, 0x3f, q;\n9.7.8.7.\nLogic and Shift Instructions:\nshf\n\nshf\nFunnel shift.\nSyntax\nshf.l.mode.b32  d, a, b, c;  // left shift\nshf.r.mode.b32  d, a, b, c;  // right shift\n\n.mode = { .clamp, .wrap };\nDescription\nShift the 64-bit value formed by concatenating operands\na\nand\nb\nleft or right by the amount\nspecified by the unsigned 32-bit value in\nc\n. Operand\nb\nholds bits\n63:32\nand operand a\nholds bits\n31:0\nof the 64-bit source value. The source is shifted left or right by the clamped\nor wrapped value in\nc\n. For\nshf.l\n, the most-significant 32-bits of the result are written\ninto\nd\n; for\nshf.r\n, the least-significant 32-bits of the result are written into\nd\n.\nSemantics\nu32  n = (.mode == .clamp) ? min(c, 32) : c & 0x1f;\nswitch (shf.dir) {  // shift concatenation of [b, a]\n    case shf.l:     // extract 32 msbs\n           u32  d = (b << n)      | (a >> (32-n));\n    case shf.r:     // extract 32 lsbs\n           u32  d = (b << (32-n)) | (a >> n);\n}\nNotes\nUse funnel shift for multi-word shift operations and for rotate operations. The shift amount is\nlimited to the range\n0..32\nin clamp mode and\n0..31\nin wrap mode, so shifting multi-word\nvalues by distances greater than 32 requires first moving 32-bit words, then using\nshf\nto shift\nthe remaining\n0..31\ndistance.\nTo shift data sizes greater than 64 bits to the right, use repeated\nshf.r\ninstructions applied\nto adjacent words, operating from least-significant word towards most-significant word. At each\nstep, a single word of the shifted result is computed. The most-significant word of the result is\ncomputed using a\nshr.{u32,s32}\ninstruction, which zero or sign fills based on the instruction\ntype.\nTo shift data sizes greater than 64 bits to the left, use repeated\nshf.l\ninstructions applied to\nadjacent words, operating from most-significant word towards least-significant word. At each step, a\nsingle word of the shifted result is computed. The least-significant word of the result is computed\nusing a\nshl\ninstruction.\nUse funnel shift to perform 32-bit left or right rotate by supplying the same value for source\narguments\na\nand\nb\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nRequires\nsm_32\nor higher.\nExample\nshf.l.clamp.b32  r3,r1,r0,16;\n\n// 128-bit left shift; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] << n\nshf.l.clamp.b32  r7,r2,r3,n;\nshf.l.clamp.b32  r6,r1,r2,n;\nshf.l.clamp.b32  r5,r0,r1,n;\nshl.b32          r4,r0,n;\n\n// 128-bit right shift, arithmetic; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] >> n\nshf.r.clamp.b32  r4,r0,r1,n;\nshf.r.clamp.b32  r5,r1,r2,n;\nshf.r.clamp.b32  r6,r2,r3,n;\nshr.s32          r7,r3,n;     // result is sign-extended\n\nshf.r.clamp.b32  r1,r0,r0,n;  // rotate right by n; n < 32\nshf.l.clamp.b32  r1,r0,r0,n;  // rotate left by n; n < 32\n\n// extract 32-bits from [r1,r0] starting at position n < 32\nshf.r.clamp.b32  r0,r0,r1,n;\n9.7.8.8.\nLogic and Shift Instructions:\nshl\n\nshl\nShift bits left, zero-fill on right.\nSyntax\nshl.type d, a, b;\n\n.type = { .b16, .b32, .b64 };\nDescription\nShift\na\nleft by the amount specified by unsigned 32-bit value in\nb\n.\nSemantics\nd = a << b;\nNotes\nShift amounts greater than the register width\nN\nare clamped to\nN\n.\nThe sizes of the destination and first source operand must match, but not necessarily the type. The\nb\noperand must be a 32-bit value, regardless of the instruction type.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExample\nshl.b32  q,a,2;\n9.7.8.9.\nLogic and Shift Instructions:\nshr\n\nshr\nShift bits right, sign or zero-fill on left.\nSyntax\nshr.type d, a, b;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nShift\na\nright by the amount specified by unsigned 32-bit value in\nb\n. Signed shifts fill with\nthe sign bit, unsigned and untyped shifts fill with\n0\n.\nSemantics\nd = a >> b;\nNotes\nShift amounts greater than the register width\nN\nare clamped to\nN\n.\nThe sizes of the destination and first source operand must match, but not necessarily the type. The\nb\noperand must be a 32-bit value, regardless of the instruction type.\nBit-size types are included for symmetry with\nshl\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExample\nshr.u16  c,a,2;\nshr.s32  i,i,1;\nshr.b16  k,i,j;\n9.7.9.\nData Movement and Conversion Instructions\n\nThese instructions copy data from place to place, and from state space to state space, possibly\nconverting it from one format to another.\nmov\n,\nld\n,\nldu\n, and\nst\noperate on both\nscalar and vector types. The\nisspacep\ninstruction is provided to query whether a generic address\nfalls within a particular state space window. The\ncvta\ninstruction converts addresses between\ngeneric\nand\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate spaces.\nInstructions\nld\n,\nst\n,\nsuld\n, and\nsust\nsupport optional cache operations.\nThe Data Movement and Conversion Instructions are:\nmov\nshfl.sync\nprmt\nld\nldu\nst\nst.async\nst.bulk\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nprefetch\n,\nprefetchu\nisspacep\ncvta\ncvt\ncvt.pack\ncp.async\ncp.async.commit_group\ncp.async.wait_group\n,\ncp.async.wait_all\ncp.async.bulk\ncp.reduce.async.bulk\ncp.async.bulk.prefetch\nmultimem.cp.async.bulk\nmultimem.cp.reduce.async.bulk\ncp.async.bulk.tensor\ncp.reduce.async.bulk.tensor\ncp.async.bulk.prefetch.tensor\ncp.async.bulk.commit_group\ncp.async.bulk.wait_group\ntensormap.replace\n9.7.9.1.\nCache Operators\n\nPTX ISA version 2.0 introduced optional cache operators on load and store instructions. The cache\noperators require a target architecture of\nsm_20\nor higher.\nCache operators on load or store instructions are treated as performance hints only. The use of a\ncache operator on an\nld\nor\nst\ninstruction does not change the memory consistency behavior of\nthe program.\nFor\nsm_20\nand higher, the cache operators have the following definitions and behavior.\nTable 30\nCache Operators for Memory Load Instructions\n\nOperator\nMeaning\n.ca\nCache at all levels, likely to be accessed again.\nThe default load instruction cache operation is ld.ca, which allocates cache lines in all\nlevels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but\nmultiple L1 caches are not coherent for global data. If one thread stores to global memory\nvia one L1 cache, and a second thread loads that address via a second L1 cache with\nld.ca\n,\nthe second thread may get stale L1 cache data, rather than the data stored by the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of parallel threads.\nStores by the first grid program are then correctly fetched by the second grid program issuing\ndefault\nld.ca\nloads cached in L1.\n.cg\nCache at global level (cache in L2 and below, not L1).\nUse\nld.cg\nto cache loads only globally, bypassing the L1 cache, and cache only in the L2\ncache.\n.cs\nCache streaming, likely to be accessed once.\nThe\nld.cs\nload cached streaming operation allocates global lines with evict-first policy in\nL1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or\ntwice. When\nld.cs\nis applied to a Local window address, it performs the\nld.lu\noperation.\n.lu\nLast use.\nThe compiler/programmer may use\nld.lu\nwhen restoring spilled registers and popping function\nstack frames to avoid needless write-backs of lines that will not be used again. The\nld.lu\ninstruction performs a load cached streaming operation (\nld.cs\n) on global addresses.\n.cv\nDon’t cache and fetch again (consider cached system memory lines stale, fetch again).\nThe ld.cv load operation applied to a global System Memory address invalidates (discards) a\nmatching L2 line and re-fetches the line on each new load.\nTable 31\nCache Operators for Memory Store Instructions\n\nOperator\nMeaning\n.wb\nCache write-back all coherent levels.\nThe default store instruction cache operation is\nst.wb\n, which writes back cache lines of\ncoherent cache levels with normal eviction policy.\nIf one thread stores to global memory, bypassing its L1 cache, and a second thread in a\ndifferent SM later loads from that address via a different L1 cache with\nld.ca\n, the second\nthread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored\nby the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of thread arrays.\nStores by the first grid program are then correctly missed in L1 and fetched by the second grid\nprogram issuing default\nld.ca\nloads.\n.cg\nCache at global level (cache in L2 and below, not L1).\nUse\nst.cg\nto cache global store data only globally, bypassing the L1 cache, and cache only\nin the L2 cache.\n.cs\nCache streaming, likely to be accessed once.\nThe\nst.cs\nstore cached-streaming operation allocates cache lines with evict-first policy to\nlimit cache pollution by streaming output data.\n.wt\nCache write-through (to system memory).\nThe\nst.wt\nstore write-through operation applied to a global System Memory address writes\nthrough the L2 cache.\n9.7.9.2.\nCache Eviction Priority Hints\n\nPTX ISA version 7.4 adds optional cache eviction priority hints on load and store\ninstructions. Cache eviction priority requires target architecture\nsm_70\nor higher.\nCache eviction priority on load or store instructions is treated as a performance hint. It is\nsupported for\n.global\nstate space and generic addresses where the address points to\n.global\nstate space.\nTable 32\nCache Eviction Priority Hints for Memory Load and Store Instructions\n\nCache Eviction Priority\nMeaning\nevict_normal\nCache data with normal eviction priority. This is the default eviction priority.\nevict_first\nData cached with this priority will be first in the eviction priority order and\nwill likely be evicted when cache eviction is required. This priority is suitable\nfor streaming data.\nevict_last\nData cached with this priority will be last in the eviction priority order and will\nlikely be evicted only after other data with\nevict_normal\nor\nevict_first\neviction priotity is already evicted. This priority is suitable for data that\nshould remain persistent in cache.\nevict_unchanged\nDo not change eviction priority order as part of this operation.\nno_allocate\nDo not allocate data to cache. This priority is suitable for streaming data.\n9.7.9.3.\nData Movement and Conversion Instructions:\nmov\n\nmov\nSet a register variable with the value of a register variable or an immediate value. Take the\nnon-generic address of a variable in global, local, or shared state space.\nSyntax\nmov.type  d, a;\nmov.type  d, sreg;\nmov.type  d, avar;       // get address of variable\nmov.type  d, avar+imm;   // get address of variable with offset\nmov.u32   d, fname;      // get address of device function\nmov.u64   d, fname;      // get address of device function\nmov.u32   d, kernel;     // get address of entry function\nmov.u64   d, kernel;     // get address of entry function\n\n.type = { .pred,\n          .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 };\nDescription\nWrite register\nd\nwith the value of\na\n.\nOperand\na\nmay be a register, special register, variable with optional offset in an addressable\nmemory space, or function name.\nFor variables declared in\n.const\n,\n.global\n,\n.local\n, and\n.shared\nstate spaces,\nmov\nplaces the non-generic address of the variable (i.e., the address of the variable in its state\nspace) into the destination register. The generic address of a variable in\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate space may be generated by first taking the address within the state\nspace with\nmov\nand then converting it to a generic address using the\ncvta\ninstruction;\nalternately, the generic address of a variable declared in\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate space may be taken directly using the\ncvta\ninstruction.\nNote that if the address of a device function parameter is moved to a register, the parameter will\nbe copied onto the stack and the address will be in the local state space.\nSemantics\nd = a;\nd = sreg;\nd = &avar;        // address is non-generic; i.e., within the variable's declared state space\nd = &avar+imm;\nNotes\nAlthough only predicate and bit-size types are required, we include the arithmetic types for the\nprogrammer’s convenience: their use enhances program readability and allows additional type\nchecking.\nWhen moving address of a kernel or a device function, only\n.u32\nor\n.u64\ninstruction types\nare allowed. However, if a signed type is used, it is not treated as a compilation error. The\ncompiler issues a warning in this case.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTaking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function\naddresses should only be used in the context of CUDA Dynamic Parallelism system calls. See the\nCUDA\nDynamic Parallelism Programming Guide\nfor details.\nTarget ISA Notes\nmov.f64\nrequires\nsm_13\nor higher.\nTaking the address of kernel entry functions requires\nsm_35\nor higher.\nExamples\nmov.f32  d,a;\nmov.u16  u,v;\nmov.f32  k,0.1;\nmov.u32  ptr, A;        // move address of A into ptr\nmov.u32  ptr, A[5];     // move address of A[5] into ptr\nmov.u32  ptr, A+20;     // move address with offset into ptr\nmov.u32  addr, myFunc;  // get address of device function 'myFunc'\nmov.u64  kptr, main;    // get address of entry function 'main'\n9.7.9.4.\nData Movement and Conversion Instructions:\nmov\n\nmov\nMove vector-to-scalar (pack) or scalar-to-vector (unpack).\nSyntax\nmov.type  d, a;\n\n.type = { .b16, .b32, .b64, .b128 };\nDescription\nWrite scalar register\nd\nwith the packed value of vector register\na\n, or write vector register\nd\nwith the unpacked values from scalar register\na\n.\nWhen destination operand\nd\nis a vector register, the sink symbol\n'_'\nmay be used for one or\nmore elements provided that at least one element is a scalar register.\nFor bit-size types,\nmov\nmay be used to pack vector elements into a scalar register or unpack\nsub-fields of a scalar register into a vector. Both the overall size of the vector and the size of\nthe scalar must match the size of the instruction type.\nSemantics\n// pack two 8-bit elements into .b16\nd = a.x | (a.y << 8)\n// pack four 8-bit elements into .b32\nd = a.x | (a.y << 8)  | (a.z << 16) | (a.w << 24)\n// pack two 16-bit elements into .b32\nd = a.x | (a.y << 16)\n// pack four 16-bit elements into .b64\nd = a.x | (a.y << 16)  | (a.z << 32) | (a.w << 48)\n// pack two 32-bit elements into .b64\nd = a.x | (a.y << 32)\n// pack four 32-bit elements into .b128\nd = a.x | (a.y << 32)  | (a.z << 64) | (a.w << 96)\n// pack two 64-bit elements into .b128\nd = a.x | (a.y << 64)\n\n// unpack 8-bit elements from .b16\n{ d.x, d.y } = { a[0..7], a[8..15] }\n// unpack 8-bit elements from .b32\n{ d.x, d.y, d.z, d.w }\n        { a[0..7], a[8..15], a[16..23], a[24..31] }\n\n// unpack 16-bit elements from .b32\n{ d.x, d.y }  = { a[0..15], a[16..31] }\n// unpack 16-bit elements from .b64\n{ d.x, d.y, d.z, d.w } =\n        { a[0..15], a[16..31], a[32..47], a[48..63] }\n\n// unpack 32-bit elements from .b64\n{ d.x, d.y } = { a[0..31], a[32..63] }\n\n// unpack 32-bit elements from .b128\n{ d.x, d.y, d.z, d.w } =\n        { a[0..31], a[32..63], a[64..95], a[96..127] }\n// unpack 64-bit elements from .b128\n{ d.x, d.y } = { a[0..63], a[64..127] }\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nTarget ISA Notes\nSupported on all target architectures.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nExamples\nmov.b32 %r1,{a,b};      // a,b have type .u16\nmov.b64 {lo,hi}, %x;    // %x is a double; lo,hi are .u32\nmov.b32 %r1,{x,y,z,w};  // x,y,z,w have type .b8\nmov.b32 {r,g,b,a},%r1;  // r,g,b,a have type .u8\nmov.b64 {%r1, _}, %x;   // %x is.b64, %r1 is .b32\nmov.b128 {%b1, %b2}, %y;   // %y is.b128, %b1 and % b2 are .b64\nmov.b128 %y, {%b1, %b2};   // %y is.b128, %b1 and % b2 are .b64\n9.7.9.5.\nData Movement and Conversion Instructions:\nshfl\n(deprecated)\n\nshfl\n(deprecated)\nRegister data shuffle within threads of a warp.\nSyntax\nshfl.mode.b32  d[|p], a, b, c;\n\n.mode = { .up, .down, .bfly, .idx };\nDeprecation Note\nThe\nshfl\ninstruction without a\n.sync\nqualifier is deprecated in PTX ISA version 6.0.\nSupport for this instruction with\n.target\nlower than\nsm_70\nmay be removed in a future PTX ISA version.\nRemoval Note\nSupport for\nshfl\ninstruction without a\n.sync\nqualifier is removed in PTX ISA version 6.4 for\n.target\nsm_70\nor higher.\nDescription\nExchange register data between threads of a warp.\nEach thread in the currently executing warp will compute a source lane index\nj\nbased on input\noperands\nb\nand\nc\nand the\nmode\n. If the computed source lane index\nj\nis in range, the\nthread will copy the input operand\na\nfrom lane\nj\ninto its own destination register\nd\n;\notherwise, the thread will simply copy its own input\na\nto destination\nd\n. The optional\ndestination predicate\np\nis set to\nTrue\nif the computed source lane is in range, and\notherwise set to\nFalse\n.\nNote that an out of range value of\nb\nmay still result in a valid computed source lane index\nj\n. In this case, a data transfer occurs and the destination predicate\np\nis True.\nNote that results are undefined in divergent control flow within a warp, if an active thread sources\na register from an inactive thread.\nOperand\nb\nspecifies a source lane or source lane offset, depending on the mode.\nOperand\nc\ncontains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index.\nSemantics\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & mask[4:0]) | (cval[4:0] & ~mask[4:0]);\nminLane = (lane[4:0] & mask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~mask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval;\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nDeprecated in PTX ISA version 6.0 in favor of\nshfl.sync\n.\nNot supported in PTX ISA version 6.4 for .target\nsm_70\nor higher.\nTarget ISA Notes\nshfl\nrequires\nsm_30\nor higher.\nshfl\nis not supported on\nsm_70\nor higher starting PTX ISA version 6.4.\nExamples\n// Warp-level INCLUSIVE PLUS SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.up.b32  Ry|p, Rx, 0x1,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x2,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x4,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x8,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x10, 0x0;\n@p  add.f32      Rx, Ry, Rx;\n\n\n    // Warp-level INCLUSIVE PLUS REVERSE-SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.down.b32  Ry|p, Rx, 0x1,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x2,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x4,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x8,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x10, 0x1f;\n@p  add.f32        Rx, Ry, Rx;\n\n\n    // BUTTERFLY REDUCTION:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.bfly.b32  Ry, Rx, 0x10, 0x1f;   // no predicate dest\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x8,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x4,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x2,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x1,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    //\n    // All threads now hold sum in Rx\n9.7.9.6.\nData Movement and Conversion Instructions:\nshfl.sync\n\nshfl.sync\nRegister data shuffle within threads of a warp.\nSyntax\nshfl.sync.mode.b32  d[|p], a, b, c, membermask;\n\n.mode = { .up, .down, .bfly, .idx };\nDescription\nExchange register data between threads of a warp.\nshfl.sync\nwill cause executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nshfl.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s\nlaneid\n.\nshfl.sync\nexchanges register data between threads in\nmembermask\n.\nEach thread in the currently executing warp will compute a source lane index\nj\nbased on input\noperands\nb\nand\nc\nand the\nmode\n. If the computed source lane index\nj\nis in range, the\nthread will copy the input operand\na\nfrom lane\nj\ninto its own destination register\nd\n;\notherwise, the thread will simply copy its own input\na\nto destination\nd\n. The optional\ndestination predicate\np\nis set to\nTrue\nif the computed source lane is in range, and\notherwise set to\nFalse\n.\nNote that an out of range value of\nb\nmay still result in a valid computed source lane index\nj\n. In this case, a data transfer occurs and the destination predicate\np\nis True.\nNote that results are undefined if a thread sources a register from an inactive thread or a thread\nthat is not in\nmembermask\n.\nOperand\nb\nspecifies a source lane or source lane offset, depending on the mode.\nOperand\nc\ncontains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index.\nThe behavior of\nshfl.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nshfl.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when\nthe\nshfl.sync\ninstruction is executed. Otherwise, the behavior is undefined.\nSemantics\n// wait for all threads in membermask to arrive\nwait_for_specified_threads(membermask);\n\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nsegmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & segmask[4:0]) | (cval[4:0] & ~segmask[4:0]);\nminLane = (lane[4:0] & segmask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~segmask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval;\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nshfl.sync.up.b32  Ry|p, Rx, 0x1,  0x0, 0xffffffff;\n9.7.9.7.\nData Movement and Conversion Instructions:\nprmt\n\nprmt\nPermute bytes from register pair.\nSyntax\nprmt.b32{.mode}  d, a, b, c;\n\n.mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 };\nDescription\nPick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination\nregister.\nIn the generic form (no mode specified), the permute control consists of four 4-bit selection\nvalues. The bytes in the two source registers are numbered from 0 to 7:\n{b,\na}\n=\n{{b7,\nb6,\nb5,\nb4},\n{b3,\nb2,\nb1,\nb0}}\n. For each byte in the target register, a 4-bit selection value is defined.\nThe 3 lsbs of the selection value specify which of the 8 source bytes should be moved into the\ntarget position. The msb defines if the byte value should be copied, or if the sign (msb of the\nbyte) should be replicated over all 8 bits of the target position (sign extend of the byte value);\nmsb=0\nmeans copy the literal value;\nmsb=1\nmeans replicate the sign. Note that the sign\nextension is only performed as part of generic form.\nThus, the four 4-bit values fully specify an arbitrary byte permute, as a\n16b\npermute code.\ndefault mode\nd.b3\nsource select\nd.b2\nsource select\nd.b1\nsource select\nd.b0\nsource select\nindex\nc[15:12]\nc[11:8]\nc[7:4]\nc[3:0]\nThe more specialized form of the permute control uses the two lsb’s of operand\nc\n(which is\ntypically an address pointer) to control the byte extraction.\nmode\nselector\nc[1:0]\nd.b3\nsource\nd.b2\nsource\nd.b1\nsource\nd.b0\nsource\nf4e\n(forward 4 extract)\n0\n3\n2\n1\n0\n1\n4\n3\n2\n1\n2\n5\n4\n3\n2\n3\n6\n5\n4\n3\nb4e\n(backward 4 extract)\n0\n5\n6\n7\n0\n1\n6\n7\n0\n1\n2\n7\n0\n1\n2\n3\n0\n1\n2\n3\nrc8\n(replicate 8)\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\necl\n(edge clamp left)\n0\n3\n2\n1\n0\n1\n3\n2\n1\n1\n2\n3\n2\n2\n2\n3\n3\n3\n3\n3\necr\n(edge clamp right)\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n2\n2\n2\n1\n0\n3\n3\n2\n1\n0\nrc16\n(replicate 16)\n0\n1\n0\n1\n0\n1\n3\n2\n3\n2\n2\n1\n0\n1\n0\n3\n3\n2\n3\n2\nSemantics\ntmp64 = (b<<32) | a;  // create 8 byte source\n\nif ( ! mode ) {\n   ctl[0] = (c >>  0) & 0xf;\n   ctl[1] = (c >>  4) & 0xf;\n   ctl[2] = (c >>  8) & 0xf;\n   ctl[3] = (c >> 12) & 0xf;\n} else {\n   ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >>  0) & 0x3;\n}\n\ntmp[07:00] = ReadByte( mode, ctl[0], tmp64 );\ntmp[15:08] = ReadByte( mode, ctl[1], tmp64 );\ntmp[23:16] = ReadByte( mode, ctl[2], tmp64 );\ntmp[31:24] = ReadByte( mode, ctl[3], tmp64 );\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nprmt\nrequires\nsm_20\nor higher.\nExamples\nprmt.b32      r1, r2, r3, r4;\nprmt.b32.f4e  r1, r2, r3, r4;\n9.7.9.8.\nData Movement and Conversion Instructions:\nld\n\nld\nLoad a register variable from an addressable state space variable.\nSyntax\nld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld.volatile{.ss}{.level::prefetch_size}{.vec}.type  d, [a];\n\nld.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.acquire.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.mmio.relaxed.sys{.global}.type  d, [a];\n\n.ss =                       { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} };\n.cop =                      { .ca, .cg, .cs, .lu, .cv };\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nLoad register variable\nd\nfrom the location specified by the source address operand\na\nin\nspecified state space. If no state space is given, perform the load using\nGeneric Addressing\n.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nIf no sub-qualifier is specified with\n.param\nstate space, then:\n::func\nis assumed when access is inside a device function.\n::entry\nis assumed when accessing kernel function parameters from entry function. Otherwise, when\naccessing device function parameters or any other\n.param\nvariables from entry function\n::func\nis assumed by default.\nFor\nld.param::entry\ninstruction, operand a must be a kernel parameter address, otherwise behavior\nis undefined. For\nld.param::func\ninstruction, operand a must be a device function parameter address,\notherwise behavior is undefined.\nInstruction\nld.param{::func}\nused for reading value returned from device function call cannot be\npredicated. See\nParameter State Space\nand\nFunction Declarations and Definitions\nfor descriptions\nof the proper use of\nld.param\n.\nThe\n.relaxed\nand\n.acquire\nqualifiers indicate memory synchronization as described in the\nMemory Consistency Model\n. The\n.scope\nqualifier\nindicates the set of threads with which an\nld.relaxed\nor\nld.acquire\ninstruction can directly\nsynchronize\n1\n. The\n.weak\nqualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means.\nThe semantic details of\n.mmio\nqualifier are described in the\nMemory Consistency Model\n.\nOnly\n.sys\nthread scope is valid for\nld.mmio\noperation. The\nqualifiers\n.mmio\nand\n.relaxed\nmust be specified together.\nThe semantic details of\n.volatile\nqualifier are described in the\nMemory Consistency Model\n.\nThe\n.weak\n,\n.volatile\n,\n.relaxed\nand\n.acquire\nqualifiers are mutually exclusive. When\nnone of these is specified, the\n.weak\nqualifier is assumed by default.\n.relaxed\nand\n.acquire\n:\nMay be used with\n.global\n,\n.shared\nspaces, or with generic addressing where the address\npoints to\n.global\nor\n.shared\nspace.\nCache operations are not allowed.\n.volatile\n:\nMay be used with\n.global\n,\n.shared\n,\n.local\nspaces, or with generic addressing where\nthe address points to\n.global\n,\n.shared\n, or\n.local\nspace.\nCache operations are not allowed.\n.mmio\n:\nMay be used only with\n.global\nspace or with generic addressing where the address points to\n.global\nspace.\nThe optional qualifier\n.unified\nmust be specified on operand\na\nif\na\nis the address of a\nvariable declared with\n.unified\nattribute as described in\nVariable and Function Attribute Directive: .attribute\n.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nd\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that data from corresponding memory location is not read.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe qualifier\n.level::prefetch_size\nmay only be used with\n.global\nstate space and with\ngeneric addressing where the address points to\n.global\nstate space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifiers\n.unified\nand\n.level::cache_hint\nare only supported for\n.global\nstate\nspace and for generic addressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\n1\nThis synchronization is further extended to other threads through the transitive nature of\ncausality order\n, as described in the memory consistency model.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See\nTable 28\nfor a description of these relaxed type-checking rules.\n.f16\ndata may be loaded using\nld.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\nor can be used in half precision floating point instructions.\n.f16x2\ndata may be loaded using\nld.b32\nand then used in half precision floating point\ninstructions.\nPTX ISA Notes\nld introduced in PTX ISA version 1.0.\nld.volatile\nintroduced in PTX ISA version 1.1.\nGeneric addressing and cache operations introduced in PTX ISA version 2.0.\nSupport for scope qualifier,\n.relaxed\n,\n.acquire\n,\n.weak\nqualifiers introduced in PTX ISA\nversion 6.0.\nSupport for generic addressing of .const space added in PTX ISA version 3.1.\nSupport for\n.level1::eviction_priority\n,\n.level::prefetch_size\nand\n.level::cache_hint\nqualifiers introduced in PTX ISA version 7.4.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for\n.unified\nqualifier introduced in PTX ISA version 8.0.\nSupport for\n.mmio\nqualifier introduced in PTX ISA version 8.2.\nSupport for\n::entry\nand\n::func\nsub-qualifiers on\n.param\nspace introduced in PTX ISA\nversion 8.3.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nSupport for\n.volatile\nqualifier with\n.local\nstate space introduced in PTX ISA version 9.1.\nTarget ISA Notes\nld.f64\nrequires\nsm_13\nor higher.\nSupport for scope qualifier,\n.relaxed\n,\n.acquire\n,\n.weak\nqualifiers require\nsm_70\nor\nhigher.\nGeneric addressing requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nSupport for\n.level::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::prefetch_size\nqualifier requires\nsm_75\nor higher.\nSupport for\n.L2::256B\nand\n.L2::cache_hint\nqualifiers requires\nsm_80\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.unified\nqualifier requires\nsm_90\nor higher.\nSupport for\n.mmio\nqualifier requires\nsm_70\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nld.global.f32    d,[a];\nld.shared.v4.b32 Q,[p];\nld.const.s32     d,[p+4];\nld.local.b32     x,[p+-8]; // negative offset\nld.local.b64     x,[240];  // immediate address\n\nld.global.b16    %r,[fs];  // load .f16 data into 32-bit reg\ncvt.f32.f16      %r,%r;    // up-convert f16 data to f32\n\nld.global.b32    %r0, [fs];     // load .f16x2 data in 32-bit reg\nld.global.b32    %r1, [fs + 4]; // load .f16x2 data in 32-bit reg\nadd.rn.f16x2     %d0, %r0, %r1; // addition of f16x2 data\nld.global.relaxed.gpu.u32 %r0, [gbl];\nld.shared.acquire.gpu.u32 %r1, [sh];\nld.global.relaxed.cluster.u32 %r2, [gbl];\nld.shared::cta.acquire.gpu.u32 %r2, [sh + 4];\nld.shared::cluster.u32 %r3, [sh + 8];\nld.global.mmio.relaxed.sys.u32 %r3, [gbl];\nld.local.volatile.u32 %r4, [lcl];\n\nld.global.f32    d,[ugbl].unified;\nld.b32           %r0, [%r1].unified;\n\nld.global.L1::evict_last.u32  d, [p];\n\nld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2\nld.L2::128B.f64         %r1, [gbl]; // Prefetch 128B to L2\nld.global.L2::256B.f64  %r2, [gbl]; // Prefetch 256B to L2\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1;\nld.global.L2::cache_hint.b64  x, [p], cache-policy;\nld.param::entry.b32 %rp1, [kparam1];\n\nld.global.b128   %r0, [gbl];   // 128-bit load\n\n// 256-bit load\nld.global.L2::evict_last.v8.f32 { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7}, [addr];\nld.global.L2::evict_last.L1::evict_last.v4.u64 { %reg0, %reg1, %reg2, %reg3}, [addr];\n9.7.9.9.\nData Movement and Conversion Instructions:\nld.global.nc\n\nld.global.nc\nLoad a register variable from global state space via non-coherent cache.\nSyntax\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type                 d, [a]{, cache-policy};\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type             d, [a]{, cache-policy};\n\nld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type      d, [a]{, cache-policy};\nld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type  d, [a]{, cache-policy};\n\n.cop  =                     { .ca, .cg, .cs };     // cache operation\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate};\n.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.vec  =                     { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nLoad register variable\nd\nfrom the location specified by the source address operand\na\nin the\nglobal state space, and optionally cache in non-coherent read-only cache.\nNote\nOn some architectures, the texture cache is larger, has higher bandwidth, and longer latency than\nthe global memory cache. For applications with sufficient parallelism to cover the longer\nlatency,\nld.global.nc\nshould offer better performance than\nld.global\non such\narchitectures.\nThe address operand\na\nshall contain a global address.\nSupported addressing modes for operand\na\nand alignment requirements are\ndescribed in\nAddresses as Operands\n.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\n,\n.s32\n,\n.u32\n, or\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nd\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that data from corresponding memory location is not read.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types.\n.f16\ndata may be loaded using\nld.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nSupport for\n.level::eviction_priority\n,\n.level::prefetch_size\nand\n.level::cache_hint\nqualifiers introduced in PTX ISA version 7.4.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nRequires\nsm_32\nor higher.\nSupport for\n.level1::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::prefetch_size\nqualifier requires\nsm_75\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nld.global.nc.f32           d, [a];\nld.gloal.nc.L1::evict_last.u32 d, [a];\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.5;\nld.global.nc.L2::cache_hint.f32  d, [a], cache-policy;\n\nld.global.nc.L2::64B.b32      d,  [a];     // Prefetch 64B to L2\nld.global.nc.L2::256B.f64     d,  [a];     // Prefetch 256B to L2\n\nld.global.nc.b128             d,  [a];\n\nld.global.nc.L2::evict_first.v4.f64 {%reg0, %reg1. %reg2, %reg3}. [a]; // 256-bit load\n9.7.9.10.\nData Movement and Conversion Instructions:\nldu\n\nldu\nLoad read-only data from an address that is common across threads in the warp.\nSyntax\nldu{.ss}.type      d, [a];       // load from address\nldu{.ss}.vec.type  d, [a];       // vec load from address\n\n.ss   = { .global };             // state space\n.vec  = { .v2, .v4 };\n.type = { .b8, .b16, .b32, .b64, .b128,\n          .u8, .u16, .u32, .u64,\n          .s8, .s16, .s32, .s64,\n                     .f32, .f64 };\nDescription\nLoad\nread-only\ndata into register variable\nd\nfrom the location specified by the source address\noperand\na\nin the global state space, where the address is guaranteed to be the same across all\nthreads in the warp. If no state space is given, perform the load using\nGeneric Addressing\n.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\n.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See\nTable 28\nfor a description of these relaxed type-checking rules.\n.f16\ndata may be loaded using\nldu.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\nor can be used in half precision floating point instructions.\n.f16x2\ndata may be loaded using\nldu.b32\nand then used in half precision floating point\ninstructions.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nTarget ISA Notes\nldu.f64\nrequires\nsm_13\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nExamples\nldu.global.f32    d,[a];\nldu.global.b32    d,[p+4];\nldu.global.v4.f32 Q,[p];\nldu.global.b128   d,[a];\n9.7.9.11.\nData Movement and Conversion Instructions:\nst\n\nst\nStore data to an addressable state space variable.\nSyntax\nst{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type   [a], b{, cache-policy};\nst{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.volatile{.ss}{.vec}.type                           [a], b;\nst.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.release.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.mmio.relaxed.sys{.global}.type         [a], b;\n\n.ss =                       { .global, .local, .param{::func}, .shared{::cta, ::cluster} };\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level2::eviction_priority = { .L2::evict_normal, .L2::evict_first, .L2::evict_last };\n.level::cache_hint =        { .L2::cache_hint };\n.cop =                      { .wb, .cg, .cs, .wt };\n.sem =                      { .relaxed, .release };\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nStore the value of operand\nb\nin the location specified by the destination address\noperand\na\nin specified state space. If no state space is given, perform the store using\nGeneric Addressing\n. Stores to const memory are illegal.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\n.\nIf\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::func\n.\nInstruction\nst.param{::func}\nused for passing arguments to device function cannot be predicated.\nSee\nParameter State Space\nand\nFunction Declarations and Definitions\nfor descriptions of the proper use\nof\nst.param\n.\nThe qualifiers\n.relaxed\nand\n.release\nindicate memory synchronization as described in the\nMemory Consistency Model\n. The\n.scope\nqualifier\nindicates the set of threads with which an\nst.relaxed\nor\nst.release\ninstruction can directly\nsynchronize\n1\n. The\n.weak\nqualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means.\nThe semantic details of\n.mmio\nqualifier are described in the\nMemory Consistency Model\n.\nOnly\n.sys\nthread scope is valid for\nst.mmio\noperation. The\nqualifiers\n.mmio\nand\n.relaxed\nmust be specified together.\nThe semantic details of\n.volatile\nqualifier are described in the\nMemory Consistency Model\n.\nThe\n.weak\n,\n.volatile\n,\n.relaxed\nand\n.release\nqualifiers are mutually exclusive. When\nnone of these is specified, the\n.weak\nqualifier is assumed by default.\n.relaxed\nand\n.release\n:\nMay be used with\n.global\n,\n.shared\nspaces or with generic addressing where the address points\nto\n.global\nor\n.shared\nspace.\nCache operations are not allowed.\n.volatile\n:\nMay be used with\n.global\n,\n.shared\n,\n.local\nspaces or with generic addressing where the\naddress points to\n.global\n,\n.shared\n, or\n.local\nspace.\nCache operations are not allowed.\n.mmio\n:\nMay be used only with\n.global\nspace or with generic addressing where the address points to\n.global\nspace.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\n,\n.s32\n,\n.u32\n, or\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nb\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that no data is being written at the corresponding destination address.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\n1\nThis synchronization is further extended to other threads through the transitive nature of\ncausality order\n, as described in the memory consistency model.\nSemantics\nd = a;                // named variable d\n*(&a+immOffset) = b;            // variable-plus-offset\n*a = b;               // register\n*(a+immOffset) = b;   // register-plus-offset\n*(immAddr) = b;       // immediate address\nNotes\nOperand\nb\nmust be in the\n.reg\nstate space.\nA source register wider than the specified type may be used. The lower\nn\nbits corresponding to\nthe instruction-type width are stored to memory. See\nTable 27\nfor a description of these relaxed type-checking rules.\n.f16\ndata resulting from a\ncvt\ninstruction may be stored using\nst.b16\n.\n.f16x2\ndata may be stored using\nst.b32\n.\nPTX ISA Notes\nst introduced in PTX ISA version 1.0.\nst.volatile\nintroduced in PTX ISA version 1.1.\nGeneric addressing and cache operations introduced in PTX ISA version 2.0.\nSupport for scope qualifier,\n.relaxed\n,\n.release\n,\n.weak\nqualifiers introduced in PTX ISA\nversion 6.0.\nSupport for\n.level1::eviction_priority\nand\n.level::cache_hint\nqualifiers introduced in PTX\nISA version 7.4.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for\n.mmio\nqualifier introduced in PTX ISA version 8.2.\nSupport for\n::func\nsub-qualifier on\n.param\nspace introduced in PTX ISA version 8.3.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nSupport for\n.volatile\nqualifier with\n.local\nstate space introduced in PTX ISA version 9.1.\nTarget ISA Notes\nst.f64\nrequires\nsm_13\nor higher.\nSupport for scope qualifier,\n.relaxed\n,\n.release\n,\n.weak\nqualifiers require\nsm_70\nor\nhigher.\nGeneric addressing requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nSupport for\n.level1::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.mmio\nqualifier requires\nsm_70\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nst.global.f32    [a],b;\nst.local.b32     [q+4],a;\nst.global.v4.s32 [p],Q;\nst.local.b32     [q+-8],a; // negative offset\nst.local.s32     [100],r7; // immediate address\n\ncvt.f16.f32      %r,%r;    // %r is 32-bit register\nst.b16           [fs],%r;  // store lower\nst.global.relaxed.sys.u32 [gbl], %r0;\nst.shared.release.cta.u32 [sh], %r1;\nst.global.relaxed.cluster.u32 [gbl], %r2;\nst.shared::cta.release.cta.u32 [sh + 4], %r1;\nst.shared::cluster.u32 [sh + 8], %r1;\nst.global.mmio.relaxed.sys.u32 [gbl], %r1;\nst.local.volatile.u32 [lcl], %r2;\n\nst.global.L1::no_allocate.f32 [p], a;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nst.global.L2::cache_hint.b32  [a], b, cache-policy;\n\nst.param::func.b64 [param1], %rp1;\n\nst.global.b128  [a], b;  // 128-bit store\n\n// 256-bit store\nst.global.L2::evict_last.v8.f32 [addr], { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7};\n9.7.9.12.\nData Movement and Conversion Instructions:\nst.async\n\nst.async\nAsynchronous store operation.\nSyntax\nst.async{.weak}{.ss}.completion_mechanism{.vec}.type [a], b, [mbar];\nst.async{.scope}{.ss}.completion_mechanism{.vec}.type [a], b, [mbar];\n\n.scope =                { .cluster };\n.ss   =                 { .shared::cluster };\n.type =                 { .b32, .b64,\n                          .u32, .u64,\n                          .s32, .s64,\n                          .f32, .f64 };\n.vec  =                 { .v2, .v4 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\nst.async{.mmio}.sem.scope{.ss}.type [a], b;\n\n.sem =                  { .release };\n.scope =                { .gpu, .sys };\n.ss =                   { .global };\n.type =                 { .b8, .b16, .b32, .b64,\n                          .u8, .u16, .u32, .u64,\n                          .s8, .s16, .s32, .s64,\n                                     .f32, .f64 };\nDescription\nst.async\nis a non-blocking instruction which initiates an asynchronous store operation that\nstores the value specified by source operand\nb\nto the destination memory location\nspecified by operand\na\n.\nOperands\na\nis a destination address, and must be either a register, or of the form\nregister\n+\nimmOff\n,\nas described in\nAddresses as Operands\n.\nb\nis a source value, of the type indicated by qualifier\n.type\n.\nmbar\nis an mbarrier object address.\nQualifiers\n.mmio\nindicates whether this is an\nmmio Operation\n.\n.sem\nspecifies the memory ordering semantics as described in the\nMemory Consistency Model\n.\nIf\n.sem\nis not specified, it defaults to\n.weak\n.\n.scope\nspecifies the set of threads with which this instruction can directly synchronize.\n.ss\nspecifies the state space of the destination operand\na\nand the mbarrier\noperand\nmbar\n.\nIf\n.ss\nis not specified,\nGeneric Addressing\nis used.\n.completion_mechanism\nspecifies the mechanism for observing the completion of the\nasynchronous operation.\nWhen\n.completion_mechanism\nis\n.mbarrier::complete_tx::bytes\n: upon completion of the\nasynchronous operation, a\ncomplete-tx\noperation will be performed on the mbarrier object specified by the operand\nmbar\n, with\ncompleteCount\nargument equal to the amount of data stored in bytes.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\n.type\nspecifies the type of the source operand\nb\n.\nConditions\nWhen\n.sem\nis\n.weak\n:\nThis is a weak store to shared memory, which signals its completion through an mbarrier object.\nThe store operation is treated as a weak memory operation.\nThe complete-tx operation on the mbarrier has\n.release\nsemantics at\n.cluster\nscope.\nRequires:\nThe shared memory addresses of destination operand\na\nand the\nmbarrier object\nmbar\nbelong\nto the same CTA within the same cluster as the executing thread.\nThe number of CTAs within the cluster is strictly greater than one;\n%cluster_nctarank\n>\n1\nis true.\nOtherwise, the behavior is undefined.\n.mmio\nmust not be specified.\nIf\n.ss\nis specified, it must be\n.shared::cluster\n.\nIf\n.ss\nis not specified, generic addressing is used for operands\na\nand\nmbar\n.\nIf the generic addresses specified do not fall within the address window of\n.shared::cluster\nstate space, the behavior is undefined.\n.completion_mechanism\nmust be specified and must be\n.mbarrier::complete_tx::bytes\n.\nWhen\n.sem\nis\n.release\n:\nThis is a release store to global memory.\nThe store operation is a strong memory operation with\n.release\nsemantics at the\nscope specified by\n.scope\n.\nIf\n.mmio\nis specified,\n.scope\nmust be\n.sys\n.\nIf\n.ss\nis specified, it must be\n.global\n.\nIf\n.ss\nis not specified, generic addressing is used for operand\na\n.\nIf the generic address specified does not fall within the address window of\n.global\nstate space, the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space, and\n.scope\nqualifier introduced in PTX ISA version 8.7.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space, and\n.scope\nqualifier require\nsm_100\nor higher.\nExamples\nst.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr];\n\nst.async.sys.release.global.u32 [addr], b;\n9.7.9.13.\nData Movement and Conversion Instructions:\nst.bulk\n\nst.bulk\nInitializes a region of memory as specified by state space.\nSyntax\nst.bulk{.weak}{.shared::cta}  [a], size, initval; // initval must be zero\nDescription\nst.bulk\ninstruction initializes a region of shared memory starting from the location specified\nby destination address operand\na\n.\nThe 32-bit or 64-bit integer operand\nsize\nspecifies the amount of memory to be initialized in terms of\nnumber of bytes.\nsize\nmust be a multiple of 8. If the value is not a multiple of 8, then the\nbehavior is undefined. The maximum value of\nsize\noperand can be 16777216.\nThe integer immediate operand\ninitval\nspecifies the initialization value for the memory\nlocations. The only numeric value allowed for operand\ninitval\nis 0.\nIf no state space is specified then\nGeneric Addressing\nis used. If the\naddress specified by\na\ndoes not fall within the address window of\n.shared\nstate space then\nthe behavior is undefined.\nThe optional qualifier\n.weak\nspecify the memory synchronizing effect of the\nst.bulk\ninstruction as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nSupport for\nsize\noperand with 32-bit length is introduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nExamples\nst.bulk.weak.shared::cta  [dst], n, 0;\n\nst.bulk                   [gdst], 4096, 0;\n9.7.9.14.\nData Movement and Conversion Instructions:\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\n\nThe multimem.* operations operate on multimem addresses and accesses all of the multiple memory\nlocations which the multimem address points to.\nMultimem addresses can only be accessed only by multimem.* operations. Accessing a multimem address\nwith\nld\n,\nst\nor any other memory operations results in undefined behavior.\nRefer to\nCUDA programming guide\nfor creation and management of the multimem addresses.\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nPerform memory operations on the multimem address.\nSyntax\n// Integer type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type      d, [a];\nmultimem.ld_reduce.weak{.ss}.op.type                 d, [a];\n\nmultimem.st{.stsem}{.scope}{.ss}.type                [a], b;\nmultimem.st.weak{.ss}.type                           [a], b;\n\nmultimem.red{.redsem}{.scope}{.ss}.op.type           [a], b;\n\n.ss =       { .global }\n.ldsem =    { .relaxed, .acquire }\n.stsem =    { .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add, .and, .or, .xor }\n.type =     { .b32, .b64,  .u32, .u64, .s32, .s64 }\n\n// Floating point type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type    d, [a];\nmultimem.ld_reduce.weak{.ss}.op{.acc_prec}{.vec}.type               d, [a];\n\nmultimem.st{.stsem}{.scope}{.ss}{.vec}.type                         [a], b;\nmultimem.st.weak{.ss}{.vec}.type                                    [a], b;\n\nmultimem.red{.redsem}{.scope}{.ss}.redop{.vec}.redtype              [a], b;\n\n.ss =       { .global }\n.ldsem =    { .relaxed, .acquire }\n.stsem =    { .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add }\n.redop  =   { .add }\n.acc_prec = { .acc::f32, .acc::f16 }\n.vec =      { .v2, .v4, .v8 }\n.type=      { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64, .e5m2, .e5m2x2, .e5m2x4, .e4m3, .e4m3x2, .e4m3x4 }\n.redtype =  { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 }\nDescription\nInstruction\nmultimem.ld_reduce\nperforms the following operations:\nload operation on the multimem address\na\n, which involves loading of data from all of the\nmultiple memory locations pointed to by the multimem address\na\n,\nreduction operation specified by\n.op\non the multiple data loaded from the multimem address\na\n.\nThe result of the reduction operation in returned in register\nd\n.\nInstruction\nmultimem.st\nperforms a store operation of the input operand\nb\nto all the memory\nlocations pointed to by the multimem address\na\n.\nInstruction\nmultimem.red\nperforms a reduction operation on all the memory locations pointed to\nby the multimem address\na\n, with operand\nb\n.\nInstruction\nmultimem.ld_reduce\nperforms reduction on the values loaded from all the memory\nlocations that the multimem address points to. In contrast, the\nmultimem.red\nperform reduction\non all the memory locations that the multimem address points to.\nAddress operand\na\nmust be a multimem address. Otherwise, the behavior is undefined.  Supported\naddressing modes for operand a and alignment requirements are described in\nAddresses as Operands\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\na\ndoes not fall within the address window of\n.global\nstate\nspace then the behavior is undefined.\nFor floating-point type multi- operations, the size of the specified type along with\n.vec\nmust\nequal either 32-bits or 64-bits or 128-bits. No other combinations of\n.vec\nand type are\nallowed. Type\n.f64\ncannot be used with\n.vec\nqualifier.\nThe following table describes the valid usage of\n.vec\nand base floating-point type:\n.vec\nBase float-type supported\nNo\n.vec\nspecified\n.f16x2\n,\n.bf16x2\n,\n.f32\n,\n.f64\n,\n.e5m2x4\n,\n.e4m3x4\n.v2\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3x2\n,\n.e4m3x4\n.v4\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\n.v8\n.f16\n,\n.bf16\n,\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\nThe following table describes the valid combinations of\n.op\nand base type:\nop\nBase type\n.add\n.u32\n,\n.u64\n,\n.s32\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.f64\n,\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nFor\nmultimem.ld_reduce\n, the default precision of the intermediate accumulation is same as the\nspecified type.\nOptionally,\n.acc_prec\nqualifier can be specified to change the precision of intermediate\naccumulation as follows:\n.type\n.acc::prec\nChanges precision to\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.acc::f32\n.f32\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\n,\n.e4m3x4\n,\n.e5m2x4\n.acc::f16\n.f16\nOptional qualifiers\n.ldsem\n,\n.stsem\nand\n.redsem\nspecify the memory synchronizing effect\nof the\nmultimem.ld_reduce\n,\nmultimem.st\nand\nmultimem.red\nrespectively, as described in\nMemory Consistency Model\n. If explicit semantics qualifiers\nare not specified, then\nmultimem.ld_reduce\nand\nmultimem.st\ndefault to\n.weak\nand\nmultimem.red\ndefaults to\n.relaxed\n.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in\nMemory Consistency Model\n. If the\n.scope\nqualifier is not specified for\nmultimem.red\nthen\n.sys\nscope is assumed by default.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.acc::f32\nqualifier introduced in PTX ISA version 8.2.\nSupport for types\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nintroduced in PTX ISA version 8.6.\nSupport for\n.acc::f16\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nTypes\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nsm_121a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.acc::f16\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nsm_121a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nmultimem.ld_reduce.and.b32                    val1_b32, [addr1];\nmultimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2];\n\nmultimem.st.relaxed.gpu.b32                [addr3], val3_b32;\nmultimem.st.release.cta.global.u32         [addr4], val4_u32;\n\nmultimem.red.relaxed.gpu.max.f64           [addr5], val5_f64;\nmultimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9};\nmultimem.ld_reduce.add.acc::f32.v2.f16x2   {val_10, val_11}, [addr7];\n\nmultimem.ld_reduce.relaxed.cta.min.v2.e4m3x2 {val_12, val_13}, [addr8];\nmultimem.ld_reduce.relaxed.cta.add.v4.e4m3   {val_14, val_15, val_16, val_17}, [addr9];\nmultimem.ld_reduce.add.acc::f16.v4.e5m2      {val_18, val_19, val_20, val_21}, [addr10];\n9.7.9.15.\nData Movement and Conversion Instructions:\nprefetch\n,\nprefetchu\n\nprefetch\n,\nprefetchu\nPrefetch line containing a generic address at a specified level of memory hierarchy, in specified\nstate space.\nSyntax\nprefetch{.space}.level                    [a];   // prefetch to data cache\nprefetch.global.level::eviction_priority  [a];   // prefetch to data cache\n\nprefetchu.L1  [a];             // prefetch to uniform cache\n\nprefetch{.tensormap_space}.tensormap [a];  // prefetch the tensormap\n\n.space =                    { .global, .local };\n.level =                    { .L1, .L2 };\n.level::eviction_priority = { .L2::evict_last, .L2::evict_normal };\n.tensormap_space =          { .const, .param };\nDescription\nThe\nprefetch\ninstruction brings the cache line containing the specified address in global or\nlocal memory state space into the specified cache level.\nIf the\n.tensormap\nqualifier is specified then the\nprefetch\ninstruction brings the cache line\ncontaining the specified address in the\n.const\nor\n.param\nmemory state space for subsequent\nuse by the\ncp.async.bulk.tensor\ninstruction.\nIf no state space is given, the\nprefetch\nuses\nGeneric Addressing\n.\nOptionally, the eviction priority to be applied on the prefetched cache line can be specified by the\nmodifier\n.level::eviction_priority\n.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe\nprefetchu\ninstruction brings the cache line containing the specified generic address into\nthe specified uniform cache level.\nA\nprefetch\nto a shared memory location performs no operation.\nA\nprefetch\ninto the uniform cache requires a generic address, and no operation occurs if the\naddress maps to a\nconst\n,\nlocal\n, or\nshared\nmemory location.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nSupport for\n.level::eviction_priority\nqualifier introduced in PTX ISA version 7.4.\nSupport for the\n.tensormap\nqualifier is introduced in PTX ISA version 8.0.\nTarget ISA Notes\nprefetch\nand\nprefetchu\nrequire\nsm_20\nor higher.\nSupport for\n.level::eviction_priority\nqualifier requires\nsm_80\nor higher.\nSupport for the\n.tensormap\nqualifier requires\nsm_90\nor higher.\nExamples\nprefetch.global.L1             [ptr];\nprefetch.global.L2::evict_last [ptr];\nprefetchu.L1  [addr];\nprefetch.const.tensormap       [ptr];\n9.7.9.16.\nData Movement and Conversion Instructions:\napplypriority\n\napplypriority\nApply the cache eviction priority to the specified address in the specified cache level.\nSyntax\napplypriority{.global}.level::eviction_priority  [a], size;\n\n.level::eviction_priority = { .L2::evict_normal };\nDescription\nThe\napplypriority\ninstruction applies the cache eviction priority specified by the\n.level::eviction_priority\nqualifier to the address range\n[a..a+size)\nin the specified cache\nlevel.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nThe operand\nsize\nis an integer constant that specifies the amount of data, in bytes, in the\nspecified cache level on which the priority is to be applied. The only supported value for the\nsize\noperand is 128.\nSupported addressing modes for operand\na\nare described in\nAddresses as Operands\n.\na\nmust be aligned to 128 bytes.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\napplypriority.global.L2::evict_normal [ptr], 128;\n9.7.9.17.\nData Movement and Conversion Instructions:\ndiscard\n\ndiscard\nDiscard the data at the specified address range and cache level.\nSyntax\ndiscard{.global}.level  [a], size;\n\n.level = { .L2 };\nDescription\nSemantically, this behaves like a weak write of an\nunstable indeterminate value\n:\nreads of memory locations with\nunstable indeterminate values\nmay return different\nbit patterns each time until the memory is overwritten.\nThis operation\nhints\nto the implementation that data in the specified cache\n.level\ncan be destructively discarded without writing it back to memory.\nThe operand\nsize\nis an integer constant that specifies the length in bytes of the\naddress range\n[a,\na\n+\nsize)\nto write\nunstable indeterminate values\ninto.\nThe only supported value for the\nsize\noperand is\n128\n.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nSupported addressing modes for address operand\na\nare described in\nAddresses as Operands\n.\na\nmust be aligned to 128 bytes.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\ndiscard.global.L2 [ptr], 128;\nld.weak.u32 r0, [ptr];\nld.weak.u32 r1, [ptr];\n// The values in r0 and r1 may differ!\n9.7.9.18.\nData Movement and Conversion Instructions:\ncreatepolicy\n\ncreatepolicy\nCreate a cache eviction policy for the specified cache level.\nSyntax\n// Range-based policy\ncreatepolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy, [a], primary-size, total-size;\n\n// Fraction-based policy\ncreatepolicy.fractional.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy{, fraction};\n\n// Converting the access property from CUDA APIs\ncreatepolicy.cvt.L2.b64            cache-policy, access-property;\n\n.level::primary_priority =   { .L2::evict_last, .L2::evict_normal,\n                               .L2::evict_first, .L2::evict_unchanged };\n.level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged };\nDescription\nThe\ncreatepolicy\ninstruction creates a cache eviction policy for the specified cache level in an\nopaque 64-bit register specified by the destination operand\ncache-policy\n. The cache eviction\npolicy specifies how cache eviction priorities are applied to global memory addresses used in memory\noperations with\n.level::cache_hint\nqualifier.\nThere are two types of cache eviction policies:\nRange-based policy\nThe cache eviction policy created using\ncreatepolicy.range\nspecifies the cache eviction\nbehaviors for the following three address ranges:\n[a\n..\na\n+\n(primary-size\n-\n1)]\nreferred to as primary range.\n[a\n+\nprimary-size\n..\na\n+\n(total-size\n-\n1)]\nreferred to as trailing secondary range.\n[a\n-\n(total-size\n-\nprimary-size)\n..\n(a\n-\n1)]\nreferred to as preceding secondary range.\nWhen a range-based cache eviction policy is used in a memory operation with\n.level::cache_hint\nqualifier, the eviction priorities are applied as follows:\nIf the memory address falls in the primary range, the eviction priority specified by\n.L2::primary_priority\nis applied.\nIf the memory address falls in any of the secondary ranges, the eviction priority specified by\n.L2::secondary_priority\nis applied.\nIf the memory address does not fall in either of the above ranges, then the applied eviction\npriority is unspecified.\nThe 32-bit operand\nprimary-size\nspecifies the size, in bytes, of the primary range. The\n32-bit operand\ntotal-size\nspecifies the combined size, in bytes, of the address range\nincluding primary and secondary ranges. The value of\nprimary-size\nmust be less than or equal\nto the value of\ntotal-size\n. Maximum allowed value of\ntotal-size\nis 4GB.\nIf\n.L2::secondary_priority\nis not specified, then it defaults to\n.L2::evict_unchanged\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nFraction-based policy\nA memory operation with\n.level::cache_hint\nqualifier can use the fraction-based cache\neviction policy to request the cache eviction priority specified by\n.L2:primary_priority\nto\nbe applied to a fraction of cache accesses specified by the 32-bit floating point operand\nfraction\n. The remainder of the cache accesses get the eviction priority specified by\n.L2::secondary_priority\n. This implies that in a memory operation that uses a fraction-based\ncache policy, the memory access has a probability specified by the operand\nfraction\nof\ngetting the cache eviction priority specified by\n.L2::primary_priority\n.\nThe valid range of values for the operand\nfraction\nis\n(0.0,..,\n1.0]\n. If the operand\nfraction\nis not specified, it defaults to 1.0.\nIf\n.L2::secondary_priority\nis not specified, then it defaults to\n.L2::evict_unchanged\n.\nThe access property created using the CUDA APIs can be converted into cache eviction policy by the\ninstruction\ncreatepolicy.cvt\n. The source operand\naccess-property\nis a 64-bit opaque\nregister. Refer to\nCUDA programming guide\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\ncreatepolicy.fractional.L2::evict_last.b64                      policy, 1.0;\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64  policy, 0.5;\n\ncreatepolicy.range.L2::evict_last.L2::evict_first.b64\n                                            policy, [ptr], 0x100000, 0x200000;\n\n// access-prop is created by CUDA APIs.\ncreatepolicy.cvt.L2.b64 policy, access-prop;\n9.7.9.19.\nData Movement and Conversion Instructions:\nisspacep\n\nisspacep\nQuery whether a generic address falls within a specified state space window.\nSyntax\nisspacep.space  p, a;    // result is .pred\n\n.space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\nDescription\nWrite predicate register\np\nwith\n1\nif generic address a falls within the specified state\nspace window and with\n0\notherwise. Destination\np\nhas type\n.pred\n; the source address\noperand must be of type\n.u32\nor\n.u64\n.\nisspacep.param{::entry}\nreturns\n1\nif the generic address falls within the window of\nKernel Function Parameters\n, otherwise returns\n0\n. If\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::entry\n.\nisspacep.global\nreturns\n1\nfor\nKernel Function Parameters\nas\n.param\nwindow is contained within the\n.global\nwindow.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nNote\nispacep.shared::cluster\nwill return 1 for every shared memory address that is accessible to\nthe threads in the cluster, whereas\nispacep.shared::cta\nwill return 1 only if the address is\nof a variable declared in the executing CTA.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nisspacep.const\nintroduced in PTX ISA version 3.1.\nisspacep.param\nintroduced in PTX ISA version 7.7.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::entry\non\n.param\nspace introduced in PTX ISA version 8.3.\nTarget ISA Notes\nisspacep\nrequires\nsm_20\nor higher.\nisspacep.param{::entry}\nrequires\nsm_70\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nExamples\nisspacep.const           iscnst, cptr;\nisspacep.global          isglbl, gptr;\nisspacep.local           islcl,  lptr;\nisspacep.shared          isshrd, sptr;\nisspacep.param::entry    isparam, pptr;\nisspacep.shared::cta     isshrdcta, sptr;\nisspacep.shared::cluster ishrdany sptr;\n9.7.9.20.\nData Movement and Conversion Instructions:\ncvta\n\ncvta\nConvert address from\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space to generic, or vice-versa. Take the generic address of a variable declared in\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space.\nSyntax\n// convert const, global, local, or shared address to generic address\ncvta.space.size  p, a;        // source address in register a\ncvta.space.size  p, var;      // get generic address of var\ncvta.space.size  p, var+imm;  // generic address of var+offset\n\n// convert generic address to const, global, local, or shared address\ncvta.to.space.size  p, a;\n\n.space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\n.size  = { .u32, .u64 };\nDescription\nConvert a\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\naddress to a generic address, or vice-versa. The\nsource and destination addresses must be the same size. Use\ncvt.u32.u64\nor\ncvt.u64.u32\nto\ntruncate or zero-extend addresses.\nFor variables declared in\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space, the generic address of the variable may be taken using\ncvta\n. The source is either a\nregister or a variable defined in\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\nmemory\nwith an optional offset.\nWhen converting a generic address into a\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\naddress, the resulting address is undefined in cases where the generic address does not fall within\nthe address window of the specified state space. A program may use\nisspacep\nto guard against\nsuch incorrect behavior.\nFor\ncvta\nwith\n.shared\nstate space, the address must belong to the space specified by\n::cta\nor\n::cluster\nsub-qualifier, otherwise the behavior is undefined. If no sub-qualifier\nis specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nIf\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::entry\n. For\n.param{::entry}\nstate space, operand\na\nmust be a kernel parameter address, otherwise\nbehavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\ncvta.const\nand\ncvta.to.const\nintroduced in PTX ISA version 3.1.\ncvta.param\nand\ncvta.to.param\nintroduced in PTX ISA version 7.7.\nNote:\nThe current implementation does not allow generic pointers to\nconst\nspace variables in\nprograms that contain pointers to constant buffers passed as kernel parameters.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::entry\non\n.param\nspace introduced in PTX ISA version 8.3.\nTarget ISA Notes\ncvta\nrequires\nsm_20\nor higher.\ncvta.param{::entry}\nand\ncvta.to.param{::entry}\nrequires\nsm_70\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nExamples\ncvta.const.u32   ptr,cvar;\ncvta.local.u32   ptr,lptr;\ncvta.shared::cta.u32  p,As+4;\ncvta.shared::cluster.u32 ptr, As;\ncvta.to.global.u32  p,gptr;\ncvta.param.u64   ptr,pvar;\ncvta.to.param::entry.u64  epptr, ptr;\n9.7.9.21.\nData Movement and Conversion Instructions:\ncvt\n\ncvt\nConvert a value from one type to another.\nSyntax\ncvt{.irnd}{.ftz}{.sat}.dtype.atype         d, a;  // integer rounding\ncvt{.frnd}{.ftz}{.sat}.dtype.atype         d, a;  // fp rounding\n\ncvt.frnd2{.relu}{.satfinite}.f16.f32       d, a;\ncvt.frnd2{.relu}{.satfinite}.f16x2.f32     d, a, b;\ncvt.rs{.relu}{.satfinite}.f16x2.f32        d, a, b, rbits;\n\ncvt.frnd2{.relu}{.satfinite}.bf16.f32      d, a;\ncvt.frnd2{.relu}{.satfinite}.bf16x2.f32    d, a, b;\ncvt.rs{.relu}{.satfinite}.bf16x2.f32       d, a, b, rbits;\n\ncvt.rna{.satfinite}.tf32.f32               d, a;\ncvt.frnd2{.satfinite}{.relu}.tf32.f32      d, a;\n\ncvt.rn.satfinite{.relu}.f8x2type.f32       d, a, b;\ncvt.rn.satfinite{.relu}.f8x2type.fp16x2    d, a;\ncvt.rn.{.relu}.f16x2.f8x2type              d, a;\ncvt.rs{.relu}.satfinite.f8x4type.f32       d, {a, b, e, f}, rbits;\n\ncvt.rn.satfinite{.relu}.f4x2type.f32        d, a, b;\ncvt.rn.satfinite{.relu}.f4x2type.fp16x2type d, a;\ncvt.rn{.relu}.f16x2.f4x2type                d, a;\ncvt.rs{.relu}.satfinite.f4x4type.f32        d, {a, b, e, f}, rbits;\n\ncvt.rn.satfinite{.relu}.f6x2type.f32        d, a, b;\ncvt.rn.satfinite{.relu}.f6x2type.fp16x2type d, a;\ncvt.rn{.relu}.f16x2.f6x2type                d, a;\ncvt.rs{.relu}.satfinite.f6x4type.f32        d, {a, b, e, f}, rbits;\n\ncvt.frnd3{.satfinite}.ue8m0x2.f32          d, a, b;\ncvt.frnd3{.satfinite}.ue8m0x2.bf16x2       d, a;\ncvt.rn.bf16x2.ue8m0x2                      d, a;\n\ncvt.rn.satfinite{.relu}{.scaled::n2::ue8m0}.s2f6x2.f32      d, a, b{, scale-factor};\ncvt.rn.satfinite{.relu}{.scaled::n2::ue8m0}.s2f6x2.bf16x2   d, a{, scale-factor};\ncvt.rn{.satfinite}{.relu}{.scaled::n2::ue8m0}.bf16x2.s2f6x2 d, a{, scale-factor};\n\n.irnd   = { .rni, .rzi, .rmi, .rpi };\n.frnd   = { .rn,  .rz,  .rm,  .rp  };\n.frnd2  = { .rn,  .rz };\n.frnd3  = { .rz,  .rp };\n.dtype = .atype = { .u8,   .u16, .u32, .u64,\n                    .s8,   .s16, .s32, .s64,\n                    .bf16, .f16, .f32, .f64 };\n.f8x2type = { .e4m3x2, .e5m2x2 };\n.f4x2type = { .e2m1x2 };\n.f6x2type = { .e2m3x2, .e3m2x2 };\n.f4x4type = { .e2m1x4 };\n.f8x4type = { .e4m3x4, .e5m2x4 };\n.f6x4type = { .e2m3x4, .e3m2x4 };\n.fp16x2type = { .f16x2, .bf16x2 };\nDescription\nConvert between different types and sizes.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, two inputs\na\nand\nb\nof\n.f32\ntype are\nconverted into\n.f16\nor\n.bf16\ntype and the converted values are packed in the destination\nregister\nd\n, such that the value converted from input\na\nis stored in the upper half of\nd\nand the value converted from input\nb\nis stored in the lower half of\nd\nFor\n.f16x2\ninstruction type, destination operand\nd\nhas\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operand\nd\nhas\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperand\nd\nhas\n.b32\ntype. For\n.tf32\ninstruction type, operand\nd\nhas\n.b32\ntype.\nWhen converting to\n.e4m3x2\n/\n.e5m2x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When converting two\n.f32\ninputs to\n.e4m3x2\n/\n.e5m2x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nand the value converted from\ninput\nb\nis stored in the lower 8 bits of\nd\n. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e4m3x2\n/\n.e5m2x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified\nformat. The converted values are packed in the destination operand\nd\nsuch that the value\nconverted from the upper 16 bits of input\na\nis stored in the upper 8 bits of\nd\nand the value\nconverted from the lower 16 bits of input\na\nis stored in the lower 8 bits of\nd\n.\nWhen converting from\n.e4m3x2\n/\n.e5m2x2\nto\n.f16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit input value in operand\na\nis converted to\n.f16\ntype. The converted values\nare packed in the destination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in the upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the lower 16 bits of\nd\n.\nWhen converting to\n.e2m1x2\ndata formats, the destination operand\nd\nhas\n.b8\ntype.\nWhen converting two\n.f32\ninputs to\n.e2m1x2\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom input\na\nis stored in the upper 4 bits of\nd\nand the value converted from input\nb\nis\nstored in the lower 4 bits of\nd\n. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e2m1x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified format.\nThe converted values are packed in\nd\nso that the value from the upper 16 bits of\na\nis stored\nin the upper 4 bits of\nd\n, and the value from the lower 16 bits of\na\nis stored in the lower 4\nbits of\nd\n.\nWhen converting from\n.e2m1x2\nto\n.f16x2\n, source operand\na\nhas\n.b8\ntype. Each 4-bit\ninput value in operand\na\nis converted to\n.f16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 4 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 4 bits of\na\nis stored in the\nlower 16 bits of\nd\n.\nWhen converting to\n.e2m1x4\ndata format, the destination operand\nd\nhas\n.b16\ntype. When\nconverting four\n.f32\ninputs to\n.e2m1x4\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom inputs\na\n,\nb\n,\ne\n,\nf\nare stored in each 4 bits starting from upper bits of\nd\n.\nWhen converting to\n.e2m3x2\n/\n.e3m2x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When converting two\n.f32\ninputs to\n.e2m3x2\n/\n.e3m2x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nwith 2 MSB bits padded with\nzeros and the value converted from input\nb\nis stored in the lower 8 bits of\nd\nwith 2 MSB bits\npadded with zeros. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e2m3x2\n/\n.e3m2x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified format. The converted\nvalues are packed in the destination operand\nd\nso that the value from the upper 16 bits of input\na\nis stored in the upper 8 bits of\nd\nwith 2 MSB bits padded with zeros, and the value from\nthe lower 16 bits of input\na\nis stored in the lower 8 bits of\nd\nwith 2 MSB bits padded with\nzeros.\nWhen converting from\n.e2m3x2\n/\n.e3m2x2\nto\n.f16x2\n, source operand\na\nhas\n.b16\ntype.\nEach 8-bit input value with 2 MSB bits 0 in operand\na\nis converted to\n.f16\ntype. The converted\nvalues are packed in the destination operand\nd\nsuch that the value converted from the upper 8 bits\nof\na\nis stored in the upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the lower 16 bits of\nd\n.\nWhen converting to\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\ndata format, the destination\noperand\nd\nhas\n.b32\ntype. When converting four\n.f32\ninputs to\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom inputs\na\n,\nb\n,\ne\n,\nf\nare stored in each 8 bits starting from upper bits of\nd\n.\nFor\n.e3m2x4\n/\n.e2m3x4\n, each 8-bit output will have 2 MSB bits padded with zeros.\nWhen converting to\n.ue8m0x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When\nconverting two\n.f32\nor two packed\n.bf16\ninputs to\n.ue8m0x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nand the value converted from\ninput\nb\nis stored in the lower 8 bits of\nd\n.\nWhen converting from\n.ue8m0x2\nto\n.bf16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit\ninput value in operand\na\nis converted to\n.bf16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the\nlower 16 bits of\nd\n.\nWhen converting to\n.s2f6x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When\nconverting two\n.f32\ninputs to\n.s2f6x2\n, each input is converted to the\n.s2f6\nvalue, and\nthe converted values are packed in the destination operand\nd\nsuch that the value converted from\ninput\na\nis stored in the upper 8 bits of\nd\nand the value converted from input\nb\nis\nstored in the lower 8 bits of\nd\n. When converting from\n.bf16x2\n, each input packed in operand\na\nis converted to\n.s2f6\nvalue and converted values are packed in the destination operand\nd\nsuch that value converted from upper 8 bits of input\na\nis stored in upper 8 bits of\noperand\nd\nand the value converted from lower 8 bits of input\na\nis stored in lower 8 bits of\noperand\nd\n. Optional operand\nscale-factor\nhas type\n.b16\nand stores two packed scaling\nfactors of type\n.ue8m0\n. For down conversion, inputs are divided by\nscale-factor\nand then\nconversion is performed. The scaling factor of input\na\n/\n.bf16\ninput from upper 16 bits of\na\nis stored in the upper 8 bits of operand\nscale-factor\nand the scaling factor of input\nb\n/\n.bf16\ninput from lower 16 bits of\na\nis stored in the lower 8 bits of operand\nscale-factor\n.\nIf not specified explicitly, value of\nscale-factor\nwill be assumed to be\n0x7f7f\n,\nthat is, assumed as value\n1\nfor both input scale factors.\nWhen converting from\n.s2f6x2\nto\n.bf16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit\ninput value in operand\na\nis converted to\n.bf16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the\nlower 16 bits of\nd\n. Optional operand\nscale-factor\nhas type\n.b16\nand stores two packed\nscaling factors of type\n.ue8m0\n. For up-conversion, inputs are converted to destination type and\nthen multiplied by\nscale-factor\n. The scaling factor of\n.s2f6\ninput from upper 8 bits of\na\nis stored in the upper 8 bits of operand\nscale-factor\nand the scaling factor of\n.s2f6\ninput\nfrom lower 8 bits of\na\nis stored in the lower 8 bits of operand\nscale-factor\n.\nIf not specified explicitly, value of\nscale-factor\nwill be assumed to\nbe\n0x7f7f\n, that is, assumed as value\n1\nfor both input scale factors.\nOptional qualifier\n.scaled::n2::ue8m0\nspecifies that the instruction uses packed\nscale-factor with 2 scale values of\nue8m0\ntype. Operand\nscale-factor\nand qualifier\n.scaled::n2::ue8m0\nmust be used together.\nrbits\nis a\n.b32\ntype register operand used for providing random bits for\n.rs\nrounding mode.\nWhen converting to\n.f16x2\n, two 16-bit values are provided from\nrbits\nwhere 13 LSBs from\nupper 16-bits are used as random bits for operand\na\nwith 3 MSBs are 0 and 13 LSBs from lower\n16-bits are used as random bits for operand\nb\nwith 3 MSBs are 0.\nWhen converting to\n.bf16x2\n, two 16-bit values are provided from\nrbits\nwhere upper 16-bits\nare used as random bits for operand\na\nand lower 16-bits are used as random bits for operand\nb\n.\nWhen converting to\n.e4m3x4\n/\n.e5m2x4\n/\n.e2m3x4\n/\n.e3m2x4\n, two 16-bit values are provided\nfrom\nrbits\nwhere lower 16-bits are used for operands\ne\n,\nf\nand upper 16 bits are used\nfor operands\na\n,\nb\n.\nWhen converting to\n.e2m1x4\n, two 16-bit values are provided from\nrbits\nwhere lower 8-bits\nfrom both 16-bits half of\nrbits\nare used for operands\ne\n,\nf\nand upper 8-bits from both\n16-bits half of\nrbits\nare used for operands\na\n,\nb\n.\nRounding modifier is mandatory in all of the following cases:\nfloat-to-float conversions, when destination type is smaller than source type\nAll float-to-int conversions\nAll int-to-float conversions\nAll conversions involving\n.f16x2\n,\n.e4m3x2,\n.e5m2x2,\n,\n.bf16x2\n,\n.tf32\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\nand\n.ue8m0x2\ninstruction types.\n.satfinite\nmodifier is only supported for conversions involving the following types:\n.e4m3x2\n,\n.e5m2x2\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\ndestination types.\n.satfinite\nmodifier is mandatory for such conversions.\n.f16\n,\n.bf16\n,\n.f16x2\n,\n.bf16x2\n,\n.tf32\n,\n.ue8m0x2\nas destination types.\nSemantics\nif (/* inst type is .f16x2 or .bf16x2 */) {\n    d[31:16] = convert(a);\n    d[15:0]  = convert(b);\n} else if (/* inst destination type is .e5m2x2 or .e4m3x2 or .ue8m0x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:8] = convert(a);\n        d[7:0]  = convert(b);\n    } else {\n        d[15:8] = convert(a[31:16]);\n        d[7:0]  = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .s2f6x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:8] = convert(a / scale-factor[15:8]);\n        d[7:0]  = convert(b / scale-factor[7:0]);\n    } else {\n        d[15:8] = convert(a[15:8] / scale-factor[15:8]);\n        d[7:0]  = convert(a[7:0] / scale-factor[7:0]);\n    }\n} else if (/* inst source type is .s2f6x2 */) {\n        d[31:16] = convert(a[15:8]) * scale-factor[15:8];\n        d[15:0]  = convert(a[7:0]) * scale-factor[7:0];\n} else if (/* inst destination type is .e2m1x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[7:4] = convert(a);\n        d[3:0] = convert(b);\n    } else {\n        d[7:4] = convert(a[31:16]);\n        d[3:0]  = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .e2m3x2 or .e3m2x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:14] = 0;\n        d[13:8] = convert(a);\n        d[7:6] = 0;\n        d[5:0] = convert(b);\n    } else {\n        d[15:14] = 0;\n        d[13:8] = convert(a[31:16]);\n        d[7:6] = 0;\n        d[5:0] = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .e2m1x4 */) {\n    d[15:12] = convert(a);\n    d[11:8] = convert(b);\n    d[7:4] = convert(e);\n    d[3:0] = convert(f);\n} else if (/* inst destination type is .e4m3x4 or .e5m2x4 */) {\n    d[31:24] = convert(a);\n    d[23:16] = convert(b);\n    d[15:8] = convert(e);\n    d[7:0] = convert(f);\n} else if (/* inst destination type is .e2m3x4 or .e3m2x4 */) {\n    d[31:30] = 0;\n    d[29:24] = convert(a);\n    d[23:22] = 0;\n    d[21:16] = convert(b);\n    d[15:14] = 0;\n    d[13:8] = convert(e);\n    d[7:6] = 0;\n    d[5:0] = convert(f);\n} else {\n    d = convert(a);\n}\n// Random bits\nrbits\nsemantics for\n.rs\nrounding:\nDestination type\n.f16\n:\nRefer\nFigure 38\nfor random bits layout details.\nFigure 38\nRandom bits layout for\n.rs\nrounding with\n.f16\ndestination type\n\nDestination type\n.bf16\n:\nRefer\nFigure 39\nfor random bits layout details.\nFigure 39\nRandom bits layout for\n.rs\nrounding with\n.bf16\ndestination type\n\nDestination type\n.e2m1x4\n:\nRefer\nFigure 40\nfor random bits layout details.\nFigure 40\nRandom bits layout for\n.rs\nrounding with\n.e2m1x4\ndestination type\n\nDestination type\n.e5m2x4\n,\n.e4m3x4\n,\n.e3m2x4\n,\n.e2m3x4\n:\nRefer\nFigure 41\nfor random bits layout details.\nFigure 41\nRandom bits layout for\n.rs\nrounding with\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\ndestination type\n\nInteger Notes\nInteger rounding is required for float-to-integer conversions, and for same-size float-to-float\nconversions where the value is rounded to an integer. Integer rounding is illegal in all other\ninstances.\nInteger rounding modifiers:\n.rni\nround to nearest integer, choosing even integer if source is equidistant between two integers\n.rzi\nround to nearest integer in the direction of zero\n.rmi\nround to nearest integer in direction of negative infinity\n.rpi\nround to nearest integer in direction of positive infinity\nIn float-to-integer conversions, depending upon conversion types,\nNaN\ninput results in following\nvalue:\nZero if source is not\n.f64\nand destination is not\n.s64\n,\n.u64\n.\nOtherwise 1 << (BitWidth(dst) - 1) corresponding to the value of (\nMAXINT\n>> 1) + 1 for unsigned type\nor\nMININT\nfor signed type.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nFor\ncvt.ftz.dtype.f32\nfloat-to-integer conversions and\ncvt.ftz.f32.f32\nfloat-to-float\nconversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier\n.ftz\ncan only be specified when either\n.dtype\nor\n.atype\nis\n.f32\nand applies only\nto single precision (\n.f32\n) inputs and results.\nsm_1x\nFor\ncvt.ftz.dtype.f32\nfloat-to-integer conversions and\ncvt.ftz.f32.f32\nfloat-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving\nzero. The optional\n.ftz\nmodifier may be specified in these cases for clarity.\nNote:\nIn PTX ISA versions 1.4 and earlier, the\ncvt\ninstruction did not flush single-precision\nsubnormal inputs or results to zero if the destination type size was 64-bits. The compiler will\npreserve this behavior for legacy PTX code.\nSaturation modifier:\n.sat\nFor integer destination types,\n.sat\nlimits the result to\nMININT..MAXINT\nfor the size of\nthe operation. Note that saturation applies to both signed and unsigned integer types.\nThe saturation modifier is allowed only in cases where the destination type’s value range is not\na superset of the source type’s value range; i.e., the\n.sat\nmodifier is illegal in cases\nwhere saturation is not possible based on the source and destination types.\nFor float-to-integer conversions, the result is clamped to the destination range by default; i.e,\n.sat\nis redundant.\nFloating Point Notes\nFloating-point rounding is required for float-to-float conversions that result in loss of precision,\nand for integer-to-float conversions. Floating-point rounding is illegal in all other instances.\nFloating-point rounding modifiers:\n.rn\nrounding to nearest, with ties to even\n.rna\nrounding to nearest, with ties away from zero\n.rz\nrounding toward zero\n.rm\nrounding toward negative infinity\n.rp\nrounding toward positive infinity\n.rs\nStochastic rounding is achieved through the use of the supplied random bits. Operation’s result\nis rounded in the direction toward zero or away from zero based on the carry out of the integer\naddition of the supplied random bits (\nrbits\n) to the truncated off (discarded) bits of\nmantissa from the input.\nA floating-point value may be rounded to an integral value using the integer rounding modifiers (see\nInteger Notes). The operands must be of the same size. The result is an integral value, stored in\nfloating-point format.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported. Modifier\n.ftz\nmay be specified to flush\nsingle-precision subnormal inputs and results to sign-preserving zero. Modifier\n.ftz\ncan only\nbe specified when either\n.dtype\nor\n.atype\nis\n.f32\nand applies only to single\nprecision (\n.f32\n) inputs and results.\nsm_1x\nSingle-precision subnormal inputs and results are flushed to sign-preserving zero. The optional\n.ftz\nmodifier may be specified in these cases for clarity.\nNote:\nIn PTX ISA versions 1.4 and earlier, the\ncvt\ninstruction did not flush\nsingle-precision subnormal inputs or results to zero if either source or destination type was\n.f64\n. The compiler will preserve this behavior for legacy PTX code. Specifically, if the PTX\nISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to\nsign-preserving zero only for\ncvt.f32.f16\n,\ncvt.f16.f32\n, and\ncvt.f32.f32\ninstructions.\nSaturation modifier:\n.sat\n:\nFor floating-point destination types,\n.sat\nlimits the result to the range [0.0, 1.0].\nNaN\nresults are flushed to positive zero. Applies to\n.f16\n,\n.f32\n, and\n.f64\ntypes.\n.relu\n:\nFor\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.e4m3x2\n,\n.e5m2x2\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\nand\n.tf32\ndestination types,\n.relu\nclamps the result to 0 if negative.\nNaN\nresults are converted\nto canonical\nNaN\n.\n.satfinite\n:\nFor\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.e4m3x2\n,\n.e5m2x2\n,\n.ue8m0x2\n,\n.e4m3x4\n,\n.e5m2x4\nand\n.tf32\ndestination formats, if the input value is\nNaN\n, then the result is\nNaN\nin the specified destination format. For\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\ndestination formats\nNaN\nresults are converted to positive\nMAX_NORM\n.\nIf the absolute value of input (ignoring sign) is greater than\nMAX_NORM\nof the specified destination\nformat, then the result is sign-preserved\nMAX_NORM\nof the destination format and a positive\nMAX_NORM\nin\n.ue8m0x2\nfor which the destination sign is not supported.\nNotes\nA source register wider than the specified type may be used, except when the source operand has\n.bf16\nor\n.bf16x2\nformat. The lower\nn\nbits corresponding to the instruction-type width\nare used in the conversion. See\nOperand Size Exceeding Instruction-Type Size\nfor a description of these relaxed\ntype-checking rules.\nA destination register wider than the specified type may be used, except when the destination\noperand has\n.bf16\n,\n.bf16x2\nor\n.tf32\nformat. The result of conversion is sign-extended to\nthe destination register width for signed integers, and is zero-extended to the destination register\nwidth for unsigned, bit-size, and floating-point types. See\nOperand Size Exceeding Instruction-Type Size\nfor a description of these relaxed\ntype-checking rules.\nFor\ncvt.f32.bf16\n,\nNaN\ninput yields unspecified\nNaN\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\n.relu\nmodifier and {\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.tf32\n} destination formats\nintroduced in PTX ISA version 7.0.\ncvt.f32.bf16\nintroduced in PTX ISA version 7.1.\ncvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}\n,\ncvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16\n, and\ncvt.tf32.f32.{relu}.{rn/rz}\nintroduced\nin PTX ISA version 7.8.\n.ftz\nqualifier for\ncvt.f32.bf16\nintroduced in PTX ISA version 7.8.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nfor\nsm_90\nor higher introduced in PTX ISA version 7.8.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nfor\nsm_90\nor higher introduced in PTX ISA version 7.8.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nfor\nsm_89\nintroduced in PTX ISA version 8.1.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nfor\nsm_89\nintroduced in PTX ISA version 8.1.\ncvt.satfinite.{f16,\nbf16,\nf16x2,\nbf16x2,\ntf32}.f32\nintroduced in PTX ISA version 8.1.\ncvt.{rn/rz}.satfinite.tf32.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}\nintroduced in PTX ISA version 8.6.\ncvt.{rp/rz}{.satfinite}{.relu}.ue8m0x2.bf16x2\nintroduced in PTX ISA version 8.6.\ncvt.{rz/rp}.satfinite.ue8m0x2.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn.bf16x2.ue8m0x2\nintroduced in PTX ISA version 8.6.\n.rs\nrounding mode introduced in PTX ISA version 8.7.\ncvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32\nintroduced in PTX ISA version 8.7.\ncvt.rn.satfinite{.relu}{.e5m2x2/.e4m3x2}{.bf16x2}\nintroduced in PTX ISA version 9.1.\ncvt.rn.satfinite{.relu}{.e2m3x2/.e3m2x2/.e2m1x2}{.f16x2/.bf16x2}\nintroduced in PTX ISA\nversion 9.1.\ncvt\nwith\n.s2f6x2\ninstruction type introduced in PTX ISA version 9.1.\nTarget ISA Notes\ncvt\nto or from\n.f64\nrequires\nsm_13\nor higher.\n.relu\nmodifier and {\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.tf32\n} destination formats require\nsm_80\nor higher.\ncvt.f32.bf16\nrequires\nsm_80\nor higher.\ncvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}\n,\ncvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16\n, and\ncvt.tf32.f32.{relu}.{rn/rz}\nrequire\nsm_90\nor higher.\n.ftz\nqualifier for\ncvt.f32.bf16\nrequires\nsm_90\nor higher.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nrequires\nsm89\nor higher.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nrequires\nsm_89\nor higher.\ncvt.{rn/rz}.satfinite.tf32.f32\nrequires\nsm_100\nor higher.\ncvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.{rz/rp}{.satfinite}{.relu}.ue8m0x2.bf16x2\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.{rz/rp}.satfinite.ue8m0x2.f32\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.rn.bf16x2.ue8m0x2\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.rs\nrounding mode is supported on following architectures:\nsm_100a\nsm_103a\ncvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32\nis supported on following architectures:\nsm_100a\nsm_103a\ncvt.rn.satfinite{.relu}{.e5m2x2/.e4m3x2}{.bf16x2}\nis supported on following family-specific\narchitectures:\nsm_100f\nor higher in the same family\nsm_110f\nor higher in the same family\nsm_120f\nor higher in the same family\ncvt.rn.satfinite{.relu}{.e2m3x2/.e3m2x2/.e2m1x2}{.f16x2/.bf16x2}\nis supported on following\nfamily-specific architectures:\nsm_100f\nor higher in the same family\nsm_110f\nor higher in the same family\nsm_120f\nor higher in the same family\ncvt\nwith\n.s2f6x2\ninstruction type is supported on following architectures:\nsm_100a\nsm_103a\nsm_110a\nsm_120a\nsm_121a\nExamples\ncvt.f32.s32 f,i;\ncvt.s32.f64 j,r;     // float-to-int saturates by default\ncvt.rni.f32.f32 x,y; // round to nearest int, result is fp\ncvt.f32.f32 x,y;     // note .ftz behavior for sm_1x targets\ncvt.rn.relu.f16.f32      b, f;        // result is saturated with .relu saturation mode\ncvt.rz.f16x2.f32         b1, f, f1;   // convert two fp32 values to packed fp16 outputs\ncvt.rn.relu.satfinite.f16x2.f32    b1, f, f1;   // convert two fp32 values to packed fp16 outputs with .relu saturation on each output\ncvt.rn.bf16.f32          b, f;        // convert fp32 to bf16\ncvt.rz.relu.satfinite.bf16.f3 2    b, f;        // convert fp32 to bf16 with .relu and .satfinite saturation\ncvt.rz.satfinite.bf16x2.f32        b1, f, f1;   // convert two fp32 values to packed bf16 outputs\ncvt.rn.relu.bf16x2.f32   b1, f, f1;   // convert two fp32 values to packed bf16 outputs with .relu saturation on each output\ncvt.rna.satfinite.tf32.f32         b1, f;       // convert fp32 to tf32 format\ncvt.rn.relu.tf32.f32     d, a;        // convert fp32 to tf32 format\ncvt.f64.bf16.rp          f, b;        // convert bf16 to f64 format\ncvt.bf16.f16.rz          b, f         // convert f16 to bf16 format\ncvt.bf16.u64.rz          b, u         // convert u64 to bf16 format\ncvt.s8.bf16.rpi          s, b         // convert bf16 to s8 format\ncvt.bf16.bf16.rpi        b1, b2       // convert bf16 to corresponding int represented in bf16 format\ncvt.rn.satfinite.e4m3x2.f32 d, a, b;  // convert a, b to .e4m3 and pack as .e4m3x2 output\ncvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu\n                                         // saturation on each output and pack as .e5m2x2\ncvt.rn.f16x2.e4m3x2 d, a;             // unpack a, convert two .e4m3 values to packed f16x2 output\ncvt.rn.satfinite.tf32.f32 d, a;       // convert fp32 to tf32 format\ncvt.rn.relu.f16x2.e2m1x2 d, a;        // unpack a, convert two .e2m1 values to packed f16x2 output\ncvt.rn.satfinite.e2m3x2.f32 d, a, b;  // convert a, b to .e2m3 and pack as .e2m3x2 output\ncvt.rn.relu.f16x2.e3m2x2 d, a;        // unpack a, convert two .e3m2 values to packed f16x2 output\n\ncvt.rs.f16x2.f32    d, a, b, rbits;  // convert 2 fp32 values to packed fp16 with applying .rs rounding\ncvt.rs.satfinite.e2m1x4.f32  d, {a, b, e, f}, rbits; // convert 4 fp32 values to packed 4 e2m1 values with applying .rs rounding\n\ncvt.rn.satfinite.relu.e2m1x2type.f16x2  d, a; // unpack a and covert to two .e2m1 values\ncvt.rn.satfinite.e2m3x2type.bf16x2  d, a; // unpack a and covert to two .e2m3 values\n\n// Convert 2 f32 values to s2f6 after applying scale factor for dividing the value\ncvt.rn.satfinite.scaled::n2::ue8m0.s2f6x2.f32 d, a, b, scale-factor;\n9.7.9.22.\nData Movement and Conversion Instructions:\ncvt.pack\n\ncvt.pack\nConvert two integer values from one integer type to another and pack the results.\nSyntax\ncvt.pack.sat.convertType.abType  d, a, b;\n    .convertType  = { .u16, .s16 }\n    .abType       = { .s32 }\n\ncvt.pack.sat.convertType.abType.cType  d, a, b, c;\n    .convertType  = { .u2, .s2, .u4, .s4, .u8, .s8 }\n    .abType       = { .s32 }\n    .cType        = { .b32 }\nDescription\nConvert two 32-bit integers\na\nand\nb\ninto specified type and pack the results into\nd\n.\nDestination\nd\nis an unsigned 32-bit integer. Source operands\na\nand\nb\nare integers of\ntype\n.abType\nand the source operand\nc\nis an integer of type\n.cType\n.\nThe inputs\na\nand\nb\nare converted to values of type specified by\n.convertType\nwith\nsaturation and the results after conversion are packed into lower bits of\nd\n.\nIf operand\nc\nis specified then remaining bits of\nd\nare copied from lower bits of\nc\n.\nSemantics\nta = a < MIN(convertType) ? MIN(convertType) : a;\nta = a > MAX(convertType) ? MAX(convertType) : a;\ntb = b < MIN(convertType) ? MIN(convertType) : b;\ntb = b > MAX(convertType) ? MAX(convertType) : b;\n\nsize = sizeInBits(convertType);\ntd = tb ;\nfor (i = size; i <= 2 * size - 1; i++) {\n    td[i] = ta[i - size];\n}\n\nif (isU16(convertType) || isS16(convertType)) {\n    d = td;\n} else {\n    for (i = 0; i < 2 * size; i++) {\n        d[i] = td[i];\n    }\n    for (i = 2 * size; i <= 31; i++) {\n        d[i] = c[i - 2 * size];\n    }\n}\n.sat\nmodifier limits the converted values to\nMIN(convertType)\n..\nMAX(convertedType)\n(no\noverflow) if the corresponding inputs are not in the range of datatype specified as\n.convertType\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nTarget ISA Notes\nRequires\nsm_72\nor higher.\nSub byte types (\n.u4\n/\n.s4\nand\n.u2\n/\n.s2\n) requires\nsm_75\nor higher.\nExamples\ncvt.pack.sat.s16.s32      %r1, %r2, %r3;           // 32-bit to 16-bit conversion\ncvt.pack.sat.u8.s32.b32   %r4, %r5, %r6, 0;        // 32-bit to 8-bit conversion\ncvt.pack.sat.u8.s32.b32   %r7, %r8, %r9, %r4;      // %r7 = { %r5, %r6, %r8, %r9 }\ncvt.pack.sat.u4.s32.b32   %r10, %r12, %r13, %r14;  // 32-bit to 4-bit conversion\ncvt.pack.sat.s2.s32.b32   %r15, %r16, %r17, %r18;  // 32-bits to 2-bit conversion\n9.7.9.23.\nData Movement and Conversion Instructions:\nmapa\n\nmapa\nMap the address of the shared variable in the target CTA.\nSyntax\nmapa{.space}.type          d, a, b;\n\n// Maps shared memory address in register a into CTA b.\nmapa.shared::cluster.type  d, a, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh + imm, b;\n\n// Maps generic address in register a into CTA b.\nmapa.type                  d, a, b;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 }\nDescription\nGet address in the CTA specified by operand\nb\nwhich corresponds to the address specified by\noperand\na\n.\nInstruction type\n.type\nindicates the type of the destination operand\nd\nand the source\noperand\na\n.\nWhen space is\n.shared::cluster\n, source\na\nis either a shared memory variable or a register\ncontaining a valid shared memory address and register\nd\ncontains a shared memory address. When\nthe optional qualifier\n.space\nis not specified, both\na\nand\nd\nare registers containing\ngeneric addresses pointing to shared memory.\nb\nis a 32-bit integer operand representing the rank of the target CTA.\nDestination register\nd\nwill hold an address in CTA\nb\ncorresponding to operand\na\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmapa.shared::cluster.u64 d1, %reg1, cta;\nmapa.shared::cluster.u32 d2, sh, 3;\nmapa.u64                 d3, %reg2, cta;\n9.7.9.24.\nData Movement and Conversion Instructions:\ngetctarank\n\ngetctarank\nGenerate the CTA rank of the address.\nSyntax\ngetctarank{.space}.type d, a;\n\n// Get cta rank from source shared memory address in register a.\ngetctarank.shared::cluster.type d, a;\n\n// Get cta rank from shared memory variable.\ngetctarank.shared::cluster.type d, var;\n\n// Get cta rank from shared memory variable+offset.\ngetctarank.shared::cluster.type d, var + imm;\n\n// Get cta rank from generic address of shared memory variable in register a.\ngetctarank.type d, a;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 }\nDescription\nWrite the destination register\nd\nwith the rank of the CTA which contains the address specified\nin operand\na\n.\nInstruction type\n.type\nindicates the type of source operand\na\n.\nWhen space is\n.shared::cluster\n, source\na\nis either a shared memory variable or a register\ncontaining a valid shared memory address. When the optional qualifier\n.space\nis not specified,\na\nis a register containing a generic addresses pointing to shared memory. Destination\nd\nis\nalways a 32-bit register which holds the rank of the CTA.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ngetctarank.shared::cluster.u32 d1, addr;\ngetctarank.shared::cluster.u64 d2, sh + 4;\ngetctarank.u64                 d3, src;\n9.7.9.25.\nData Movement and Conversion Instructions: Asynchronous copy\n\nAn asynchronous copy operation performs the underlying operation asynchronously in the background,\nthus allowing the issuing threads to perform subsequent tasks.\nAn asynchronous copy operation can be a\nbulk\noperation that operates on a large amount of data, or\na\nnon-bulk\noperation that operates on smaller sized data. The amount of data handled by a bulk\nasynchronous operation must be a multiple of 16 bytes.\nAn asynchronous copy operation typically includes the following sequence:\nOptionally, reading from the tensormap.\nReading data from the source location(s).\nWriting data to the destination location(s).\nWrites being made visible to the executing thread or other threads.\n9.7.9.25.1.\nCompletion Mechanisms for Asynchronous Copy Operations\n\nA thread must explicitly wait for the completion of an asynchronous copy operation in order to\naccess the result of the operation. Once an asynchronous copy operation is initiated, modifying the\nsource memory location or tensor descriptor or reading from the destination memory location before\nthe asynchronous operation completes, exhibits undefined behavior.\nThis section describes two asynchronous copy operation completion mechanisms supported in PTX:\nAsync-group mechanism and mbarrier-based mechanism.\nAsynchronous operations may be tracked by either of the completion mechanisms or both mechanisms.\nThe tracking mechanism is instruction/instruction-variant specific.\n9.7.9.25.1.1.\nAsync-group mechanism\n\nWhen using the async-group completion mechanism, the issuing thread specifies a group of\nasynchronous operations, called\nasync-group\n, using a\ncommit\noperation and tracks the completion\nof this group using a\nwait\noperation. The thread issuing the asynchronous operation must create\nseparate\nasync-groups\nfor bulk and non-bulk asynchronous operations.\nA\ncommit\noperation creates a per-thread\nasync-group\ncontaining all prior asynchronous operations\ntracked by\nasync-group\ncompletion and initiated by the executing thread but none of the asynchronous\noperations following the commit operation. A committed asynchronous operation belongs to a single\nasync-group\n.\nWhen an\nasync-group\ncompletes, all the asynchronous operations belonging to that group are\ncomplete and the executing thread that initiated the asynchronous operations can read the result of\nthe asynchronous operations. All\nasync-groups\ncommitted by an executing thread always complete in\nthe order in which they were committed. There is no ordering between asynchronous operations within\nan\nasync-group\n.\nA typical pattern of using\nasync-group\nas the completion mechanism is as follows:\nInitiate the asynchronous operations.\nGroup the asynchronous operations into an\nasync-group\nusing a\ncommit\noperation.\nWait for the completion of the async-group using the wait operation.\nOnce the\nasync-group\ncompletes, access the results of all asynchronous operations in that\nasync-group\n.\n9.7.9.25.1.2.\nMbarrier-based mechanism\n\nA thread can track the completion of one or more asynchronous operations using the current phase of\nan\nmbarrier object\n. When the current phase of the\nmbarrier object\nis complete, it implies that\nall asynchronous operations tracked by this phase are complete, and all threads participating in\nthat\nmbarrier object\ncan access the result of the asynchronous operations.\nThe\nmbarrier object\nto be used for tracking the completion of an asynchronous operation can be\neither specified along with the asynchronous operation as part of its syntax, or as a separate\noperation. For a bulk asynchronous operation, the\nmbarrier object\nmust be specified in the\nasynchronous operation, whereas for non-bulk operations, it can be specified after the asynchronous\noperation.\nA typical pattern of using mbarrier-based completion mechanism is as follows:\nInitiate the asynchronous operations.\nSet up an\nmbarrier object\nto track the asynchronous operations in its current phase, either as\npart of the asynchronous operation or as a separate operation.\nWait for the\nmbarrier object\nto complete its current phase using\nmbarrier.test_wait\nor\nmbarrier.try_wait\n.\nOnce the\nmbarrier.test_wait\nor\nmbarrier.try_wait\noperation returns\nTrue\n, access the\nresults of the asynchronous operations tracked by the\nmbarrier object\n.\n9.7.9.25.2.\nAsync Proxy\n\nThe\ncp{.reduce}.async.bulk\noperations are performed in the\nasynchronous proxy\n(or\nasync\nproxy\n).\nAccessing the same memory location across multiple proxies needs a cross-proxy fence. For the\nasync proxy\n,\nfence.proxy.async\nshould be used to synchronize memory between\ngeneric\nproxy\nand the\nasync proxy\n.\nThe completion of a\ncp{.reduce}.async.bulk\noperation is followed by an implicit\ngeneric-async\nproxy fence. So the result of the asynchronous operation is made visible to the generic proxy as\nsoon as its completion is observed.\nAsync-group\nOR\nmbarrier-based\ncompletion mechanism must\nbe used to wait for the completion of the\ncp{.reduce}.async.bulk\ninstructions.\n9.7.9.25.3.\nData Movement and Conversion Instructions: Non-bulk copy\n\n9.7.9.25.3.1.\nData Movement and Conversion Instructions:\ncp.async\n\ncp.async\nInitiates an asynchronous copy operation from one state space to another.\nSyntax\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, src-size}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, src-size}{, cache-policy} ;\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, ignore-src}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, ignore-src}{, cache-policy} ;\n\n.level::cache_hint =     { .L2::cache_hint }\n.level::prefetch_size =  { .L2::64B, .L2::128B, .L2::256B }\ncp-size =                { 4, 8, 16 }\nDescription\ncp.async\nis a non-blocking instruction which initiates an asynchronous copy operation of data\nfrom the location specified by source address operand\nsrc\nto the location specified by\ndestination address operand\ndst\n. Operand\nsrc\nspecifies a location in the global state space\nand\ndst\nspecifies a location in the shared state space.\nOperand\ncp-size\nis an integer constant which specifies the size of data in bytes to be copied to\nthe destination\ndst\n.\ncp-size\ncan only be 4, 8 and 16.\nInstruction\ncp.async\nallows optionally specifying a 32-bit integer operand\nsrc-size\n. Operand\nsrc-size\nrepresents the size of the data in bytes to be copied from\nsrc\nto\ndst\nand must\nbe less than\ncp-size\n. In such case, remaining bytes in destination\ndst\nare filled with\nzeros. Specifying\nsrc-size\nlarger than\ncp-size\nresults in undefined behavior.\nThe optional and non-immediate predicate argument\nignore-src\nspecifies whether the data from the\nsource location\nsrc\nshould be ignored completely. If the source data is ignored then zeros will\nbe copied to destination\ndst\n. If the argument\nignore-src\nis not specified then it defaults\nto\nFalse\n.\nSupported alignment requirements and addressing modes for operand\nsrc\nand\ndst\nare described\nin\nAddresses as Operands\n.\nThe mandatory\n.async\nqualifier indicates that the\ncp\ninstruction will initiate the memory\ncopy operation asynchronously and control will return to the executing thread before the copy\noperation is complete. The executing thread can then use\nasync-group based completion mechanism\nor the\nmbarrier based completion mechanism\nto wait for completion of the asynchronous copy operation.\nNo other synchronization mechanism guarantees the completion of the asynchronous\ncopy operations.\nThere is no ordering guarantee between two\ncp.async\noperations if they are not explicitly\nsynchronized using\ncp.async.wait_all\nor\ncp.async.wait_group\nor\nmbarrier instructions\n.\nAs described in\nCache Operators\n, the\n.cg\nqualifier indicates\ncaching of data only at global level cache L2 and not at L1 whereas\n.ca\nqualifier indicates\ncaching of data at all levels including L1 cache. Cache operator are treated as performance hints\nonly.\ncp.async\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe qualifier\n.level::prefetch_size\nmay only be used with\n.global\nstate space and with\ngeneric addressing where the address points to\n.global\nstate space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for\n.level::cache_hint\nand\n.level::prefetch_size\nqualifiers introduced in PTX ISA\nversion 7.4.\nSupport for\nignore-src\noperand introduced in PTX ISA version 7.5.\nSupport for sub-qualifier\n::cta\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nExamples\ncp.async.ca.shared.global  [shrd],    [gbl + 4], 4;\ncp.async.ca.shared::cta.global  [%r0 + 8], [%r1],     8;\ncp.async.cg.shared.global  [%r2],     [%r3],     16;\n\ncp.async.cg.shared.global.L2::64B   [%r2],      [%r3],     16;\ncp.async.cg.shared.global.L2::128B  [%r0 + 16], [%r1],     16;\ncp.async.cg.shared.global.L2::256B  [%r2 + 32], [%r3],     16;\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 0.25;\ncp.async.ca.shared.global.L2::cache_hint [%r2], [%r1], 4, cache-policy;\n\ncp.async.ca.shared.global                   [shrd], [gbl], 4, p;\ncp.async.cg.shared.global.L2::cache_hint   [%r0], [%r2], 16, q, cache-policy;\n9.7.9.25.3.2.\nData Movement and Conversion Instructions:\ncp.async.commit_group\n\ncp.async.commit_group\nCommits all prior initiated but uncommitted\ncp.async\ninstructions into a\ncp.async-group\n.\nSyntax\ncp.async.commit_group ;\nDescription\ncp.async.commit_group\ninstruction creates a new\ncp.async-group\nper thread and batches all\nprior\ncp.async\ninstructions initiated by the executing thread but not committed to any\ncp.async-group\ninto the new\ncp.async-group\n. If there are no uncommitted\ncp.async\ninstructions then\ncp.async.commit_group\nresults in an empty\ncp.async-group.\nAn executing thread can wait for the completion of all\ncp.async\noperations in a\ncp.async-group\nusing\ncp.async.wait_group\n.\nThere is no memory ordering guarantee provided between any two\ncp.async\noperations within the\nsame\ncp.async-group\n. So two or more\ncp.async\noperations within a\ncp.async-group\ncopying data\nto the same location results in undefined behavior.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example 1:\ncp.async.ca.shared.global [shrd], [gbl], 4;\ncp.async.commit_group ; // Marks the end of a cp.async group\n\n// Example 2:\ncp.async.ca.shared.global [shrd1],   [gbl1],   8;\ncp.async.ca.shared.global [shrd1+8], [gbl1+8], 8;\ncp.async.commit_group ; // Marks the end of cp.async group 1\n\ncp.async.ca.shared.global [shrd2],    [gbl2],    16;\ncp.async.cg.shared.global [shrd2+16], [gbl2+16], 16;\ncp.async.commit_group ; // Marks the end of cp.async group 2\n9.7.9.25.3.3.\nData Movement and Conversion Instructions:\ncp.async.wait_group\n/\ncp.async.wait_all\n\ncp.async.wait_group\n,\ncp.async.wait_all\nWait for completion of prior asynchronous copy operations.\nSyntax\ncp.async.wait_group N;\ncp.async.wait_all ;\nDescription\ncp.async.wait_group\ninstruction will cause executing thread to wait till only\nN\nor fewer of\nthe most recent\ncp.async-group\ns are pending and all the prior\ncp.async-group\ns committed by\nthe executing threads are complete. For example, when\nN\nis 0, the executing thread waits on all\nthe prior\ncp.async-group\ns to complete. Operand\nN\nis an integer constant.\ncp.async.wait_all\nis equivalent to :\ncp.async.commit_group;\ncp.async.wait_group 0;\nAn empty\ncp.async-group\nis considered to be trivially complete.\nWrites performed by\ncp.async\noperations are made visible to the executing thread only after:\nThe completion of\ncp.async.wait_all\nor\nThe completion of\ncp.async.wait_group\non the\ncp.async-group\nin which the\ncp.async\nbelongs to or\nmbarrier.test_wait\nreturns\nTrue\non an\nmbarrier object\nwhich is tracking the completion of the\ncp.async\noperation.\nThere is no ordering between two\ncp.async\noperations that are not synchronized with\ncp.async.wait_all\nor\ncp.async.wait_group\nor\nmbarrier objects\n.\ncp.async.wait_group\nand\ncp.async.wait_all\ndoes not provide any ordering and visibility\nguarantees for any other memory operation apart from\ncp.async\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example of .wait_all:\ncp.async.ca.shared.global [shrd1], [gbl1], 4;\ncp.async.cg.shared.global [shrd2], [gbl2], 16;\ncp.async.wait_all;  // waits for all prior cp.async to complete\n\n// Example of .wait_group :\ncp.async.ca.shared.global [shrd3], [gbl3], 8;\ncp.async.commit_group;  // End of group 1\n\ncp.async.cg.shared.global [shrd4], [gbl4], 16;\ncp.async.commit_group;  // End of group 2\n\ncp.async.cg.shared.global [shrd5], [gbl5], 16;\ncp.async.commit_group;  // End of group 3\n\ncp.async.wait_group 1;  // waits for group 1 and group 2 to complete\n9.7.9.25.4.\nData Movement and Conversion Instructions: Bulk copy\n\n9.7.9.25.4.1.\nData Movement and Conversion Instructions:\ncp.async.bulk\n\ncp.async.bulk\nInitiates an asynchronous copy operation from one state space to another.\nSyntax\n// global -> shared::cta\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, cache-policy}\n\n.dst =                  { .shared::cta }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n\n\n// global -> shared::cluster\ncp.async.bulk.dst.src.completion_mechanism{.multicast}{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> shared::cluster\ncp.async.bulk.dst.src.completion_mechanism [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n\n\n// shared::cta -> global\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}{.cp_mask}\n                      [dstMem], [srcMem], size {, cache-policy} {, byteMask}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint =    { .L2::cache_hint }\nDescription\ncp.async.bulk\nis a non-blocking instruction which initiates an asynchronous bulk-copy operation\nfrom the location specified by source address operand\nsrcMem\nto the location specified by\ndestination address operand\ndstMem\n.\nThe direction of bulk-copy is from the state space specified by the\n.src\nmodifier to the state\nspace specified by the\n.dst\nmodifiers.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied, in terms of number of\nbytes.\nsize\nmust be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined. The memory range\n[dstMem,\ndstMem\n+\nsize\n-\n1]\nmust not overflow the destination memory\nspace and the memory range\n[srcMem,\nsrcMem\n+\nsize\n-\n1]\nmust not overflow the source memory\nspace. Otherwise, the behavior is undefined. The addresses\ndstMem\nand\nsrcMem\nmust be aligned\nto 16 bytes.\nWhen the destination of the copy is\n.shared::cta\nthe destination address has to be in the shared\nmemory of the executing CTA within the cluster, otherwise the behavior is undefined.\nWhen the source of the copy is\n.shared::cta\nand the destination is\n.shared::cluster\n, the\ndestination has to be in the shared memory of a different CTA within the cluster.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\n.mbarrier::...\n.shared::cta\n.global\nmbarrier based\n.shared::cluster\n.global\n.shared::cluster\n.shared::cta\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.async.bulk\nvariant uses\nmbarrier based completion mechanism. The\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.bulk_group\nspecifies that the\ncp.async.bulk\nvariant uses\nbulk async-group\nbased completion mechanism.\nThe optional modifier\n.multicast::cluster\nallows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand\nctaMask\nspecifies the destination CTAs in the\ncluster such that each bit position in the 16-bit\nctaMask\noperand corresponds to the\n%ctaid\nof the destination CTA. The source data is multicast to the same CTA-relative offset as\ndstMem\nin the shared memory of each destination CTA. The mbarrier signal is also multicast to the same\nCTA-relative offset as\nmbar\nin the shared memory of the destination CTA.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nWhen the optional qualifier\n.cp_mask\nis specified, the argument\nbyteMask\nis required.\nThe i-th bit in the 16-bit wide\nbyteMask\noperand specifies whether the i-th byte of each 16-byte\nwide chunk of source data is copied to the destination. If the bit is set, the byte is copied.\nThe copy operation in\ncp.async.bulk\nis treated as a weak memory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nNotes\n.multicast::cluster\nqualifier is optimized for target architecture\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\nand may have substantially reduced performance on other\ntargets and hence\n.multicast::cluster\nis advised to be used with\n.target\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.shared::cta\nas destination state space is introduced in PTX ISA version 8.6.\nSupport for\n.cp_mask\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.multicast::cluster\nqualifier advised to be used with\n.target\nsm_90a\nor\nsm_100f\nor\nsm_100a\nor\nsm_103f\nor\nsm_103a\nor\nsm_110f\nor\nsm_110a\n.\nSupport for\n.cp_mask\nqualifier requires\nsm_100\nor higher.\nExamples\n// .global -> .shared::cta (strictly non-remote):\ncp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n// .global -> .shared::cluster:\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                                             [dstMem], [srcMem], size, [mbar], ctaMask;\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n\n// .shared::cta -> .shared::cluster (strictly remote):\ncp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\n// .shared::cta -> .global:\ncp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy;\n\n// .shared::cta -> .global with .cp_mask:\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.cp_mask [dstMem], [srcMem], size, cache-policy, byteMask;\n9.7.9.25.4.2.\nData Movement and Conversion Instructions:\ncp.reduce.async.bulk\n\ncp.reduce.async.bulk\nInitiates an asynchronous reduction operation.\nSyntax\ncp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type\n              [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .b32, .u32, .s32, .b64, .u64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type\n               [dstMem], [srcMem], size{, cache-policy}\n\n.dst =                  { .global      }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint    = { .L2::cache_hint }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type\n               [dstMem], [srcMem], size{, cache-policy}\n.dst  =                 { .global }\n.src  =                 { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.type =                 { .f16, .bf16 }\nDescription\ncp.reduce.async.bulk\nis a non-blocking instruction which initiates an asynchronous reduction\noperation on an array of memory locations specified by the destination address operand\ndstMem\nwith the source array whose location is specified by the source address operand\nsrcMem\n. The size\nof the source and the destination array must be the same and is specified by the operand\nsize\n.\nEach data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier\n.redOp\n. The type of each\ndata element in the source and the destination array is specified by the modifier\n.type\n.\nThe source address operand\nsrcMem\nis located in the state space specified by\n.src\nand the\ndestination address operand\ndstMem\nis located in the state specified by the\n.dst\n.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied from the source location and\nused in the reduction operation, in terms of number of bytes.\nsize\nmust be a multiple of 16. If\nthe value is not a multiple of 16, then the behavior is undefined. The memory range\n[dstMem,\ndstMem\n+\nsize\n-\n1]\nmust not overflow the destination memory space and the memory range\n[srcMem,\nsrcMem\n+\nsize\n-\n1]\nmust not overflow the source memory space. Otherwise, the behavior is\nundefined. The addresses\ndstMem\nand\nsrcMem\nmust be aligned to 16 bytes.\nThe operations supported by\n.redOp\nare classified as follows:\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n, and\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..x]\nwhere\nx\nis the value at the source\nstate space.\nThe floating point operation\n.add\nrounds to the nearest even. The current implementation of\ncp.reduce.async.bulk.add.f32\nflushes subnormal inputs and results to sign-preserving zero. The\ncp.reduce.async.bulk.add.f16\nand\ncp.reduce.async.bulk.add.bf16\noperations require\n.noftz\nqualifier. It preserves input and result subnormals, and does not flush them to zero.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.dst\n.redOp\nElement type\n.shared::cluster\n.add\n.u32\n,\n.s32\n,\n.u64\n.min\n,\n.max\n.u32\n,\n.s32\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n.global\n.add\n.u32\n,\n.s32\n,\n.u64\n,\n.f32\n,\n.f64\n,\n.f16\n,\n.bf16\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n,\n.f16\n,\n.bf16\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\n.mbarrier::...\n.shared::cluster\n.global\nmbarrier based\n.shared::cluster\n.shared::cta\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.reduce.async.bulk\nvariant\nuses mbarrier based completion mechanism. The\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.bulk_group\nspecifies that the\ncp.reduce.async.bulk\nvariant uses\nbulk\nasync-group\nbased completion mechanism.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nEach reduction operation performed by the\ncp.reduce.async.bulk\nhas individually\n.relaxed.gpu\nmemory ordering semantics. The load operations in\ncp.reduce.async.bulk\nare treated as weak\nmemory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;\n9.7.9.25.4.3.\nData Movement and Conversion Instructions:\ncp.async.bulk.prefetch\n\ncp.async.bulk.prefetch\nProvides a hint to the system to initiate the asynchronous prefetch of data to the cache.\nSyntax\ncp.async.bulk.prefetch.L2.src{.level::cache_hint}   [srcMem], size {, cache-policy}\n\n.src =                { .global }\n.level::cache_hint =  { .L2::cache_hint }\nDescription\ncp.async.bulk.prefetch\nis a non-blocking instruction which may initiate an asynchronous prefetch\nof data from the location specified by source address operand\nsrcMem\n, in\n.src\nstatespace, to\nthe L2 cache.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be prefetched in terms of number of\nbytes.\nsize\nmust be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined.  The address\nsrcMem\nmust be aligned to 16 bytes.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\ncp.async.bulk.prefetch\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.prefetch.L2.global                 [srcMem], size;\n\ncp.async.bulk.prefetch.L2.global.L2::cache_hint  [srcMem], size, policy;\n9.7.9.26.\nData Movement and Conversion Instructions:\nmultimem.cp.async.bulk\n\nmultimem.cp.async.bulk\nInitiates an asynchronous copy operation to a multimem address range.\nSyntax\nmultimem.cp.async.bulk.dst.src.completion_mechanism{.cp_mask}\n    [dstMem], [srcMem], size{, byteMask};\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\nDescription\nInstruction\nmultimem.cp.async.bulk\ninitiates an asynchronous bulk-copy operation from source\naddress range\n[srcMem,\nsrcMem\n+\nsize)\nto memory locations residing on each GPU’s memory referred\nto by the destination multimem address range\n[dstMem,\ndstMem\n+\nsize)\n. The direction of\nbulk-copy is from the state space specified by the\n.src\nmodifier to the state space specified\nby the\n.dst\nmodifiers.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied, in terms of number of\nbytes. Operand\nsize\nmust be a multiple of\n16\n. The memory range\n[dstMem,\ndstMem\n+\nsize)\nmust not overflow the destination multimem memory space. The memory range\n[srcMem,\nsrcMem\n+\nsize)\nmust not overflow the source memory space. The addresses\ndstMem\nand\nsrcMem\nmust be aligned\nto\n16\nbytes. If any of these pre-conditions is not met, the behavior is undefined.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported by the\ninstruction. The modifier\n.bulk_group\nspecifies that the\nmultimem.cp.async.bulk\ninstruction\nuses bulk async-group based completion mechanism.\nWhen the optional modifier\n.cp_mask\nis specified, the argument\nbyteMask\nis required. The\ni-th bit in the 16-bit wide\nbyteMask\noperand specifies whether the i-th byte of each 16-byte\nwide chunk of source data is copied to the destination. If the bit is set, the byte is copied.\nThe reads and writes of the copy operation in\nmultimem.cp.async.bulk\nare weak memory operations\nas described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nSupport for\n.cp_mask\nqualifier requires\nsm_100\nor higher.\nExamples\nmultimem.cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\nmultimem.cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], 512;\n\nmultimem.cp.async.bulk.global.shared::cta.bulk_group.cp_mask [dstMem], [srcMem], size, byteMask;\n9.7.9.27.\nData Movement and Conversion Instructions:\nmultimem.cp.reduce.async.bulk\n\nmultimem.cp.reduce.async.bulk\nInitiates an asynchronous reduction operation to a multimem address range.\nSyntax\nmultimem.cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type  [dstMem], [srcMem], size;\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\n    .redOp                = { .and, .or, .xor,\n                              .add, .inc, .dec,\n                              .min, .max }\n    .type                 = { .f16, .bf16,\n                              .b32, .u32, .s32,\n                              .b64, .u64, .s64,\n                              .f32, .f64 }\n\nmultimem.cp.reduce.async.bulk.dst.src.completion_mechanism.add.noftz.type  [dstMem], [srcMem], size;\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\n    .type                 = { .f16, .bf16 }\nDescription\nInstruction\nmultimem.cp.reduce.async.bulk\ninitiates an element-wise asynchronous reduction\noperation with elements from source memory address range\n[srcMem,\nsrcMem\n+\nsize)\nto memory\nlocations residing on each GPU’s memory referred to by the multimem destination address range\n[dstMem,\ndstMem\n+\nsize)\n.\nEach data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier\n.redOp\n. The type of each\ndata element in the source and the destination array is specified by the modifier\n.type\n.\nThe source address operand\nsrcMem\nis in the state space specified by\n.src\nand the\ndestination address operand\ndstMem\nis in the state specified by the\n.dst\n.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied from the source location\nand used in the reduction operation, in terms of number of bytes. Operand\nsize\nmust be a\nmultiple of 16. The memory range\n[dstMem,\ndstMem\n+\nsize)\nmust not overflow the destination\nmultimem memory space. The memory range\n[srcMem,\nsrcMem\n+\nsize)\nmust not overflow the source\nmemory space. The addresses\ndstMem\nand\nsrcMem\nmust be aligned to 16 bytes. If any of these\npreconditions is not met, the behavior is undefined.\nThe operations supported by\n.redOp\nare classified as follows:\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n, and\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..x]\nwhere\nx\nis the value at the source\nstate space.\nThe floating point operation\n.add\nrounds to the nearest even, preserve input and result\nsubnormals, and does not flush them to zero, except for the current implementation of\nmultimem.cp.reduce.async.bulk.add.f32\nwhich flushes subnormal inputs and results to\nsign-preserving zero. The\nmultimem.cp.reduce.async.bulk.add.f16\nand\nmultimem.cp.reduce.async.bulk.add.bf16\noperations require\n.noftz\nqualifier. It preserves\ninput and result subnormals, and does not flush them to zero.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.redOp\nelement type\n.add\n.u32, .s32, .u64, .f32, .f64,\n.f16, .bf16\n.min,\n.max\n.u32, .s32, .u64, .s64, .f16,\n.bf16\n.inc,\n.dec\n.u32\n.and,\n.or,\n.xor\n.b32, .b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported by the\ninstruction. The modifier\n.bulk_group\nspecifies that the\nmultimem.cp.reduce.async.bulk\nuses\nbulk async-group based completion mechanism.\nEach reduction operation performed by the\nmultimem.cp.reduce.async.bulk\nhas individually\n.relaxed.sys\nmemory ordering semantics. The load operations in\nmultimem.cp.reduce.async.bulk\nare treated as weak memory operations as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.xor.b64 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.inc.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.dec.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.max.s32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.min.bf16 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.bf16 [dstMem], [srcMem], size;\n9.7.9.27.1.\nData Movement and Conversion Instructions: Tensor copy\n\n9.7.9.27.1.1.\nRestriction on Tensor Copy instructions\n\nFollowing are the restrictions on the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n:\ncp.reduce.async.bulk\ndoesn’t support the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\ncp.async.bulk.tensor\nwith the direction\n.global.shared::cta\ndoesn’t support the\ntype\n.b4x16_p64\n.\ncp.async.bulk.tensor\nwith the direction\n.shared::cluster.global\ndoesn’t support\nthe sub-byte types on\nsm_120a\n.\nOOB-NaN fill mode doesn’t support the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\nBox-Size[0] must be exactly:\n96B for\nb6x16_p32\nand\n.b6p2x16\n.\n64B for\nb4x16_p64\n.\nTensor-Size[0] must be a multiple of:\n96B for\nb6x16_p32\nand\n.b6p2x16\n.\n64B for\nb4x16_p64\n.\nFor\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n, the first coordinate in the tensorCoords\nargument vector must be a multiple of 128.\nFor\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n, the global memory address must be 32B aligned.\nAdditionally, tensor stride in every dimension must be 32B aligned.\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\nsupports the following swizzling modes:\nNone.\n128B (With all potential swizzle atomicity values except: 32B with 8B flip)\nFollowing are the restrictions on the 96B swizzle mode:\nThe\n.swizzle_atomicity\nmust be 16B.\nThe\n.interleave_layout\nmust not be set.\nBox-Size[0] must be less than or equal to 96B.\nThe type must not be among following:\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\nThe\n.load_mode\nmust not be set to\n.im2col::w::128\n.\nFollowing are the restrictions on the\n.global.shared::cta\ndirection:\nStarting co-ordinates for Bounding Box (\ntensorCoords\n) must be non-negative.\nThe bounding box along the D, W and H dimensions must stay within the tensor boundaries.\nThis implies:\nBounding-Box Lower-Corner must be non-negative.\nBounding-Box Upper-Corner must be non-positive.\nFollowing are the restrictions for\nsm_120a\n:\ncp.async.bulk.tensor\nwith the direction\n.shared::cluster.global\ndoesn’t support:\nthe sub-byte types\nthe qualifier\n.swizzle_atomicity\nFollowing are the restrictions for\nsm_103a\nwhile using type\n.b6p2x16\non\ncp.async.bulk.tensor\nwith the direction\n.global.shared::cta\n:\nBox-Size[0] must be exactly either of 48B or 96B.\nThe global memory address must be 16B aligned.\nTensor Stride in every dimension must be 16B aligned.\nThe first coordinate in the tensorCoords argument vector must be a multiple of 64.\nTensor-Size[0] must be a multiple of 48B.\nThe following swizzle modes are supported:\nNone.\n128B (With all potential swizzle atomicity values except: 32B with 8B flip)\n64B swizzle with 16B swizzle atomicity\n9.7.9.27.1.2.\nData Movement and Conversion Instructions:\ncp.async.bulk.tensor\n\ncp.async.bulk.tensor\nInitiates an asynchronous copy operation on the tensor data from one state space to another.\nSyntax\n// global -> shared::cta\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.cta_group}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo} {, cache-policy}\n\n.dst =                  { .shared::cta }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.cta_group =            { .cta_group::1, .cta_group::2 }\n.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =    { .L2::cache_hint }\n\n\n// global -> shared::cluster\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.cta_group}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo}\n                                   {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.cta_group =            { .cta_group::1, .cta_group::2 }\n.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> global\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint}\n                                   [tensorMap, tensorCoords], [srcMem] {, cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .tile::scatter4, .im2col_no_offs }\n.level::cache_hint =    { .L2::cache_hint }\nDescription\ncp.async.bulk.tensor\nis a non-blocking instruction which initiates an asynchronous copy\noperation of tensor data from the location in\n.src\nstate space to the location in the\n.dst\nstate space.\nThe operand\ndstMem\nspecifies the location in the\n.dst\nstate space into which the tensor data\nhas to be copied and\nsrcMem\nspecifies the location in the\n.src\nstate space from which the\ntensor data has to be copied.\nWhen\n.dst\nis specified as\n.shared::cta\n, the address\ndstMem\nmust be in the shared memory\nof the executing CTA within the cluster, otherwise the behavior is undefined.\nWhen\n.dst\nis specified as\n.shared::cluster\n, the address\ndstMem\ncan be in the shared memory\nof any of the CTAs within the current cluster.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nThe dimension of the tensor data is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates in the tensor data in the\nglobal memory from or to which the copy operation has to be performed. The individual tensor\ncoordinates in\ntensorCoords\nare of type\n.s32\n. The format of vector argument\ntensorCoords\nis dependent on\n.load_mode\nspecified and is as follows:\n.load_mode\ntensorCoords\nSemantics\n.tile::scatter4\n{col_idx, row_idx0, row_idx1, row_idx2, row_idx3}\nFixed length vector of size 5.\nThe five elements together specify the start\nco-ordinates of the four rows.\n.tile::gather4\nRest all\n{d0, .., dn}\nfor n = .dim\nVector of n elements where n = .dim.\nThe elements indicate the offset in each of the\ndimension.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\nNeeded for completion of\nentire Async operation\noptionally can be used for the completion of\nreading of the tensormap object\n.mbarrier::...\n.shared::cta\n.global\nmbarrier based\nBulk async-group\nbased\n.shared::cluster\n.global\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.async.bulk.tensor\nvariant\nuses mbarrier based completion mechanism. Upon the completion of the asynchronous copy operation, the\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.cta_group\ncan only be specified with the mbarrier based completion mechanism. The\nmodifier\n.cta_group\nis used to signal either the odd numbered CTA or the even numbered CTA among\nthe\nCTA-Pair\n. When\n.cta_group::1\nis specified, the mbarrier object\nmbar\nthat is specified must be in the shared memory of the same CTA as the shared memory destination\ndstMem\n.\nWhen\n.cta_group::2\nis specified, the mbarrier object\nmbar\ncan be in shared memory of either the\nsame CTA as the shared memory destination\ndstMem\nor in its\npeer-CTA\n. If\n.cta_group\nis not specified, then it defaults to\n.cta_group::1\n.\nThe modifier\n.bulk_group\nspecifies that the\ncp.async.bulk.tensor\nvariant uses\nbulk\nasync-group\nbased completion mechanism.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n.\nIn\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination.\nIn\n.tile::gather4\nmode, four rows in 2-dimnesional source tensor are combined to form a single 2-dimensional\ndestination tensor. In\n.tile::scatter4\nmode, single 2-dimensional source tensor is divided into four rows\nin 2-dimensional destination tensor. Details of\n.tile::scatter4\n/\n.tile::gather4\nmodes are described\nin\n.tile::scatter4 and .tile::gather4 modes\n.\nIn\n.im2col\nand\n.im2col::*\nmodes, some dimensions of the source tensors are unrolled in a single\ndimensional column at the destination. Details of the\nim2col\nand\n.im2col::*\nmodes are described\nin\nim2col mode\nand\nim2col::w and im2col::w::128 modes\nrespectively. In\n.im2col\nand\n.im2col::*\nmodes, the tensor has to be at least 3-dimensional. The vector\noperand\nim2colInfo\ncan be specified only when\n.load_mode\nis\n.im2col\nor\n.im2col::w\nor\n.im2col::w::128\n. The format of the vector argument\nim2colInfo\nis dependent on the exact im2col mode\nand is as follows:\nExact im2col mode\nim2colInfo argument\nSemantics\n.im2col\n{ i2cOffW , i2cOffH , i2cOffD }\nfor\n.dim\n=\n.5d\nA vector of im2col offsets whose vector size is two\nless than number of dimensions .dim.\n.im2col::w\n{ wHalo, wOffset }\nA vector of 2 arguments containing\nwHalo\nand\nwOffset\narguments.\n.im2col::w::128\n.im2col_no_offs\nim2colInfo\nis not applicable.\nim2colInfo\nis not applicable.\nArgument\nwHalo\nis a 16bit unsigned integer whose valid set of values differs on the load-mode and is as follows:\n- Im2col::w mode : valid range is [0, 512).\n- Im2col::w::128 mode : valid range is [0, 32).\nArgument\nwOffset\nis a 16bit unsigned integer whose valid range of values is [0, 32).\nThe optional modifier\n.multicast::cluster\nallows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand\nctaMask\nspecifies the destination CTAs in the\ncluster such that each bit position in the 16-bit\nctaMask\noperand corresponds to the\n%ctaid\nof the destination CTA. The source data is multicast to the same offset as\ndstMem\nin the shared\nmemory of each destination CTA. When\n.cta_group\nis specified as:\n.cta_group::1\n: The mbarrier signal is also multicasted to the same offset as\nmbar\nin\nthe shared memory of the destination CTA.\n.cta_group::2\n: The mbarrier signal is multicasted either to all the odd numbered CTAs or the\neven numbered CTAs within the corresponding\nCTA-Pair\n. For each destination\nCTA specified in the\nctaMask\n, the mbarrier signal is sent either to the destination CTA or its\npeer-CTA\nbased on CTAs\n%cluster_ctarank\nparity of shared memory where\nthe mbarrier object\nmbar\nresides.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nThe copy operation in\ncp.async.bulk.tensor\nis treated as a weak memory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nNotes\n.multicast::cluster\nqualifier is optimized for target architecture\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\nand may have substantially reduced performance on other\ntargets and hence\n.multicast::cluster\nis advised to be used with\n.target\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.shared::cta\nas destination state space is introduced in PTX ISA version 8.6.\nSupport for qualifiers\n.tile::gather4\nand\n.tile::scatter4\nintroduced in PTX ISA version 8.6.\nSupport for qualifiers\n.im2col::w\nand\n.im2col::w::128\nintroduced in PTX ISA version 8.6.\nSupport for qualifier\n.cta_group\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.multicast::cluster\nqualifier advised to be used with\n.target\nsm_90a\nor\nsm_100f\nor\nsm_100a\nor\nsm_103f\nor\nsm_103a\nor\nsm_110f\nor\nsm_110a\n.\nQualifiers\n.tile::gather4\nand\n.im2col::w\nrequire:\nsm_100a\nwhen destination state space is\n.shared::cluster\nand is supported on\nsm_100f\nfrom PTX ISA version 8.8.\nsm_100\nor higher when destination state space is\n.shared::cta\n.\nQualifier\n.tile::scatter4\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.im2col::w::128\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.cta_group\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n.reg .b16 ctaMask;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.tensor.1d.shared::cta.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.5d.shared::cta.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\ncp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                     [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask;\n\n@p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                     [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy;\n\n@p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group  [tensorMap3, {tc0}], [sMem3];\n\ncp.async.bulk.tensor.2d.tile::gather4.shared::cluster.global.mbarrier::complete_tx::bytes\n                     [sMem5], [tensorMap6, {x0, y0, y1, y2, y3}], [mbar5];\n\ncp.async.bulk.tensor.3d.im2col::w.shared::cluster.global.mbarrier::complete_tx::bytes\n                     [sMem4], [tensorMap5, {t0, t1, t2}], [mbar4], {im2colwHalo, im2colOff};\n\ncp.async.bulk.tensor.1d.shared::cluster.global.tile.cta_group::2\n                     [sMem6], [tensorMap7, {tc0}], [peerMbar];\n9.7.9.27.1.3.\nData Movement and Conversion Instructions:\ncp.reduce.async.bulk.tensor\n\ncp.reduce.async.bulk.tensor\nInitiates an asynchronous reduction operation on the tensor data.\nSyntax\n// shared::cta -> global:\ncp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint}\n                                          [tensorMap, tensorCoords], [srcMem] {,cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .im2col_no_offs }\n.redOp =                { .add, .min, .max, .inc, .dec, .and, .or, .xor}\nDescription\ncp.reduce.async.bulk.tensor\nis a non-blocking instruction which initiates an asynchronous\nreduction operation of tensor data in the\n.dst\nstate space with tensor data in the\n.src\nstate space.\nThe operand\nsrcMem\nspecifies the location of the tensor data in the\n.src\nstate space using\nwhich the reduction operation has to be performed.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nEach element of the tensor data in the\n.dst\nstate space is reduced inline with the corresponding\nelement from the tensor data in the\n.src\nstate space. The modifier\n.redOp\nspecifies the\nreduction operation used for the inline reduction. The type of each tensor data element in the\nsource and the destination tensor is specified in\nTensor-map\n.\nThe dimension of the tensor is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates of the tensor data in the\nglobal memory on which the reduce operation is to be performed. The number of tensor coordinates in\nthe vector argument\ntensorCoords\nshould be equal to the dimension specified by the modifier\n.dim\n. The individual tensor coordinates are of the type\n.s32\n.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.redOp\nElement type\n.add\n.u32\n,\n.s32\n,\n.u64\n,\n.f32\n,\n.f16\n,\n.bf16\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n,\n.f16\n,\n.bf16\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. Value\n.bulk_group\nof the modifier\n.completion_mechanism\nspecifies that\ncp.reduce.async.bulk.tensor\ninstruction uses\nbulk async-group\nbased completion mechanism.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n. In\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination. In\n.im2col_no_offs\nmode, some dimensions of the source tensors are unrolled in a single dimensional\ncolumn at the destination. Details of the\nim2col\nmode are described in\nim2col mode\n. In\n.im2col\nmode, the tensor has to be at least\n3-dimensional.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nEach reduction operation performed by\ncp.reduce.async.bulk.tensor\nhas individually\n.relaxed.gpu\nmemory ordering semantics. The load operations in\ncp.reduce.async.bulk.tensor\nare treated as weak memory operations and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group\n                                             [tensorMap0, {tc0}], [sMem0];\n\ncp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint\n                                             [tensorMap1, {tc0, tc1}], [sMem1] , policy;\n\ncp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group\n                                             [tensorMap2, {tc0, tc1, tc2}], [sMem2]\n9.7.9.27.1.4.\nData Movement and Conversion Instructions:\ncp.async.bulk.prefetch.tensor\n\ncp.async.bulk.prefetch.tensor\nProvides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache.\nSyntax\n// global -> shared::cluster:\ncp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords]\n                                                             {, im2colInfo } {, cache-policy}\n\n.src =                { .global }\n.dim =                { .1d, .2d, .3d, .4d, .5d }\n.load_mode =          { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =  { .L2::cache_hint }\nDescription\ncp.async.bulk.prefetch.tensor\nis a non-blocking instruction which may initiate an asynchronous\nprefetch of tensor data from the location in\n.src\nstatespace to the L2 cache.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nThe dimension of the tensor data is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates in the tensor data in the\nglobal memory from which the copy operation has to be performed. The individual tensor\ncoordinates in\ntensorCoords\nare of type\n.s32\n. The format of vector argument\ntensorCoords\nis dependent on\n.load_mode\nspecified and is as follows:\n.load_mode\ntensorCoords\nSemantics\n.tile::gather4\n{col_idx, row_idx0, row_idx1, row_idx2, row_idx3}\nFixed length vector of size 5.\nThe five elements together specify the start\nco-ordinates of the four rows.\nRest all\n{d0, .., dn}\nfor n = .dim\nVector of n elements where n = .dim.\nThe elements indicate the offset in each of the\ndimension.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n.\nIn\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination.\nIn\n.tile::gather4\nmode, four rows in the 2-dimnesional source tensor are fetched to L2 cache.\nDetails of\n.tile::gather4\nmodes are described\nin\n.tile::scatter4 and .tile::gather4 modes\n.\nIn\n.im2col\nand\n.im2col::*\nmodes, some dimensions of the source tensors are unrolled in a single\ndimensional column at the destination. Details of the\nim2col\nand\n.im2col::*\nmodes are described in\nim2col mode\nand\nim2col::w and im2col::w::128 modes\nrespectively. In\n.im2col\nand\n.im2col::*\nmodes, the tensor has to be at least 3-dimensional. The vector\noperand\nim2colInfo\ncan be specified only when\n.load_mode\nis\n.im2col\nor\n.im2col::w\nor\n.im2col::w::128\n. The format of the vector argument\nim2colInfo\nis dependent on the exact im2col mode\nand is as follows:\nExact im2col mode\nim2colInfo argument\nSemantics\n.im2col\n{ i2cOffW , i2cOffH , i2cOffD }\nfor\n.dim\n=\n.5d\nA vector of im2col offsets whose vector size is two\nless than number of dimensions .dim.\n.im2col::w\n{ wHalo, wOffset }\nA vector of 2 arguments containing\nwHalo\nand\nwOffset\narguments.\n.im2col::w::128\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\ncp.async.bulk.prefetch.tensor\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for qualifier\n.tile::gather4\nintroduced in PTX ISA version 8.6.\nSupport for qualifiers\n.im2col::w\nand\n.im2col::w::128\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nQualifier\n.tile::gather4\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifiers\n.im2col::w\nand\n.im2col::w::128\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n.reg .b16 ctaMask, im2colwHalo, im2colOff;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.prefetch.tensor.1d.L2.global.tile  [tensorMap0, {tc0}];\n\n@p cp.async.bulk.prefetch.tensor.2d.L2.global    [tensorMap1, {tc0, tc1}];\n\n@p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col\n                      [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint\n                      [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy;\n\ncp.async.bulk.prefetch.tensor.2d.L2.global.tile::gather4 [tensorMap5, {col_idx, row_idx0, row_idx1, row_idx2, row_idx3}];\n\ncp.async.bulk.prefetch.tensor.4d.L2.global.im2col::w::128\n                      [tensorMap4, {t0, t1, t2, t3}], {im2colwHalo, im2colOff};\n9.7.9.27.2.\nData Movement and Conversion Instructions: Bulk and Tensor copy completion instructions\n\n9.7.9.27.2.1.\nData Movement and Conversion Instructions:\ncp.async.bulk.commit_group\n\ncp.async.bulk.commit_group\nCommits all prior initiated but uncommitted\ncp.async.bulk\ninstructions into a\ncp.async.bulk-group\n.\nSyntax\ncp.async.bulk.commit_group;\nDescription\ncp.async.bulk.commit_group\ninstruction creates a new per-thread\nbulk async-group\nand batches\nall prior\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions satisfying the following\nconditions into the new\nbulk async-group\n:\nThe prior\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions use\nbulk_group\nbased\ncompletion mechanism, and\nThey are initiated by the executing thread but not committed to any\nbulk async-group\n.\nIf there are no uncommitted\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions then\ncp.async.bulk.commit_group\nresults in an empty\nbulk async-group\n.\nAn executing thread can wait for the completion of all\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\noperations in a\nbulk async-group\nusing\ncp.async.bulk.wait_group\n.\nThere is no memory ordering guarantee provided between any two\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\noperations within the same\nbulk async-group\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.commit_group;\n9.7.9.27.2.2.\nData Movement and Conversion Instructions:\ncp.async.bulk.wait_group\n\ncp.async.bulk.wait_group\nWait for completion of\nbulk async-groups\n.\nSyntax\ncp.async.bulk.wait_group{.read} N;\nDescription\ncp.async.bulk.wait_group\ninstruction will cause the executing thread to wait until only N or\nfewer of the most recent\nbulk async-groups\nare pending and all the prior\nbulk async-groups\ncommitted by the executing threads are complete. For example, when N is 0, the executing thread\nwaits on all the prior\nbulk async-groups\nto complete. Operand N is an integer constant.\nBy default,\ncp.async.bulk.wait_group\ninstruction will cause the executing thread to wait until\ncompletion of all the bulk async operations in the specified\nbulk async-group\n. A bulk async\noperation includes the following:\nOptionally, reading from the tensormap.\nReading from the source locations.\nWriting to their respective destination locations.\nWrites being made visible to the executing thread.\nThe optional\n.read\nmodifier indicates that the waiting has to be done until all the bulk\nasync operations in the specified\nbulk async-group\nhave completed:\nreading from the tensormap\nthe reading from their source locations.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.wait_group.read   0;\ncp.async.bulk.wait_group        2;\n9.7.9.28.\nData Movement and Conversion Instructions:\ntensormap.replace\n\ntensormap.replace\nModifies the field of a tensor-map object.\nSyntax\ntensormap.replace.mode.field1{.ss}.b1024.type  [addr], new_val;\ntensormap.replace.mode.field2{.ss}.b1024.type  [addr], ord, new_val;\ntensormap.replace.mode.field3{.ss}.b1024.type  [addr], new_val;\n\n.mode    = { .tile }\n.field1  = { .global_address, .rank }\n.field2  = { .box_dim, .global_dim, .global_stride, .element_stride  }\n.field3  = { .elemtype,  .interleave_layout, .swizzle_mode, .swizzle_atomicity, .fill_mode }\n.ss      = { .global, .shared::cta }\n.type    = { .b32, .b64 }\nDescription\nThe\ntensormap.replace\ninstruction replaces the field, specified by\n.field\nqualifier,\nof the tensor-map object at the location specified by the address operand\naddr\nwith a\nnew value. The new value is specified by the argument\nnew_val\n.\nQualifier\n.mode\nspecifies the mode of the\ntensor-map\nobject\nlocated at the address operand\naddr\n.\nInstruction type\n.b1024\nindicates the size of the\ntensor-map\nobject, which is 1024 bits.\nOperand\nnew_val\nhas the type\n.type\n. When\n.field\nis specified as\n.global_address\nor\n.global_stride\n,\n.type\nmust be\n.b64\n. Otherwise,\n.type\nmust be\n.b32\n.\nThe immediate integer operand\nord\nspecifies the ordinal of the field across the rank of the\ntensor which needs to be replaced in the\ntensor-map\nobject.\nFor field\n.rank\n, the operand\nnew_val\nmust be ones less than the desired tensor rank as\nthis field uses zero-based numbering.\nWhen\n.field3\nis specified, the operand\nnew_val\nmust be an immediate and the\nTable 33\nshows the mapping of the operand\nnew_val\nacross various fields.\nTable 33\nTensormap new_val validity\n\nnew_val\n.field3\n.elemtype\n.interleave_layout\n.swizzle_mode\n.swizzle_atomicity\n.fill_mode\n0\n.u8\nNo interleave\nNo swizzling\n16B\nZero fill\n1\n.u16\n16B interleave\n32B swizzling\n32B\nOOB-NaN fill\n2\n.u32\n32B interleave\n64B swizzling\n32B + 8B flip\nx\n3\n.s32\nx\n128B swizzling\n64B\nx\n4\n.u64\nx\n96B swizzling\nx\nx\n5\n.s64\nx\nx\nx\nx\n6\n.f16\nx\nx\nx\nx\n7\n.f32\nx\nx\nx\nx\n8\n.f32.ftz\nx\nx\nx\nx\n9\n.f64\nx\nx\nx\nx\n10\n.bf16\nx\nx\nx\nx\n11\n.tf32\nx\nx\nx\nx\n12\n.tf32.ftz\nx\nx\nx\nx\n13\n.b4x16\nx\nx\nx\nx\n14\n.b4x16_p64\nx\nx\nx\nx\n15\n.b6x16_p32\nor\n.b6p2x16\nx\nx\nx\nx\nNote\nThe values of\n.elemtype\ndo not correspond to the values of the\nCUtensorMapDataType\nenum used in the driver API.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the address specified by\naddr\ndoes not fall within the address window of\n.global\nor\n.shared::cta\nstate space then the behavior is undefined.\ntensormap.replace\nis treated as a weak memory operation, on the entire 1024-bit opaque\ntensor-map\nobject, in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nQualifier\n.swizzle_atomicity\nintroduced in PTX ISA version 8.6.\nQualifier\n.elemtype\nwith values from\n13\nto\n15\n, both inclusive, is\nsupported in PTX ISA version 8.7 onwards.\nQualifier\n.swizzle_mode\nwith value\n4\nis supported from PTX ISA version 8.8 onwards.\nTarget ISA Notes\nSupported on following architectures:\nsm_90a\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nQualifier\n.swizzle_atomicity\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\n(refer to\nsection\nfor restrictions on sm_120a)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.field3\nvariant\n.elemtype\ncorresponding to\nnew_val\nvalues\n13\n,\n14\nand\n15\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\n(refer to\nsection\nfor restrictions on sm_120a)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.field3\nvariant\n.swizzle_mode\ncorresponding to\nnew_val\nvalue\n4\nis supported on\nfollowing architectures:\nsm_103a\n(refer to\nsection\nfor restrictions on sm_103a)\nExamples\ntensormap.replace.tile.global_address.shared::cta.b1024.b64   [sMem], new_val;\n9.7.10.\nTexture Instructions\n\nThis section describes PTX instructions for accessing textures and samplers. PTX supports the\nfollowing operations on texture and sampler descriptors:\nStatic initialization of texture and sampler descriptors.\nModule-scope and per-entry scope definitions of texture and sampler descriptors.\nAbility to query fields within texture and sampler descriptors.\n9.7.10.1.\nTexturing Modes\n\nFor working with textures and samplers, PTX has two modes of operation. In the\nunified mode,\ntexture and sampler information is accessed through a single\n.texref\nhandle. In the\nindependent\nmode\n, texture and sampler information each have their own handle, allowing them to be defined\nseparately and combined at the site of usage in the program.\nThe advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior\nto\nsm_3x\n), with the restriction that they correspond 1-to-1 with the 256 possible textures per\nkernel (128 for architectures prior to\nsm_3x\n). The advantage of independent mode is that\ntextures and samplers can be mixed and matched, but the number of samplers is greatly restricted to\n32 per kernel (16 for architectures prior to\nsm_3x\n).\nTable 34\nsummarizes the number of textures, samplers and\nsurfaces available in different texturing modes.\nTable 34\nTexture, sampler and surface limits\n\nTexturing mode\nResource\nsm_1x\n,\nsm_2x\nsm_3x+\nUnified mode\nTextures\n128\n256\nSamplers\n128\n256\nSurfaces\n8\n16\nIndependent mode\nTextures\n128\n256\nSamplers\n16\n32\nSurfaces\n8\n16\nThe texturing mode is selected using\n.target\noptions\ntexmode_unified\nand\ntexmode_independent\n. A PTX module may declare only one texturing mode. If no texturing mode is\ndeclared, the module is assumed to use unified mode.\nExample\n: calculate an element’s power contribution as element’s power/total number of elements.\n.target texmode_independent\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n...\n.entry compute_power\n  ( .param .texref tex1 )\n{\n  txq.width.b32  r6, [tex1]; // get tex1's width\n  txq.height.b32 r5, [tex1]; // get tex1's height\n  tex.2d.v4.f32.f32  {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}];\n  mul.u32 r5, r5, r6;\n  add.f32 r1, r1, r2;\n  add.f32 r3, r3, r4;\n  add.f32 r1, r1, r3;\n  cvt.f32.u32 r5, r5;\n  div.f32 r1, r1, r5;\n}\n9.7.10.2.\nMipmaps\n\nA\nmipmap\nis a sequence of textures, each of which is a progressively lower resolution\nrepresentation of the same image. The height and width of each image, or level of detail (LOD), in\nthe mipmap is a power of two smaller than the previous level. Mipmaps are used in graphics\napplications to improve rendering speed and reduce aliasing artifacts. For example, a\nhigh-resolution mipmap image is used for objects that are close to the user; lower-resolution images\nare used as the object appears farther away. Mipmap filtering modes are provided when switching\nbetween two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity.\nExample:\nIf the texture has a basic size of 256 by 256 pixels, then the associated mipmap set\nmay contain a series of eight images, each one-fourth the total area of the previous one: 128x128\npixels, 64x64, 32x32, 16x16, 8x8, 4x4, 2x2, 1x1 (a single pixel). If, for example, a scene is\nrendering this texture in a space of 40x40 pixels, then either a scaled up version of the 32x32\n(without trilinear interpolation) or an interpolation of the 64x64 and the 32x32 mipmaps (with\ntrilinear interpolation) would be used.\nThe total number of LODs in a complete mipmap pyramid is calculated through the following equation:\nnumLODs = 1 + floor(log2(max(w, h, d)))\nThe finest LOD is called the base level and is the 0th level. The next (coarser) level is the 1st\nlevel, and so on. The coarsest level is the level of size (1 x 1 x 1). Each successively smaller\nmipmap level has half the {width, height, depth} of the previous level, but if this half value is a\nfractional value, it’s rounded down to the next largest integer. Essentially, the size of a mipmap\nlevel can be specified as:\nmax(1, floor(w_b / 2^i)) x\nmax(1, floor(h_b / 2^i)) x\nmax(1, floor(d_b / 2^i))\nwhere\ni\nis the ith level beyond the 0th level (the base level). And\nw_b\n,\nh_b\nand\nd_b\nare the\nwidth, height and depth of the base level respectively.\nPTX support for mipmaps\nThe PTX\ntex\ninstruction supports three modes for specifying the LOD:\nbase\n,\nlevel\n, and\ngrad\nient. In base mode, the instruction always picks level 0. In level mode, an additional\nargument is provided to specify the LOD to fetch from. In gradmode, two floating-point vector\narguments provide\npartials\n(e.g.,\n{ds/dx,\ndt/dx}\nand\n{ds/dy,\ndt/dy}\nfor a 2d texture),\nwhich the\ntex\ninstruction uses to compute the LOD.\nThese instructions provide access to texture memory.\ntex\ntld4\ntxq\n9.7.10.3.\nTexture Instructions:\ntex\n\ntex\nPerform a texture memory lookup.\nSyntax\ntex.geom.v4.dtype.ctype  d, [a, c] {, e} {, f};\ntex.geom.v4.dtype.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\ntex.geom.v2.f16x2.ctype  d[|p], [a, c] {, e} {, f};\ntex.geom.v2.f16x2.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n// mipmaps\ntex.base.geom.v4.dtype.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v4.dtype.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v4.dtype.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\ntex.base.geom.v2.f16x2.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v2.f16x2.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v2.f16x2.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms };\n.dtype = { .u32, .s32, .f16,  .f32 };\n.ctype = {       .s32, .f32 };          // .cube, .acube require .f32\n                                        // .2dms, .a2dms require .s32\nDescription\ntex.{1d,2d,3d}\nTexture lookup using a texture coordinate vector. The instruction loads data from the texture named\nby operand\na\nat coordinates given by operand\nc\ninto destination\nd\n. Operand\nc\nis a\nscalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a\nfour-element vector for 3d textures, where the fourth element is ignored. An optional texture\nsampler\nb\nmay be specified. If no sampler is specified, the sampler behavior is a property of\nthe named texture. The optional destination predicate\np\nis set to\nTrue\nif data from texture\nat specified coordinates is resident in memory,\nFalse\notherwise. When optional destination\npredicate\np\nis set to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data\nat specified coordinates is dependent on execution environment setup using Driver API calls, prior\nto kernel launch. Refer to Driver API documentation for more details including any\nsystem/implementation specific behavior.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nvalues that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand\ne\nis a singleton tuple for 1d textures; is a two\nelement vector 2d textures; and is four-element vector for 3d textures, where the fourth element is\nignored.\nAn optional operand\nf\nmay be specified for\ndepth\ntextures\n. Depth textures are special type\nof textures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in\nf\noperand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering. When using depth compare operand, the elements in texture coordinate vector\nc\nhave\n.f32\ntype.\nDepth compare operand is not supported for\n3d\ntextures.\nThe instruction returns a two-element vector for destination type\n.f16x2\n. For all other\ndestination types, the instruction returns a four-element vector. Coordinates may be given in either\nsigned 32-bit integer or 32-bit floating point form.\nA texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\ntex.{a1d,a2d}\nTexture array selection, followed by texture lookup. The instruction first selects a texture from\nthe texture array named by operand\na\nusing the index given by the first element of the array\ncoordinate vector\nc\n. The instruction then loads data from the selected texture at coordinates\ngiven by the remaining elements of operand\nc\ninto destination\nd\n. Operand\nc\nis a bit-size\ntype vector or tuple containing an index into the array of textures followed by coordinates within\nthe selected texture, as follows:\nFor 1d texture arrays, operand\nc\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the second element is interpreted as\na 1d texture coordinate of type\n.ctype\n.\nFor 2d texture arrays, operand\nc\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the next two elements are\ninterpreted as 2d texture coordinates of type\n.ctype\n. The fourth element is ignored.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nvalues that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand\ne\nis a singleton tuple for 1d texture arrays; and is\na two element vector 2d texture arrays.\nAn optional operand\nf\nmay be specified for depth textures arrays. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth textures. When using depth compare\noperand, the coordinates in texture coordinate vector\nc\nhave\n.f32\ntype.\nThe instruction returns a two-element vector for destination type\n.f16x2\n. For all other\ndestination types, the instruction returns a four-element vector. The texture array index is a\n32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point\nvalues.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.cube\nCubemap\ntexture lookup. The instruction loads data from the cubemap texture named by operand\na\nat coordinates given by operand\nc\ninto destination\nd\n. Cubemap textures are special\ntwo-dimensional layered textures consisting of six layers that represent the faces of a cube. All\nlayers in a cubemap are of the same size and are square (i.e., width equals height).\nWhen accessing a cubemap, the texture coordinate vector\nc\nhas type\n.v4.f32\n, and comprises\nthree floating-point coordinates (\ns\n,\nt\n,\nr\n) and a fourth padding argument which is\nignored. Coordinates (\ns\n,\nt\n,\nr\n) are projected onto one of the six cube faces. The (\ns\n,\nt\n,\nr\n) coordinates can be thought of as a direction vector emanating from the center of the\ncube. Of the three coordinates (\ns\n,\nt\n,\nr\n), the coordinate of the largest magnitude (the\nmajor axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by\nthe absolute value of the major axis to produce a new (\ns\n,\nt\n) coordinate pair to lookup into\nthe selected cube face.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nOffset vector operand\ne\nis not supported for cubemap textures.\nan optional operand\nf\nmay be specified for cubemap depth textures. operand\nf\nis\n.f32\nscalar value that specifies depth compare value for cubemap depth textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.acube\nCubemap array selection, followed by cubemap lookup. The instruction first selects a cubemap texture\nfrom the cubemap array named by operand\na\nusing the index given by the first element of the\narray coordinate vector\nc\n. The instruction then loads data from the selected cubemap texture at\ncoordinates given by the remaining elements of operand\nc\ninto destination\nd\n.\nCubemap array\ntextures consist of an array of cubemaps, i.e., the total number of layers is a\nmultiple of six. When accessing a cubemap array texture, the coordinate vector\nc\nhas type\n.v4.b32\n. The first element is interpreted as an unsigned integer index (\n.u32\n) into the\ncubemap array, and the remaining three elements are interpreted as floating-point cubemap\ncoordinates (\ns\n,\nt\n,\nr\n), used to lookup in the selected cubemap as described above.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nOffset vector operand\ne\nis not supported for cubemap texture arrays.\nAn optional operand\nf\nmay be specified for cubemap depth texture arrays. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for cubemap depth textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.2dms\nMulti-sample texture lookup using a texture coordinate vector. Multi-sample textures consist of\nmultiple samples per data element. The instruction loads data from the texture named by operand\na\nfrom sample number given by first element of the operand\nc\n, at coordinates given by\nremaining elements of operand\nc\ninto destination\nd\n. When accessing a multi-sample texture,\ntexture coordinate vector\nc\nhas type\n.v4.b32\n. The first element in operand\nc\nis\ninterpreted as unsigned integer sample number (\n.u32\n), and the next two elements are interpreted\nas signed integer (\n.s32\n) 2d texture coordinates. The fourth element is ignored. An optional\ntexture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior is a\nproperty of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7.\nDepth compare operand\nf\nis not supported for multi-sample textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.a2dms\nMulti-sample texture array selection, followed by multi-sample texture lookup. The instruction first\nselects a multi-sample texture from the multi-sample texture array named by operand a using the\nindex given by the first element of the array coordinate vector\nc\n. The instruction then loads\ndata from the selected multi-sample texture from sample number given by second element of the\noperand\nc\n, at coordinates given by remaining elements of operand\nc\ninto destination\nd\n. When accessing a multi-sample texture array, texture coordinate vector\nc\nhas type\n.v4.b32\n. The first element in operand c is interpreted as unsigned integer sampler number, the\nsecond element is interpreted as unsigned integer index (\n.u32\n) into the multi-sample texture\narray and the next two elements are interpreted as signed integer (\n.s32\n) 2d texture\ncoordinates. An optional texture sampler\nb\nmay be specified. If no sampler is specified, the\nsampler behavior is a property of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nvalues\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7.\nDepth compare operand\nf\nis not supported for multi-sample texture arrays.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nMipmaps\n.base\n(lod zero)\nPick level 0 (base level). This is the default if no mipmap mode is specified. No additional arguments.\n.level\n(lod explicit)\nRequires an additional 32-bit scalar argument,\nlod\n, which contains the LOD to fetch from. The\ntype of\nlod\nfollows\n.ctype\n(either\n.s32\nor\n.f32\n). Geometries\n.2dms\nand\n.a2dms\nare not supported in this mode.\n.grad\n(lod gradient)\nRequires two\n.f32\nvectors,\ndPdx\nand\ndPdy\n, that specify the partials. The vectors are\nsingletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are\nfour-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d\nand cube geometries. Geometries\n.2dms\nand\n.a2dms\nare not supported in this mode.\nFor mipmap texture lookup, an optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7. Offset vector operand is not supported for cube\nand cubemap geometries.\nAn optional operand\nf\nmay be specified for mipmap textures. Operand\nf\nis\n.f32\nscalar\nvalue that specifies depth compare value for depth textures. When using depth compare operand, the\ncoordinates in texture coordinate vector\nc\nhave\n.f32\ntype.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nDepth compare operand is not supported for\n3d\ntextures.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nNotes\nFor compatibility with prior versions of PTX, the square brackets are not required and\n.v4\ncoordinate vectors are allowed for any geometry, with the extra elements being ignored.\nPTX ISA Notes\nUnified mode texturing introduced in PTX ISA version 1.0. Extension using opaque\n.texref\nand\n.samplerref\ntypes and independent mode texturing introduced in PTX ISA version 1.5.\nTexture arrays\ntex.{a1d,a2d}\nintroduced in PTX ISA version 2.3.\nCubemaps and cubemap arrays introduced in PTX ISA version 3.0.\nSupport for mipmaps introduced in PTX ISA version 3.1.\nIndirect texture access introduced in PTX ISA version 3.1.\nMulti-sample textures and multi-sample texture arrays introduced in PTX ISA version 3.2.\nSupport for textures returning\n.f16\nand\n.f16x2\ndata introduced in PTX ISA version 4.2.\nSupport for\ntex.grad.{cube,\nacube}\nintroduced in PTX ISA version 4.3.\nOffset vector operand introduced in PTX ISA version 4.3.\nDepth compare operand introduced in PTX ISA version 4.3.\nSupport for optional destination predicate introduced in PTX ISA version 7.1.\nTarget ISA Notes\nSupported on all target architectures.\nThe cubemap array geometry (\n.acube\n) requires\nsm_20\nor higher.\nMipmaps require\nsm_20\nor higher.\nIndirect texture access requires\nsm_20\nor higher.\nMulti-sample textures and multi-sample texture arrays require\nsm_30\nor higher.\nTexture fetch returning\n.f16\nand\n.f16x2\ndata require\nsm_53\nor higher.\ntex.grad.{cube,\nacube}\nrequires\nsm_20\nor higher.\nOffset vector operand requires\nsm_30\nor higher.\nDepth compare operand requires\nsm_30\nor higher.\nSupport for optional destination predicate requires\nsm_60\nor higher.\nExamples\n// Example of unified mode texturing\n // - f4 is required to pad four-element tuple and is ignored\n tex.3d.v4.s32.s32  {r1,r2,r3,r4}, [tex_a,{f1,f2,f3,f4}];\n\n // Example of independent mode texturing\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,smpl_x,{f1}];\n\n // Example of 1D texture array, independent texturing mode\n tex.a1d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,smpl_x,{idx,s1}];\n\n // Example of 2D texture array, unified texturing mode\n // - f3 is required to pad four-element tuple and is ignored\n tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2,f3}];\n\n // Example of cubemap array, unified textureing mode\n tex.acube.v4.f32.f32 {r0,r1,r2,r3}, [tex_cuarray,{idx,f1,f2,f3}];\n\n // Example of multi-sample texture, unified texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture, independent texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms, smpl_x,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture array, unified texturing mode\n tex.a2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ams,{idx,sample,r6,r7}];\n\n // Example of texture returning .f16 data\n tex.1d.v4.f16.f32  {h1,h2,h3,h4}, [tex_a,smpl_x,{f1}];\n\n // Example of texture returning .f16x2 data\n tex.1d.v2.f16x2.f32  {h1,h2}, [tex_a,smpl_x,{f1}];\n\n // Example of 3d texture array access with tex.grad,unified texturing mode\n tex.grad.3d.v4.f32.f32 {%f4,%f5,%f6,%f7},[tex_3d,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n// Example of cube texture array access with tex.grad,unified texturing mode\n tex.grad.cube.v4.f32.f32{%f4,%f5,%f6,%f7},[tex_cube,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n // Example of 1d texture lookup with offset, unified texturing mode\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a, {f1}], {r5};\n\n // Example of 2d texture array lookup with offset, unified texturing mode\n tex.a2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{idx,f1,f2}], {f5,f6};\n\n // Example of 2d mipmap texture lookup with offset, unified texturing mode\n tex.level.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}],\n                          flvl, {r7, r8};\n\n // Example of 2d depth texture lookup with compare, unified texturing mode\n tex.1d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a, {f1}], f0;\n\n // Example of depth 2d texture array lookup with offset, compare\n tex.a2d.v4.s32.f32  {f0,f1,f2,f3}, [tex_a,{idx,f4,f5}], {r5,r6}, f6;\n\n // Example of destination predicate use\n tex.3d.v4.s32.s32 {r1,r2,r3,r4}|p, [tex_a,{f1,f2,f3,f4}];\n9.7.10.4.\nTexture Instructions:\ntld4\n\ntld4\nPerform a texture fetch of the 4-texel bilerp footprint.\nSyntax\ntld4.comp.2d.v4.dtype.f32    d[|p], [a, c] {, e} {, f};\ntld4.comp.geom.v4.dtype.f32  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n.comp  = { .r, .g, .b, .a };\n.geom  = { .2d, .a2d, .cube, .acube };\n.dtype = { .u32, .s32, .f32 };\nDescription\nTexture fetch of the 4-texel bilerp footprint using a texture coordinate vector. The instruction\nloads the bilerp footprint from the texture named by operand\na\nat coordinates given by operand\nc\ninto vector destination\nd\n. The texture component fetched for each texel sample is\nspecified by\n.comp\n. The four texel samples are placed into destination vector\nd\nin\ncounter-clockwise order starting at lower left.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nAn optional operand\nf\nmay be specified for\ndepth textures\n. Depth textures are special type of\ntextures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in\nf\noperand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering.\nA texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\ntld4.2d\nFor 2D textures, operand\nc\nspecifies coordinates as a two-element, 32-bit floating-point vector.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7.\ntld4.a2d\nTexture array selection, followed by\ntld4\ntexture fetch of 2d texture. For 2d texture arrays\noperand\nc\nis a four element, 32-bit vector. The first element in operand c is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the next two elements are interpreted\nas 32-bit floating point coordinates of 2d texture. The fourth element is ignored.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7.\ntld4.cube\nFor cubemap textures, operand\nc\nspecifies four-element vector which comprises three\nfloating-point coordinates (s, t, r) and a fourth padding argument which is ignored.\nCubemap textures are special two-dimensional layered textures consisting of six layers that\nrepresent the faces of a cube. All layers in a cubemap are of the same size and are square (i.e.,\nwidth equals height).\nCoordinates (s, t, r) are projected onto one of the six cube faces. The (s, t, r) coordinates can be\nthought of as a direction vector emanating from the center of the cube. Of the three coordinates (s,\nt, r), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the\nother two coordinates (the minor axes) are divided by the absolute value of the major axis to\nproduce a new (s, t) coordinate pair to lookup into the selected cube face.\nOffset vector operand\ne\nis not supported for cubemap textures.\ntld4.acube\nCubemap array selection, followed by\ntld4\ntexture fetch of cubemap texture. The first element in\noperand\nc\nis interpreted as an unsigned integer index (\n.u32\n) into the cubemap texture array,\nand the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r),\nused to lookup in the selected cubemap.\nOffset vector operand\ne\nis not supported for cubemap texture arrays.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.2.\nIndirect texture access introduced in PTX ISA version 3.1.\ntld4.{a2d,cube,acube}\nintroduced in PTX ISA version 4.3.\nOffset vector operand introduced in PTX ISA version 4.3.\nDepth compare operand introduced in PTX ISA version 4.3.\nSupport for optional destination predicate introduced in PTX ISA version 7.1.\nTarget ISA Notes\ntld4\nrequires\nsm_20\nor higher.\nIndirect texture access requires\nsm_20\nor higher.\ntld4.{a2d,cube,acube}\nrequires\nsm_30\nor higher.\nOffset vector operand requires\nsm_30\nor higher.\nDepth compare operand requires\nsm_30\nor higher.\nSupport for optional destination predicate requires\nsm_60\nor higher.\nExamples\n//Example of unified mode texturing\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}];\n\n// Example of independent mode texturing\ntld4.r.2d.v4.u32.f32  {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}];\n\n// Example of unified mode texturing using offset\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};\n\n// Example of unified mode texturing using compare\ntld4.r.2d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7;\n\n// Example of optional destination predicate\ntld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7;\n9.7.10.5.\nTexture Instructions:\ntxq\n\ntxq\nQuery texture and sampler attributes.\nSyntax\ntxq.tquery.b32         d, [a];       // texture attributes\ntxq.level.tlquery.b32  d, [a], lod;  // texture attributes\ntxq.squery.b32         d, [a];       // sampler attributes\n\n.tquery  = { .width, .height, .depth,\n             .channel_data_type, .channel_order,\n             .normalized_coords, .array_size,\n             .num_mipmap_levels, .num_samples};\n\n.tlquery = { .width, .height, .depth };\n\n.squery  = { .force_unnormalized_coords, .filter_mode,\n             .addr_mode_0, addr_mode_1, addr_mode_2 };\nDescription\nQuery an attribute of a texture or sampler. Operand\na\nis either a\n.texref\nor\n.samplerref\nvariable, or a\n.u64\nregister.\nQuery\nReturns\n.width\n.height\n.depth\nvalue in elements\n.channel_data_type\nUnsigned integer corresponding to source language’s channel data type\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both\nchannel_data_type\nand channel_order queries.\n.channel_order\nUnsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.normalized_coords\n1\n(\nTrue\n) or\n0\n(\nFalse\n).\n.force_unnormalized_coords\n1\n(\nTrue)\nor\n0\n(\nFalse).\nDefined only for\n.samplerref\nvariables in independent texture mode. Overrides the\nnormalized_coords\nfield of a\n.texref\nvariable used with a\n.samplerref\nin a\ntex\ninstruction.\n.filter_mode\nInteger from\nenum\n{\nnearest,\nlinear\n}\n.addr_mode_0\n.addr_mode_1\n.addr_mode_2\nInteger from\nenum\n{\nwrap,\nmirror,\nclamp_ogl,\nclamp_to_edge,\nclamp_to_border\n}\n.array_size\nFor a texture array, number of textures in array, 0 otherwise.\n.num_mipmap_levels\nFor a mipmapped texture, number of levels of details (LOD), 0 otherwise.\n.num_samples\nFor a multi-sample texture, number of samples, 0 otherwise.\nTexture attributes are queried by supplying a\n.texref\nargument to\ntxq\n. In unified mode,\nsampler attributes are also accessed via a\n.texref\nargument, and in independent mode sampler\nattributes are accessed via a separate\n.samplerref\nargument.\ntxq.level\ntxq.level\nrequires an additional 32bit integer argument,\nlod\n, which specifies LOD and\nqueries requested attribute for the specified LOD.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.5.\nChannel data type and channel order queries were added in PTX ISA version 2.1.\nThe\n.force_unnormalized_coords\nquery was added in PTX ISA version 2.2.\nIndirect texture access introduced in PTX ISA version 3.1.\n.array_size\n,\n.num_mipmap_levels\n,\n.num_samples\nsamples queries were added in PTX ISA\nversion 4.1.\ntxq.level\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nSupported on all target architectures.\nIndirect texture access requires\nsm_20\nor higher.\nQuerying the number of mipmap levels requires\nsm_20\nor higher.\nQuerying the number of samples requires\nsm_30\nor higher.\ntxq.level\nrequires\nsm_30\nor higher.\nExamples\ntxq.width.b32       %r1, [tex_A];\ntxq.filter_mode.b32 %r1, [tex_A];   // unified mode\ntxq.addr_mode_0.b32 %r1, [smpl_B];  // independent mode\ntxq.level.width.b32 %r1, [tex_A], %r_lod;\n9.7.10.6.\nTexture Instructions:\nistypep\n\nistypep\nQuery whether a register points to an opaque variable of a specified type.\nSyntax\nistypep.type   p, a;  // result is .pred\n\n.type = { .texref, .samplerref, .surfref };\nDescription\nWrite predicate register\np\nwith 1 if register\na\npoints to an opaque variable of the\nspecified type, and with 0 otherwise. Destination\np\nhas type\n.pred\n; the source address\noperand must be of type\n.u64\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.0.\nTarget ISA Notes\nistypep requires\nsm_30\nor higher.\nExamples\nistypep.texref istex, tptr;\nistypep.samplerref issampler, sptr;\nistypep.surfref issurface, surfptr;\n9.7.11.\nSurface Instructions\n\nThis section describes PTX instructions for accessing surfaces. PTX supports the following\noperations on surface descriptors:\nStatic initialization of surface descriptors.\nModule-scope and per-entry scope definitions of surface descriptors.\nAbility to query fields within surface descriptors.\nThese instructions provide access to surface memory.\nsuld\nsust\nsured\nsuq\n9.7.11.1.\nSurface Instructions:\nsuld\n\nsuld\nLoad from surface memory.\nSyntax\nsuld.b.geom{.cop}.vec.dtype.clamp  d, [a, b];  // unformatted\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d };\n.cop   = { .ca, .cg, .cs, .cv };               // cache operation\n.vec   = { none, .v2, .v4 };\n.dtype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero };\nDescription\nsuld.b.{1d,2d,3d}\nLoad from surface memory using a surface coordinate vector. The instruction loads data from the\nsurface named by operand\na\nat coordinates given by operand\nb\ninto destination\nd\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a scalar or singleton tuple\nfor 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d\nsurfaces, where the fourth element is ignored. Coordinate elements are of type\n.s32\n.\nsuld.b\nperforms an unformatted load of binary data. The lowest dimension coordinate represents a\nbyte offset into the surface and is not scaled, and the size of the data transfer matches the size\nof destination operand\nd\n.\nsuld.b.{a1d,a2d}\nSurface layer selection, followed by a load from the selected surface. The instruction first selects\na surface layer from the surface array named by operand\na\nusing the index given by the first\nelement of the array coordinate vector\nb\n. The instruction then loads data from the selected\nsurface at coordinates given by the remaining elements of operand\nb\ninto destination\nd\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a bit-size\ntype vector or tuple containing an index into the array of surfaces followed by coordinates within\nthe selected surface, as follows:\nFor 1d surface arrays, operand\nb\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the second element is interpreted as a\n1d surface coordinate of type\n.s32\n.\nFor 2d surface arrays, operand\nb\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the next two elements are interpreted\nas 2d surface coordinates of type\n.s32\n. The fourth element is ignored.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nloads data at the nearest surface location (sized appropriately)\n.zero\nloads zero for out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nsuld.b.trap\nintroduced in PTX ISA version 1.5.\nAdditional clamp modifiers and cache operations introduced in PTX ISA version 2.0.\nsuld.b.3d\nand\nsuld.b.{a1d,a2d}\nintroduced in PTX ISA version 3.0.\nIndirect surface access introduced in PTX ISA version 3.1.\nTarget ISA Notes\nsuld.b\nsupported on all target architectures.\nsm_1x\ntargets support only the\n.trap\nclamping modifier.\nsuld.3d\nand\nsuld.{a1d,a2d}\nrequire\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nExamples\nsuld.b.1d.v4.b32.trap  {s1,s2,s3,s4}, [surf_B, {x}];\nsuld.b.3d.v2.b64.trap  {r1,r2}, [surf_A, {x,y,z,w}];\nsuld.b.a1d.v2.b32      {r0,r1}, [surf_C, {idx,x}];\nsuld.b.a2d.b32         r0, [surf_D, {idx,x,y,z}];  // z ignored\n9.7.11.2.\nSurface Instructions:\nsust\n\nsust\nStore to surface memory.\nSyntax\nsust.b.{1d,2d,3d}{.cop}.vec.ctype.clamp  [a, b], c;  // unformatted\nsust.p.{1d,2d,3d}.vec.b32.clamp          [a, b], c;  // formatted\n\nsust.b.{a1d,a2d}{.cop}.vec.ctype.clamp   [a, b], c;  // unformatted\n\n.cop   = { .wb, .cg, .cs, .wt };                     // cache operation\n.vec   = { none, .v2, .v4 };\n.ctype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero };\nDescription\nsust.{1d,2d,3d}\nStore to surface memory using a surface coordinate vector. The instruction stores data from operand\nc\nto the surface named by operand\na\nat coordinates given by operand\nb\n. Operand\na\nis\na\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a scalar or singleton tuple for 1d\nsurfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces,\nwhere the fourth element is ignored. Coordinate elements are of type\n.s32\n.\nsust.b\nperforms an unformatted store of binary data. The lowest dimension coordinate represents\na byte offset into the surface and is not scaled. The size of the data transfer matches the size of\nsource operand\nc\n.\nsust.p\nperforms a formatted store of a vector of 32-bit data values to a surface sample. The\nsource vector elements are interpreted left-to-right as\nR\n,\nG\n,\nB\n, and\nA\nsurface\ncomponents. These elements are written to the corresponding surface sample components. Source\nelements that do not occur in the surface sample are ignored. Surface sample components that do not\noccur in the source vector will be written with an unpredictable value. The lowest dimension\ncoordinate represents a sample offset rather than a byte offset.\nThe source data interpretation is based on the surface sample format as follows: If the surface\nformat contains\nUNORM\n,\nSNORM\n, or\nFLOAT\ndata, then\n.f32\nis assumed; if the surface\nformat contains\nUINT\ndata, then\n.u32\nis assumed; if the surface format contains\nSINT\ndata, then\n.s32\nis assumed. The source data is then converted from this type to the surface\nsample format.\nsust.b.{a1d,a2d}\nSurface layer selection, followed by an unformatted store to the selected surface. The instruction\nfirst selects a surface layer from the surface array named by operand\na\nusing the index given by\nthe first element of the array coordinate vector\nb\n. The instruction then stores the data in\noperand\nc\nto the selected surface at coordinates given by the remaining elements of operand\nb\n. Operand\na\nis a .surfref variable or\n.u64\nregister. Operand\nb\nis a bit-size type\nvector or tuple containing an index into the array of surfaces followed by coordinates within the\nselected surface, as follows:\nFor 1d surface arrays, operand\nb\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the second element is interpreted as\na 1d surface coordinate of type\n.s32\n.\nFor 2d surface arrays, operand\nb\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the next two elements are\ninterpreted as 2d surface coordinates of type\n.s32\n. The fourth element is ignored.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nstores data at the nearest surface location (sized appropriately)\n.zero\ndrops stores to out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nsust.b.trap\nintroduced in PTX ISA version 1.5.\nsust.p\n, additional clamp modifiers, and\ncache operations introduced in PTX ISA version 2.0.\nsust.b.3d\nand\nsust.b.{a1d,a2d}\nintroduced in PTX ISA version 3.0.\nIndirect surface access introduced in PTX ISA version 3.1.\nTarget ISA Notes\nsust.b\nsupported on all target architectures.\nsm_1x\ntargets support only the\n.trap\nclamping modifier.\nsust.3d\nand\nsust.{a1d,a2d}\nrequire\nsm_20\nor higher.\nsust.p\nrequires\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nExamples\nsust.p.1d.v4.b32.trap  [surf_B, {x}], {f1,f2,f3,f4};\nsust.b.3d.v2.b64.trap  [surf_A, {x,y,z,w}], {r1,r2};\nsust.b.a1d.v2.b64      [surf_C, {idx,x}], {r1,r2};\nsust.b.a2d.b32         [surf_D, {idx,x,y,z}], r0;  // z ignored\n9.7.11.3.\nSurface Instructions:\nsured\n\nsured\nReduce surface memory.\nSyntax\nsured.b.op.geom.ctype.clamp  [a,b],c; // byte addressing\nsured.p.op.geom.ctype.clamp  [a,b],c; // sample addressing\n\n.op    = { .add, .min, .max, .and, .or };\n.geom  = { .1d, .2d, .3d };\n.ctype = { .u32, .u64, .s32, .b32, .s64 };  // for sured.b\n.ctype = { .b32, .b64 };                    // for sured.p\n.clamp = { .trap, .clamp, .zero };\nDescription\nReduction to surface memory using a surface coordinate vector. The instruction performs a reduction\noperation with data from operand\nc\nto the surface named by operand\na\nat coordinates given by\noperand\nb\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a\nscalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a\nfour-element vector for 3d surfaces, where the fourth element is ignored. Coordinate elements are of\ntype\n.s32\n.\nsured.b\nperforms an unformatted reduction on\n.u32\n,\n.s32\n,\n.b32\n,\n.u64\n, or\n.s64\ndata. The lowest dimension coordinate represents a byte offset into the surface and is not\nscaled. Operation\nadd\napplies to\n.u32\n,\n.u64\n, and\n.s32\ntypes;\nmin\nand\nmax\napply to\n.u32\n,\n.s32\n,\n.u64\nand\n.s64\ntypes; operations\nand\nand\nor\napply to\n.b32\ntype.\nsured.p\nperforms an unformatted reduction on sample-addressed data. The lowest dimension coordinate\nrepresents a sample offset rather than a byte offset and is scaled by the data-size given by\n.ctype\n.\nThe instruction type\n.b64\nis restricted to\nmin\nand\nmax\noperations. For type\n.b32\n,\nthe data is interpreted as\n.u32\nor\n.s32\nbased on the surface sample format as follows:\nif the surface format contains\nUINT\ndata, then\n.u32\nis assumed; if the surface format contains\nSINT\ndata, then\n.s32\nis assumed. For type\n.b64\n, if the surface format contains\nUINT\ndata,\nthen\n.u64\nis assumed; if the surface format contains\nSINT\ndata, then\n.s64\nis assumed.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nstores data at the nearest surface location (sized appropriately)\n.zero\ndrops stores to out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nIndirect surface access introduced in PTX ISA version 3.1.\n.u64\n/\n.s64\n/\n.b64\ntypes with\n.min\n/\n.max\noperations introduced in PTX ISA version\n8.1.\nTarget ISA Notes\nsured requires\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\n.u64\n/\n.s64\n/\n.b64\ntypes with\n.min\n/\n.max\noperations requires\nsm_50\nor higher.\nExamples\nsured.b.add.2d.u32.trap  [surf_A, {x,y}], r1;\nsured.p.min.1d.u32.trap  [surf_B, {x}], r1;\nsured.b.max.1d.u64.trap  [surf_C, {x}], r1;\nsured.p.min.1d.b64.trap  [surf_D, {x}], r1;\n9.7.11.4.\nSurface Instructions:\nsuq\n\nsuq\nQuery a surface attribute.\nSyntax\nsuq.query.b32   d, [a];\n\n.query = { .width, .height, .depth,\n           .channel_data_type, .channel_order,\n           .array_size, .memory_layout };\nDescription\nQuery an attribute of a surface. Operand\na\nis a\n.surfref\nvariable or a\n.u64\nregister.\nQuery\nReturns\n.width\n.height\n.depth\nvalue in elements\n.channel_data_type\nUnsigned integer corresponding to source language’s channel data\ntype enumeration. If the source language combines channel data\ntype and channel order into a single enumeration type, that value\nis returned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.channel_order\nUnsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and\nchannel order into a single enumeration type, that value is\nreturned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.array_size\nFor a surface array, number of surfaces in array, 0 otherwise.\n.memory_layout\n1\nfor surface with linear memory layout;\n0\notherwise\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.5.\nChannel data type and channel order queries added in PTX ISA version 2.1.\nIndirect surface access introduced in PTX ISA version 3.1.\nThe\n.array_size\nquery was added in PTX ISA version 4.1.\nThe\n.memory_layout\nquery was added in PTX ISA version 4.2.\nTarget ISA Notes\nSupported on all target architectures.\nIndirect surface access requires\nsm_20\nor higher.\nExamples\nsuq.width.b32       %r1, [surf_A];\n9.7.12.\nControl Flow Instructions\n\nThe following PTX instructions and syntax are for controlling execution in a PTX program:\n{}\n@\nbra\ncall\nret\nexit\n9.7.12.1.\nControl Flow Instructions:\n{}\n\n{}\nInstruction grouping.\nSyntax\n{ instructionList }\nDescription\nThe curly braces create a group of instructions, used primarily for defining a function body. The\ncurly braces also provide a mechanism for determining the scope of a variable: any variable declared\nwithin a scope is not available outside the scope.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n{ add.s32  a,b,c; mov.s32  d,a; }\n9.7.12.2.\nControl Flow Instructions:\n@\n\n@\nPredicated execution.\nSyntax\n@{!}p    instruction;\nDescription\nExecute an instruction or instruction block for threads that have the guard predicate\nTrue\n. Threads with a\nFalse\nguard predicate do nothing.\nSemantics\nIf\n{!}p\nthen instruction\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsetp.eq.f32  p,y,0;     // is y zero?\n@!p div.f32      ratio,x,y  // avoid division by zero\n\n@q  bra L23;                // conditional branch\n9.7.12.3.\nControl Flow Instructions:\nbra\n\nbra\nBranch to a target and continue execution there.\nSyntax\n@p   bra{.uni}  tgt;           // tgt is a label\n     bra{.uni}  tgt;           // unconditional branch\nDescription\nContinue execution at the target. Conditional branches are specified by using a guard predicate. The\nbranch target must be a label.\nbra.uni\nis guaranteed to be non-divergent, i.e. all active threads in a warp that are currently\nexecuting this instruction have identical values for the guard predicate and branch target.\nSemantics\nif (p) {\n    pc = tgt;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nUnimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nbra.uni  L_exit;    // uniform unconditional jump\n@q  bra      L23;   // conditional branch\n9.7.12.4.\nControl Flow Instructions:\nbrx.idx\n\nbrx.idx\nBranch to a label indexed from a list of potential branch targets.\nSyntax\n@p    brx.idx{.uni} index, tlist;\n      brx.idx{.uni} index, tlist;\nDescription\nIndex into a list of possible destination labels, and continue execution from the chosen\nlabel. Conditional branches are specified by using a guard predicate.\nbrx.idx.uni\nguarantees that the branch is non-divergent, i.e. all active threads in a warp that\nare currently executing this instruction have identical values for the guard predicate and the\nindex\nargument.\nThe\nindex\noperand is a\n.u32\nregister. The\ntlist\noperand must be the label of a\n.branchtargets\ndirective. It is accessed as a zero-based sequence using\nindex\n. Behaviour is\nundefined if the value of\nindex\nis greater than or equal to the length of\ntlist\n.\nThe\n.branchtargets\ndirective must be defined in the local function scope before it is used. It\nmust refer to labels within the current function.\nSemantics\nif (p) {\n    if (index < length(tlist)) {\n      pc = tlist[index];\n    } else {\n      pc = undefined;\n    }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\n.function foo () {\n    .reg .u32 %r0;\n    ...\n    L1:\n    ...\n    L2:\n    ...\n    L3:\n    ...\n    ts: .branchtargets L1, L2, L3;\n    @p brx.idx %r0, ts;\n    ...\n}\n9.7.12.5.\nControl Flow Instructions:\ncall\n\ncall\nCall a function, recording the return location.\nSyntax\n// direct call to named function, func is a symbol\ncall{.uni} (ret-param), func, (param-list);\ncall{.uni} func, (param-list);\ncall{.uni} func;\n\n// indirect call via pointer, with full list of call targets\ncall{.uni} (ret-param), fptr, (param-list), flist;\ncall{.uni} fptr, (param-list), flist;\ncall{.uni} fptr, flist;\n\n// indirect call via pointer, with no knowledge of call targets\ncall{.uni} (ret-param), fptr, (param-list), fproto;\ncall{.uni} fptr, (param-list), fproto;\ncall{.uni} fptr, fproto;\nDescription\nThe\ncall\ninstruction stores the address of the next instruction, so execution can resume at that\npoint after executing a\nret\ninstruction. A\ncall\nis assumed to be divergent unless the\n.uni\nsuffix is present. The\n.uni\nsuffix indicates that the\ncall\nis guaranteed to be\nnon-divergent, i.e. all active threads in a warp that are currently executing this instruction have\nidentical values for the guard predicate and\ncall\ntarget.\nFor direct calls, the called location\nfunc\nmust be a symbolic function name; for indirect calls,\nthe called location\nfptr\nmust be an address of a function held in a register. Input arguments\nand return values are optional. Arguments may be registers, immediate constants, or variables in\n.param\nspace. Arguments are pass-by-value.\nIndirect calls require an additional operand,\nflist\nor\nfproto\n, to communicate the list of\npotential\ncall\ntargets or the common function prototype of all\ncall\ntargets,\nrespectively. In the first case,\nflist\ngives a complete list of potential\ncall\ntargets and\nthe optimizing backend is free to optimize the calling convention. In the second case, where the\ncomplete list of potential\ncall\ntargets may not be known, the common function prototype is given\nand the\ncall\nmust obey the ABI’s calling convention.\nThe\nflist\noperand is either the name of an array (call table) initialized to a list of function\nnames; or a label associated with a\n.calltargets\ndirective, which declares a list of potential\ncall\ntargets. In both cases the fptr register holds the address of a function listed in the call\ntable or\n.calltargets\nlist, and the\ncall\noperands are type-checked against the type\nsignature of the functions indicated by\nflist\n.\nThe fproto operand is the name of a label associated with a\n.callprototype\ndirective. This\noperand is used when a complete list of potential targets is not known. The\ncall\noperands are\ntype-checked against the prototype, and code generation will follow the ABI calling convention. If a\nfunction that doesn’t match the prototype is called, the behavior is undefined.\nCall tables may be declared at module scope or local scope, in either the constant or global state\nspace. The\n.calltargets\nand\n.callprototype\ndirectives must be declared within a function\nbody. All functions must be declared prior to being referenced in a\ncall\ntable initializer or\n.calltargets\ndirective.\nPTX ISA Notes\nDirect\ncall\nintroduced in PTX ISA version 1.0. Indirect\ncall\nintroduced in PTX ISA version 2.1.\nTarget ISA Notes\nDirect\ncall\nsupported on all target architectures. Indirect\ncall\nrequires\nsm_20\nor higher.\nExamples\n// examples of direct call\n    call     init;    // call function 'init'\n    call.uni g, (a);  // call function 'g' with parameter 'a'\n@p  call     (d), h, (a, b);  // return value into register d\n\n// call-via-pointer using jump table\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n\n.global .u32 jmptbl[5] = { foo, bar, baz };\n      ...\n@p    ld.global.u32  %r0, [jmptbl+4];\n@p    ld.global.u32  %r0, [jmptbl+8];\n      call  (retval), %r0, (x, y), jmptbl;\n\n// call-via-pointer using .calltargets directive\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n      ...\n@p    mov.u32  %r0, foo;\n@q    mov.u32  %r0, baz;\nFtgt: .calltargets foo, bar, baz;\n      call  (retval), %r0, (x, y), Ftgt;\n\n// call-via-pointer using .callprototype directive\n.func dispatch (.reg .u32 fptr, .reg .u32 idx)\n{\n...\nFproto: .callprototype _ (.param .u32 _, .param .u32 _);\n      call  %fptr, (x, y), Fproto;\n...\n9.7.12.6.\nControl Flow Instructions:\nret\n\nret\nReturn from function to instruction after call.\nSyntax\nret{.uni};\nDescription\nReturn execution to caller’s environment. A divergent return suspends threads until all threads are\nready to return to the caller. This allows multiple divergent\nret\ninstructions.\nA\nret\nis assumed to be divergent unless the\n.uni\nsuffix is present, indicating that the\nreturn is guaranteed to be non-divergent.\nAny values returned from a function should be moved into the return parameter variables prior to\nexecuting the\nret\ninstruction.\nA return instruction executed in a top-level entry routine will terminate thread execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nret;\n@p  ret;\n9.7.12.7.\nControl Flow Instructions:\nexit\n\nexit\nTerminate a thread.\nSyntax\nexit;\nDescription\nEnds execution of a thread.\nBarriers exclusively waiting on arrivals from exited threads are always released.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nexit;\n@p  exit;\n9.7.13.\nParallel Synchronization and Communication Instructions\n\nThese instructions are:\nbar{.cta}\n,\nbarrier{.cta}\nbar.warp.sync\nbarrier.cluster\nmembar\natom\nred\nred.async\nvote\nmatch.sync\nactivemask\nredux.sync\ngriddepcontrol\nelect.sync\nmbarrier.init\nmbarrier.inval\nmbarrier.arrive\nmbarrier.arrive_drop\nmbarrier.test_wait\nmbarrier.try_wait\nmbarrier.pending_count\ncp.async.mbarrier.arrive\ntensormap.cp_fenceproxy\nclusterlaunchcontrol.try_cancel\nclusterlaunchcontrol.query_cancel\n9.7.13.1.\nParallel Synchronization and Communication Instructions:\nbar\n,\nbarrier\n\nbar\n,\nbar.cta\n,\nbarrier\n,\nbarrier.cta\nBarrier synchronization.\nSyntax\nbarrier{.cta}.sync{.aligned}      a{, b};\nbarrier{.cta}.arrive{.aligned}    a, b;\n\nbarrier{.cta}.red.popc{.aligned}.u32  d, a{, b}, {!}c;\nbarrier{.cta}.red.op{.aligned}.pred   p, a{, b}, {!}c;\n\nbar{.cta}.sync      a{, b};\nbar{.cta}.arrive    a, b;\n\nbar{.cta}.red.popc.u32  d, a{, b}, {!}c;\nbar{.cta}.red.op.pred   p, a{, b}, {!}c;\n\n.op = { .and, .or };\nDescription\nPerforms barrier synchronization and communication within a CTA. Each CTA instance has sixteen\nbarriers numbered\n0..15\n.\nbarrier{.cta}\ninstructions can be used by the threads within the CTA for synchronization and\ncommunication.\nOperands\na\n,\nb\n, and\nd\nhave type\n.u32\n; operands\np\nand\nc\nare predicates. Source\noperand\na\nspecifies a logical barrier resource as an immediate constant or register with value\n0\nthrough\n15\n. Operand\nb\nspecifies the number of threads participating in the barrier. If\nno thread count is specified, all threads in the CTA participate in the barrier. When specifying a\nthread count, the value must be a multiple of the warp size. Note that a non-zero thread count is\nrequired for\nbarrier{.cta}.arrive\n.\nDepending on operand\nb\n, either specified number of threads (in multiple of warp size) or all\nthreads in the CTA participate in\nbarrier{.cta}\ninstruction. The\nbarrier{.cta}\ninstructions\nsignal the arrival of the executing threads at the named barrier.\nbarrier{.cta}\ninstruction causes executing thread to wait for all non-exited threads from its\nwarp and marks warps’ arrival at barrier. In addition to signaling its arrival at the barrier, the\nbarrier{.cta}.red\nand\nbarrier{.cta}.sync\ninstructions causes executing thread to wait for\nnon-exited threads of all other warps participating in the barrier to\narrive.\nbarrier{.cta}.arrive\ndoes not cause executing thread to wait for threads of other\nparticipating warps.\nWhen a barrier completes, the waiting threads are restarted without delay, and the barrier is\nreinitialized so that it can be immediately reused.\nThe\nbarrier{.cta}.sync\nor\nbarrier{.cta}.red\nor\nbarrier{.cta}.arrive\ninstruction\nguarantees that when the barrier completes, prior memory accesses requested by this thread are\nperformed relative to all threads participating in the barrier. The\nbarrier{.cta}.sync\nand\nbarrier{.cta}.red\ninstruction further guarantees that no new memory access is requested by this\nthread before the barrier completes.\nA memory read (e.g., by\nld\nor\natom\n) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread participating in the barrier. A\nmemory write (e.g., by\nst\n,\nred\nor\natom\n) has been performed when the value written has\nbecome visible to other threads participating in the barrier, that is, when the previous value can\nno longer be read.\nbarrier{.cta}.red\nperforms a reduction operation across threads. The\nc\npredicate (or its\ncomplement) from all threads in the CTA are combined using the specified reduction operator. Once\nthe barrier count is reached, the final value is written to the destination register in all threads\nwaiting at the barrier.\nThe reduction operations for\nbarrier{.cta}.red\nare population-count (\n.popc\n),\nall-threads-True (\n.and\n), and any-thread-True (\n.or\n). The result of\n.popc\nis the number of\nthreads with a\nTrue\npredicate, while\n.and\nand\n.or\nindicate if all the threads had a\nTrue\npredicate or if any of the threads had a\nTrue\npredicate.\nInstruction\nbarrier{.cta}\nhas optional\n.aligned\nmodifier. When specified, it indicates that\nall threads in CTA will execute the same\nbarrier{.cta}\ninstruction. In conditionally executed\ncode, an aligned\nbarrier{.cta}\ninstruction should only be used if it is known that all threads\nin CTA evaluate the condition identically, otherwise behavior is undefined.\nDifferent warps may execute different forms of the\nbarrier{.cta}\ninstruction using the same\nbarrier name and thread count. One example mixes\nbarrier{.cta}.sync\nand\nbarrier{.cta}.arrive\nto implement producer/consumer models. The producer threads execute\nbarrier{.cta}.arrive\nto\nannounce their arrival at the barrier and continue execution without delay to produce the next\nvalue, while the consumer threads execute the\nbarrier{.cta}.sync\nto wait for a resource to be\nproduced. The roles are then reversed, using a different barrier, where the producer threads execute\na\nbarrier{.cta}.sync\nto wait for a resource to consumed, while the consumer threads announce\nthat the resource has been consumed with\nbarrier{.cta}.arrive\n. Care must be taken to keep a warp\nfrom executing more\nbarrier{.cta}\ninstructions than intended (\nbarrier{.cta}.arrive\nfollowed\nby any other\nbarrier{.cta}\ninstruction to the same barrier) prior to the reset of the\nbarrier.\nbarrier{.cta}.red\nshould not be intermixed with\nbarrier{.cta}.sync\nor\nbarrier{.cta}.arrive\nusing the same active barrier. Execution in this case is unpredictable.\nThe optional\n.cta\nqualifier simply indicates CTA-level applicability of the barrier and it\ndoesn’t change the semantics of the instruction.\nbar{.cta}.sync\nis equivalent to\nbarrier{.cta}.sync.aligned\n.\nbar{.cta}.arrive\nis\nequivalent to\nbarrier{.cta}.arrive.aligned\n.\nbar{.cta}.red\nis equivalent to\nbarrier{.cta}.red.aligned\n.\nNote\nFor .target\nsm_6x\nor below,\nbarrier{.cta}\ninstruction without\n.aligned\nmodifier is equivalent to\n.aligned\nvariant and has the same restrictions as of\n.aligned\nvariant.\nAll threads in warp (except for those have exited) must execute\nbarrier{.cta}\ninstruction\nin convergence.\nPTX ISA Notes\nbar.sync\nwithout a thread count introduced in PTX ISA version 1.0.\nRegister operands, thread count, and\nbar.{arrive,red}\nintroduced in PTX ISA version 2.0.\nbarrier\ninstruction introduced in PTX ISA version 6.0.\n.cta\nqualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nRegister operands, thread count, and\nbar{.cta}.{arrive,red}\nrequire\nsm_20\nor higher.\nOnly\nbar{.cta}.sync\nwith an immediate barrier number is supported for\nsm_1x\ntargets.\nbarrier{.cta}\ninstruction requires\nsm_30\nor higher.\nExamples\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for all threads in CTA to also arrive:\n    st.shared [r0],r1;  // write my result to shared memory\n    bar.cta.sync  1;    // arrive, wait for others to arrive\n    ld.shared r2,[r3];  // use shared results from other threads\n\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for fixed number of cooperating threads to arrive:\n    #define CNT1 (8*12) // Number of cooperating threads\n\n    st.shared [r0],r1;     // write my result to shared memory\n    bar.cta.sync  1, CNT1; // arrive, wait for others to arrive\n    ld.shared r2,[r3];     // use shared results from other threads\n\n// Use bar.red.and to compare results across the entire CTA:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.and.pred r3,1,p; // r3=AND(p) forall threads in CTA\n\n// Use bar.red.popc to compute the size of a group of threads\n// that have a specific condition True:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.popc.u32 r3,1,p; // r3=SUM(p) forall threads in CTA\n\n// Examples of barrier.cta.sync\n    st.shared         [r0],r1;\n    barrier.cta.sync  0;\n    ld.shared         r1, [r0];\n\n/* Producer/consumer model. The producer deposits a value in\n * shared memory, signals that it is complete but does not wait\n * using bar.arrive, and begins fetching more data from memory.\n * Once the data returns from memory, the producer must wait\n * until the consumer signals that it has read the value from\n * the shared memory location. In the meantime, a consumer\n * thread waits until the data is stored by the producer, reads\n * it, and then signals that it is done (without waiting).\n */\n    // Producer code places produced value in shared memory.\n    st.shared   [r0],r1;\n    bar.arrive  0,64;\n    ld.global   r1,[r2];\n    bar.sync    1,64;\n    ...\n\n    // Consumer code, reads value from shared memory\n    bar.sync   0,64;\n    ld.shared  r1,[r0];\n    bar.arrive 1,64;\n    ...\n9.7.13.2.\nParallel Synchronization and Communication Instructions:\nbar.warp.sync\n\nbar.warp.sync\nBarrier synchronization for threads in a warp.\nSyntax\nbar.warp.sync      membermask;\nDescription\nbar.warp.sync\nwill cause executing thread to wait until all threads corresponding to\nmembermask\nhave executed a\nbar.warp.sync\nwith the same\nmembermask\nvalue before resuming\nexecution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s\nlaneid\n.\nThe behavior of\nbar.warp.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nbar.warp.sync\nalso guarantee memory ordering among threads participating in barrier. Thus,\nthreads within warp that wish to communicate via memory can store to memory, execute\nbar.warp.sync\n, and then safely read values stored by other threads in warp.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nbar.warp.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when the\nbar.warp.sync\ninstruction is executed. Otherwise, the behavior is\nundefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nst.shared.u32 [r0],r1;         // write my result to shared memory\nbar.warp.sync  0xffffffff;     // arrive, wait for others to arrive\nld.shared.u32 r2,[r3];         // read results written by other threads\n9.7.13.3.\nParallel Synchronization and Communication Instructions:\nbarrier.cluster\n\nbarrier.cluster\nBarrier synchronization within a cluster.\nSyntax\nbarrier.cluster.arrive{.sem}{.aligned};\nbarrier.cluster.wait{.acquire}{.aligned};\n\n.sem = {.release, .relaxed}\nDescription\nPerforms barrier synchronization and communication within a cluster.\nbarrier.cluster\ninstructions can be used by the threads within the cluster for synchronization\nand communication.\nbarrier.cluster.arrive\ninstruction marks warps’ arrival at barrier without causing executing\nthread to wait for threads of other participating warps.\nbarrier.cluster.wait\ninstruction causes the executing thread to wait for all non-exited threads\nof the cluster to perform\nbarrier.cluster.arrive\n.\nIn addition,\nbarrier.cluster\ninstructions cause the executing thread to wait for all non-exited\nthreads from its warp.\nWhen all non-exited threads in the cluster have executed\nbarrier.cluster.arrive\n, the barrier\ncompletes and is automatically reinitialized. After using\nbarrier.cluster.wait\nto detect completion\nof the barrier, a thread may immediately arrive at the barrier once again.\nEach thread must arrive at the barrier only once before the barrier completes.\nThe\nbarrier.cluster.wait\ninstruction guarantees that when it completes the execution, memory\naccesses (except asynchronous operations) requested, in program order, prior to the preceding\nbarrier.cluster.arrive\nby all threads in the cluster are complete and visible to the executing\nthread.\nThere is no memory ordering and visibility guarantee for memory accesses requested by the executing\nthread, in program order, after\nbarrier.cluster.arrive\nand prior to\nbarrier.cluster.wait\n.\nThe optional\n.relaxed\nqualifier on\nbarrier.cluster.arrive\nspecifies that there are no memory\nordering and visibility guarantees provided for the memory accesses performed prior to\nbarrier.cluster.arrive\n.\nThe optional\n.sem\nand\n.acquire\nqualifiers on instructions\nbarrier.cluster.arrive\nand\nbarrier.cluster.wait\nspecify the memory synchronization as described in the\nMemory Consistency Model\n. If the optional\n.sem\nqualifier is absent for\nbarrier.cluster.arrive\n,\n.release\nis assumed by default. If the optional\n.acquire\nqualifier is absent for\nbarrier.cluster.wait\n,\n.acquire\nis assumed by default.\nThe optional\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nbarrier.cluster\ninstruction. In conditionally executed code, an aligned\nbarrier.cluster\ninstruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nSupport for\n.acquire\n,\n.relaxed\n,\n.release\nqualifiers introduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n// use of arrive followed by wait\nld.shared::cluster.u32 r0, [addr];\nbarrier.cluster.arrive.aligned;\n...\nbarrier.cluster.wait.aligned;\nst.shared::cluster.u32 [addr], r1;\n\n// use memory fence prior to arrive for relaxed barrier\n@cta0 ld.shared::cluster.u32 r0, [addr];\nfence.cluster.acq_rel;\nbarrier.cluster.arrive.relaxed.aligned;\n...\nbarrier.cluster.wait.aligned;\n@cta1 st.shared::cluster.u32 [addr], r1;\n9.7.13.4.\nParallel Synchronization and Communication Instructions:\nmembar\n/\nfence\n\nmembar\n,\nfence\nEnforce an ordering of memory operations.\nSyntax\n// Thread fence:\nfence{.sem}.scope;\n\n// Thread fence:\nfence.acquire.sync_restrict::shared::cluster.cluster;\nfence.release.sync_restrict::shared::cta.cluster;\n\n// Operation fence:\nfence.op_restrict.release.cluster;\n\n// Proxy fence (bi-directional):\nfence.proxy.proxykind;\n\n// Proxy fence (uni-directional):\nfence.proxy.to_proxykind::from_proxykind.release.scope;\nfence.proxy.to_proxykind::from_proxykind.acquire.scope  [addr], size;\nfence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\nfence.proxy.async::generic.release.sync_restrict::shared::cta.cluster;\n\n// Old style membar:\nmembar.level;\nmembar.proxy.proxykind;\n\n.sem       = { .sc, .acq_rel, .acquire, .release };\n.scope     = { .cta, .cluster, .gpu, .sys };\n.level     = { .cta, .gl, .sys };\n.proxykind = { .alias, .async, .async.global, .async.shared::{cta, cluster} };\n.op_restrict = { .mbarrier_init };\n.to_proxykind::from_proxykind = {.tensormap::generic};\nDescription\nThe\nmembar\ninstruction guarantees that prior memory accesses requested by this thread (\nld\n,\nst\n,\natom\nand\nred\ninstructions) are performed at the specified\nlevel\n, before later\nmemory operations requested by this thread following the\nmembar\ninstruction. The\nlevel\nqualifier specifies the set of threads that may observe the ordering effect of this operation.\nA memory read (e.g., by\nld\nor\natom\n) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread at the indicated level. A memory\nwrite (e.g., by\nst\n,\nred\nor\natom\n) has been performed when the value written has become\nvisible to other threads at the specified level, that is, when the previous value can no longer be\nread.\nThe\nfence\ninstruction establishes an ordering between memory accesses requested by this thread\n(\nld\n,\nst\n,\natom\nand\nred\ninstructions) as described in the\nMemory Consistency Model\n. The scope qualifier specifies the set of threads that may\nobserve the ordering effect of this operation.\nfence.acq_rel\nis a light-weight fence that is sufficient for memory synchronization in most\nprograms. Instances of\nfence.acq_rel\nsynchronize when combined with additional memory operations\nas described in\nacquire\nand\nrelease\npatterns in the\nMemory Consistency Model\n.\nIf the optional\n.sem\nqualifier is absent,\n.acq_rel\nis assumed by default.\nfence.sc\nis a slower fence that can restore\nsequential consistency\nwhen used in sufficient\nplaces, at the cost of performance. Instances of\nfence.sc\nwith sufficient scope always\nsynchronize by forming a total order per scope, determined at runtime. This total order can be\nconstrained further by other synchronization in the program.\nQualifiers\n.op_restrict\nand\n.sync_restrict\nrestrict the class of memory operations\nfor which the\nfence\ninstruction provides the memory ordering guarantees. When\n.op_restrict\nis\n.mbarrier_init\n, the synchronizing effect of the fence only applies to the prior\nmbarrier.init\noperations executed by the same thread on\nmbarrier objects\nin\n.shared::cta\nstate space. When\n.sync_restrict\nis\n.sync_restrict::shared::cta\n,\n.sem\nmust be\n.release\n, and the effect of the fence only applies to operations performed on objects in\n.shared::cta\nstate space. Likewise, when\n.sync_restrict\nis\n.sync_restrict::shared::cluster\n,\n.sem\nmust be\n.acquire\n, and the effect of the fence only applies to operations performed on\nobjects in\n.shared::cluster\nstate space. When either\n.sync_restrict::shared::cta\nor\n.sync_restrict::shared::cluster\nis present, the\n.scope\nmust be specified as\n.cluster\n.\nThe address operand\naddr\nand the operand\nsize\ntogether specify the memory range\n[addr,\naddr+size-1]\non which the ordering guarantees on the memory accesses across the proxies is to be\nprovided. The only supported value for the\nsize\noperand is 128, which must be a constant integer literal.\nGeneric Addressing\nis used unconditionally, and the address specified by\nthe operand\naddr\nmust fall within the\n.global\nstate space. Otherwise, the behavior is undefined.\nOn\nsm_70\nand higher\nmembar\nis a synonym for\nfence.sc\n1\n, and the\nmembar\nlevels\ncta\n,\ngl\nand\nsys\nare synonymous with the\nfence\nscopes\ncta\n,\ngpu\nand\nsys\nrespectively.\nmembar.proxy\nand\nfence.proxy\ninstructions establish an ordering between memory accesses that\nmay happen through different\nproxies\n.\nA\nuni-directional\nproxy ordering from the\nfrom-proxykind\nto the\nto-proxykind\nestablishes\nordering between a prior memory access performed via the\nfrom-proxykind\nand a subsequent memory access\nperformed via the\nto-proxykind\n.\nA\nbi-directional\nproxy ordering between two proxykinds establishes two\nuni-directional\nproxy orderings\n: one from the first proxykind to the second proxykind and the other from the second proxykind to the first\nproxykind.\nThe\n.proxykind\nqualifier indicates the\nbi-directional\nproxy ordering that is established between the memory\naccesses done between the generic proxy and the proxy specified by\n.proxykind\n.\nValue\n.alias\nof the\n.proxykind\nqualifier refers to memory accesses performed using virtually\naliased addresses to the same memory location. Value\n.async\nof the\n.proxykind\nqualifier specifies\nthat the memory ordering is established between the async proxy and the generic proxy. The memory\nordering is limited only to operations performed on objects in the state space specified. If no state space\nis specified, then the memory ordering applies on all state spaces.\nA\n.release\nproxy fence can form a release sequence that synchronizes with an acquire\nsequence that contains a\n.acquire\nproxy fence. The\n.to_proxykind\nand\n.from_proxykind\nqualifiers indicate the\nuni-directional\nproxy ordering that is established.\nOn\nsm_70\nand higher,\nmembar.proxy\nis a synonym for\nfence.proxy\n.\n1\nThe semantics of\nfence.sc\nintroduced with\nsm_70\nis a superset of the semantics of\nmembar\nand the two are compatible; when executing on\nsm_70\nor later architectures,\nmembar\nacquires the full semantics of\nfence.sc\n.\nPTX ISA Notes\nmembar.{cta,gl}\nintroduced in PTX ISA version 1.4.\nmembar.sys\nintroduced in PTX ISA version 2.0.\nfence\nintroduced in PTX ISA version 6.0.\nmembar.proxy\nand\nfence.proxy\nintroduced in PTX ISA version 7.5.\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\n.op_restrict\nqualifier introduced in PTX ISA version 8.0.\nfence.proxy.async\nis introduced in PTX ISA version 8.0.\n.to_proxykind::from_proxykind\nqualifier introduced in PTX ISA version 8.3.\n.acquire\nand\n.release\nqualifiers for\nfence\ninstruction introduced in PTX ISA version 8.6.\n.sync_restrict\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nmembar.{cta,gl}\nsupported on all target architectures.\nmembar.sys\nrequires\nsm_20\nor higher.\nfence\nrequires\nsm_70\nor higher.\nmembar.proxy\nrequires\nsm_60\nor higher.\nfence.proxy\nrequires\nsm_70\nor higher.\n.cluster\nscope qualifier requires\nsm_90\nor higher.\n.op_restrict\nqualifier requires\nsm_90\nor higher.\nfence.proxy.async\nrequires\nsm_90\nor higher.\n.to_proxykind::from_proxykind\nqualifier requires\nsm_90\nor higher.\n.acquire\nand\n.release\nqualifiers for\nfence\ninstruction require\nsm_90\nor higher..\n.sync_restrict\nqualifier requires\nsm_90\nor higher..\nExamples\nmembar.gl;\nmembar.cta;\nmembar.sys;\nfence.sc.cta;\nfence.sc.cluster;\nfence.proxy.alias;\nmembar.proxy.alias;\nfence.mbarrier_init.release.cluster;\nfence.proxy.async;\nfence.proxy.async.shared::cta;\nfence.proxy.async.shared::cluster;\nfence.proxy.async.global;\n\ntensormap.replace.tile.global_address.global.b1024.b64   [gbl], new_addr;\nfence.proxy.tensormap::generic.release.gpu;\ncvta.global.u64  tmap, gbl;\nfence.proxy.tensormap::generic.acquire.gpu [tmap], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [tmap, {tc0}], [mbar0];\n\n// Acquire remote barrier state via async proxy.\nbarrier.cluster.wait.acquire;\nfence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\n\n// Release local barrier state via generic proxy.\nmbarrier.init [bar];\nfence.mbarrier_init.release.cluster;\nbarrier.cluster.arrive.relaxed;\n\n// Acquire local shared memory via generic proxy.\nmbarrier.try_wait.relaxed.cluster.shared::cta.b64 complete, [addr], parity;\nfence.acquire.sync_restrict::shared::cluster.cluster;\n\n// Release local shared memory via generic proxy.\nfence.release.sync_restrict::shared::cta.cluster;\nmbarrier.arrive.relaxed.cluster.shared::cluster.b64 state, [bar];\n9.7.13.5.\nParallel Synchronization and Communication Instructions:\natom\n\natom\nAtomic reduction operations for thread-to-thread communication.\nSyntax\nAtomic operation with scalar type:\natom{.sem}{.scope}{.space}.op{.level::cache_hint}.type d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.op.type d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b16 d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b128 d, [a], b, c;\natom{.sem}{.scope}{.space}.exch{.level::cache_hint}.b128 d, [a], b {, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16     d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2   d, [a], b{, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16    d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2  d, [a], b{, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                { .relaxed, .acquire, .release, .acq_rel };\n.scope =              { .cta, .cluster, .gpu, .sys };\n\n.op =                 { .and, .or, .xor,\n                        .cas, .exch,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };\nAtomic operation with vector type:\natom{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32                  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_16_bit.half_word_type  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type     d, [a], b{, cache-policy};\n\n.sem =               { .relaxed, .acquire, .release, .acq_rel };\n.scope =             { .cta, .cluster, .gpu, .sys };\n.op =                { .add, .min, .max };\n.half_word_type =    { .f16, .bf16 };\n.packed_type =       { .f16x2, .bf16x2 };\n.vec_16_bit =        { .v2, .v4, .v8 }\n.vec_32_bit =        { .v2, .v4 };\n.level::cache_hint = { .L2::cache_hint }\nDescription\nAtomically loads the original value at location\na\ninto destination register\nd\n, performs a\nreduction operation with operand\nb\nand the value in location\na\n, and stores the result of the\nspecified operation at location\na\n, overwriting the original value. Operand\na\nspecifies a\nlocation in the specified state space. If no state space is given, perform the memory accesses using\nGeneric Addressing\n.\natom\nwith scalar type may be used only\nwith\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\natom\nwith vector type may be used only with\n.global\nspace\nand with generic addressing where the address points to\n.global\nspace.\nFor\natom\nwith vector type, operands\nd\nand\nb\nare brace-enclosed vector expressions, size\nof which is equal to the size of vector qualifier.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.relaxed\nis assumed by default.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is absent,\n.gpu\nscope is\nassumed by default.\nFor\natom\nwith vector type, the supported combinations of vector qualifier and types, and atomic\noperations supported on these combinations are depicted in the following table:\nVector qualifier\nTypes\n.f16\n/\nbf16\n.f16x2\n/\nbf16x2\n.f32\n.v2\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v4\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v8\n.add\n,\n.min\n,\n.max\nNot supported\nNot Supported\nTwo atomic operations (\natom\nor\nred\n) are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write.\natom\ninstruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch cases, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire\natom\nis not guaranteed to be atomic as a single access.\nFor\nsm_6x\nand earlier architectures,\natom\noperations on\n.shared\nstate space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory atomic\ninstructions, e.g., by inserting barriers between normal stores and atomic operations to a common\naddress, or by using atom.exch to store to locations accessed by other atomic operations.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe bit-size operations are\n.and\n,\n.or\n,\n.xor\n,\n.cas\n(compare-and-swap), and\n.exch\n(exchange).\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n,\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\nThe floating-point operation\n.add\noperation rounds to nearest even. Current implementation of\natom.add.f32\non global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas\natom.add.f32\non shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero.\natom.add.f16\n,\natom.add.f16x2\n,\natom.add.bf16\nand\natom.add.bf16x2\noperation requires\nthe\n.noftz\nqualifier; it preserves subnormal inputs and results, and does not flush them to\nzero.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\natomic {\n    d = *a;\n    *a = (operation == cas) ? operation(*a, b, c)\n                            : operation(*a, b);\n}\nwhere\n    inc(r, s)  = (r >= s) ? 0 : r+1;\n    dec(r, s)  = (r==0 || r > s)  ? s : r-1;\n    exch(r, s) =  s;\n    cas(r,s,t) = (r == s) ? t : r;\nNotes\nSimple reductions may be specified by using the\nbit bucket\ndestination operand\n_\n.\nPTX ISA Notes\n32-bit atom.global introduced in PTX ISA version 1.1.\natom.shared\nand 64-bit\natom.global.{add,cas,exch}\nintroduced in PTX ISA 1.2.\natom.add.f32\nand 64-bit\natom.shared.{add,cas,exch}\nintroduced in PTX ISA 2.0.\n64-bit\natom.{and,or,xor,min,max}\nintroduced in PTX ISA 3.1.\natom.add.f64\nintroduced in PTX ISA 5.0.\n.scope\nqualifier introduced in PTX ISA 5.0.\n.sem\nqualifier introduced in PTX ISA version 6.0.\natom.add.noftz.f16x2\nintroduced in PTX ISA 6.2.\natom.add.noftz.f16\nand\natom.cas.b16\nintroduced in PTX ISA 6.3.\nPer-element atomicity of\natom.f16x2\nclarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2.\nSupport for\n.level::cache_hint\nqualifier introduced in PTX ISA version 7.4.\natom.add.noftz.bf16\nand\natom.add.noftz.bf16x2\nintroduced in PTX ISA 7.8.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for vector types introduced in PTX ISA version 8.1.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nTarget ISA Notes\natom.global\nrequires\nsm_11\nor higher.\natom.shared\nrequires\nsm_12\nor higher.\n64-bit\natom.global.{add,cas,exch}\nrequire\nsm_12\nor higher.\n64-bit\natom.shared.{add,cas,exch}\nrequire\nsm_20\nor higher.\n64-bit\natom.{and,or,xor,min,max}\nrequire\nsm_32\nor higher.\natom.add.f32\nrequires\nsm_20\nor higher.\natom.add.f64\nrequires\nsm_60\nor higher.\n.scope\nqualifier requires\nsm_60\nor higher.\n.sem\nqualifier requires\nsm_70\nor higher.\nUse of generic addressing requires\nsm_20\nor higher.\natom.add.noftz.f16x2\nrequires\nsm_60\nor higher.\natom.add.noftz.f16\nand\natom.cas.b16\nrequires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\natom.add.noftz.bf16\nand\natom.add.noftz.bf16x2\nrequire\nsm_90\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for vector types requires\nsm_90\nor higher.\nSupport for\n.b128\ntype requires\nsm_90\nor higher.\nExamples\natom.global.add.s32  d,[a],1;\natom.shared::cta.max.u32  d,[x+4],0;\n@p  atom.global.cas.b32  d,[p],my_val,my_new_val;\natom.global.sys.add.u32 d, [a], 1;\natom.global.acquire.sys.inc.u32 ans, [gbl], %r0;\natom.add.noftz.f16x2 d, [a], b;\natom.add.noftz.f16   hd, [ha], hb;\natom.global.cas.b16  hd, [ha], hb, hc;\natom.add.noftz.bf16   hd, [a], hb;\natom.add.noftz.bf16x2 bd, [b], bb;\natom.add.shared::cluster.noftz.f16   hd, [ha], hb;\natom.shared.b128.cas d, a, b, c; // 128-bit atom\natom.global.b128.exch d, a, b;   // 128-bit atom\n\natom.global.cluster.relaxed.add.u32 d, [a], 1;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\natom.global.add.L2::cache_hint.s32  d, [a], 1, cache-policy;\n\natom.global.v8.f16.max.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v8.bf16.add.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v2.f16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v2.bf16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v4.b16x2.min.noftz  {%hd0, %hd1, %hd2, %hd3}, [gbl], {%h0, %h1, %h2, %h3};\natom.global.v4.f32.add  {%f0, %f1, %f2, %f3}, [gbl], {%f0, %f1, %f2, %f3};\natom.global.v2.f16x2.min.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.bf16x2.max.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};\n9.7.13.6.\nParallel Synchronization and Communication Instructions:\nred\n\nred\nReduction operations on global and shared memory.\nSyntax\nReduction operation with scalar type:\nred{.sem}{.scope}{.space}.op{.level::cache_hint}.type          [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16    [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2  [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16\n                                                      [a], b {, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2\n                                                      [a], b {, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                {.relaxed, .release};\n.scope =              {.cta, .cluster, .gpu, .sys};\n\n.op =                 { .and, .or, .xor,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };\nReduction operation with vector type:\nred{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32 [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}. vec_16_bit.half_word_type [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy};\n\n.sem =                { .relaxed, .release };\n.scope =              { .cta, .cluster, .gpu, .sys };\n.op =                 { .add, .min, .max };\n.half_word_type =     { .f16, .bf16 };\n.packed_type =        { .f16x2,.bf16x2 };\n.vec_16_bit =         { .v2, .v4, .v8 }\n.vec_32_bit =         { .v2, .v4 };\n.level::cache_hint =  { .L2::cache_hint }\nDescription\nPerforms a reduction operation with operand\nb\nand the value in location\na\n, and stores the\nresult of the specified operation at location\na\n, overwriting the original value. Operand\na\nspecifies a location in the specified state space. If no state space is given, perform the memory\naccesses using\nGeneric Addressing\n.\nred\nwith scalar type may\nbe used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address\npoints to\n.global\nor\n.shared\nspace.\nred\nwith vector type may be used only with\n.global\nspace and with generic addressing where the address points to\n.global\nspace.\nFor\nred\nwith vector type, operand\nb\nis brace-enclosed vector expressions, size of which is\nequal to the size of vector qualifier.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.relaxed\nis assumed by default.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is absent,\n.gpu\nscope is\nassumed by default.\nFor\nred\nwith vector type, the supported combinations of vector qualifier, types and reduction\noperations supported on these combinations are depicted in following table:\nVector qualifier\nTypes\n.f16\n/\nbf16\n.f16x2\n/\nbf16x2\n.f32\n.v2\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v4\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v8\n.add\n,\n.min\n,\n.max\nNot supported\nNot Supported\nTwo atomic operations (\natom\nor\nred\n) are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write.\nred\ninstruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch case, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire\nred\nis not guaranteed to be atomic as a single access.\nFor\nsm_6x\nand earlier architectures,\nred\noperations on\n.shared\nstate space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory reduction\ninstructions, e.g., by inserting barriers between normal stores and reduction operations to a common\naddress, or by using\natom.exch\nto store to locations accessed by other reduction operations.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n,\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\nThe floating-point operation\n.add\noperation rounds to nearest even. Current implementation of\nred.add.f32\non global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas\nred.add.f32\non shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero.\nred.add.f16\n,\nred.add.f16x2\n,\nred.add.bf16\nand\nred.add.bf16x2\noperation requires the\n.noftz\nqualifier; it preserves subnormal inputs and results, and does not flush them to zero.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\n*a = operation(*a, b);\n\nwhere\n    inc(r, s) = (r >= s) ? 0 : r+1;\n    dec(r, s) = (r==0 || r > s)  ? s : r-1;\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2.\nred.add.f32\nand\nred.shared.add.u64\nintroduced in PTX ISA 2.0.\n64-bit\nred.{and,or,xor,min,max}\nintroduced in PTX ISA 3.1.\nred.add.f64\nintroduced in PTX ISA 5.0.\n.scope\nqualifier introduced in PTX ISA 5.0.\n.sem\nqualifier introduced in PTX ISA version 6.0.\nred.add.noftz.f16x2\nintroduced in PTX ISA 6.2.\nred.add.noftz.f16\nintroduced in PTX ISA 6.3.\nPer-element atomicity of\nred.f16x2\nclarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2\nSupport for\n.level::cache_hint\nqualifier introduced in PTX ISA version 7.4.\nred.add.noftz.bf16\nand\nred.add.noftz.bf16x2\nintroduced in PTX ISA 7.8.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for vector types introduced in PTX ISA version 8.1.\nTarget ISA Notes\nred.global\nrequires\nsm_11\nor higher\nred.shared\nrequires\nsm_12\nor higher.\nred.global.add.u64\nrequires\nsm_12\nor higher.\nred.shared.add.u64\nrequires\nsm_20\nor higher.\n64-bit\nred.{and,or,xor,min,max}\nrequire\nsm_32\nor higher.\nred.add.f32\nrequires\nsm_20\nor higher.\nred.add.f64\nrequires\nsm_60\nor higher.\n.scope\nqualifier requires\nsm_60\nor higher.\n.sem\nqualifier requires\nsm_70\nor higher.\nUse of generic addressing requires\nsm_20\nor higher.\nred.add.noftz.f16x2\nrequires\nsm_60\nor higher.\nred.add.noftz.f16\nrequires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nred.add.noftz.bf16\nand\nred.add.noftz.bf16x2\nrequire\nsm_90\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for vector types requires\nsm_90\nor higher.\nExamples\nred.global.add.s32  [a],1;\nred.shared::cluster.max.u32  [x+4],0;\n@p  red.global.and.b32  [p],my_val;\nred.global.sys.add.u32 [a], 1;\nred.global.acquire.sys.add.u32 [gbl], 1;\nred.add.noftz.f16x2 [a], b;\nred.add.noftz.bf16   [a], hb;\nred.add.noftz.bf16x2 [b], bb;\nred.global.cluster.relaxed.add.u32 [a], 1;\nred.shared::cta.min.u32  [x+4],0;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nred.global.and.L2::cache_hint.b32 [a], 1, cache-policy;\n\nred.global.v8.f16.add.noftz  [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v8.bf16.min.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v2.f16.add.noftz [gbl], {%h0, %h1};\nred.global.v2.bf16.add.noftz [gbl], {%h0, %h1};\nred.global.v4.f16x2.max.noftz [gbl], {%h0, %h1, %h2, %h3};\nred.global.v4.f32.add  [gbl], {%f0, %f1, %f2, %f3};\nred.global.v2.f16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.bf16x2.add.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};\n9.7.13.7.\nParallel Synchronization and Communication Instructions:\nred.async\n\nred.async\nAsynchronous reduction operation.\nSyntax\n// Increment and Decrement reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  =                 { .relaxed };\n.scope =                { .cluster };\n.ss   =                 { .shared::cluster };\n.op   =                 { .inc, .dec };\n.type =                 { .u32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n\n// MIN and MAX reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.op   = { .min, .max };\n.type = { .u32, .s32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// Bitwise AND, OR and XOR reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.op   = { .and, .or, .xor };\n.type = { .b32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// ADD reductions\nred.async.sem.scope{.ss}.completion_mechanism.add.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.type = { .u32, .s32, .u64 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\nred.async{.mmio}.sem.scope{.ss}.add.type [a], b;\n\n.sem  = { .release };\n.scope = { .gpu, .cluster };\n.ss   = { .global };\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nred.async\nis a non-blocking instruction which initiates an asynchronous reduction operation\nspecified by\n.op\n, with the operand\nb\nand the value at destination shared memory location\nspecified by operand\na\n.\nOperands\na\nis a destination address, and must be either a register, or of the form\nregister\n+\nimmOff\n, as described in\nAddresses as Operands\n.\nb\nis a source value, of the type indicated by qualifier\n.type\n.\nmbar\nis an mbarrier object address.\nQualifiers\n.mmio\nindicates whether this is an\nmmio Operation\n.\n.sem\nspecifies the memory ordering semantics as described in the\nMemory Consistency Model\n.\n.scope\nspecifies the set of threads with which this instruction can\ndirectly synchronize.\n.ss\nspecifies the state space of the destination operand\na\nand the\nmbarrier operand\nmbar\n.\nIf\n.ss\nis not specified,\nGeneric Addressing\nis used.\n.completion_mechanism\nspecifies the mechanism for observing the\ncompletion of the asynchronous operation.\nWhen\n.completion_mechanism\nis\n.mbarrier::complete_tx::bytes\n: upon\ncompletion of the asynchronous operation, a\ncomplete-tx\noperation will be performed on the mbarrier object specified by the operand\nmbar\n,\nwith\ncompleteCount\nargument equal to the amount of data stored in bytes.\nWhen\n.completion_mechanism\nis not specified: the completion of the store\nsynchronizes with the end of the CTA.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\n.op\nspecifies the reduction operation.\nThe\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\n.type\nspecifies the type of the source operand\nb\n.\nConditions\nWhen\n.sem\nis\n.relaxed\n:\nThe reduce operation is a relaxed memory operation.\nThe complete-tx operation on the mbarrier has\n.release\nsemantics at\n.cluster\nscope.\nThe shared-memory addresses of the destination operand\na\nand the\nmbarrier operand\nmbar\nmust meet all of the following conditions:\nThey belong to the same CTA.\nThe CTA to which they belong is different from the CTA of the executing thread,\nbut must be within the same cluster.\nOtherwise, the behavior is undefined.\n.mmio\nmust not be specified.\nIf\n.ss\nis specified, it must be\n.shared::cluster\n.\nIf\n.ss\nis not specified, generic addressing is used for operands\na\nand\nmbar\n.\nIf the generic addresses specified do not fall within the address window of\n.shared::cluster\nstate space, the behavior is undefined.\nIf\n.completion_mechanism\nis specified, it must be\n.mbarrier::complete_tx::bytes\n.\nIf\n.completion_mechanism\nis not specified, it defaults to\n.mbarrier::complete_tx::bytes\n.\nWhen\n.sem\nis\n.release\n:\nThe reduce operation is a strong memory operation with\n.release\nsemantics\nat the scope specified by\n.scope\n.\nIf\n.mmio\nis specified,\n.scope\nmust be\n.sys\n.\nIf\n.ss\nis specified, it must be\n.global\n.\nIf\n.ss\nis not specified, generic addressing is used for operand\na\n.\nIf the generic address specified does not fall within the address window of\n.global\nstate space, the behavior is undefined.\n.completion_mechanism\nmust not be specified.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space,\nand\n.gpu\nand\n.sys\nscopes introduced in PTX ISA version 8.7.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space,\nand\n.gpu\nand\n.sys\nscopes require\nsm_100\nor higher.\nExamples\nred.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr];\n\nred.async.release.sys.global.add.u32 [addr], b;\n9.7.13.8.\nParallel Synchronization and Communication Instructions:\nvote\n(deprecated)\n\nvote\n(deprecated)\nVote across thread group.\nSyntax\nvote.mode.pred  d, {!}a;\nvote.ballot.b32 d, {!}a;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni };\nDeprecation Note\nThe\nvote\ninstruction without a\n.sync\nqualifier is deprecated in PTX ISA version 6.0.\nSupport for this instruction with\n.target\nlower than\nsm_70\nmay be removed in a future PTX\nISA version.\nRemoval Note\nSupport for\nvote\ninstruction without a\n.sync\nqualifier is removed in PTX ISA version 6.4 for\n.target\nsm_70\nor higher.\nDescription\nPerforms a reduction of the source predicate across all active threads in a warp. The destination\npredicate value is the same across all threads in the warp.\nThe reduction modes are:\n.all\nTrue\nif source predicate is\nTrue\nfor all active threads in warp. Negate the source\npredicate to compute\n.none\n.\n.any\nTrue\nif source predicate is\nTrue\nfor some active thread in warp. Negate the source\npredicate to compute\n.not_all\n.\n.uni\nTrue\nif source predicate has the same value in all active threads in warp. Negating the\nsource predicate also computes\n.uni\n.\nIn the\nballot\nform,\nvote.ballot.b32\nsimply copies the predicate from each thread in a warp\ninto the corresponding bit position of destination register\nd\n, where the bit position\ncorresponds to the thread’s lane id.\nAn inactive thread in warp will contribute a 0 for its entry when participating in\nvote.ballot.b32\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2.\nDeprecated in PTX ISA version 6.0 in favor of\nvote.sync\n.\nNot supported in PTX ISA version 6.4 for .target\nsm_70\nor higher.\nTarget ISA Notes\nvote\nrequires\nsm_12\nor higher.\nvote.ballot.b32\nrequires\nsm_20\nor higher.\nvote\nis not supported on\nsm_70\nor higher starting PTX ISA version 6.4.\nRelease Notes\nNote that\nvote\napplies to threads in a single warp, not across an entire CTA.\nExamples\nvote.all.pred    p,q;\nvote.uni.pred    p,q;\nvote.ballot.b32  r1,p;  // get 'ballot' across warp\n9.7.13.9.\nParallel Synchronization and Communication Instructions:\nvote.sync\n\nvote.sync\nVote across thread group.\nSyntax\nvote.sync.mode.pred  d, {!}a, membermask;\nvote.sync.ballot.b32 d, {!}a, membermask;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni };\nDescription\nvote.sync\nwill cause executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nvote.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s\nlaneid\n. Operand\na\nis a\npredicate register.\nIn the\nmode\nform,\nvote.sync\nperforms a reduction of the source predicate across all non-exited\nthreads in\nmembermask\n. The destination operand\nd\nis a predicate register and its value is\nthe same across all threads in\nmembermask\n.\nThe reduction modes are:\n.all\nTrue\nif source predicate is\nTrue\nfor all non-exited threads in\nmembermask\n. Negate the\nsource predicate to compute\n.none\n.\n.any\nTrue\nif source predicate is\nTrue\nfor some thread in\nmembermask\n. Negate the source\npredicate to compute\n.not_all\n.\n.uni\nTrue\nif source predicate has the same value in all non-exited threads in\nmembermask\n. Negating the source predicate also computes\n.uni\n.\nIn the\nballot\nform, the destination operand\nd\nis a\n.b32\nregister. In this form,\nvote.sync.ballot.b32\nsimply copies the predicate from each thread in\nmembermask\ninto the\ncorresponding bit position of destination register\nd\n, where the bit position corresponds to the\nthread’s lane id.\nA thread not specified in\nmembermask\nwill contribute a 0 for its entry in\nvote.sync.ballot.b32\n.\nThe behavior of\nvote.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nvote.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when\nthe\nvote.sync\ninstruction is executed. Otherwise, the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nvote.sync.all.pred    p,q,0xffffffff;\nvote.sync.ballot.b32  r1,p,0xffffffff;  // get 'ballot' across warp\n9.7.13.10.\nParallel Synchronization and Communication Instructions:\nmatch.sync\n\nmatch.sync\nBroadcast and compare a value across threads in warp.\nSyntax\nmatch.any.sync.type  d, a, membermask;\nmatch.all.sync.type  d[|p], a, membermask;\n\n.type = { .b32, .b64 };\nDescription\nmatch.sync\nwill cause executing thread to wait until all non-exited threads from\nmembermask\nhave executed\nmatch.sync\nwith the same qualifiers and same\nmembermask\nvalue before resuming\nexecution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid.\nmatch.sync\nperforms broadcast and compare of operand\na\nacross all non-exited threads in\nmembermask\nand sets destination\nd\nand optional predicate\np\nbased on mode.\nOperand\na\nhas instruction type and\nd\nhas\n.b32\ntype.\nDestination\nd\nis a 32-bit mask where bit position in mask corresponds to thread’s laneid.\nThe matching operation modes are:\n.all\nd\nis set to mask corresponding to non-exited threads in\nmembermask\nif all non-exited\nthreads in\nmembermask\nhave same value of operand\na\n; otherwise\nd\nis set\nto 0. Optionally predicate\np\nis set to true if all non-exited threads in\nmembermask\nhave\nsame value of operand\na\n; otherwise\np\nis set to false. The sink symbol ‘_’ may be used in\nplace of any one of the destination operands.\n.any\nd\nis set to mask of non-exited threads in\nmembermask\nthat have same value of operand\na\n.\nThe behavior of\nmatch.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_70\nor higher.\nRelease Notes\nNote that\nmatch.sync\napplies to threads in a single warp, not across an entire CTA.\nExamples\nmatch.any.sync.b32    d, a, 0xffffffff;\nmatch.all.sync.b64    d|p, a, mask;\n9.7.13.11.\nParallel Synchronization and Communication Instructions:\nactivemask\n\nactivemask\nQueries the active threads within a warp.\nSyntax\nactivemask.b32 d;\nDescription\nactivemask\nqueries predicated-on active threads from the executing warp and sets the destination\nd\nwith 32-bit integer mask where bit position in the mask corresponds to the thread’s\nlaneid\n.\nDestination\nd\nis a 32-bit destination register.\nAn active thread will contribute 1 for its entry in the result and exited or inactive or\npredicated-off thread will contribute 0 for its entry in the result.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.2.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nactivemask.b32  %r1;\n9.7.13.12.\nParallel Synchronization and Communication Instructions:\nredux.sync\n\nredux.sync\nPerform reduction operation on the data from each predicated active thread in the thread group.\nSyntax\nredux.sync.op.type dst, src, membermask;\n.op   = {.add, .min, .max}\n.type = {.u32, .s32}\n\nredux.sync.op.b32 dst, src, membermask;\n.op   = {.and, .or, .xor}\n\nredux.sync.op{.abs.}{.NaN}.f32 dst, src, membermask;\n.op   = { .min, .max }\nDescription\nredux.sync\nwill cause the executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nredux.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s\nlaneid\n.\nredux.sync\nperforms a reduction operation\n.op\nof the 32 bit source register\nsrc\nacross\nall non-exited threads in the\nmembermask\n. The result of the reduction operation is written to\nthe 32 bit destination register\ndst\n.\nReduction operation can be one of the bitwise operation in\n.and\n,\n.or\n,\n.xor\nor arithmetic\noperation in\n.add\n,\n.min\n,\n.max\n.\nFor the\n.add\noperation result is truncated to 32 bits.\nFor\n.f32\ninstruction type, if the input value is 0.0 then +0.0 > -0.0.\nIf\n.abs\nqualifier is specified, then the absolute value of the input is considered for the\nreduction operation.\nIf the\n.NaN\nqualifier is specified, then the result of the reduction operation is canonical NaN\nif the input to the reduction operation from any participating thread is NaN.\nIn the absence of\n.NaN\nqualifier, only non-NaN values are considered for the reduction operation\nand the result will be canonical NaN when all inputs are NaNs.\nThe behavior of\nredux.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for\n.f32\ntype is introduced in PTX ISA version 8.6.\nSupport for\n.abs\nand\n.NaN\nqualifiers is introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\n.f32\ntype requires\nsm_100a\nand is supported on\nsm_100f\nfrom PTX ISA version 8.8.\nQualifiers\n.abs\nand\n.NaN\nrequire\nsm_100a\nand are supported on\nsm_100f\nor\nhigher in the same family from PTX ISA version 8.8.\nRelease Notes\nNote that\nredux.sync\napplies to threads in a single warp, not across an entire CTA.\nExamples\n.reg .b32 dst, src, init, mask;\nredux.sync.add.s32 dst, src, 0xff;\nredux.sync.xor.b32 dst, src, mask;\n\nredux.sync.min.abs.NaN.f32 dst, src, mask;\n9.7.13.13.\nParallel Synchronization and Communication Instructions:\ngriddepcontrol\n\ngriddepcontrol\nControl execution of dependent grids.\nSyntax\ngriddepcontrol.action;\n\n.action   = { .launch_dependents, .wait }\nDescription\nThe\ngriddepcontrol\ninstruction allows the dependent grids and prerequisite grids as defined by\nthe runtime, to control execution in the following way:\n.launch_dependents\nmodifier signals that specific dependents the runtime system designated to\nreact to this instruction can be scheduled as soon as all other CTAs in the grid issue the same\ninstruction or have completed. The dependent may launch before the completion of the current\ngrid. There is no guarantee that the dependent will launch before the completion of the current\ngrid. Repeated invocations of this instruction by threads in the current CTA will have no additional\nside effects past that of the first invocation.\n.wait\nmodifier causes the executing thread to wait until all prerequisite grids in flight have\ncompleted and all the memory operations from the prerequisite grids are performed and made visible\nto the current grid.\nNote\nIf the prerequisite grid is using\ngriddepcontrol.launch_dependents\n, then the dependent grid\nmust use\ngriddepcontrol.wait\nto ensure correct functional execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ngriddepcontrol.launch_dependents;\ngriddepcontrol.wait;\n9.7.13.14.\nParallel Synchronization and Communication Instructions:\nelect.sync\n\nelect.sync\nElect a leader thread from a set of threads.\nSyntax\nelect.sync d|p, membermask;\nDescription\nelect.sync\nelects one predicated active leader thread from among a set of threads specified by\nmembermask\n.\nlaneid\nof the elected thread is returned in the 32-bit destination operand\nd\n. The sink symbol ‘_’ can be used for destination operand\nd\n. The predicate destination\np\nis set to\nTrue\nfor the leader thread, and\nFalse\nfor all other threads.\nOperand\nmembermask\nspecifies a 32-bit integer indicating the set of threads from which a leader\nis to be elected. The behavior is undefined if the executing thread is not in\nmembermask\n.\nElection of a leader thread happens deterministically, i.e. the same leader thread is elected for\nthe same\nmembermask\nevery time.\nThe mandatory\n.sync\nqualifier indicates that\nelect\ncauses the executing thread to wait until\nall threads in the\nmembermask\nexecute the\nelect\ninstruction before resuming execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nelect.sync    %r0|%p0, 0xffffffff;\n9.7.13.15.\nParallel Synchronization and Communication Instructions:\nmbarrier\n\nmbarrier\nis a barrier created in shared memory that supports :\nSynchronizing any subset of threads within a CTA\nOne-way synchronization of threads across CTAs of a cluster. As noted in\nmbarrier support with shared memory\n, threads can\nperform only\narrive\noperations but not\n*_wait\non an mbarrier located in\nshared::cluster\nspace.\nWaiting for completion of asynchronous memory operations initiated by a thread and making them\nvisible to other threads.\nAn\nmbarrier object\nis an opaque object in memory which can be initialized and invalidated using :\nmbarrier.init\nmbarrier.inval\nOperations supported on\nmbarrier object\ns are :\nmbarrier.expect_tx\nmbarrier.complete_tx\nmbarrier.arrive\nmbarrier.arrive_drop\nmbarrier.test_wait\nmbarrier.try_wait\nmbarrier.pending_count\ncp.async.mbarrier.arrive\nPerforming any\nmbarrier\noperation except\nmbarrier.init\non an uninitialized\nmbarrier object\nresults in undefined behavior.\nPerforming any\nnon-mbarrier\nor\nmbarrier.init\noperations on an initialized\nmbarrier object\nresults in undefined behavior.\nUnlike\nbar{.cta}\n/\nbarrier{.cta}\ninstructions which can access a limited number of barriers\nper CTA,\nmbarrier objects\nare user defined and are only limited by the total shared memory size\navailable.\nmbarrier\noperations enable threads to perform useful work after the arrival at the\nmbarrier\nand\nbefore waiting for the\nmbarrier\nto complete.\n9.7.13.15.1.\nSize and alignment of mbarrier object\n\nAn mbarrier object is an opaque object with the following type and alignment requirements :\nType\nAlignment (bytes)\nMemory space\n.b64\n8\n.shared\n9.7.13.15.2.\nContents of the mbarrier object\n\nAn opaque\nmbarrier object\nkeeps track of the following information :\nCurrent phase of the\nmbarrier object\nCount of pending arrivals for the current phase of the\nmbarrier object\nCount of expected arrivals for the next phase of the\nmbarrier object\nCount of pending asynchronous memory operations (or transactions) tracked by the current phase of\nthe\nmbarrier object\n. This is also referred to as\ntx-count\n.\nAn\nmbarrier object\nprogresses through a sequence of phases where each phase is defined by threads\nperforming an expected number of\narrive-on\noperations.\nThe valid range of each of the counts is as shown below:\nCount name\nMinimum value\nMaximum value\nExpected arrival count\n1\n2\n20\n- 1\nPending arrival count\n0\n2\n20\n- 1\ntx-count\n-(2\n20\n- 1)\n2\n20\n- 1\n9.7.13.15.3.\nLifecycle of the mbarrier object\n\nThe\nmbarrier object\nmust be initialized prior to use.\nAn\nmbarrier object\nis used to synchronize threads and asynchronous memory operations.\nAn\nmbarrier object\nmay be used to perform a sequence of such synchronizations.\nAn\nmbarrier object\nmust be invalidated to repurpose its memory for any purpose,\nincluding repurposing it for another mbarrier object.\n9.7.13.15.4.\nPhase of the mbarrier object\n\nThe phase of an\nmbarrier object\nis the number of times the\nmbarrier object\nhas been used to\nsynchronize threads and\nasynchronous\noperations. In each phase {0, 1, 2, …}, threads perform in program order :\narrive-on\noperations to complete the current phase and\ntest_wait\n/\ntry_wait\noperations to check for the completion of the current phase.\nAn\nmbarrier object\nis automatically reinitialized upon completion of the current phase for\nimmediate use in the next phase. The current phase is incomplete and all prior phases are complete.\nFor each phase of the mbarrier object, at least one\ntest_wait\nor\ntry_wait\noperation must be\nperformed which returns\nTrue\nfor\nwaitComplete\nbefore an\narrive-on\noperation\nin the subsequent phase.\n9.7.13.15.5.\nTracking asynchronous operations by the mbarrier object\n\nStarting with the Hopper architecture (\nsm_9x\n),\nmbarrier object\nsupports a new count, called\ntx-count\n, which is used for tracking the completion of asynchronous memory operations or\ntransactions.\ntx-count\ntracks the number of asynchronous transactions, in units specified by the\nasynchronous memory operation, that are outstanding and yet to be complete.\nThe\ntx-count\nof an\nmbarrier object\nmust be set to the total amount of asynchronous memory\noperations, in units as specified by the asynchronous operations, to be tracked by the current\nphase. Upon completion of each of the asynchronous operations, the\ncomplete-tx\noperation will be performed on the\nmbarrier object\nand thus progress the mbarrier towards the\ncompletion of the current phase.\n9.7.13.15.5.1.\nexpect-tx operation\n\nThe\nexpect-tx\noperation, with an\nexpectCount\nargument, increases the\ntx-count\nof an\nmbarrier object\nby the value specified by\nexpectCount\n. This sets the current phase of the\nmbarrier object\nto expect and track the completion of additional asynchronous transactions.\n9.7.13.15.5.2.\ncomplete-tx operation\n\nThe\ncomplete-tx\noperation, with an\ncompleteCount\nargument, on an\nmbarrier object\nconsists of the following:\nmbarrier signaling\nSignals the completion of asynchronous transactions that were tracked by the current phase. As a\nresult of this,\ntx-count\nis decremented by\ncompleteCount\n.\nmbarrier potentially completing the current phase\nIf the current phase has been completed then the mbarrier transitions to the next phase. Refer to\nPhase Completion of the mbarrier object\nfor details on phase completion requirements and phase transition process.\n9.7.13.15.6.\nPhase Completion of the mbarrier object\n\nThe requirements for completion of the current phase are described below. Upon completion of the\ncurrent phase, the phase transitions to the subsequent phase as described below.\nCurrent phase completion requirements\nAn\nmbarrier object\ncompletes the current phase when all of the following conditions are met:\nThe count of the pending arrivals has reached zero.\nThe\ntx-count\nhas reached zero.\nPhase transition\nWhen an\nmbarrier\nobject completes the current phase, the following actions are performed\natomically:\nThe\nmbarrier object\ntransitions to the next phase.\nThe pending arrival count is reinitialized to the expected arrival count.\n9.7.13.15.7.\nArrive-on operation on mbarrier object\n\nAn\narrive-on\noperation, with an optional\ncount\nargument, on an\nmbarrier object\nconsists of the\nfollowing 2 steps :\nmbarrier signalling:\nSignals the arrival of the executing thread OR completion of the asynchronous instruction which\nsignals the arrive-on operation initiated by the executing thread on the\nmbarrier object\n. As a\nresult of this, the pending arrival count is decremented by\ncount\n. If the\ncount\nargument is\nnot specified, then it defaults to 1.\nmbarrier potentially completing the current phase:\nIf the current phase has been completed then the mbarrier transitions to the next phase. Refer to\nPhase Completion of the mbarrier object\nfor details on phase completion requirements and phase transition process.\n9.7.13.15.8.\nmbarrier support with shared memory\n\nThe following table summarizes the support of various mbarrier operations on\nmbarrier objects\nlocated at different shared memory locations:\nmbarrier operations\n.shared::cta\n.shared::cluster\nmbarrier.arrive\nSupported\nSupported, cannot return result\nmbarrier.expect_tx\nSupported\nSupported\nmbarrier.complete_tx\nSupported\nSupported\nOther mbarrier operations\nSupported\nNot supported\n9.7.13.15.9.\nParallel Synchronization and Communication Instructions:\nmbarrier.init\n\nmbarrier.init\nInitialize the\nmbarrier object\n.\nSyntax\nmbarrier.init{.shared{::cta}}.b64 [addr], count;\nDescription\nmbarrier.init\ninitializes the\nmbarrier object\nat the location specified by the address operand\naddr\nwith the unsigned 32-bit integer\ncount\n. The value of operand count must be in the range\nas specified in\nContents of the mbarrier object\n.\nInitialization of the\nmbarrier object\ninvolves :\nInitializing the current phase to 0.\nInitializing the expected arrival count to\ncount\n.\nInitializing the pending arrival count to\ncount\n.\nInitializing the\ntx-count\nto 0.\nThe valid range of values for the operand\ncount\nis [1, …, 2\n20\n- 1].\nRefer\nContents of the mbarrier object\nfor the\nvalid range of values for the various constituents of the mbarrier.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe behavior of performing an\nmbarrier.init\noperation on a memory location containing a\nvalid\nmbarrier object\nis undefined; invalidate the\nmbarrier object\nusing\nmbarrier.inval\nfirst, before repurposing the memory location for any other purpose, including another\nmbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.shared .b64 shMem, shMem2;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n\ncvta.shared.u64          addr, shMem2;\nmbarrier.init.b64        [addr],   %r1;\nbar.cta.sync             0;\n// ... other mbarrier operations on addr\n\nmbarrier.init.shared::cta.b64 [shMem], 12;\nbar.sync                 0;\n// ... other mbarrier operations on shMem\n9.7.13.15.10.\nParallel Synchronization and Communication Instructions:\nmbarrier.inval\n\nmbarrier.inval\nInvalidates the\nmbarrier object\n.\nSyntax\nmbarrier.inval{.shared{::cta}}.b64 [addr];\nDescription\nmbarrier.inval\ninvalidates the\nmbarrier object\nat the location specified by the address\noperand\naddr\n.\nAn\nmbarrier object\nmust be invalidated before using its memory location for any other purpose.\nPerforming any\nmbarrier\noperation except\nmbarrier.init\non a memory location that does not\ncontain a valid\nmbarrier object\n, results in undefined behaviour.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.shared .b64 shmem;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n.reg    .pred t0;\n\n// Example 1 :\nbar.sync                      0;\n@t0 mbarrier.init.b64     [addr], %r1;\n// ... other mbarrier operations on addr\nbar.sync                      0;\n@t0 mbarrier.inval.b64    [addr];\n\n\n// Example 2 :\nbar.cta.sync                  0;\nmbarrier.init.shared.b64           [shmem], 12;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared.b64      [shmem];\n\n// shmem can be reused here for unrelated use :\nbar.cta.sync                  0;\nst.shared.b64                      [shmem], ...;\n\n// shmem can be re-initialized as mbarrier object :\nbar.cta.sync                  0;\n@t0 mbarrier.init.shared.b64       [shmem], 24;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared::cta.b64 [shmem];\n9.7.13.15.11.\nParallel Synchronization and Communication Instructions:\nmbarrier.expect_tx\n\nmbarrier.expect_tx\nPerfoms\nexpect-tx\noperation on the\nmbarrier object\n.\nSyntax\nmbarrier.expect_tx{.sem.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster }\nDescription\nA thread executing\nmbarrier.expect_tx\nperforms an\nexpect-tx\noperation on the\nmbarrier object\nat the location specified by the address operand\naddr\n. The\n32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to the\nexpect-tx\noperation.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nare as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmbarrier.expect_tx.b64                       [addr], 32;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj1], 512;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj2], 512;\n9.7.13.15.12.\nParallel Synchronization and Communication Instructions:\nmbarrier.complete_tx\n\nmbarrier.complete_tx\nPerfoms\ncomplete-tx\noperation on the\nmbarrier object\n.\nSyntax\nmbarrier.complete_tx{.sem.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster }\nDescription\nA thread executing\nmbarrier.complete_tx\nperforms a\ncomplete-tx\noperation on the\nmbarrier object\nat the location specified by the address operand\naddr\n. The\n32-bit unsigned integer operand\ntxCount\nspecifies the\ncompleteCount\nargument to the\ncomplete-tx\noperation.\nmbarrier.complete_tx\ndoes not involve any asynchronous memory operations and only simulates the\ncompletion of an asynchronous memory operation and its side effect of signaling to the\nmbarrier\nobject\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nare as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmbarrier.complete_tx.b64             [addr],     32;\nmbarrier.complete_tx.shared.b64      [mbarObj1], 512;\nmbarrier.complete_tx.relaxed.cta.b64 [addr2],    32;\n9.7.13.15.13.\nParallel Synchronization and Communication Instructions:\nmbarrier.arrive\n\nmbarrier.arrive\nPerforms\narrive-on operation\non the\nmbarrier object\n.\nSyntax\nmbarrier.arrive{.sem.scope}{.shared{::cta}}.b64           state, [addr]{, count};\nmbarrier.arrive{.sem.scope}{.shared::cluster}.b64         _, [addr] {,count}\nmbarrier.arrive.expect_tx{.sem.scope}{.shared{::cta}}.b64 state, [addr], txCount;\nmbarrier.arrive.expect_tx{.sem.scope}{.shared::cluster}.b64   _, [addr], txCount;\nmbarrier.arrive.noComplete{.release.cta}{.shared{::cta}}.b64  state, [addr], count;\n\n.sem   = { .release, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nA thread executing\nmbarrier.arrive\nperforms an\narrive-on\noperation\non the\nmbarrier object\nat the location specified by the address operand\naddr\n. The 32-bit\nunsigned integer operand\ncount\nspecifies the\ncount\nargument to the\narrive-on\noperation.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional qualifier\n.expect_tx\nspecifies that an\nexpect-tx\noperation is performed prior to the\narrive-on\noperation. The 32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to\nthe\nexpect-tx\noperation. When both qualifiers\n.arrive\nand\n.expect_tx\nare specified, then\nthe count argument of the\narrive-on\noperation is assumed to be 1.\nA\nmbarrier.arrive\noperation with\n.noComplete\nqualifier must not cause the\nmbarrier\nto\ncomplete its current phase, otherwise the behavior is undefined.\nThe value of the operand\ncount\nmust be in the range as specified in\nContents of the mbarrier object\n.\nNote: for\nsm_8x\n, when the argument\ncount\nis specified, the modifier\n.noComplete\nis\nrequired.\nmbarrier.arrive\noperation on an\nmbarrier object\nlocated in\n.shared::cta\nreturns an opaque\n64-bit register capturing the phase of the\nmbarrier object\nprior to the\narrive-on operation\nin the\ndestination operand\nstate.\nContents of the\nstate\noperand are implementation\nspecific. Optionally, sink symbol\n'_'\ncan be used for the\nstate\nargument.\nmbarrier.arrive\noperation on an\nmbarrier object\nlocated in\n.shared::cluster\nbut not in\n.shared::cta\ncannot return a value. Sink symbol ‘_’ is mandatory for the destination operand for\nsuch cases.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.release\nis assumed by default.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is not specified then it\ndefaults to\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state space where the\nmbarrier resides.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sink symbol ‘_’ as the destination operand is introduced in PTX ISA version 7.1.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\ncount\nargument without the modifier\n.noComplete\nintroduced in PTX ISA version\n7.8.\nSupport for sub-qualifier\n::cluster\nintroduced in PTX ISA version 8.0.\nSupport for qualifier\n.expect_tx\nis introduced in PTX ISA version 8.0.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSupport for\ncount\nargument without the modifier\n.noComplete\nrequires\nsm_90\nor higher.\nQualifier\n.expect_tx\nrequires\nsm_90\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n.reg .b32 cnt, remoteAddr32, remoteCTAId, addr32;\n.reg .b64 %r<5>, addr, remoteAddr64;\n.shared .b64 shMem, shMem2;\n\ncvta.shared.u64            addr, shMem2;\nmov.b32                    addr32, shMem2;\nmapa.shared::cluster.u32   remoteAddr32, addr32, remoteCTAId;\nmapa.u64                   remoteAddr64, addr,   remoteCTAId;\n\ncvta.shared.u64          addr, shMem2;\n\nmbarrier.arrive.shared.b64                       %r0, [shMem];\nmbarrier.arrive.shared::cta.b64                  %r0, [shMem2];\nmbarrier.arrive.release.cta.shared::cluster.b64  _, [remoteAddr32];\nmbarrier.arrive.release.cluster.b64              _, [remoteAddr64], cnt;\nmbarrier.arrive.expect_tx.release.cluster.b64    _, [remoteAddr64], tx_count;\nmbarrier.arrive.noComplete.b64                   %r1, [addr], 2;\nmbarrier.arrive.relaxed.cta.b64                  %r2, [addr], 4;\nmbarrier.arrive.b64                              %r2, [addr], cnt;\n9.7.13.15.14.\nParallel Synchronization and Communication Instructions:\nmbarrier.arrive_drop\n\nmbarrier.arrive_drop\nDecrements the expected count of the\nmbarrier object\nand performs\narrive-on operation\n.\nSyntax\nmbarrier.arrive_drop{.sem.scope}{.shared{::cta}}.b64 state,           [addr]{, count};\nmbarrier.arrive_drop{.sem.scope}{.shared::cluster}.b64           _,   [addr] {,count};\nmbarrier.arrive_drop.expect_tx{.sem.scope}{.shared{::cta}}.b64 state, [addr], tx_count;\nmbarrier.arrive_drop.expect_tx{.sem.scope}{.shared::cluster}.b64   _, [addr], tx_count;\nmbarrier.arrive_drop.noComplete{.release.cta}{.shared{::cta}}.b64 state,  [addr], count;\n\n.sem   = { .release, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nA thread executing\nmbarrier.arrive_drop\non the\nmbarrier object\nat the location specified by\nthe address operand\naddr\nperforms the following steps:\nDecrements the expected arrival count of the\nmbarrier object\nby the value specified by the\n32-bit integer operand\ncount\n. If\ncount\noperand is not specified, it defaults to 1.\nPerforms an\narrive-on operation\non the\nmbarrier object\n. The operand\ncount\nspecifies the\ncount\nargument to the\narrive-on operation\n.\nThe decrement done in the expected arrivals count of the\nmbarrier object\nwill be for all the\nsubsequent phases of the\nmbarrier object\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional qualifier\n.expect_tx\nspecifies that an\nexpect-tx\noperation is performed prior to the\narrive_drop\noperation, i.e. the decrement of arrival count and\narrive-on\noperation. The 32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to\nthe\nexpect-tx\noperation. When both qualifiers\n.arrive_drop\nand\n.expect_tx\nare specified, then\nthe count argument of the\narrive-on\noperation is assumed to be 1.\nmbarrier.arrive_drop\noperation with\n.release\nqualifier forms the\nrelease\npattern as\ndescribed in the Memory Consistency Model and synchronizes with the\nacquire\npatterns.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.release\nis assumed by default.  The\n.relaxed\nqualifier does not provide any memory\nordering semantics and visibility guarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that an\nmbarrier.arrive_drop\ninstruction can directly synchronize. If the\n.scope\nqualifier is not specified then it defaults\nto\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state space where the mbarrier\nresides.\nA\nmbarrier.arrive_drop\nwith\n.noComplete\nqualifier must not complete the\nmbarrier,\notherwise the behavior is undefined.\nThe value of the operand\ncount\nmust be in the range as specified in\nContents of the mbarrier object\n.\nNote: for\nsm_8x\n, when the argument\ncount\nis specified, the modifier\n.noComplete\nis\nrequired.\nA thread that wants to either exit or opt out of participating in the\narrive-on operation\ncan use\nmbarrier.arrive_drop\nto drop itself from the\nmbarrier\n.\nmbarrier.arrive_drop\noperation on an\nmbarrier object\nlocated in\n.shared::cta\nreturns an\nopaque 64-bit register capturing the phase of the\nmbarrier object\nprior to the\narrive-on\noperation\nin the destination operand\nstate\n. Contents of the returned state are implementation\nspecific. Optionally, sink symbol\n'_'\ncan be used for the\nstate\nargument.\nmbarrier.arrive_drop\noperation on an\nmbarrier\nobject located in\n.shared::cluster\nbut not\nin\n.shared::cta\ncannot return a value. Sink symbol ‘_’ is mandatory for the destination operand\nfor such cases.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\ncount\nargument without the modifier\n.noComplete\nintroduced in PTX ISA version\n7.8.\nSupport for qualifier\n.expect_tx\nis introduced in PTX ISA version 8.0.\nSupport for sub-qualifier\n::cluster\nintroduced in PTX ISA version 8.0.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSupport for\ncount\nargument without the modifier\n.noComplete\nrequires\nsm_90\nor higher.\nQualifier\n.expect_tx\nrequires\nsm_90\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n.reg .b32 cnt;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\n// Example 1\n@p mbarrier.arrive_drop.shared.b64 _, [shMem];\n@p exit;\n@p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a;\n@p2 exit;\n..\n@!p mbarrier.arrive.shared.b64   %r1, [shMem];\n@!p mbarrier.test_wait.shared.b64  q, [shMem], %r1;\n\n// Example 2\nmbarrier.arrive_drop.shared::cluster.b64 _, [addr];\nmbarrier.arrive_drop.shared::cta.release.cluster.b64     _, [addr], cnt;\n\n// Example 3\nmbarrier.arrive_drop.expect_tx.shared::cta.relaxed.cluster.b64 state, [addr], tx_count;\n9.7.13.15.15.\nParallel Synchronization and Communication Instructions:\ncp.async.mbarrier.arrive\n\ncp.async.mbarrier.arrive\nMakes the\nmbarrier object\ntrack all prior\ncp.async\noperations initiated by the\nexecuting thread.\nSyntax\ncp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr];\nDescription\nCauses an\narrive-on operation\nto be\ntriggered by the system on the\nmbarrier object\nupon the completion of all prior\ncp.async\noperations initiated by the\nexecuting thread. The\nmbarrier object\nis at the location specified by the operand\naddr\n. The\narrive-on operation\nis\nasynchronous to execution of\ncp.async.mbarrier.arrive\n.\nWhen\n.noinc\nmodifier is not specified, the pending count of the mbarrier object is incremented\nby 1 prior to the asynchronous\narrive-on operation\n. This\nresults in a zero-net change for the pending count from the asynchronous\narrive-on\noperation\nduring the current phase. The pending count of the\nmbarrier object\nafter the increment should not\nexceed the limit as mentioned in\nContents of the mbarrier object\n. Otherwise,\nthe behavior is undefined.\nWhen the\n.noinc\nmodifier is specified, the increment to the pending count of the\nmbarrier\nobject\nis not performed. Hence the decrement of the pending count done by the asynchronous\narrive-on operation\nmust be\naccounted for in the initialization of the\nmbarrier object\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example 1: no .noinc\nmbarrier.init.shared.b64 [shMem], threadCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n....\n// Absence of .noinc accounts for arrive-on from completion of prior cp.async operations.\n// So mbarrier.init must only account for arrive-on from mbarrier.arrive.\ncp.async.mbarrier.arrive.shared.b64 [shMem];\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n\n\n\n// Example 2: with .noinc\n\n// Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive.\n\n// All threads participating in the mbarrier perform cp.async\nmov.b32 copyOperationCnt, threadCount;\n\n// 3 arrive-on operations will be triggered per-thread\nmul.lo.u32 copyArrivalCnt, copyOperationCnt, 3;\n\nadd.u32 totalCount, threadCount, copyArrivalCnt;\n\nmbarrier.init.shared.b64 [shMem], totalCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n...\n// Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 1st instance\n....\ncp.async.ca.shared.global [shard3], [gbl3], 4;\ncp.async.ca.shared.global [shard4], [gbl4], 16;\ncp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem]; // 2nd instance\n....\ncp.async.ca.shared.global [shard5], [gbl5], 4;\ncp.async.cg.shared.global [shard6], [gbl6], 16;\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 3rd and last instance\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n9.7.13.15.16.\nParallel Synchronization and Communication Instructions:\nmbarrier.test_wait\n/\nmbarrier.try_wait\n\nmbarrier.test_wait\n,\nmbarrier.try_wait\nChecks whether the\nmbarrier object\nhas completed the phase.\nSyntax\nmbarrier.test_wait{.sem.scope}{.shared{::cta}}.b64        waitComplete, [addr], state;\nmbarrier.test_wait.parity{.sem.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity;\n\nmbarrier.try_wait{.sem.scope}{.shared{::cta}}.b64         waitComplete, [addr], state\n                                                            {, suspendTimeHint};\n\nmbarrier.try_wait.parity{.sem.scope}{.shared{::cta}}.b64  waitComplete, [addr], phaseParity\n                                                            {, suspendTimeHint};\n\n.sem   = { .acquire, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nThe\ntest_wait\nand\ntry_wait\noperations test for the completion of the current or the immediately\npreceding phase of an\nmbarrier object\nat the location specified by the operand\naddr\n.\nmbarrier.test_wait\nis a non-blocking instruction which tests for the completion of the phase.\nmbarrier.try_wait\nis a potentially blocking instruction which tests for the completion of the\nphase. If the phase is not complete, the executing thread may be suspended. Suspended thread resumes\nexecution when the specified phase completes OR before the phase completes following a\nsystem-dependent time limit. The optional 32-bit unsigned integer operand\nsuspendTimeHint\nspecifies the time limit, in nanoseconds, that may be used for the time limit instead of the\nsystem-dependent limit.\nmbarrier.test_wait\nand\nmbarrier.try_wait\ntest for completion of the phase :\nSpecified by the 64-bit unsigned integer operand\nstate\n, which was returned by an\nmbarrier.arrive\ninstruction on the same\nmbarrier object\nduring the current or the\nimmediately preceding phase. Or\nIndicated by the 32-bit unsigned integer operand\nphaseParity\n, which is the integer parity\nof either the current phase or the immediately preceding phase of the\nmbarrier object\n.\nThe\n.parity\nvariant of the instructions test for the completion of the phase indicated by the\noperand\nphaseParity\n, which is the integer parity of either the current phase or the immediately\npreceding phase of the\nmbarrier object\n. An even phase has integer parity 0 and an odd phase has\ninteger parity of 1. So the valid values of\nphaseParity\noperand are 0 and 1.\nNote: the use of the\n.parity\nvariants of the instructions requires tracking the phase of an\nmbarrier object\nthroughout its lifetime.\nThe\ntest_wait\nand\ntry_wait\noperations are valid only for :\nthe current incomplete phase, for which\nwaitComplete\nreturns\nFalse\n.\nthe immediately preceding phase, for which\nwaitComplete\nreturns\nTrue\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nWhen\nmbarrier.test_wait\nand\nmbarrier.try_wait\noperations with\n.acquire\nqualifier\nreturns\nTrue\n, they form the\nacquire\npattern as described in the\nMemory Consistency Model\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.acquire\nis assumed by default.  The\n.relaxed\nqualifier does not provide any memory\nordering semantics and visibility guarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that the\nmbarrier.test_wait\nand\nmbarrier.try_wait\ninstructions can directly synchronize. If the\n.scope\nqualifier is not\nspecified then it defaults to\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state\nspace where the mbarrier resides.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nThe following ordering of memory operations hold for the executing thread when\nmbarrier.test_wait\nor\nmbarrier.try_wait\nhaving acquire semantics returns\nTrue\n:\nAll memory accesses (except\nasync operations\n) requested prior, in program\norder, to\nmbarrier.arrive\nhaving release semantics during the completed phase by\nthe participating threads of the CTA are performed and are visible to the executing thread.\nAll\ncp.async\noperations\nrequested prior, in program order, to\ncp.async.mbarrier.arrive\nduring the completed phase by\nthe participating threads of the CTA are performed and made visible to the executing thread.\nAll\ncp.async.bulk\nasynchronous operations using the same\nmbarrier object\nrequested prior,\nin program order, to\nmbarrier.arrive\nhaving release semantics during the completed\nphase by the participating threads of the CTA are performed and made visible to the executing thread.\nAll memory accesses requested after the\nmbarrier.test_wait\nor\nmbarrier.try_wait\n, in\nprogram order, are not performed and not visible to memory accesses performed prior to\nmbarrier.arrive\nhaving release semantics, in program order, by other threads\nparticipating in the\nmbarrier\n.\nThere is no ordering and visibility guarantee for memory accesses requested by the thread after\nmbarrier.arrive\nhaving release semantics and prior to\nmbarrier.test_wait\n,\nin program order.\nPTX ISA Notes\nmbarrier.test_wait\nintroduced in PTX ISA version 7.0.\nModifier\n.parity\nis introduced in PTX ISA version 7.1.\nmbarrier.try_wait\nintroduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nmbarrier.test_wait\nrequires\nsm_80\nor higher.\nmbarrier.try_wait\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n// Example 1a, thread synchronization with test_wait:\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.test_wait.shared.b64    complete, [shMem], %r1;\n@!complete nanosleep.u32 20;\n@!complete bra waitLoop;\n\n// Example 1b, thread synchronization with try_wait :\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.try_wait.relaxed.cluster.shared.b64    complete, [shMem], %r1;\n@!complete bra waitLoop;\n\n\n// Example 2, thread synchronization using phase parity :\n\n.reg .b32 i, parArg;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmov.b32 i, 0;\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nloopStart :                           // One phase per loop iteration\n    ...\n    mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads\n    ...\n    and.b32 parArg, i, 1;\n    waitLoop:\n    mbarrier.test_wait.parity.shared.b64  complete, [shMem], parArg;\n    @!complete nanosleep.u32 20;\n    @!complete bra waitLoop;\n    ...\n    add.u32 i, i, 1;\n    setp.lt.u32 p, i, IterMax;\n@p bra loopStart;\n\n\n// Example 3, Asynchronous copy completion waiting :\n\n.reg .b64 state;\n.shared .b64 shMem2;\n.shared .b64 shard1, shard2;\n.global .b64 gbl1, gbl2;\n\nmbarrier.init.shared.b64 [shMem2], threadCount;\n...\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n\n// Absence of .noinc accounts for arrive-on from prior cp.async operation\ncp.async.mbarrier.arrive.shared.b64 [shMem2];\n...\nmbarrier.arrive.shared.b64 state, [shMem2];\n\nwaitLoop:\nmbarrier.test_wait.shared::cta.b64 p, [shMem2], state;\n@!p bra waitLoop;\n\n// Example 4, Synchronizing the CTA0 threads with cluster threads\n.reg .b64 %r1, addr, remAddr;\n.shared .b64 shMem;\n\ncvta.shared.u64          addr, shMem;\nmapa.u64                 remAddr, addr, 0;     // CTA0's shMem instance\n\n// One thread from CTA0 executing the below initialization operation\n@p0 mbarrier.init.shared::cta.b64 [shMem], N;  // N = no of cluster threads\n\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Entire cluster executing the below arrive operation\nmbarrier.arrive.release.cluster.b64              _, [remAddr];\n\n// computation not requiring mbarrier synchronization ...\n\n// Only CTA0 threads executing the below wait operation\nwaitLoop:\nmbarrier.try_wait.parity.acquire.cluster.shared::cta.b64  complete, [shMem], 0;\n@!complete bra waitLoop;\n9.7.13.15.17.\nParallel Synchronization and Communication Instructions:\nmbarrier.pending_count\n\nmbarrier.pending_count\nQuery the pending arrival count from the opaque mbarrier state.\nSyntax\nmbarrier.pending_count.b64 count, state;\nDescription\nThe pending count can be queried from the opaque mbarrier state using\nmbarrier.pending_count\n.\nThe\nstate\noperand is a 64-bit register that must be the result of a prior\nmbarrier.arrive.noComplete\nor\nmbarrier.arrive_drop.noComplete\ninstruction. Otherwise, the\nbehavior is undefined.\nThe destination register\ncount\nis a 32-bit unsigned integer representing the pending count of\nthe\nmbarrier object\nprior to the\narrive-on operation\nfrom\nwhich the\nstate\nregister was obtained.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.reg .b32 %r1;\n.reg .b64 state;\n.shared .b64 shMem;\n\nmbarrier.arrive.noComplete.b64 state, [shMem], 1;\nmbarrier.pending_count.b64 %r1, state;\n9.7.13.16.\nParallel Synchronization and Communication Instructions:\ntensormap.cp_fenceproxy\n\ntensormap.cp_fenceproxy\nA fused copy and fence operation.\nSyntax\ntensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned  [dst], [src], size;\n\n.cp_qualifiers    = { .global.shared::cta }\n.fence_qualifiers = { .to_proxy::from_proxy.release.scope }\n.to_proxy::from_proxy  = { .tensormap::generic }\n.scope            = { .cta, .cluster, .gpu , .sys }\nDescription\nThe\ntensormap.cp_fenceproxy\ninstructions perform the following operations in order :\nCopies data of size specified by the\nsize\nargument, in bytes, from the location specified\nby the address operand\nsrc\nin shared memory to the location specified by the address operand\ndst\nin the global memory, in the generic proxy.\nEstablishes a\nuni-directional\nproxy release pattern on the ordering from the copy operation\nto the subsequent access performed in the tensormap proxy on the address\ndst\n.\nThe valid value of immediate operand\nsize\nis 128.\nThe operands\nsrc\nand\ndst\nspecify non-generic addresses in\nshared::cta\nand\nglobal\nstate space respectively.\nThe\n.scope\nqualifier specifies the set of threads that can directly observe the proxy\nsynchronizing effect of this operation, as described in\nMemory Consistency Model\n.\nThe mandatory\n.sync\nqualifier indicates that\ntensormap.cp_fenceproxy\ncauses the executing\nthread to wait until all threads in the warp execute the same\ntensormap.cp_fenceproxy\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\ntensormap.cp_fenceproxy\ninstruction. In conditionally executed code, an aligned\ntensormap.cp_fenceproxy\ninstruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n// Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor\n\n.reg .b64 new_addr;\n.global .align 128 .b8 gbl[128];\n.shared .align 128 .b8 sMem[128];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar];\n...\ntry_wait_loop:\nmbarrier.try_wait.shared.b64 p, [mbar], state;\n@!p bra try_wait loop;\n\ntensormap.replace.tile.global_address.shared.b1024.b64   [sMem], new_addr;\ntensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.release.gpu.sync.aligned\n                                                         [gbl], [sMem], 128;\nfence.proxy.tensormap::generic.acquire.gpu [gbl], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [gbl, {tc0}], [mbar0];\n9.7.13.17.\nParallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.try_cancel\n\nclusterlaunchcontrol.try_cancel\nRequests cancellation of cluster which is not launched yet.\nSyntax\nclusterlaunchcontrol.try_cancel.async{.space}.completion_mechanism{.multicast::cluster::all}.b128 [addr], [mbar];\n\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n.space = { .shared::cta };\nDescription\nThe\nclusterlaunchcontrol.try_cancel\ninstruction requests atomically cancelling the launch of\na cluster that has not started running yet. It asynchronously writes an opaque response to shared\nmemory indicating whether the operation succeeded or failed. The completion of the asynchronous\noperation is tracked using the mbarrier completion mechanism at\n.cluster\nscope.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nOn success, the opaque response contains the\nctaid\nof the first CTA of the canceled cluster; no\nother successful response from other\nclusterlaunchcontrol.try_cancel\noperations from the same\ngrid will contain that id.\nThe mandatory\n.async\nqualifier indicates that the instruction will initiate the cancellation\noperation asynchronously and control will return to the executing thread before the requested\noperation is complete.\nThe\n.space\nqualifier is specified, both operands\naddr\nand\nmbar\nmust be in the\n.shared::cta\nstate space. Otherwise, generic addressing will be assumed for both. The result\nis undefined if any of address operands do not fall within the address window of\n.shared::cta\n.\nThe qualifier\n.completion_mechanism\nspecifies that upon completion of the asynchronous operation,\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data stored in bytes, will be performed\non the mbarrier object specified by the operand\nmbar\n.\nThe executing thread can then use\nmbarrier instructions\nto wait for completion\nof the asynchronous operation. No other synchronization mechanisms described in\nMemory Consistency Model\ncan be used to guarantee the completion of the asynchronous copy operations.\nThe\n.multicast::cluster::all\nqualifier indicates that the response is asynchronously written using\nweak async-proxy writes to the corresponding local shared memory\naddr\nof each CTA in the requesting\ncluster. The completion of the writes to\naddr\nof a particular CTA is signaled via a complete-tx operation\nto the mbarrier object on the shared memory of that CTA.\nThe behavior of instruction with\n.multicast::cluster::all\nqualifier is undefined if any CTA in the\ncluster is exited.\nOperand\naddr\nspecifies the naturally aligned address of the 16-byte wide shared memory location where\nthe request’s response is written.\nThe response of\nclusterlaunchcontrol.try_cancel\ninstruction will be 16-byte opaque value and will be\nit available at location specified by operand\naddr\n. After loading this response into 16-byte register,\ninstruction\nclusterlaunchcontrol.query_cancel\ncan be used to check if request was successful and to\nretrieve\nctaid\nof the first CTA of the canceled cluster.\nIf the executing CTA has already observed the completion of a\nclusterlaunchcontrol.try_cancel\ninstruction\nas failed, then the behavior of issuing a subsequent\nclusterlaunchcontrol.try_cancel\ninstruction is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nQualifier\n.multicast::cluster::all\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Assumption: 1D cluster (cluster_ctaid.y/.z == 1) with 1 thread per CTA.\n\n// Current Cluster to be processed: initially the launched cluster:\nmov.b32 xctaid, %ctaid.x;\n\n// Establish full synchronization across all CTAs of the cluster for the first iteration.\n// Weaker synchronization may suffice depending on initialization sequence.\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Iteration loop over all cluster CTAs\nprocessCluster:\n  mov.u32  %r0, %tid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  // Elect a leader thread (thread idx 0) for each CTA to arrive and expect_tx at\n  // each CTA local shared memory barrier:\n  mov.u32  %r0, %tid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  // All other threads skip to processing the work of the current cluster:\n  @!p0 bra processCurrentCluster;\n\n  // All CTAs in the cluster arrive at their local SMEM barrier and set 16B handle tx count:\n  mbarrier.arrive.expect_tx.cluster.relaxed.shared::cta.b64 state, [mbar], 16;\n\n  // First CTA in Cluster attempts to cancel a not-yet-started cluster:\n  mov.u32  %r0, %cluster_ctaid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  @p0 clusterlaunchcontrol.try_cancel.async.mbarrier::complete_tx::bytes.multicast::cluster::all.b128 [addr], [mbar];\n\n  processCurrentCluster:\n    // ...process current cluster (\"xctaid\") while cancellation request for next cluster runs asynchronously...\n\n  // After processing current cluster, wait on cancellation request response for next cluster via specified mbarrier:\n  waitLoop:\n    // .acquire prevents weak handle read (\"ld.shared handle, [addr]\") from overtaking this mbarrier.try_wait:\n    mbarrier.try_wait.cluster.acquire.shared::cta.b64   complete, [mbar], state;\n    @!complete bra waitLoop;\n   // Cancellation request has completed.\n\n  // Generic-proxy weak read of cancellation request into 16-byte wide register:\n  ld.shared.b128 handle, [addr];\n\n  // Check whether cancellation succeeded:\n  clusterlaunchcontrol.query_cancel.is_canceled.pred.b128 p, handle;\n  // If cancellation request failed, we couldn't cancel any other cluster, so all current cluster CTAs exit.\n  @!p ret;\n\n  // Otherwise, cancellation request succeeded.\n  // Extract \"ctaid\" of first cancelled-cluster CTA which we'll process in next \"processCluster\" loop iteration:\n  @p clusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xctaid, _, _, _},  handle;\n\n  // Release current iteration generic-proxy weak read of handle (\"ld.shared handle, [addr]\")\n  // before next iteration async-proxy write to handle (\"clusterlaunchcontrol.try_cancel [addr]\")\n  fence.proxy.async::generic.release.sync_restrict::shared::cta.cluster;\n\n  // Arrive and wait at the next iteration cluster barrier with relaxed semantics.\n  barrier.cluster.arrive.relaxed;\n  barrier.cluster.wait;\n\n  // Acquire prior iteration generic-proxy weak read of handle (\"ld.shared handle, [addr]\")\n  // before current iteration async-proxy write to handle (\"clusterlaunchcontrol.try_cancel [addr]\")\n  fence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\n\n  bra processCluster;\n9.7.13.18.\nParallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.query_cancel\n\nclusterlaunchcontrol.query_cancel\nQueries response of\nclusterlaunchcontrol.try_cancel\noperation.\nSyntax\nclusterlaunchcontrol.query_cancel.is_canceled.pred.b128 pred, try_cancel_response;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xdim, ydim, zdim, _},  try_cancel_response;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid{::dimension}.b32.b128 reg, try_cancel_response;\n\n::dimension = { ::x, ::y, ::z };\nDescription\nInstruction\nclusterlaunchcontrol.query_cancel\ncan be used to decode opaque response\nwritten by instruction\nclusterlaunchcontrol.try_cancel\n.\nAfter loading response from\nclusterlaunchcontrol.try_cancel\ninstruction into 16-byte\nregister it can be further queried using\nclusterlaunchcontrol.query_cancel\ninstruction\nas follows:\nclusterlaunchcontrol.query_cancel.is_canceled.pred.b128\n: If the cluster is canceled\nsuccessfully, predicate\np\nis set to\ntrue\n; otherwise, it is set to\nfalse\n.\nIf the request succeeded, the instruction\nclusterlaunchcontrol.query_cancel.get_first_ctaid\nextracts the CTA id of the first CTA in the canceled cluster. By default, the instruction\nreturns a\n.v4\nvector whose first three elements are the\nx\n,\ny\nand\nz\ncoordinate\nof first CTA in canceled cluster. The contents of the 4th element are unspecified. The\nexplicit\n.get_first_ctaid::x\n,\n.get_first_ctaid::y\n, or\n.get_first_ctaid::z\nqualifiers can be used to extract individual\nx\n,\ny\nor\nz\ncoordinates into a 32-bit\nregister.\nIf the request fails the behavior of\nclusterlaunchcontrol.query_cancel.get_first_ctaid\nis undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nExamples\nclusterlaunchcontrol.query_cancel.is_canceled pred.b128 p, handle;\n\n@p clusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xdim, ydim, zdim, ignr}  handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::x.b32.b128 reg0, handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::y.b32.b128 reg1, handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::z.b32.b128 reg2, handle;\n9.7.14.\nWarp Level Matrix Multiply-Accumulate Instructions\n\nThe matrix multiply and accumulate operation has the following form:\nD = A * B + C\nwhere\nD\nand\nC\nare called accumulators and may refer to the same matrix.\nPTX provides two ways to perform matrix multiply-and-accumulate computation:\nUsing\nwmma\ninstructions:\nThis warp-level computation is performed collectively by all threads in the warp as follows:\nLoad matrices A, B and C from memory into registers using the\nwmma.load\noperation. When\nthe operation completes, the destination registers in each thread hold a fragment of the\nloaded matrix.\nPerform the matrix multiply and accumulate operation using the\nwmma.mma\noperation on the\nloaded matrices. When the operation completes, the destination registers in each thread hold\na fragment of the result matrix returned by the\nwmma.mma\noperation.\nStore result Matrix D back to memory using the\nwmma.store\noperation. Alternately, result\nmatrix D can also be used as argument C for a subsequent\nwmma.mma\noperation.\nThe\nwmma.load\nand\nwmma.store\ninstructions implicitly handle the organization of matrix\nelements when loading the input matrices from memory for the\nwmma.mma\noperation and when\nstoring the result back to memory.\nUsing\nmma\ninstruction:\nSimilar to\nwmma\n,\nmma\nalso requires computation to be performed collectively by all\nthreads in the warp however distribution of matrix elements across different threads in warp\nneeds to be done explicitly before invoking the\nmma\noperation. The\nmma\ninstruction\nsupports both dense as well as sparse matrix A. The sparse variant can be used when A is a\nstructured sparse matrix as described in\nSparse matrix storage\n.\n9.7.14.1.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and C. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\n, where A is an\nMxK\nmatrix, B is a\nKxN\nmatrix, while C and D are\nMxN\nmatrices.\nThe following matrix shapes are supported for the specified types:\nInstruction\nScale\nSparsity\nMultiplicand Data-type\nShape\nPTX ISA version\nwmma\nNA\nDense\nFloating-point -\n.f16\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 6.0\nwmma\nDense\nAlternate floating-point format -\n.bf16\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 7.0\nwmma\nDense\nAlternate floating-point format -\n.tf32\n.m16n16k8\nPTX ISA version 7.0\nwmma\nDense\nInteger -\n.u8\n/\n.s8\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 6.3\nwmma\nDense\nSub-byte integer -\n.u4\n/\n.s4\n.m8n8k32\nPTX ISA version 6.3\nwmma\nDense\nSingle-bit -\n.b1\n.m8n8k128\nPTX ISA version 6.3\nmma\nNA\nDense\nFloating-point -\n.f64\n.m8n8k4\nPTX ISA version 7.0\n.m16n8k4\n,\n.m16n8k8\n,\nand\n.m16n8k16\nPTX ISA version 7.8\nmma\nDense\nFloating-point -\n.f16\n.m8n8k4\nPTX ISA version 6.4\n.m16n8k8\nPTX ISA version 6.5\n.m16n8k16\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.bf16\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.tf32\n.m16n8k4\nand\n.m16n8k8\nPTX ISA version 7.0\nmma\nDense\nInteger -\n.u8\n/\n.s8\n.m8n8k16\nPTX ISA version 6.5\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.0\nmma\nDense\nSub-byte integer -\n.u4\n/\n.s4\n.m8n8k32\nPTX ISA version 6.5\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 7.0\nmma\nDense\nSingle-bit -\n.b1\n.m8n8k128\n,\n.m16n8k128\n,\nand\n.m16n8k256\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k32\nPTX ISA version 8.4\nmma\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k16\nPTX ISA version 8.7\nmma\nDense\nAlternate floating-point format -\n.e3m2\n/\n.e2m3\n/\n.e2m1\n.m16n8k32\nPTX ISA version 8.7\nmma\nYes\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nX\n(Scale)\n.ue8m0\n.m16n8k32\nPTX ISA version 8.7\nmma\nDense\nAlternate floating-point format -\n.e2m1\nX\n(Scale)\n.ue8m0\n/\n.ue4m3\n.m16n8k64\nPTX ISA version 8.7\nmma\nNA\nSparse\nFloating-point -\n.f16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.bf16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.tf32\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 7.1\nmma\nSparse\nInteger -\n.u8\n/\n.s8\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 7.1\nmma\nSparse\nSub-byte integer -\n.u4\n/\n.s4\n.m16n8k64\nand\n.m16n8k128\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k64\nPTX ISA version 8.4\nmma\nSparse\nwith\nordered\nmetadata\nFloating-point -\n.f16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.bf16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.tf32\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nInteger -\n.u8\n/\n.s8\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nSub-byte integer -\n.u4\n/\n.s4\n.m16n8k64\nand\n.m16n8k128\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k64\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e3m2\n/\n.e2m3\n/\n.e2m1\n.m16n8k64\nPTX ISA version 8.7\nmma\nYes\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nX\n(Scale)\n.ue8m0\n.m16n8k64\nPTX ISA version 8.7\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e2m1\nX\n(Scale)\n.ue8m0\n/\n.ue4m3\n.m16n8k128\nPTX ISA version 8.7\n9.7.14.2.\nMatrix Data-types\n\nThe matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point.\nFor floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later.\nData-type\nMultiplicands (A or B)\nAccumulators (C or D)\nInteger\n.u8\n,\n.s8\n.s32\nFloating Point\n.f16\n.f16\n,\n.f32\nAlternate floating Point\n.bf16\n.f32\nAlternate floating Point\n.tf32\n.f32\nAlternate floating Point\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n.f16\n,\n.f32\nAlternate floating Point\nwith scale\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\nX (Scale)\n.ue8m0\n.f32\nAlternate floating Point\nwith scale\n.e2m1\nX (Scale)\n.ue8m0\nor\n.ue4m3\n.f32\nFloating Point\n.f64\n.f64\nSub-byte integer\nboth\n.u4\nor both\n.s4\n.s32\nSingle-bit integer\n.b1\n.s32\n9.7.14.3.\nBlock Scaling for\nmma.sync\n\nThe\nmma\ninstruction with the following\n.kind\nqualifier:\n.kind::mxf8f6f4\n.kind::mxf4\n.kind::mxf4nvf4\nperform matrix multiplication with block scaling. This operation has the following form:\nD\n=\n(A\n*\nscale_A)\n*\n(B\n*\nscale_B)\n+\nC\n.\nFor a\nscale_A\nmatrix of shape\nM x SFA_N\n, each row of matrix\nA\nis divided into\nSFA_N\nnumber of chunks and each chunk of a row is multiplied with the corresponding\nelement (henceforth referred as\nSF_A\n) from the same row of\nscale_A\n.\nSimilarly, for a\nscale_B\nmatrix of shape\nSFB_M x N\n, each column of matrix\nB\nis\ndivided into the\nSFB_M\nnumber of chunks and each chunk of a column is multiplied with\nthe corresponding element (henceforth referred as\nSF_B\n) from the same column of\nscale_B\n.\nFigure 42\nshows an example of\nmma\nwith block scaling of\nscale_vec::2X\n.\nFigure 42\nmma\nwith block scaling of\n.scale_vec::2X\n\nThe shapes for\nscale_A\nand\nscale_B\nmatrices depend upon the qualifier\n.scale_vec_size\nas shown in\nTable 35\n.\nTable 35\nShapes for scale matrices depending upon\n.scale_vec_size\nqualifier\n\n.scale_vec_size\nShape of scale_A\nShape of scale_B\n.scale_vec::1X\nM x 1\n1 x N\n.scale_vec::2X\nM x 2\n2 x N\n.scale_vec::4X\nM x 4\n4 x N\nThe valid combination of the exact element types and the\n.scale_vec_size\nare listed in\nTable 36\n.\nTable 36\nValid combinations of\n.scale_vec_size\nand\n.kind\nqualifier\n\n.kind::*\nElement Data Type\n.atype and .btype\nScale Data Type\n.stype\n.scale_vec_size\n.kind::mxf8f6f4\n.e4m3\n,\n.e5m2\n.e3m2\n,\n.e2m3\n.e2m1\n.ue8m0\n.scale_vec::1X\n.kind::mxf4\n.e2m1\n.ue8m0\n.scale_vec::2X\n.kind::mxf4nvf4\n.e2m1\n.ue8m0\n.scale_vec::2X\n,\n.scale_vec::4X\n.e2m1\n.ue4m3\n.scale_vec::4X\nThe\nscale-a-data\nand\nscale-b-data\nargument provides metadata for\nscale_A\nand\nscale_B\nmatrices respectively. The tuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nprovides the selector information to choose elements\nSF_A\nand\nSF_B\nfrom corresponding metadata arguments\nscale-a-data\nand\nscale-b-data\n.\nThe tuple\n{byte-id-a,\nthread-id-a}\nallows to select the scale matrix element\nSF_A\nfrom\nscale-a-data\n. Similarly, the tuple\n{byte-id-b,\nthread-id-b}\nallows to select\nthe scale matrix element\nSF_B\nfrom\nscale-b-data\n.\nThe components\nthread-id-a\n,\nthread-id-b\ndecides which threads among the quad\ncontribute the\nSF_A\nand\nSF_B\nvalues. The following listing describes the impact\nof thread selector component\nthread-id-a\n,\nthread-id-b\n:\nOne thread-pair within the quad determined by\nthread-id-a\n, contributes the\nSF_A\nvalues. The value of 0 selects lower two threads whereas value of 1 selects upper two\nthreads from the quad. In other words, when\nthread-id-a\nset to 0, thread-pair\nsatisfying:\n%laneid\n% 4 == 0 or 1 provides the\nSF_A\n. In contrast when\nthread-id-a\nset to 1, thread-pair satisfying:\n%laneid\n% 4 == 2 or 3 provides\nthe\nSF_A\n. Refer\nFigure 43\nfor more details.\nFigure 43\nSelection of set of values for\nSF_A\nbased on\nthread-id-a\n\nOne thread within the quad, determined by\nthread-id-b\n, contributes the\nSF_B\nvalue. In other words, each thread satisfying:\n%laneid\n% 4 ==\nthread-id-b\nprovides the\nSF_B\n. Refer\nFigure 44\nfor more details.\nFigure 44\nSelection of set of values for\nSF_B\nbased on\nthread-id-b\n\nThe arguments\nbyte-id-a\n,\nbyte-id-b\nselects which bytes from the\nscale-a-data\n,\nscale-b-data\ncontribute the\nSF_A\nand\nSF_B\nvalues. The following listing describes\nimplications of\n.scale_vec_size\nqualifier on byte selector component\nbyte-id-a\n,\nbyte-id-b\n:\nWhen\n.scale_vec_size\nis\n.scale_vec::1X\nOne byte each within\nscale-a-data\nand\nscale-b-data\ndetermined by\nbyte-id-a\n,\nbyte-id-b\nrespectively contributes the\nSF_A\nand\nSF_B\nvalues.\nWhen\n.scale_vec_size\nis\n.scale_vec::2X\nOne byte-pair (two bytes) within\nscale-a-data\nand\nscale-b-data\ndetermined by\nbyte-id-a\nand\nbyte-id-b\ncontributes the\nSF_A\nand\nSF_B\nvalues. The value\nof 0 selects lower two bytes whereas value of 2 selects upper two bytes from the\ncorresponding metadata value.\nWhen\n.scale_vec_size\nis\n.scale_vec::4X\nAll four bytes within\nscale-a-data\nand\nscale-b-data\ncontribute the values.\nHence,\nbyte-id-a\n,\nbyte-id-b\nmust be zero.\nRefer\nFigure 45\nfor more details.\nFigure 45\nSelection of set of values for\nSF_A\nor\nSF_B\nbased on\nbyte-id-a\nor\nbyte-id-b\n\nTable 37\nenumerates the valid values for\nvarious selector components. Any other value results in an undefined behavior.\nTable 37\nValid values for various selector components\n\n.scale_vec_size\nSelector Components\nbyte-id-a\nthread-id-a\nbyte-id-b\nthread-id-b\nscale_vec::1X\n[0, 1, 2, 3]\n[0, 1]\n[0, 1, 2, 3]\n[0, 1, 2, 3]\nscale_vec::2X\n[0, 2]\n[0, 2]\nscale_vec::4X\n0\n0\n9.7.14.4.\nMatrix multiply-accumulate operation using\nwmma\ninstructions\n\nThis section describes warp level\nwmma.load,\nwmma.mma\nand\nwmma.store\ninstructions and the\norganization of various matrices invovled in these instruction.\n9.7.14.4.1.\nMatrix Fragments for WMMA\n\nEach thread in the warp holds a fragment of the matrix. The distribution of fragments loaded by the\nthreads in a warp is unspecified and is target architecture dependent, and hence the identity of the\nfragment within the matrix is also unspecified and is target architecture dependent. The fragment\nreturned by a\nwmma\noperation can be used as an operand for another\nwmma\noperation if the\nshape, layout and element type of the underlying matrix matches. Since fragment layout is\narchitecture dependent, using the fragment returned by a\nwmma\noperation in one function as an\noperand for a\nwmma\noperation in a different function may not work as expected if the two\nfunctions are linked together but were compiled for different link-compatible SM architectures. Note\npassing\nwmma\nfragment to a function having\n.weak\nlinkage is unsafe since at link time\nreferences to such function may get resolved to a function in different compilation module.\nEach fragment is a vector expression whose contents are determined as follows. The identity of\nindividual matrix elements in the fragment is unspecified.\nInteger fragments\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.u8\nor\n.s8\n.m16n16k16\nA\nA vector expression of two\n.b32\nregisters, with each\nregister containing four elements from the matrix.\nB\nA vector expression of two\n.b32\nregisters, with each\nregister containing four elements from the matrix.\n.m8n32k16\nA\nA vector expression containing a single\n.b32\nregister\ncontaining four elements from the matrix.\nB\nA vector expression of four\n.b32\nregisters, with each\nregister containing four elements from the matrix.\n.m32n8k16\nA\nA vector expression of four\n.b32\nregisters, with each\nregister containing four elements from the matrix.\nB\nA vector expression containing single\n.b32\nregister,\nwith each containing four elements from the matrix.\nAccumulators (C or D):\nData-type\nShape\nFragment\n.s32\n.m16n16k16\nA vector expression of eight\n.s32\nregisters.\n.m8n32k16\n.m32n8k16\nFloating point fragments\nData-type\nMatrix\nFragment\n.f16\nA or B\nA vector expression of eight\n.f16x2\nregisters.\n.f16\nC or D\nA vector expression of four\n.f16x2\nregisters.\n.f32\nA vector expression of eight\n.f32\nregisters.\nFloating point fragments for\n.bf16\ndata format\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.bf16\n.m16n16k16\nA\nA vector expression of four\n.b32\nregisters, with each\nregister containing two elements from the matrix.\nB\n.m8n32k16\nA\nA vector expression containing a two\n.b32\nregisters,\nwith containing two elements from the matrix.\nB\nA vector expression of eight\n.b32\nregisters, with\neach register containing two elements from the matrix.\n.m32n8k16\nA\nA vector expression of eight\n.b32\nregisters, with\neach register containing two elements from the matrix.\nB\nA vector expression containing two\n.b32\nregisters,\nwith each containing two elements from the matrix.\nAccumulators (C or D):\nData-type\nMatrix\nFragment\n.f32\nC or D\nA vector expression containing eight\n.f32\nregisters.\nFloating point fragments for\n.tf32\ndata format\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.tf32\n.m16n16k8\nA\nA vector expression of four\n.b32\nregisters.\nB\nA vector expression of four\n.b32\nregisters.\nAccumulators (C or D):\nData-type\nShape\nMatrix\nFragment\n.f32\n.m16n16k8\nC or D\nA vector expression containing eight\n.f32\nregisters.\nDouble precision floating point fragments\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.f64\n.m8n8k4\nA or B\nA vector expression of single\n.f64\nregister.\nAccumulators (C or D):\nData-type\nShape\nMatrix\nFragment\n.f64\n.m8n8k4\nC or D\nA vector expression containing single\n.f64\nregister.\nSub-byte integer and single-bit fragments\nMultiplicands (A or B):\nData-type\nShape\nFragment\n.u4\nor\n.s4\n.m8n8k32\nA vector expression containing a single\n.b32\nregister, containing eight elements from the matrix.\n.b1\n.m8n8k128\nA vector expression containing a single\n.b32\nregister, containing 32 elements from the matrix.\nAccumulators (C or D):\nData-type\nShape\nFragment\n.s32\n.m8n8k32\nA vector expression of two\n.s32\nregisters.\n.m8n8k128\nA vector expression of two\n.s32\nregisters.\nManipulating fragment contents\nThe contents of a matrix fragment can be manipulated by reading and writing to individual\nregisters in the fragment, provided the following conditions are satisfied:\nAll matrix element in the fragment are operated on uniformly across threads, using the same\nparameters.\nThe order of the matrix elements is not changed.\nFor example, if each register corresponding to a given matrix is multiplied by a uniform constant\nvalue, then the resulting matrix is simply the scaled version of the original matrix.\nNote that type conversion between\n.f16\nand\n.f32\naccumulator fragments is not supported in\neither direction. The result is undefined even if the order of elements in the fragment remains\nunchanged.\n9.7.14.4.2.\nMatrix Storage for WMMA\n\nEach matrix can be stored in memory with a\nrow-major\nor\ncolumn-major\nlayout. In a\nrow-major\nformat, consecutive elements of each row are stored in contiguous memory locations, and the row is\ncalled the\nleading dimension\nof the matrix. In a\ncolumn-major\nformat, consecutive elements of\neach column are stored in contiguous memory locations and the column is called the\nleading\ndimension\nof the matrix.\nConsecutive instances of the\nleading dimension\n(rows or columns) need not be stored contiguously\nin memory. The\nwmma.load\nand\nwmma.store\noperations accept an optional argument\nstride\nthat specifies the offset from the beginning of each row (or column) to the next, in terms of matrix\nelements (and not bytes). For example, the matrix being accessed by a\nwmma\noperation may be a\nsubmatrix from a larger matrix stored in memory. This allows the programmer to compose a\nmultiply-and-accumulate operation on matrices that are larger than the shapes supported by the\nwmma\noperation.\nAddress Alignment\nThe starting address of each instance of the leading dimension (row or column) must be aligned\nwith the size of the corresponding fragment in bytes. Note that the starting address is\ndetermined by the base pointer and the optional\nstride\n.\nConsider the following instruction as an example:\nwmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s;\nFragment size in bytes = 32 (eight elements of type\n.f16x2\n)\nActual\nstride\nin bytes = 2 *\ns\n(since\nstride\nis specified in terms of\n.f16\nelements, not bytes)\nFor each row of this matrix to be aligned at fragment size the following must be true:\np\nis a multiple of 32.\n2*s\nis a multiple of 32.\nDefault value for stride\nThe default value of the\nstride\nis the size of the\nleading dimension\nof the matrix. For\nexample, for an\nMxK\nmatrix, the\nstride\nis\nK\nfor a\nrow-major\nlayout and\nM\nfor a\ncolumn-major\nlayout. In particular, the default strides for the supported matrix shapes are as\nfollows:\nShape\nA (row)\nA (column)\nB (row)\nB (column)\nAccumulator (row)\nAccumulator (column)\n16x16x16\n16\n16\n16\n16\n16\n16\n8x32x16\n16\n8\n32\n16\n32\n8\n32x8x16\n16\n32\n8\n16\n8\n32\n8x8x32\n32\n8\n8\n32\n8\n8\n8x8x128\n128\n8\n8\n128\n8\n8\n16x16x8\n8\n16\n16\n8\n16\n16\n8x8x4\n4\n8\n8\n4\n8\n8\n9.7.14.4.3.\nWarp-level Matrix Load Instruction:\nwmma.load\n\nwmma.load\nCollectively load a matrix from memory for WMMA\nSyntax\nFloating point format\n.f16\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride};\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride};\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f16, .s8, .u8};\n.btype  = {.f16, .s8, .u8};\n.ctype  = {.f16, .f32, .s32};\nAlternate floating point format\n.bf16\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.bf16 };\n.btype  = {.bf16 };\n.ctype  = {.f32 };\nAlternate floating point format\n.tf32\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.tf32 };\n.btype  = {.tf32 };\n.ctype  = {.f32 };\nDouble precision Floating point\n.f64\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f64 };\n.btype  = {.f64 };\n.ctype  = {.f64 };\nSub-byte loads:\nwmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\n.ctype  = {.s32};\nSingle-bit loads:\nwmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.b1};\n.btype  = {.b1};\n.ctype  = {.s32};\nDescription\nCollectively load a matrix across all threads in a warp from the location indicated by address\noperand\np\nin the specified state space into destination register\nr\n.\nIf no state space is given, perform the memory accesses using\nGeneric Addressing\n.\nwmma.load\noperation may be used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\nThe mutually exclusive qualifiers\n.a\n,\n.b\nand\n.c\nindicate whether matrix A, B or C is\nbeing loaded respectively for the\nwmma\ncomputation.\nThe destination operand\nr\nis a brace-enclosed vector expression that can hold the fragment\nreturned by the load operation, as described in\nMatrix Fragments for WMMA\n.\nThe\n.shape\nqualifier indicates the dimensions of all the matrix arguments involved in the\nintended\nwmma\ncomputation.\nThe\n.layout\nqualifier indicates whether the matrix to be loaded is stored in\nrow-major\nor\ncolumn-major\nformat.\nstride\nis an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the\nleading dimension\n(rows or columns). The default\nvalue of\nstride\nis described in\nMatrix Storage for WMMA\nand must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior.\nThe required alignment for address\np\nand\nstride\nis described in the\nMatrix Storage for WMMA\n.\nThe mandatory\n.sync\nqualifier indicates that\nwmma.load\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.load\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.load\ninstruction. In conditionally executed code, a\nwmma.load\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.load\nis undefined if all threads do not use the same qualifiers and the\nsame values of\np\nand\nstride\n, or if any thread in the warp has exited.\nwmma.load\nis treated as a\nweak\nmemory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\n.m8n8k4\nand\n.m16n16k8\non\nwmma\nintroduced in PTX ISA version 7.0.\nDouble precision and alternate floating point precision\nwmma\nintroduced in PTX ISA version 7.0.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision and alternate floating point precision\nwmma\nrequires\nsm_80\nor higher.\nExamples\n// Load elements from f16 row-major matrix B\n.reg .b32 x<8>;\n\nwmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr];\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from f32 column-major matrix C and scale the values:\n.reg .b32 x<8>;\n\nwmma.load.c.sync.aligned.m16n16k16.col.f32\n                 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr];\n\nmul.f32 x0, x0, 0.1;\n// repeat for all registers x<8>;\n...\nmul.f32 x7, x7, 0.1;\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from integer matrix A:\n.reg .b32 x<4>\n// destination registers x<4> contain four packed .u8 values each\nwmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr];\n\n// Load elements from sub-byte integer matrix A:\n.reg .b32 x0;\n// destination register x0 contains eight packed .s4 values\nwmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr];\n\n// Load elements from .bf16 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k16.row.bf16\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .tf32 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k8.row.tf32\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .f64 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m8n8k4.row.f64\n                {x0}, [ptr];\n9.7.14.4.4.\nWarp-level Matrix Store Instruction:\nwmma.store\n\nwmma.store\nCollectively store a matrix into memory for WMMA\nSyntax\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f16, .f32, .s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32, .m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.type   = {.s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.type   = {.f64};\nDescription\nCollectively store a matrix across all threads in a warp at the location indicated by address\noperand\np\nin the specified state space from source register\nr\n.\nIf no state space is given, perform the memory accesses using\nGeneric Addressing\n.\nwmma.load\noperation may be used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\nThe source operand\nr\nis a brace-enclosed vector expression that matches the shape of the\nfragment expected by the store operation, as described in\nMatrix Fragments for WMMA\n.\nThe\n.shape\nqualifier indicates the dimensions of all the matrix arguments involved in the\nintended\nwmma\ncomputation. It must match the\n.shape\nqualifier specified on the\nwmma.mma\ninstruction that produced the D matrix being stored.\nThe\n.layout\nqualifier indicates whether the matrix to be loaded is stored in\nrow-major\nor\ncolumn-major\nformat.\nstride\nis an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the\nleading dimension\n(rows or columns). The default\nvalue of\nstride\nis described in\nMatrix Storage for WMMA\nand must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior.\nThe required alignment for address\np\nand\nstride\nis described in the\nMatrix Storage for WMMA\n.\nThe mandatory\n.sync\nqualifier indicates that\nwmma.store\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.store\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.store\ninstruction. In conditionally executed code, a\nwmma.store\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.store\nis undefined if all threads do not use the same qualifiers and the\nsame values of\np\nand\nstride\n, or if any thread in the warp has exited.\nwmma.store\nis treated as a\nweak\nmemory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\n.m16n16k8\nintroduced in PTX ISA version 7.0.\nDouble precision\nwmma\nintroduced in PTX ISA version 7.0.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision\nwmma\nand shape\n.m16n16k8\nrequires\nsm_80\nor higher.\nExamples\n// Storing f32 elements computed by a wmma.mma\n.reg .b32 x<8>;\n\nwmma.mma.sync.m16n16k16.row.col.f32.f32\n              {d0, d1, d2, d3, d4, d5, d6, d7}, ...;\nwmma.store.d.sync.m16n16k16.row.f32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m16n16k16 shape:\n.reg .b32 d<8>;\nwmma.store.d.sync.aligned.m16n16k16.row.s32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m8n8k128 shape:\n.reg .b32 d<2>\nwmma.store.d.sync.aligned.m8n8k128.row.s32\n[ptr], {d0, d1};\n\n// Store f64 accumulator for m8n8k4 shape:\n.reg .f64 d<2>;\nwmma.store.d.sync.aligned.m8n8k4.row.f64\n              [ptr], {d0, d1};\n9.7.14.4.5.\nWarp-level Matrix Multiply-and-Accumulate Instruction:\nwmma.mma\n\nwmma.mma\nPerform a single matrix multiply-and-accumulate operation across a warp\nSyntax\n// Floating point (.f16 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype d, a, b, c;\n\n// Integer (.u8/.s8 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape  =  {.m16n16k16, .m8n32k16, .m32n8k16};\n.dtype   = {.f16, .f32};\n.atype   = {.s8, .u8};\n.btype   = {.s8, .u8};\n.ctype   = {.f16, .f32};\nFloating point format\n.bf16\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k16, .m8n32k16, .m32n8k16};\n.atype   = {.bf16 };\n.btype   = {.bf16};\nFloating point format\n.tf32\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k8 };\n.atype   = {.tf32 };\n.btype   = {.tf32};\nFloating point Double precision\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape{.rnd}.f64.f64.f64.f64 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m8n8k4 };\n.rnd = { .rn, .rz, .rm, .rp };\nSub-byte (\n.u4\n/\n.s4\nmultiplicands)\nwmma.mma\n:\nwmma.mma.sync.aligned.row.col.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n.shape  = {.m8n8k32};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\nSingle-bit (\n.b1\nmultiplicands)\nwmma.mma\n:\nwmma.mma.op.popc.sync.aligned.row.col.shape.s32.atype.btype.s32 d, a, b, c;\n.shape  = {.m8n8k128};\n.atype  = {.b1};\n.btype  = {.b1};\n.op     = {.xor, .and}\nDescription\nPerform a warp-level matrix multiply-and-accumulate computation\nD\n=\nA\n*\nB\n+\nC\nusing matrices A,\nB and C loaded in registers\na\n,\nb\nand\nc\nrespectively, and store the result matrix in\nregister\nd\n. The register arguments\na\n,\nb\n,\nc\nand\nd\nhold unspecified fragments of\nthe corresponding matrices as described in\nMatrix Fragments for WMMA\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively.\nFor\nwmma.mma\nwithout explicit\n.atype\nand\n.btype\n:\n.atype\nand\n.btype\nare\nimplicitly set to\n.f16\n.\nFor integer\nwmma\n,\n.ctype\nand\n.dtype\nmust be specified as\n.s32\n. Also, the values for\n.atype\nand\n.btype\nmust be the same, i.e., either both are\n.s8\nor both are\n.u8\n.\nFor sub-byte single-bit\nwmma\n,\n.ctype\nand\n.dtype\nmust be specified as\n.s32\n. Also, the\nvalues for\n.atype\nand\n.btype\nmust be the same; i.e., either both are\n.s4\n, both are\n.u4\n, or both are\n.b1\n.\nFor single-bit\nwmma\n, multiplication is replaced by a sequence of logical operations;\nspecifically,\nwmma.xor.popc\nand\nwmma.and.popc\ncomputes the XOR, AND respectively of a\n128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result\n(\npopc\n). This result is added to the corresponding element of C and written into D.\nThe qualifiers\n.alayout\nand\n.blayout\nmust match the layout specified on the\nwmma.load\ninstructions that produce the contents of operands\na\nand\nb\nrespectively. Similarly, the\nqualifiers\n.atype\n,\n.btype\nand\n.ctype\nmust match the corresponding qualifiers on the\nwmma.load\ninstructions that produce the contents of operands\na\n,\nb\nand\nc\nrespectively.\nThe\n.shape\nqualifier must match the\n.shape\nqualifier used on the\nwmma.load\ninstructions\nthat produce the contents of all three input operands\na\n,\nb\nand\nc\nrespectively.\nThe destination operand\nd\nis a brace-enclosed vector expression that matches the\n.shape\nof\nthe fragment computed by the\nwmma.mma\ninstruction.\nSaturation at the output:\nThe optional qualifier\n.satfinite\nindicates that the final values in the destination register\nare saturated as follows:\nThe output is clamped to the minimum or maximum 32-bit signed integer value. Otherwise, if the\naccumulation would overflow, the value wraps.\nPrecision and rounding for\n.f16\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with\nat least single precision. When both\n.ctype\nand\n.dtype\nare specified as\n.f16\n, the\naccumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs is unspecified.\nPrecision and rounding for\n.bf16\n,\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding and handling of subnormal inputs is unspecified.\nRounding modifiers on double precision\nwmma.mma\n(default is\n.rn\n):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe mandatory\n.sync\nqualifier indicates that\nwmma.mma\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.mma\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.mma\ninstruction. In conditionally executed code, a\nwmma.mma\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.mma\nis undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\nDouble precision and alternate floating point precision\nwmma\nintroduced in PTX ISA version 7.0.\nSupport for\n.and\noperation in single-bit\nwmma\nintroduced in PTX ISA version 7.1.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n.satfinite\non floating point\nwmma.mma\nis deprecated in PTX ISA version 6.4 and\nis removed from PTX ISA version 6.5.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision, alternate floating point precision\nwmma\nrequire\nsm_80\nor higher.\n.and\noperation in single-bit\nwmma\nrequires\nsm_80\nor higher.\nExamples\n.global .align 32 .f16 A[256], B[256];\n.global .align 32 .f32 C[256], D[256];\n.reg .b32 a<8> b<8> c<8> d<8>;\n\nwmma.load.a.sync.aligned.m16n16k16.global.row.f16\n        {a0, a1, a2, a3, a4, a5, a6, a7}, [A];\nwmma.load.b.sync.aligned.m16n16k16.global.col.f16\n        {b0, b1, b2, b3, b4, b5, b6, b7}, [B];\n\nwmma.load.c.sync.aligned.m16n16k16.global.row.f32\n        {c0, c1, c2, c3, c4, c5, c6, c7}, [C];\n\nwmma.mma.sync.aligned.m16n16k16.row.col.f32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3, a4, a5, a6, a7},\n        {b0, b1, b2, b3, b4, b5, b6, b7},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\nwmma.store.d.sync.aligned.m16n16k16.global.col.f32\n        [D], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Compute an integer WMMA:\n.reg .b32  a, b<4>;\n.reg .b32 c<8>, d<8>;\nwmma.mma.sync.aligned.m8n32k16.row.col.s32.s8.s8.s32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a}, {b0, b1, b2,  b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\n// Compute sub-byte WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute single-bit type WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.xor.popc.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute double precision wmma\n.reg .f64 a, b, c<2>, d<2>;\nwmma.mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute alternate floating point precision wmma\n.reg .b32 a<2>, b<2>, c<8>, d<8>;\nwmma.mma.sync.aligned.m16n16k8.row.col.f32.tf32.tf32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3}, {b0, b1, b2, b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n9.7.14.5.\nMatrix multiply-accumulate operation using\nmma\ninstruction\n\nThis section describes warp-level\nmma\n,\nldmatrix\n,\nstmatrix\n, and\nmovmatrix\ninstructions and the organization of various matrices involved in these instructions.\n9.7.14.5.1.\nMatrix Fragments for\nmma.m8n8k4\nwith\n.f16\nfloating point type\n\nA warp executing\nmma.m8n8k4\nwith\n.f16\nfloating point type will compute 4 MMA operations of shape\n.m8n8k4\n.\nElements of 4 matrices need to be distributed across the threads in a warp. The following table\nshows distribution of matrices for MMA operations.\nMMA Computation\nThreads participating in MMA computation\nMMA computation 1\nThreads with\n%laneid\n0-3 (low group) and 16-19 (high group)\nMMA computation 2\nThreads with\n%laneid\n4-7 (low group) and 20-23 (high group)\nMMA computation 3\nThreads with\n%laneid\n8-11 (low group) and 24-27 (high group)\nMMA computation 4\nThreads with\n%laneid\n12-15 (low group) and 28-31 (high group)\nFor each of the individual MMA computation shown above, each of the required thread holds a fragment\nof the matrix for performing mma operation as follows:\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters,\nwith each register containing two\n.f16\nelements from\nthe matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown below:\nFragment layout for Row Major matrix A is shown in\nFigure 46\n.\nFigure 46\nMMA .m8n8k4 fragment layout for row-major matrix A with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\ncol\n=\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nFragment layout for Column Major matrix A is shown in\nFigure 47\n.\nThe layout of the fragments held by different threads is shown below:\nFigure 47\nMMA .m8n8k4 fragment layout for column-major matrix A with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\ni\n%\n4\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nif\n%\nlaneid\n<\n16\n(\ni\n%\n4\n)\n+\n4\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\notherwise\ncol\n=\n%\nlaneid\n%\n4\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown below:\nFragment layout for Row Major matrix B is shown in\nFigure 48\n.\nFigure 48\nMMA .m8n8k4 fragment layout for row-major matrix B with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\ncol\n=\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nif\n%\nlaneid\n<\n16\ni\n+\n4\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\notherwise\nFragment layout for Column Major matrix B is shown in\nFigure 49\n.\nFigure 49\nMMA .m8n8k4 fragment layout for column-major matrix B with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\nAccumulators C (or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing four\n.f16x2\nregisters, with\neach register containing two\n.f16\nelements from the matrix\nC (or D).\nc0, c1, c2, c3, c4, c5, c6, c7\n.f32\nA vector expression of eight\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown below:\nFragment layout for accumulator matrix when\n.ctype\nis\n.f16\nis shown in\nFigure 50\n.\nFigure 50\nMMA .m8n8k4 fragment layout for matrix C/D with\n.ctype\n=\n.f16\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\ncol\n=\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nFragment layout for accumulator matrix when\n.ctype\nis\n.f32\nis shown in\nFigure 51\nand\nFigure 52\n.\nFigure 51\nMMA .m8n8k4 computation 1 and 2 fragment layout for matrix C/D with\n.ctype\n=\n.f32\n\nFigure 52\nMMA .m8n8k4 computation 3 and 4 fragment layout for matrix C/D with\n.ctype\n=\n.f32\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\nX\nif\n%\nlaneid\n<\n16\nX\n+\n4\notherwise\nwhere\nX\n=\n(\n%\nlaneid\n&\n0b1\n)\n+\n(\ni\n&\n0b10\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\n(\ni\n&\n0b100\n)\n+\n(\n%\nlaneid\n&\n0b10\n)\n+\n(\ni\n&\n0b1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\n9.7.14.5.2.\nMatrix Fragments for\nmma.m8n8k4\nwith\n.f64\nfloating point type\n\nA warp executing\nmma.m8n8k4\nwith\n.f64\nfloating point type will compute an MMA operation of\nshape\n.m8n8k4\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing a single\n.f64\nregister, containing\nsingle\n.f64\nelement from the matrix A.\na0\nThe layout of the fragments held by different threads is shown in\nFigure 53\n.\nFigure 53\nMMA .m8n8k4 fragment layout for matrix A  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n>>\n2\ncol\n=\n%\nlaneid\n%\n4\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression containing a single\n.f64\nregister, containing a single\n.f64\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 54\n.\nFigure 54\nMMA .m8n8k4 fragment layout for matrix B  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\ncol\n=\n%\nlaneid\n>>\n2\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing of two\n.f64\nregisters containing two\n.f64\nelements from the matrix C.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 55\n.\nFigure 55\nMMA .m8n8k4 fragment layout for accumulator matrix C/D  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.3.\nMatrix Fragments for\nmma.m8n8k16\n\nA warp executing\nmma.m8n8k16\nwill compute an MMA operation of shape\n.m8n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.s8\nor\n.u8\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 56\n.\nFigure 56\nMMA .m8n8k16 fragment layout for matrix A with\n.u8\n/\n.s8\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.s8\nor\n.u8\nelements from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 57\n.\nFigure 57\nMMA .m8n8k16 fragment layout for matrix B with\n.u8\n/\n.s8\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing of two\n.s32\nregisters.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 58\n.\nFigure 58\nMMA .m8n8k16 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.4.\nMatrix Fragments for\nmma.m8n8k32\n\nA warp executing\nmma.m8n8k32\nwill compute an MMA operation of shape\n.m8n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister, containing\neight\n.s4\nor\n.u4\nelements from the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 59\n.\nFigure 59\nMMA .m8n8k32 fragment layout for matrix A with\n.u4\n/\n.s4\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister, containing\neight\n.s4\nor\n.u4\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 60\n.\nFigure 60\nMMA .m8n8k32 fragment layout for matrix B with\n.u4\n/\n.s4\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression of two\n.s32\nregisters.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 61\n:\nFigure 61\nMMA .m8n8k32 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.5.\nMatrix Fragments for\nmma.m8n8k128\n\nA warp executing\nmma.m8n8k128\nwill compute an MMA operation of shape\n.m8n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister, containing thirty\ntwo\n.b1\nelements from the matrix A.\na0, a1, … a30, a31\nThe layout of the fragments held by different threads is shown in\nFigure 62\n.\nFigure 62\nMMA .m8n8k128 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n31\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister, containing thirty\ntwo\n.b1\nelements from the matrix B.\nb0, b1, …, b30, b31\nThe layout of the fragments held by different threads is shown in\nFigure 63\n.\nFigure 63\nMMA .m8n8k128 fragment layout for matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n31\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing two\n.s32\nregisters, containing two\n.s32\nelements from the matrix C (or D).\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 64\n.\nFigure 64\nMMA .m8n8k128 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.6.\nMatrix Fragments for\nmma.m16n8k4\n\nA warp executing\nmma.m16n8k4\nwill compute an MMA operation of shape\n.m16n8k4\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.tf32\n:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing two\n.b32\nregisters, containing two\n.tf32\nelements from the matrix A.\na0, a1\nThe layout of the fragments held by different threads is shown in\nFigure 65\n.\nFigure 65\nMMA .m16n8k4 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\nthreadID_in_group\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing two\n.f64\nregisters, containing two\n.f64\nelements from the matrix A.\na0, a1\nThe layout of the fragments held by different threads is shown in\nFigure 66\n.\nFigure 66\nMMA .m16n8k4 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\nthreadID_in_group\nMultiplicand B:\n.tf32\n:\n.btype\nFragment\nElements (low to high)\n.tf32\nA vector expression of a single\n.b32\nregister, containing a single\n.tf32\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 67\n.\nFigure 67\nMMA .m16n8k4 fragment layout for matrix B with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\ncol\n=\ngroupID\n.f64\n:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression of a single\n.f64\nregister, containing a single\n.f64\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 68\n.\nFigure 68\nMMA .m16n8k4 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\ncol\n=\ngroupID\nAccumulators (C or D):\n.tf32\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f32\nA vector expression containing four\n.f32\nregisters, containing four\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 69\n.\nFigure 69\nMMA .m16n8k4 fragment layout for accumulator matrix C/D with\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.f64\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, containing four\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 70\n.\nFigure 70\nMMA .m16n8k4 fragment layout for accumulator matrix C/D with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.7.\nMatrix Fragments for\nmma.m16n8k8\n\nA warp executing\nmma.m16n8k8\nwill compute an MMA operation of shape\n.m16n8k8\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.f16\nand\n.bf16\n:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\n/\n.bf16\nelements from the\nmatrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 71\n.\nFigure 71\nMMA .m16n8k8 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na1\ngroupID\n+\n8\nfor\na2\nand\na3\ncol\n=\nthreadID_in_group\n*\n2\n+\n(\ni\n&\n0x1\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.tf32\n:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters, containing four\n.tf32\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 72\n.\nFigure 72\nMMA .m16n8k8 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\nthreadID_in_group\nfor\na0\nand\na1\nthreadID_in_group\n+\n4\nfor\na2\nand\na3\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, containing four\n.f64\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 73\n.\nFigure 73\nMMA .m16n8k8 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\nthreadID_in_group\nfor\na0\nand\na1\nthreadID_in_group\n+\n4\nfor\na2\nand\na3\nMultiplicand B:\n.f16\nand\n.bf16\n:\n.btype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing a single\n.f16x2\nregister, containing\ntwo\n.f16\n/\n.bf16\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 74\n.\nFigure 74\nMMA .m16n8k8 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,\n1\n}\ncol\n=\ngroupID\n.tf32\n:\n.btype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing two\n.b32\nregisters, containing two\n.tf32\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 75\n.\nFigure 75\nMMA .m16n8k8 fragment layout for matrix B with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\nfor\nb0\nthreadID_in_group\n+\n4\nfor\nb1\ncol\n=\ngroupID\n.f64\n:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression containing two\n.f64\nregisters, containing two\n.f64\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 76\n.\nFigure 76\nMMA .m16n8k8 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\nfor\nb0\nthreadID_in_group\n+\n4\nfor\nb1\ncol\n=\ngroupID\nAccumulators (C or D):\n.f16\n,\n.bf16\nand\n.tf32\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression of four\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown in\nFigure 77\n.\nFigure 77\nMMA .m16n8k8 fragment layout for accumulator matrix C/D with\n.f16x2\n/\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.f64\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression of four\n.f64\nregisters containing four\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 78\n.\nFigure 78\nMMA .m16n8k8 fragment layout for accumulator matrix C/D with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.8.\nMatrix Fragments for\nmma.m16n8k16\nwith floating point type\n\nA warp executing\nmma.m16n8k16\nfloating point types will compute an MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.f16\nand\n.bf16\n:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.f16x2\nregisters, with\neach register containing two\n.f16\n/\n.bf16\nelements\nfrom the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 79\n.\nFigure 79\nMMA .m16n8k16 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n2\n||\n4\n<=\ni\n<\n6\ngroupID\n+\n8\nOtherwise\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nai\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\n+\n8\nfor\nai\nwhere\ni\n>=\n4\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing eight\n.f64\nregisters, with each\nregister containing one\n.f64\nelement from the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 80\n.\nFigure 80\nMMA .m16n8k16 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n%\n2\n=\n0\ngroupID\n+\n8\nOtherwise\ncol\n=\n(\ni\n*\n2\n)\n+\nthreadID_in_group\nfor\nai\nwhere\ni\n%\n2\n=\n0\n(\ni\n*\n2\n)\n-\n2\n+\n(\nthreadID_in_group\nOtherwise\nMultiplicand B:\n.f16\nand\n.bf16\n:\n.btype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing two\n.f16x2\nregisters, with\neach register containing two\n.f16\n/\n.bf16\nelements\nfrom the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 81\n.\nFigure 81\nMMA .m16n8k16 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nwhere the row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nbi\nwhere\ni\n<\n2\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\n+\n8\nfor\nbi\nwhere\ni\n>=\n2\ncol\n=\ngroupID\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, with each\nregister containing one\n.f64\nelement from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 82\n.\nFigure 82\nMMA .m16n8k16 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\n+\n(\ni\n*\n4\n)\nfor\nbi\nwhere\ni\n<\n4\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters containing\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters containing\nfour\n.f32\nelements from the matrix C (or D).\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nThe layout of the fragments held by different threads is shown in\nFigure 83\n.\nFigure 83\nMMA .m16n8k16 fragment layout for accumulator matrix matrix C/D.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.9.\nMatrix Fragments for\nmma.m16n8k16\nwith integer type\n\nA warp executing\nmma.m16n8k16\nwill compute an MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.u8\n/\n.s8\nelements from the\nmatrix A.\na0, a1, a2, a3, a4, a5, a6, a7\n.e4m3\n/\n.e5m2\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\nelements from the\nmatrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 84\n.\nFigure 84\nMMA .m16n8k16 fragment layout for matrix A with\n.u8\n/\n.s8\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n4\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n4\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.u8\n/\n.s8\nelements from the matrix B.\nb0, b1, b2, b3\n.e4m3\n/\n.e5m2\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.e4m3\n/\n.e5m2\nelements from the matrix B.\nb0, b1. b2. b3\nThe layout of the fragments held by different threads is shown in\nFigure 85\n.\nFigure 85\nMMA .m16n8k16 fragment layout for matrix B with\n.u8\n/\n.s8\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing\nfour\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c1, c2\nThe layout of the fragments held by different threads is shown in\nFigure 86\n.\nFigure 86\nMMA .m16n8k16 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.10.\nMatrix Fragments for\nmma.m16n8k32\n\nA warp executing\nmma.m16n8k32\nwill compute an MMA operation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.s4\nor\n.u4\n:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.u4\n/\n.s4\nelements from the\nmatrix A.\na0, a1, …, a14, a15\nThe layout of the fragments held by different threads is shown in\nFigure 87\n.\nFigure 87\nMMA .m16n8k32 fragment layout for matrix A with\n.u4\n/\n.s4\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n8\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n8\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n15\n}\n.s8\nor\n.u8\nor\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.s8\n/\n.u8\nelements from the\nmatrix A.\na0, a1, .., a14, a15\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from the matrix A.\na0, a1, …, a14, a15\nThe layout of the fragments held by different threads is shown in\nFigure 88\n.\nFigure 88\nMMA .m16n8k32 fragment layout for matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\n||\n8\n<=\ni\n<\n12\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nai\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\n+\n16\nfor\nai\nwhere\ni\n>=\n8\nMultiplicand B:\n.s4\nor\n.u4\n:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister,\ncontaining eight\n.s4\n/\n.u4\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 89\n.\nFigure 89\nMMA .m16n8k32 fragment layout for matrix B with\n.u4\n/\n.s4\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\ngroupID\n.s8\nor\n.u8\nor\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n:\n.btype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.s8\n/\n.u8\nelements from the\nmatrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 90\nand\nFigure 91\n.\nFigure 90\nMMA .m16n8k32 fragment layout for rows 0–15 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 91\nMMA .m16n8k32 fragment layout for rows 16–31 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nbi\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\n+\n16\nfor\nbi\nwhere\ni\n>=\n4\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing\nfour\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 92\n.\nFigure 92\nMMA .m16n8k32 fragment layout for accumulator matrix C/D with\n.s32\n/\n.f32\n/\n.f16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.11.\nMatrix Fragments for\nmma.m16n8k64\n\nA warp executing\nmma.m16n8k64\nwill compute an MMA operation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight\n.s4\n/\n.u4\nelements from the\nmatrix A.\na0, a1, …, a30, a31\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight\n.e2m1\nelements from the matrix A.\na0, a1, …, a30, a31\nThe layout of the fragments held by different threads is shown in\nFigure 93\n.\nFigure 93\nMMA .m16n8k64 fragment layout for matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\n||\n16\n<=\ni\n<\n24\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nai\nwhere\ni\n<\n16\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\n+\n32\nfor\nai\nwhere\ni\n>=\n16\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.s4\n/\n.u4\nelements from the\nmatrix B.\nb0, b1, …, b14, b15\n.e2m1\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.e2m1\nelements from the matrix B.\nb0, b1, …, b14, b15\nThe layout of the fragments held by different threads is shown in\nFigure 94\nand\nFigure 95\n.\nFigure 94\nMMA .m16n8k64 fragment layout for rows 0–31 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 95\nMMA .m16n8k64 fragment layout for rows 32–63 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nbi\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\n+\n32\nfor\nbi\nwhere\ni\n>=\n8\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing four\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing four\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 96\n.\nFigure 96\nMMA .m16n8k64 fragment layout for accumulator matrix C/D with\n.s32\n/\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.12.\nMatrix Fragments for\nmma.m16n8k128\n\nA warp executing\nmma.m16n8k128\nwill compute an MMA operation of shape\n.m16n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing two\n.b32\nregisters, with each register containing\nthirty two\n.b1\nelements from the matrix A.\na0, a1, …, a62, a63\nThe layout of the fragments held by different threads is shown in\nFigure 97\n.\nFigure 97\nMMA .m16n8k128 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n32\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n32\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,\n...,\n63\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister containing thirty\ntwo\n.b1\nelements from the matrix B.\nb0, b1, … , b30, b31\nThe layout of the fragments held by different threads is shown in\nFigure 98\n.\nFigure 98\nMMA .m16n8k128 fragment layout for matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,...,\n31\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 99\n.\nFigure 99\nMMA .m16n8k128 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n,\n2\n,\n3\n}\n9.7.14.5.13.\nMatrix Fragments for\nmma.m16n8k256\n\nA warp executing\nmma.m16n8k256\nwill compute an MMA operation of shape\n.m16n8k256\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing four\n.b32\nregisters, with each register\ncontaining thirty two\n.b1\nelements from the matrix A.\na0, a1, …, a126, a127\nThe layout of the fragments held by different threads is shown in\nFigure 100\n.\nFigure 100\nMMA .m16n8k256 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n32\n||\n64\n<=\ni\n<\n96\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nai\nwhere\ni\n<\n64\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\n+\n128\nfor\nai\nwhere\ni\n>=\n64\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing two\n.b32\nregisters, with each register\ncontaining thirty two\n.b1\nelements from the matrix B.\nb0, b1, …, b62, b63\nThe layout of the fragments held by different threads is shown in\nFigure 101\nand\nFigure 102\n.\nFigure 101\nMMA .m16n8k256 fragment layout for rows 0–127 of matrix B with\n.b1\ntype.\n\nFigure 102\nMMA .m16n8k256 fragment layout for rows 128–255 of matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\nfor\nbi\nwhere\ni\n<\n32\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\n+\n128\nfor\nbi\nwhere\ni\n>=\n32\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 103\n.\nFigure 103\nMMA .m16n8k256 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n,\n2\n,\n3\n}\n9.7.14.5.14.\nMultiply-and-Accumulate Instruction:\nmma\n\nmma\nPerform matrix multiply-and-accumulate operation\nSyntax\nHalf precision floating point type:\nmma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.ctype   = {.f16, .f32};\n.dtype   = {.f16, .f32};\nAlternate floating point type:\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32        d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.f32.atype.btype.f32      d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32       d, a, b, c;\nmma.sync.aligned.shape.row.col.dtype.f8type.f8type.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k32.row.col.kind.dtype.f8f6f4type.f8f6f4type.ctype d, a, b, c;\n\n.atype      = {.bf16, .tf32};\n.btype      = {.bf16, .tf32};\n.f8type     = {.e4m3, .e5m2};\n.f8f6f4type = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.ctype      = {.f16, .f32};\n.dtype      = {.f16, .f32};\n.shape      = {.m16n8k16, .m16n8k32};\n.kind       = {.kind::f8f6f4};\nAlternate floating point type with block scaling:\nmma.sync.aligned.m16n8k64.row.col.kind.block_scale{.scale_vec_size}.f32.e2m1.e2m1.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf4};\n.scale_vec_size = {.scale_vec::2X};\n.stype          = {.ue8m0};\n\nmma.sync.aligned.m16n8k64.row.col.kind.block_scale.scale_vec_size.f32.e2m1.e2m1.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf4nvf4};\n.scale_vec_size = {.scale_vec::2X, .scale_vec::4X};\n.stype          = {.ue8m0, .ue4m3};\n\nmma.sync.aligned.m16n8k32.row.col.kind.block_scale{.scale_vec_size}.f32.f8f6f4type.f8f6f4type.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf8f6f4};\n.scale_vec_size = {.scale_vec::1X};\n.f8f6f4type     = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.stype          = {.ue8m0};\nDouble precision floating point type:\nmma.sync.aligned.shape.row.col.f64.f64.f64.f64 d, a, b, c;\n\n.shape   = {.m8n84, .m16n8k4, .m16n8k8, .m16n8k16};\nInteger type:\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k16, .m16n8k16, .m16n8k32}\n.atype   = {.u8, .s8};\n.btype   = {.u8, .s8};\n\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k32, .m16n8k32, .m16n8k64}\n.atype   = {.u4, .s4};\n.btype   = {.u4, .s4};\nSingle bit:\nmma.sync.aligned.shape.row.col.s32.b1.b1.s32.bitOp.popc d, a, b, c;\n\n.bitOp = {.xor, .and}\n.shape = {.m8n8k128, .m16n8k128, .m16n8k256}\nDescription\nPerform a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+C\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the C and D matrices are\nMxN\n.\nQualifier\n.block_scale\nspecifies that the matrices A and B are scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply and accumulate operation\nas specified in the section\nBlock Scaling\n. The data type\ncorresponding to each of the element within\nscale_A\nand\nScale_B\nmatrices is specified\nby\n.stype\n. Qualifier\n.scale_vec_size\nspecifies the number of columns of\nscale_A\nmatrix\nand number of rows in the matrix\nscale_B\n.\nThe valid combinations of\n.kind\n,\n.stype\nand\n.scale_vec_size\nare described in\nTable 36\n. For\nmma\nwith\n.kind::mxf4\nwhen the\nqualifier\n.scale_vec_size\nis not specified, then it defaults to\n2X\n. In contrast, when\n.kind\nis specified as\n.kind::mxf8f6f4\nthen the qualifier\n.scale_vec_size\ndefaults\nto\n1X\n. However, for\n.kind::mxf4nvf4\n, it is mandatory to provide valid\n.scale_vec_size\n.\nA warp executing\nmma.sync.m8n8k4\ninstruction computes 4 matrix multiply and accumulate\noperations. Rest of the\nmma.sync\noperations compute a single matrix mutliply and accumulate\noperation per warp.\nFor single-bit\nmma.sync\n, multiplication is replaced by a sequence of logical operations;\nspecifically,\nmma.xor.popc\nand\nmma.and.popc\ncomputes the XOR, AND respectively of a k-bit\nrow of A with a k-bit column of B, then counts the number of set bits in the result (\npopc\n). This\nresult is added to the corresponding element of C and written into D.\nOperands\na\nand\nb\nrepresent two multiplicand matrices A and B, while\nc\nand\nd\nrepresent the accumulator and destination matrices, distributed across the threads in warp.\nWhen\n.block_scale\nqualifier is specified, operand\nscale-a-data\n,\nscale-b-data\nrepresents\nthe scale matrix metadata corresponding to\nscale_A\nand\nscale_B\nmatrices respectively. The\ntuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nrepresent selectors for matrices\nscale_A\nand\nscale_B\nrespectively from their corresponding metadata arguments\nscale-a-data\n,\nscale-b-data\n. The operands\nscale-a-data\n,\nscale-b-data\nare of type\n.b32\n. The operands\nbyte-id-a\n,\nthread-id-a\n,\nbyte-id-b\n,\nthread-id-b\nare unsigned 16-bit integer values.\nFor more details on selector arguments refer\nBlock Scaling\nsection.\nThe registers in each thread hold a fragment of matrix as described in\nMatrix multiply-accumulate operation using mma instruction\n.\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively. The qualifier\n.stype\nindicate the data-type\nof the elements in the matrices\nscale_A\nand\nscale_B\n. Specific shapes have type restrictions :\n.m8n8k4\n: When\n.ctype\nis\n.f32\n,\n.dtype\nmust also be\n.f32\n.\n.m16n8k8\n:\n.dtype\nmust be the same as\n.ctype\n.\n.atype\nmust be the same as\n.btype\n.\n.m16n8k16\nand\n.m16n8k32\n:\n.dtype\nmust be the same as\n.ctype\n.\nThe qualifiers\n.alayout\nand\n.blayout\nindicate the row-major or column-major layouts of\nmatrices A and B respectively.\nWhen\n.kind\nis either of\n.kind::mxf8f6f4\nor\n.kind::f8f6f4\n, the individual 4-bit and the\n6-bit floating point type elements must be packed in an 8-bit container. The matrix element of type\n.e2m1\nresides in central 4 bits of the 8-bit container with padding in the upper 2 bits and\nlower 2 bits of the container. When the matrix element is of type\n.e3m2\nor\n.e2m3\n, the\nmatrix element resides in the lower 6 bits of the 8-bit container with padding in the upper 2 bits\nof the container. In contrast, note that when using\nmma\nwith\n.kind::mxf4\nor\n.kind::mxf4nvf4\n, no explicit padding is necessary even though matrix elements are of type\n.e2m1\n.\nPrecision and rounding :\n.f16\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single\nprecision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values\nis performed with at least single precision. When both\n.ctype\nand\n.dtype\nare specified\nas\n.f16\n, the accumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.e4m3\n,\n.e5m2\n,\n.e3m2\n,\n.e2m3\n,\n.e2m1\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.f64\nfloating point operations :\nPrecision of the element-wise multiplication and addition operation is identical to that of\n.f64\nprecision fused multiply-add. Supported rounding modifiers are :\n.rn\n: mantissa LSB rounds to nearest even. This is the default.\n.rz\n: mantissa LSB rounds towards zero.\n.rm\n: mantissa LSB rounds towards negative infinity.\n.rp\n: mantissa LSB rounds towards positive infinity.\nInteger operations :\nThe integer\nmma\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the range\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nmma\ninstruction causes the executing thread to\nwait until all threads in the warp execute the same\nmma\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmma\ninstruction. In conditionally executed code, a\nmma\ninstruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined.\nThe behavior of\nmma\ninstruction is undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited.\nNotes\nPrograms using double precision floating point\nmma\ninstruction with shapes\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nrequire at least 64 registers for compilation.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.4.\n.f16\nfloating point type\nmma\noperation with\n.m8n8k4\nshape introduced in PTX ISA version\n6.4.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k8\nshape introduced in PTX ISA version\n6.5.\n.u8/.s8\ninteger type\nmma\noperation with\n.m8n8k16\nshape introduced in PTX ISA version\n6.5.\n.u4/.s4\ninteger type\nmma\noperation with\n.m8n8k32\nshape introduced in PTX ISA version\n6.5.\n.f64\nfloating point type\nmma\noperation with\n.m8n8k4\nshape introduced in PTX ISA version\n7.0.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k16\nshape introduced in PTX ISA\nversion 7.0.\n.bf16\nalternate floating point type\nmma\noperation with\n.m16n8k8\nand\n.m16n8k16\nshapes\nintroduced in PTX ISA version 7.0.\n.tf32\nalternate floating point type\nmma\noperation with\n.m16n8k4\nand\n.m16n8k8\nshapes\nintroduced in PTX ISA version 7.0.\n.u8/.s8\ninteger type\nmma\noperation with\n.m16n8k16\nand\n.m16n8k32\nshapes introduced in\nPTX ISA version 7.0.\n.u4/.s4\ninteger type\nmma\noperation with\n.m16n8k32\nand\n.m16n8k64\nshapes introduced in\nPTX ISA version 7.0.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m8n8k128\n,\n.m16n8k128\nand\n.m16n8k256\nshapes introduced in PTX ISA version 7.0.\nSupport for\n.and\noperation in single-bit\nmma\nintroduced in PTX ISA version 7.1.\n.f64\nfloating point type\nmma\noperation with\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nshapes introduced in PTX ISA version 7.8.\nSupport for\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation introduced in\nPTX ISA version 8.4.\nSupport for shape\n.m16n8k16\nand\n.f16\ndtype\n/\nctype\nwith\n.e4m3\n/\n.e5m2\nalternate\nfloating point type mma operation introduced in PTX ISA version 8.7.\nSupport for\n.e3m2\n,\n.e2m3\n,\n.e2m1\nalternate floating point type\nmma\noperation introduced\nin PTX ISA version 8.7.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier introduced in PTX ISA version 8.7.\nSupport for\n.scale_vec::4X\non\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nis introduced in\nPTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_70\nor higher.\n.f16\nfloating point type\nmma\noperation with\n.m8n8k4\nshape requires\nsm_70\nor higher.\nNote\nmma.sync.m8n8k4\nis optimized for target architecture\nsm_70\nand may have substantially\nreduced performance on other target architectures.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k8\nshape requires\nsm_75\nor higher.\n.u8/.s8\ninteger type\nmma\noperation with\n.m8n8k16\nshape requires\nsm_75\nor higher.\n.u4/.s4\ninteger type\nmma\noperation with\n.m8n8k32\nshape\nsm_75\nor higher.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m8n8k128\nshape\nsm_75\nor higher.\n.f64\nfloating point type\nmma\noperation with\n.m8n8k4\nshape requires\nsm_80\nor higher.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k16\nshape requires\nsm_80\nor\nhigher.\n.bf16\nalternate floating point type\nmma\noperation with\n.m16n8k8\nand\n.m16n8k16\nshapes\nrequires\nsm_80\nor higher.\n.tf32\nalternate floating point type\nmma\noperation with\n.m16n8k4\nand\n.m16n8k8\nshapes\nrequires\nsm_80\nor higher.\n.u8/.s8\ninteger type\nmma\noperation with\n.m16n8k16\nand\n.m16n8k32\nshapes requires\nsm_80\nor higher.\n.u4/.s4\ninteger type\nmma\noperation with\n.m16n8k32\nand\n.m16n8k64\nshapes requires\nsm_80\nor higher.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m16n8k128\nand\n.m16n8k256\nshapes\nrequires\nsm_80\nor higher.\n.and\noperation in single-bit\nmma\nrequires\nsm_80\nor higher.\n.f64\nfloating point type\nmma\noperation with\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nshapes require\nsm_90\nor higher.\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation requires\nsm_89\nor higher.\n.e3m2\n,\n.e2m3\nand\n.e2m1\nalternate floating point type\nmma\noperation requires\nsm_120a\nand is supported on\nsm_120f\nfrom PTX ISA version 8.8.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier requires\nsm_120a\nand are\nsupported on\nsm_120f\nor higher in the same family from PTX ISA version 8.8.\nExamples of half precision floating point type\n// f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4> %Rd<4>\nmma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n\n// f16 elements in C and f32 elements in D\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4>\n.reg .f32 %Rd<8>\nmma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n // f32 elements in C and D\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<4>, %Rb<2>, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .f16 %Ra<4>, %Rb<2>;\n.reg .f32 %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of alternate floating point type\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Rb2, %Rb3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f32.e4m3.e5m2.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f16.e4m3.e5m2.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f16.e5m2.e5m2.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.kind::f8f6f4.f32.e3m2.e2m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.kind::f8f6f4.f16.e2m3.e2m1.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\nExamples of integer type\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\n\n// s8 elements in A and u8 elements in B\nmma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// u4 elements in A and B matrix\nmma.sync.aligned.m8n8k32.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// s8 elements in A and u8 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u4 elements in A and s4 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.u4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// s8 elements in A and s8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u8 elements in A and u8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k64.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1 },\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of single bit type\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of\n.f64\nfloating point type\n.reg .f64 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .f64 %Ra<8>, %Rb<4>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k8.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k16.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3, %Ra4, %Ra5, %Ra6, %Ra7},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of\nmma\nwith block scale\n.reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n mma.sync.aligned.m16n8k64.row.col.kind::mxf4.block_scale.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {2, 1}, scaleBData, {2, 3};\n\n .reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .u16 bidA, bidB, tidA, tidB;\n mma.sync.aligned.m16n8k64.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue4m3\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .u16 bidA, bidB, tidA, tidB;\nmma.sync.aligned.m16n8k64.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\nmma.sync.aligned.m16n8k32.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e3m2.e2m1.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  scaleAData, {0, 1}, scaleBData, {0, 1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\nmma.sync.aligned.m16n8k32.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e4m3.e5m2.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2,  %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  scaleAData, {0, 1}, scaleBData, {0, 0};\n9.7.14.5.15.\nWarp-level matrix load instruction:\nldmatrix\n\nldmatrix\nCollectively load one or more matrices from shared memory for\nmma\ninstruction\nSyntax\nldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p];\n\nldmatrix.sync.aligned.m8n16.num{.ss}.dst_fmt.src_fmt        r, [p];\nldmatrix.sync.aligned.m16n16.num.trans{.ss}.dst_fmt.src_fmt r, [p];\n\n.shape   = {.m8n8, .m16n16};\n.num     = {.x1, .x2, .x4};\n.ss      = {.shared{::cta}};\n.type    = {.b16, .b8};\n.dst_fmt = { .b8x16 };\n.src_fmt = { .b6x16_p32, .b4x16_p64 };\nDescription\nCollectively load one or more matrices across all threads in a warp from the location indicated by\nthe address operand\np\n, from\n.shared\nstate space into destination register\nr\n. If no state\nspace is provided, generic addressing is used, such that the address in\np\npoints into\n.shared\nspace. If the generic address doesn’t fall in\n.shared\nstate space, then the behavior\nis undefined.\nThe\n.shape\nqualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit or 8-bit or 6-bit or 4-bit data.\nFollowing table shows the matrix load case for each\n.shape\n.\n.shape\nMatrix shape\nElement size\n.m8n8\n8x8\n16-bit\n.m16n16\n16x16\n8-bit or 6-bit or 4-bit\n.m8n16\n8x16\n6-bit or 4-bit\nFollowing table shows the valid use of 6-bit or 4-bit data load.\n.src_fmt\n.shape\nSource data\nPadding\n.dst_fmt\n.b6x16_p32\n.m8n16\n16 6-bit elements\n32 bits\n.b8x16\n(16 8-bit\nelements)\n.m16n16\n.b4x16_p64\n.m8n16\n16 4-bit elements\n64 bits\n.m16n16\nFor\n.b6x16_p32\nformat source data is 16 unsigned 6-bit elements with 32 bits padding.\nFor\n.b4x16_p64\nformat source data is 16 unsigned 4-bit elements with 64 bits padding.\nThe values\n.x1\n,\n.x2\nand\n.x4\nfor\n.num\nindicate one, two or four matrices\nrespectively. When\n.shape\nis\n.m16n16\n, only\n.x1\nand\n.x2\nare valid values for\n.num\n.\nThe mandatory\n.sync\nqualifier indicates that\nldmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nldmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nldmatrix\ninstruction. In conditionally executed code, an\nldmatrix\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined.\nThe behavior of\nldmatrix\nis undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited.\nThe destination operand\nr\nis a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of\n.num\n. Each component of the vector expression holds a fragment\nfrom the corresponding matrix.\nSupported addressing modes for\np\nare described in\nAddresses as Operands\n.\nConsecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of\n.num\nas shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on.\n.num\nThreads 0–7\nThreads 8–15\nThreads 16–23\nThreads 24–31\n.x1\naddr0–addr7\n–\n–\n–\n.x2\naddr0–addr7\naddr8–addr15\n–\n–\n.x4\naddr0–addr7\naddr8–addr15\naddr16–addr23\naddr24–addr31\nNote\nFor .target\nsm_75\nor below, all threads must contain valid addresses. Otherwise, the behavior\nis undefined. For\n.num\n=\n.x1\nand\n.num\n=\n.x2\n, addresses contained in lower threads can be\ncopied to higher threads to achieve the expected behavior.\nWhen reading 8x8 matrices, a group of four consecutive threads loads 16 bytes. The matrix addresses\nmust be naturally aligned accordingly.\nEach thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its\nregister\nr\n, and so on. A group of four threads loads an entire row of the matrix as shown in\nFigure 104\n.\nFigure 104\nldmatrix fragment layout for one 8x8 Matrix with 16-bit elements\n\nWhen\n.num\n=\n.x2\n, the elements of the second matrix are loaded in the next destination\nregister in each thread as per the layout in above table. Similarly, when\n.num\n=\n.x4\n,\nelements of the third and fourth matrices are loaded in the subsequent destination registers in each\nthread.\nFor matrix shape 16x16, two destination registers\nr0\nand\nr1\nof type\n.b32\nmust be\nspecified and in each register four 8-bit elements are loaded. For 4-bit or 6-bit data, 8-bit\nelement will have 4 bits or 2 bits of padding respectively.\nRefer\nOptional Decompression\nfor more details\non these formats.\nAn entire row of the matrix can be loaded by a group of four consecutive and aligned threads.\nEach thread in a warp loads 4 consecutive columns across 2 rows as shown in the\nFigure 105\n.\nFigure 105\nldmatrix fragment layout for one 16x16 matrix with 8-bit elements\n\nFor matrix shape 8x16, one destination register\nr0\nof type\n.b32\nmust be specified where four\n8-bit elements are loaded in the register. For 4-bit or 6-bit data, 8-bit element will have 4 bits\nor 2 bits of padding respectively.\nAn entire row of the matrix can be loaded by a group of four consecutive and aligned threads.\nEach thread in a warp loads 4 consecutive columns as shown in\nFigure 106\n.\nFigure 106\nldmatrix fragment layout for one 8x16 matrix with 8-bit elements containing 4-bit/6-bit data\n\nOptional qualifier\n.trans\nindicates that the matrix is loaded in column-major format. However,\nfor 16x16 matrices,\n.trans\nis mandatory.\nThe\nldmatrix\ninstruction is treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nSupport for\n.m16n16\n,\n.m8n16\nshapes introduced in PTX ISA version 8.6.\nSupport for\n.b8\ntype with\nldmatrix\nis introduced in PTX ISA version 8.6.\nSupport for\n.src_fmt\n,\n.dst_fmt\nqualifiers introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nShapes\n.m16n16\n,\n.m8n16\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nType\n.b8\nwith\nldmatrix\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nQualifiers\n.src_fmt\n,\n.dst_fmt\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Load a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 d;\nldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr];\n\n// Load two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr];\n\n// Load four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr];\n\n// Load one 16x16 matrices of 64-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m16n16.x1.trans.shared.b8 {d0, d1}, [addr];\n\n// Load two 16x16 matrices of 64-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m16n16.x2.trans.shared::cta.b8 {d0, d1, d2, d3}, [addr];\n\n// Load two 16x16 matrices of 6-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m16n16.x2.trans.shared::cta.b8x16.b6x16_p32 {d0, d1, d2, d3}, [addr];\n9.7.14.5.16.\nWarp-level matrix store instruction:\nstmatrix\n\nstmatrix\nCollectively store one or more matrices to shared memory.\nSyntax\nstmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r;\n\n.shape  = {.m8n8, .m16n8};\n.num    = {.x1, .x2, .x4};\n.ss     = {.shared{::cta}};\n.type   = {.b16, .b8};\nDescription\nCollectively store one or more matrices across all threads in a warp to the location indicated by\nthe address operand\np\n, in\n.shared\nstate space. If no state space is provided, generic\naddressing is used, such that the address in\np\npoints into\n.shared\nspace. If the generic\naddress doesn’t fall in\n.shared\nstate space, then the behavior is undefined.\nThe\n.shape\nqualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit or 8-bit data as indicated by the\n.type\nqualifier.\n.m16n8\nshape is valid only for\n.b8\ntype.\nThe values\n.x1\n,\n.x2\nand\n.x4\nfor\n.num\nindicate one, two or four matrices\nrespectively.\nThe mandatory\n.sync\nqualifier indicates that\nstmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nstmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nstmatrix\ninstruction. In conditionally executed code, an\nstmatrix\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined.\nThe behavior of\nstmatrix\nis undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited.\nThe source operand\nr\nis a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of\n.num\n. Each component of the vector expression holds a fragment\nfrom the corresponding matrix.\nSupported addressing modes for\np\nare described in\nAddresses as Operands\n.\nConsecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of\n.num\nas shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on.\n.num\nThreads 0–7\nThreads 8–15\nThreads 16–23\nThreads 24–31\n.x1\naddr0–addr7\n–\n–\n–\n.x2\naddr0–addr7\naddr8–addr15\n–\n–\n.x4\naddr0–addr7\naddr8–addr15\naddr16–addr23\naddr24–addr31\nWhen storing 8x8 matrices, a group of four consecutive threads stores 16 bytes. The matrix addresses\nmust be naturally aligned accordingly.\nEach thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its\nregister\nr\n, and so on. A group of four threads stores an entire row of the matrix as shown in\nFigure 107\n.\nFigure 107\nstmatrix fragment layout for one 8x8 matrix with 16-bit elements\n\nWhen\n.num\n=\n.x2\n, the elements of the second matrix are storedd from the next source register\nin each thread as per the layout in above table. Similarly, when\n.num\n=\n.x4\n, elements of the\nthird and fourth matrices are stored from the subsequent source registers in each thread.\nFor 16x8 matrix shape, each of the 32 threads in the warp provides four elements of data per matrix.\nEach element in the source operand\nr\nis of type\n.b32\nand contains four 8 bit elements\ne0\n,\ne1\n,\ne2\n,\ne3\nwith\ne0\nand\ne3\ncontaining the LSB and MSB respectively of register\nr\n.\nFigure 108\nstmatrix fragment layout for one 16x8 matrix with 8 bit elements\n\nOptional qualifier\n.trans\nindicates that the matrix is stored in column-major format. However,\nfor 16x8 matrices,\n.trans\nis mandatory.\nThe\nstmatrix\ninstruction is treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nSupport for\n.m16n8\nshape is introduced in PTX ISA version 8.6.\nSupport for\n.b8\ntype with\nstmatrix\nis introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nShape\n.m16n8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nType\n.b8\nwith\nstmatrix\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Store a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r};\n\n// Store two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1};\n\n// Store four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3};\n\n// Store a single 16x8 matrix using generic addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m16n8.x1.trans.shared.b8 [addr], {r};\n\n// Store two 16x8 matrices\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m16n8.x2.trans.shared::cta.b8 [addr],{r0, r1};\n\n// Store four 16x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m16n8.x4.b8 [addr], {r0, r1, r2, r3};\n9.7.14.5.17.\nWarp-level matrix transpose instruction:\nmovmatrix\n\nmovmatrix\nTranspose a matrix in registers across the warp.\nSyntax\nmovmatrix.sync.aligned.shape.trans.type d, a;\n\n.shape  = {.m8n8};\n.type   = {.b16};\nDescription\nMove a row-major matrix across all threads in a warp, reading elements from source\na\n, and\nwriting the transposed elements to destination\nd\n.\nThe\n.shape\nqualifier indicates the dimensions of the matrix being transposed. Each matrix\nelement holds 16-bit data as indicated by the\n.type\nqualifier.\nThe mandatory\n.sync\nqualifier indicates that\nmovmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nmovmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmovmatrix\ninstruction. In conditionally executed code, a\nmovmatrix\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nthe behavior is undefined.\nOperands\na\nand\nd\nare 32-bit registers containing fragments of the input matrix and the\nresulting matrix respectively. The mandatory qualifier\n.trans\nindicates that the resulting\nmatrix in\nd\nis a transpose of the input matrix specified by\na\n.\nEach thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first\nfragment in register\na\n, and so on. A group of four threads holds an entire row of the input\nmatrix as shown in\nFigure 109\n.\nFigure 109\nmovmatrix source matrix fragment layout\n\nEach thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the\nfirst fragment in register\nd\n, and so on. A group of four threads holds an entire column of the\nresult matrix as shown in\nFigure 110\n.\nFigure 110\nmovmatrix result matrix fragment layout\n\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\n.reg .b32 d, a;\nmovmatrix.sync.aligned.m8n8.trans.b16 d, a;\n9.7.14.6.\nMatrix multiply-accumulate operation using\nmma.sp\ninstruction with sparse matrix A\n\nThis section describes warp-level\nmma.sp{::ordered_metadata}\ninstruction with sparse matrix A.\nThis variant of the\nmma\noperation can be used when A is a structured sparse matrix with 50%\nzeros in each row distributed in a shape-specific granularity. For an\nMxNxK\nsparse\nmma.sp{::ordered_metadata}\noperation, the\nMxK\nmatrix A is packed into\nMxK/2\nelements.\nFor each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements\nare packed in the operand representing matrix A. The mapping of these K/2 elements to the\ncorresponding K-wide row is provided explicitly as metadata.\n9.7.14.6.1.\nSparse matrix storage\n\nGranularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a\n16x16\nmatrix A, sparsity is expected to be at 2:4\ngranularity, i.e. each 4-element vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row\ncontains 2 zeros. Index of each non-zero element in a sub-chunk is stored in the metadata\noperand. Values\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nare invalid values for metadata and\nwill result in undefined behavior. In a group of four consecutive threads, one or more threads store\nthe metadata for the whole group depending upon the matrix shape. These threads are specified using\nan additional\nsparsity selector\noperand.\nFigure 111\nshows an example of a 16x16 matrix A represented in sparse format and sparsity\nselector indicating which thread in a group of four consecutive threads stores the metadata.\nFigure 111\nSparse MMA storage example\n\nGranularities for different matrix shapes and data types are described below.\nSparse\nmma.sp{::ordered_metadata}\nwith half-precision and\n.bf16\ntype\nFor the\n.m16n8k16\nand\n.m16n8k32\nmma.sp{::ordered_metadata}\noperations, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements\nin a row of matrix A has two zeros and two non-zero elements. Only the two non-zero elements are\nstored in the operand representing matrix A and their positions in the four-wide chunk in matrix\nA are indicated by two 2-bit indices in the metadata operand. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values\nof indices; any other values result in an undefined behavior.\nFigure 112\nSparse MMA metadata example for\n.f16\n/\n.bf16\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k16\n: One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}.\nm16n8k32\n: A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior.\nSparse\nmma.sp{::ordered_metadata}\nwith\n.tf32\ntype\nWhen matrix A has\n.tf32\nelements, matrix A is structured sparse at a granularity of 1:2. In\nother words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero\nelement. Only the non-zero elements are stored in the operand for matrix A and their positions in a\ntwo-wide chunk in matrix A are indicated by the 4-bit index in the metadata.\n0b1110\nand\n0b0100\nare the only meaningful index values; any other values result in an undefined behavior.\nFigure 113\nSparse MMA metadata example for\n.tf32\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k8\n: One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}.\nm16n8k16\n: A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior.\nSparse\nmma.sp{::ordered_metadata}\nwith integer type\nWhen matrices A and B have\n.u8\n/\n.s8\nelements, matrix A is structured sparse at a granularity\nof 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes\nand two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their\npositions in the four-wide chunk are indicated by two 2-bit indices in the metadata. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 114\nSparse MMA metadata example for\n.u8\n/\n.s8\ntype.\n\nwhen matrices A and B have\n.u4\n/\n.s4\nelements, matrix A is pair-wise structured sparse at a\ngranularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has\nfour zeroes and four non-zero values. Further, the zero and non-zero values are clustered in\nsub-chunks of two elements each within the eight-wide chunk. i.e., each two-wide sub-chunk within\nthe eight-wide chunk must be all zeroes or all non-zeros. Only the four non-zero values are stored\nin sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the\neight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 115\nSparse MMA metadata example for\n.u4\n/\n.s4\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k32\nwith\n.u8\n/\n.s8\ntype and\nm16n8k64\nwith\n.u4\n/\n.s4\ntype: A thread-pair\nwithin a group of four consecutive threads contributes the sparsity metadata. Hence, the sparsity\nselector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an\nundefined behavior.\nm16n8k64\nwith\n.u8\n/\n.s8\ntype and\nm16n8k128\nwith\n.u4\n/\n.s4\ntype: All threads\nwithin a group of four consecutive threads contribute the sparsity metadata. Hence, the sparsity\nselector in this case must be 0. Any other value of sparsity selector results in an undefined\nbehavior.\nSparse\nmma.sp{::ordered_metadata}\noperating on\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype with\n.kind::f8f6f4\nor\n.kind::mxf8f6f4\nWhen matrices A and B have\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a\nrow of matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored\nin sparse matrix and their positions in the four-wide chunk are indicated by two 2-bit indices in the\nmetadata.\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful\nvalues of indices; any other values result in an undefined behavior.\nFigure 116\nSparse MMA metadata example for\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k64\n: All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior.\nSparse\nmma.sp::ordered_metadata\noperating on\n.e2m1\ntype with\n.kind::mxf4\nor\n.kind::mxf4nvf4\nWhen matrices A and B have\n.e2m1\nelements, matrix A is pair-wise structured sparse at a granularity\nof 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has four zeroes and\nfour non-zero values. Further, the zero and non-zero values are clustered in sub-chunks of two elements\neach within the eight-wide chunk. i.e., each two-wide sub-chunk within the eight-wide chunk must be all\nzeroes or all non-zeros. Only the four non-zero values are stored in sparse matrix and the positions of\nthe two two-wide sub-chunks with non-zero values in the eight-wide chunk of a row of matrix A are\nindicated by two 2-bit indices in the metadata.\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 117\nSparse MMA metadata example for\n.e2m1\ntype with\n.kind::mxf4\nor\n.kind::mxf4nvf4\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k128\n: All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior.\n9.7.14.6.2.\nMatrix fragments for multiply-accumulate operation with sparse matrix A\n\nIn this section we describe how the contents of thread registers are associated with fragments of\nvarious matrices and the sparsity metadata. The following conventions are used throughout this\nsection:\nFor matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data.\nFor matrix B, when the combination of matrix dimension and the supported data type is not already\ncovered in\nMatrix multiply-accumulate operation using mma instruction\n, a pictorial representation of matrix\nfragments is provided.\nFor matrices C and D, since the matrix dimension - data type combination is the same for all\nsupported shapes, and is already covered in\nMatrix multiply-accumulate operation using mma instruction\n, the pictorial representations\nof matrix fragments are not included in this section.\nFor the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included.\nTk:\n[m..n]\npresent\nin cell\n[x][y..z]\nindicates that bits\nm\nthrough\nn\n(with\nm\nbeing higher) in the\nmetadata operand of thread with\n%laneid=k\ncontains the indices of the non-zero elements from\nthe chunk\n[x][y]..[x][z]\nof matrix A.\n9.7.14.6.2.1.\nMatrix Fragments for sparse\nmma.m16n8k16\nwith\n.f16\nand\n.bf16\ntypes\n\nA warp executing sparse\nmma.m16n8k16\nwith\n.f16\n/\n.bf16\nfloating point type will compute\nan MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.f16\n/\n.bf16\nA vector expression containing two\n.b32\nregisters,\nwith each register containing two non-zero\n.f16\n/\n.bf16\nelements out of 4 consecutive elements from\nmatrix A.\nMapping of the non-zero elements is as\ndescribed in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 118\n.\nFigure 118\nSparse MMA .m16n8k16 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na1\ngroupID\n+\n8\nfor\na2\nand\na3\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n4\nlastcol\n=\nfirstcol\n+\n3\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\nfor\n.f16\n/\n.b16\nformats.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors each storing the index of a non-zero\nelement of a 4-wide chunk of matrix A as shown in\nFigure 119\n.\nFigure 119\nSparse MMA .m16n8k16 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.14.6.2.2.\nMatrix Fragments for sparse\nmma.m16n8k32\nwith\n.f16\nand\n.bf16\ntypes\n\nA warp executing sparse\nmma.m16n8k32\nwith\n.f16\n/\n.bf16\nfloating point type will compute\nan MMA operation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters,\nwith each register containing two non-zero\n.f16\n/\n.bf16\nelements out of 4 consecutive elements from\nmatrix A.\nMapping of the non-zero elements is as\ndescribed in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 120\n.\nFigure 120\nSparse MMA .m16n8k32 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n2\n||\n4\n<=\ni\n<\n6\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n4\nFor\nai\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n4\n)\n+\n16\nfor\nai\nwhere\ni\n>=\n4\nlastcol\n=\nfirstcol\n+\n3\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters, each\ncontaining two\n.f16\n/\n.bf16\nelements from matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 121\n.\nFigure 121\nSparse MMA .m16n8k32 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\nfor\n.f16\n/\n.b16\nformats.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero element from a 4-wide chunk of matrix A as shown in\nFigure 122\n.\nFigure 122\nSparse MMA .m16n8k32 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.14.6.2.3.\nMatrix Fragments for sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type\n\nA warp executing sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type will compute an MMA\noperation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.tf32\nA vector expression containing four\n.b32\nregisters, with each\nregister containing one non-zero\n.tf32\nelement out of 2\nconsecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 123\n.\nFigure 123\nSparse MMA .m16n8k16 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n2\nfor\na0\nand\na1\n(\nthreadID_in_group\n*\n2\n)\n+\n8\nfor\na2\nand\na3\nlastcol\n=\nfirstcol\n+\n1\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters, each\ncontaining four\n.tf32\nelements from matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 124\n.\nFigure 124\nSparse MMA .m16n8k16 fragment layout for matrix B with\n.tf32\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\n.\nMetadata: A\n.b32\nregister containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in\nFigure 125\n.\nFigure 125\nSparse MMA .m16n8k16 metadata layout for\n.tf32\ntype.\n\n9.7.14.6.2.4.\nMatrix Fragments for sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type\n\nA warp executing sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type will compute an MMA\noperation of shape\n.m16n8k8\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.tf32\nA vector expression containing two\n.b32\nregisters, each\ncontaining one non-zero\n.tf32\nelement out of 2\nconsecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 126\n.\nFigure 126\nSparse MMA .m16n8k8 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n2\nlastcol\n=\nfirstcol\n+\n1\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k8\nfor\n.tf32\nformat.\nMetadata: A\n.b32\nregister containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in\nFigure 127\n.\nFigure 127\nSparse MMA .m16n8k8 metadata layout for\n.tf32\ntype.\n\n9.7.14.6.2.5.\nMatrix Fragments for sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type\n\nA warp executing sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type will compute an MMA\noperation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u8\n/\n.s8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four non-zero\n.u8\n/\n.s8\nelements out\nof 8 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 128\n.\nFigure 128\nSparse MMA .m16n8k32 fragment layout for matrix A with\n.u8\n/\n.s8\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n8\nlastcol\n=\nfirstcol\n+\n7\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k32\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in\nFigure 129\n.\nFigure 129\nSparse MMA .m16n8k32 metadata layout for\n.u8\n/\n.s8\ntype.\n\n9.7.14.6.2.6.\nMatrix Fragments for sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\ntype\n\nA warp executing sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype will compute an MMA operation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u8\n/\n.s8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four non-zero\n.u8\n/\n.s8\nelements out\nof 8 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four non-zero\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements out of 8 consecutive\nelements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 130\nand\nFigure 131\n.\nFigure 130\nSparse MMA .m16n8k64 fragment layout for columns 0–31 of matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 131\nSparse MMA .m16n8k64 fragment layout for columns 32–63 of matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\n||\n8\n<=\ni\n<\n12\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n8\nFor\nai\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n8\n)\n+\n32\nFor\nai\nwhere\ni\n>=\n8\nlastcol\n=\nfirstcol\n+\n7\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing four\n.b32\nregisters,\neach containing four\n.u8\n/\n.s8\nelements from\nmatrix B.\nb0, b1, b2, b3, …, b15\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters,\neach containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from matrix B.\nThe layout of the fragments held by different threads is shown in\nFigure 132\n,\nFigure 133\n,\nFigure 134\nand\nFigure 135\n.\nFigure 132\nSparse MMA .m16n8k64 fragment layout for rows 0–15 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 133\nSparse MMA .m16n8k64 fragment layout for rows 16–31 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 134\nSparse MMA .m16n8k64 fragment layout for rows 32–47 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 135\nSparse MMA .m16n8k64 fragment layout for rows 48–63 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with integer type\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in\nFigure 136\nand\nFigure 137\n.\nFigure 136\nSparse MMA .m16n8k64 metadata layout for columns 0–31 for\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 137\nSparse MMA .m16n8k64 metadata layout for columns 32–63 for\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\n9.7.14.6.2.7.\nMatrix Fragments for sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type\n\nA warp executing sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type will compute an MMA\noperation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u4\n/\n.s4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight non-zero\n.u4\n/\n.s4\nelements\nout of 16 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 138\n.\nFigure 138\nSparse MMA .m16n8k64 fragment layout for matrix A with\n.u4\n/\n.s4\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n16\nlastcol\n=\nfirstcol\n+\n15\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k64\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in\nFigure 139\n.\nFigure 139\nSparse MMA .m16n8k64 metadata layout for\n.u4\n/\n.s4\ntype.\n\n9.7.14.6.2.8.\nMatrix Fragments for sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\ninteger type\n\nA warp executing sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\n/\n.e2m1\ninteger type will compute an MMA\noperation of shape\n.m16n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u4\n/\n.s4\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight non-zero\n.u4\n/\n.s4\nelements out\nof 16 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight non-zero\n.e2m1\nelements out\nof 16 consecutive elements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 140\nand\nFigure 141\n.\nFigure 140\nSparse MMA .m16n8k128 fragment layout for columns 0–63 of matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 141\nSparse MMA .m16n8k128 fragment layout for columns 64–127 of matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\n||\n16\n<=\ni\n<\n24\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n16\nFor\nai\nwhere\ni\n<\n16\n(\nthreadID_in_group\n*\n16\n)\n+\n64\nFor\nai\nwhere\ni\n>=\n16\nlastcol\n=\nfirstcol\n+\n15\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.u4\n/\n.s4\nA vector expression containing four\n.b32\nregisters, each containing\neight\n.u4\n/\n.s4\nelements from matrix B.\nb0, b1, b2, b3, …, b31\n.e2m1\nA vector expression containing four\n.b32\nregisters, each containing\neight\n.e2m1\nelements from matrix B.\nThe layout of the fragments held by different threads is shown in\nFigure 142\n,\nFigure 143\n,\nFigure 144\n,\nFigure 145\n.\nFigure 142\nSparse MMA .m16n8k128 fragment layout for rows 0–31 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 143\nSparse MMA .m16n8k128 fragment layout for rows 32–63 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 144\nSparse MMA .m16n8k128 fragment layout for rows 64–95 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 145\nSparse MMA .m16n8k128 fragment layout for rows 96–127 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k64\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in\nFigure 146\nand\nFigure 147\n.\nFigure 146\nSparse MMA .m16n8k128 metadata layout for  columns 0–63 for\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 147\nSparse MMA .m16n8k128 metadata layout for  columns 64–127 for\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\n9.7.14.6.3.\nMultiply-and-Accumulate Instruction:\nmma.sp\n/\nmma.sp::ordered_metadata\n\nmma.sp\n,\nmma.sp::ordered_metadata\nPerform matrix multiply-and-accumulate operation with sparse matrix A\nSyntax\nHalf precision floating point type:\nmma.spvariant.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\n\n.ctype     = {.f16, .f32};\n.dtype     = {.f16, .f32};\n.spvariant = {.sp, .sp::ordered_metadata};\nAlternate floating point type:\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32      d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.tf32.tf32.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k64.row.col.f32.f8type.f8type.f32 d, a, b, c, e, f;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind.dtype.f8f6f4type.f8f6f4type.ctype d, a, b, c, e, f;\n\n.f8type     = {.e4m3, .e5m2};\n.spvariant  = {.sp, .sp::ordered_metadata};\n.f8f6f4type = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.kind       = {kind::f8f6f4};\n.ctype      = {.f16, .f32};\n.dtype      = {.f16, .f32};\nAlternate floating point type with block scaling:\nmma.spvariant.sync.aligned.m16n8k128.row.col.kind.block_scale{.scale_vec_size}.f32.e2m1.e2m1.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf4};\n.scale_vec_size = {.scale_vec::2X};\n.stype          = {.ue8m0};\n\nmma.spvariant.sync.aligned.m16n8k128.row.col.kind.block_scale.scale_vec_size.f32.e2m1.e2m1.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf4nvf4};\n.scale_vec_size = {.scale_vec::2X, .scale_vec::4X};\n.stype          = {.ue8m0, .ue4m3};\n\nmma.spvariant.sync.aligned.m16n8k64.row.col.kind.block_scale{.scale_vec_size}.f32.f8f6f4type.f8f6f4type.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf8f6f4};\n.scale_vec_size = {.scale_vec::1X};\n.f8f6f4type     = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.stype          = {.ue8m0};\nInteger type:\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k32, .m16n8k64}\n.atype     = {.u8, .s8};\n.btype     = {.u8, .s8};\n.spvariant = {.sp, .sp::ordered_metadata};\n\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k64, .m16n8k128}\n.atype     = {.u4, .s4};\n.btype     = {.u4, .s4};\n.spvariant = {.sp, .sp::ordered_metadata};\nDescription\nPerform a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+C\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the C and D matrices are\nMxN\n.\nA warp executing\nmma.sp.sync/mma.sp::ordered_metadata.sync\ninstruction compute a single matrix\nmultiply and accumulate operation.\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply and accumulate operation\nas specified in the section\nBlock Scaling\n. The data type corresponding\nto each of the element within\nscale_A\nand\nscale_B\nmatrices is specified by\n.stype\n.\nQualifier\n.scale_vec_size\nspecifies the number of columns of\nscale_A\nmatrix and number of\nrows in the matrix\nscale_B\n.\nThe valid combinations of\n.kind\n,\n.stype\nand\n.scale_vec_size\nare described in\nTable 36\n. For\nmma\nwith\n.kind::mxf4\nwhen the\nqualifier\n.scale_vec_size\nis not specified, then it defaults to\n2X\n. In contrast,\nwhen\n.kind\nis specified as\n.kind::mxf8f6f4\nthen the qualifier\n.scale_vec_size\ndefaults to\n1X\n. However, for\n.kind::mxf4nvf4\n, it is mandatory to provide valid\n.scale_vec_size\n.\nOperands\na\nand\nb\nrepresent two multiplicand matrices A and B, while\nc\nand\nd\nrepresent the accumulator and destination matrices, distributed across the threads in warp. Matrix A\nis structured sparse as described in\nSparse matrix storage\nOperands\ne\nand\nf\nrepresent sparsity\nmetadata and sparsity selector respectively. Operand\ne\nis a 32-bit integer and operand\nf\nis\na 32-bit integer constant with values in the range 0..3.\nWhen\n.block_scale\nqualifier is specified, operand\nscale-a-data\n,\nscale-b-data\nrepresents\nthe scale matrix metadata corresponding to\nscale_A\nand\nscale_B\nmatrices respectively.\nThe tuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nrepresent selectors for\nmatrices\nscale_A\nand\nscale_B\nrespectively from their corresponding metadata arguments\nscale-a-data\n,\nscale-b-data\n. The operands\nscale-a-data\n,\nscale-b-data\nare of type\n.b32\n. The operands\nbyte-id-a\n,\nthread-id-a\n,\nbyte-id-b\n,\nthread-id-b\nare unsigned\n16-bit integer values. For more details on selector arguments refer\nBlock Scaling for mma.sync\nsection.\nInstruction\nmma.sp::ordered_metadata\nrequires the indices in the sparsity metadata to be sorted\nin an increasing order starting from LSB, otherwise behavior is undefined.\nThe registers in each thread hold a fragment of matrix as described in\nMatrix fragments for multiply-accumulate operation with sparse matrix A\n.\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively. The qualifier\n.stype\nindicate the\ndata-type of the elements in the matrices\nscale_A\nand\nscale_B\n. In case of shapes\n.m16n8k16\n,\n.m16n8k32\nand\n.m16n8k64\n,\n.dtype\nmust be the same as\n.ctype\n.\nWhen\n.kind\nis either of\n.kind::mxf8f6f4\nor\n.kind::f8f6f4\n, the individual 4-bit and\nthe 6-bit floating point type elements must be packed in an 8-bit container. The matrix element\nof type\n.e2m1\nresides in central 4 bits of the 8-bit container with padding in the upper 2\nbits and lower 2 bits of the container. When the matrix element is of type\n.e3m2\nor\n.e2m3\n,\nthe matrix element resides in the lower 6 bits of the 8-bit container with padding in the upper\n2 bits of the container. In contrast, note that when using\nmma\nwith\n.kind::mxf4\nor\n.kind::mxf4nvf4\n, no explicit padding is necessary even though matrix elements are of type\n.e2m1\n.\nPrecision and rounding :\n.f16\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with at least single\nprecision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values\nis performed with at least single precision. When both\n.ctype\nand\n.dtype\nare specified\nas\n.f16\n, the accumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.e4m3\n,\n.e5m2\n,\n.e3m2\n,\n.e2m3\n,\n.e2m1\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations :\nThe integer\nmma.sp/mma.sp::ordered_metadata\noperation is performed with\n.s32\naccumulators.\nThe\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the range\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nmma.sp/mma.sp::ordered_metadata\ninstruction causes\nthe executing thread to wait until all threads in the warp execute the same\nmma.sp/mma.sp::ordered_metadata\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmma.sp/mma.sp::ordered_metadata\ninstruction. In conditionally executed code, a\nmma.sp/mma.sp::ordered_metadata\ninstruction should only be used if it is known that all threads in the warp evaluate the condition identically,\notherwise behavior is undefined.\nThe behavior of\nmma.sp/mma.sp::ordered_metadata\ninstruction is undefined if all threads in the same warp\ndo not use the same qualifiers, or if any thread in the warp has exited.\nNotes\nmma.sp\ninstruction may have substantially reduced performance on some target architectures.\nHence, it is advised to use\nmma.sp::ordered_metadata\ninstruction.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.1.\nSupport for\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation introduced in\nPTX ISA version 8.4.\nmma.sp::ordered_metadata\nintroduced in PTX ISA version 8.5.\nSupport for shape\n.m16n8k32\nand\n.f16\ndtype/ctype with\n.e4m3\n/\n.e5m2\nalternate floating\npoint type\nmma\noperation introduced in PTX ISA version 8.7.\nSupport for\n.e3m2\n,\n.e2m3\n,\n.e2m1\nalternate floating point type\nmma\noperation introduced\nin PTX ISA version 8.7.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier introduced in PTX ISA version 8.7.\nSupport for\n.scale_vec::4X\non\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nis introduced in\nPTX ISA version 9.1\nTarget ISA Notes\nRequires\nsm_80\nor higher.\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation requires\nsm_89\nor higher.\nmma.sp::ordered_metadata\nrequires\nsm_80\nor higher.\nSupport for shape\n.m16n8k32\nand\n.f16\ndtype/ctype with\n.e4m3\n/\n.e5m2\nalternate floating\npoint type\nmma\noperation requires\nsm_120\n.\n.e3m2\n,\n.e2m3\nand\n.e2m1\nalternate floating point type\nmma\noperation requires\nsm_120a\nand are supported on\nsm_120f\nor higher in the same family from PTX ISA version 8.8.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier requires\nsm_120a\nand are\nsupported on\nsm_120f\nand later generation targets in the same family from PTX ISA version 8.8 except for\n.kind::mxf4nvf4\n/\n.kind::mxf4\n.\nQualifiers\n.kind::mxf4nvf4\nand\n.kind::mxf4\nare supported on following architectures:\nsm_120a\nsm_121a\nExamples of half precision floating point type\n// f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\n\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\n\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\nExamples of alternate floating point type\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k64.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::f8f6f4.f32.e3m2.e2m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::f8f6f4.f16.e2m3.e2m1.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1}, %Re, 0;\nExamples of integer type\n.reg .b32 %Ra<4>, %Rb<4>, %Rc<4>, %Rd<4>;\n.reg .u32 %Re;\n\n// u8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k32.row.col.satfinite.s32.u8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// s8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// s8 elements in A and B matrix with ordered metadata\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.s32.s4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k128.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\nExamples of mma with block scale\n.reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .b32 %Re;\n mma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4.block_scale.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1, %Rb2, %Rb3},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   %Re, 0,\n   scaleAData, {2, 1}, scaleBData, {2, 3};\n\n .reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .u16 bidA, bidB, tidA, tidB;\n .reg .b32 %Re;\n mma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue4m3\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1, %Rb2, %Rb3},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   %Re, 0,\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .u16 bidA, bidB, tidA, tidB;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue8m0\n {%Rd0, %Rd1, %Rd2, %Rd3},\n {%Ra0, %Ra1, %Ra2, %Ra3},\n {%Rb0, %Rb1, %Rb2, %Rb3},\n {%Rc0, %Rc1, %Rc2, %Rc3},\n %Re, 0,\n scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e3m2.e2m1.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  %Re, 0,\n  scaleAData, {0, 1}, scaleBData, {0, 1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e4m3.e5m2.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2,  %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  %Re, 0,\n  scaleAData, {0, 1}, scaleBData, {0, 0};\n9.7.15.\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions\n\nThe warpgroup level matrix multiply and accumulate operation has either of the following forms,\nwhere matrix\nD\nis called accumulator:\nD\n=\nA\n*\nB\n+\nD\nD\n=\nA\n*\nB\n, where the input from accumulator D is disabled.\nThe\nwgmma\ninstructions perform warpgroup level matrix multiply-and-accumulate operation by\nhaving all threads in a warpgroup collectively perform the following actions:\nLoad matrices A, B and D into registers or into shared memory.\nPerform the following\nfence\noperations:\nwgmma.fence\noperations to indicate that the register/shared-memory across the warpgroup\nhave been written into.\nfence.proxy.async\noperation to make the generic proxy operations visible to the async\nproxy.\nIssue the asynchronous matrix multiply and accumulate operations using the\nwgmma.mma_async\noperation on the input matrices. The\nwgmma.mma_async\noperation is performed in the async\nproxy.\nCreate a wgmma-group and commit all the prior outstanding\nwgmma.mma_async\noperations into the\ngroup, by using\nwgmma.commit_group\noperation.\nWait for the completion of the required wgmma-group.\nOnce the wgmma-group completes, all the\nwgmma.mma_async\noperations have been performed and\ncompleted.\n9.7.15.1.\nWarpgroup\n\nA warpgroup is a set of four contiguous warps such that the\nwarp-rank\nof the first warp is a\nmultiple of 4.\nwarp-rank of a warp is defined as:\n(%tid.x + %tid.y * %ntid.x  + %tid.z * %ntid.x * %ntid.y) / 32\n9.7.15.2.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\n, where A is an\nMxK\nmatrix, B is a\nKxN\nmatrix, while D is a\nMxN\nmatrix.\nThe following matrix shapes are supported for the specified types for the\nwgmma.mma_async\noperation:\nMultiplicand Data type\nSparsity\nShape\nFloating-point -\n.f16\nDense\n.m64n8k16\n,\n.m64n16k16\n,\n.m64n24k16\n,\n.m64n32k16\n,\n.m64n40k16\n,\n.m64n48k16\n,\n.m64n56k16\n,\n.m64n64k16\n,\n.m64n72k16\n,\n.m64n80k16\n,\n.m64n88k16\n,\n.m64n96k16\n,\n.m64n104k16\n,\n.m64n112k16\n,\n.m64n120k16\n,\n.m64n128k16\n,\n.m64n136k16\n,\n.m64n144k16\n,\n.m64n152k16\n,\n.m64n160k16\n,\n.m64n168k16\n,\n.m64n176k16\n,\n.m64n184k16\n,\n.m64n192k16\n,\n.m64n200k16\n,\n.m64n208k16\n,\n.m64n216k16\n,\n.m64n224k16\n,\n.m64n232k16\n,\n.m64n240k16\n,\n.m64n248k16\n,\n.m64n256k16\nAlternate floating-point\nformat -\n.bf16\nAlternate floating-point\nformat -\n.tf32\nSparse\nAlternate floating-point\nformat -\n.tf32\nDense\n.m64n8k8\n,\n.m64n16k8\n,\n.m64n24k8\n,\n.m64n32k8\n,\n.m64n40k8\n,\n.m64n48k8\n,\n.m64n56k8\n,\n.m64n64k8\n,\n.m64n72k8\n,\n.m64n80k8\n,\n.m64n88k8\n,\n.m64n96k8\n,\n.m64n104k8\n,\n.m64n112k8\n,\n.m64n120k8\n,\n.m64n128k8\n,\n.m64n136k8\n,\n.m64n144k8\n,\n.m64n152k8\n,\n.m64n160k8\n,\n.m64n168k8\n,\n.m64n176k8\n,\n.m64n184k8\n,\n.m64n192k8\n,\n.m64n200k8\n,\n.m64n208k8\n,\n.m64n216k8\n,\n.m64n224k8\n,\n.m64n232k8\n,\n.m64n240k8\n,\n.m64n248k8\n,\n.m64n256k8\nAlternate floating-point\nformat -\n.e4m3\n/\n.e5m2\nDense\n.m64n8k32\n,\n.m64n16k32\n,\n.m64n24k32\n,\n.m64n32k32\n,\n.m64n40k32\n,\n.m64n48k32\n,\n.m64n56k32\n,\n.m64n64k32\n,\n.m64n72k32\n,\n.m64n80k32\n,\n.m64n88k32\n,\n.m64n96k32\n,\n.m64n104k32\n,\n.m64n112k32\n,\n.m64n120k32\n,\n.m64n128k32\n,\n.m64n136k32\n,\n.m64n144k32\n,\n.m64n152k32\n,\n.m64n160k32\n,\n.m64n168k32\n,\n.m64n176k32\n,\n.m64n184k32\n,\n.m64n192k32\n,\n.m64n200k32\n,\n.m64n208k32\n,\n.m64n216k32\n,\n.m64n224k32\n,\n.m64n232k32\n,\n.m64n240k32\n,\n.m64n248k32\n,\n.m64n256k32\nFloating point -\n.f16\nSparse\nAltername floating-point\nformat -\n.bf16\nInteger -\n.u8\n/\n.s8\nDense\n.m64n8k32\n,\n.m64n16k32\n,\n.m64n24k32\n,\n.m64n32k32\n,\n.m64n48k32\n,\n.m64n64k32\n,\n.m64n80k32\n,\n.m64n96k32\n,\n.m64n112k32\n,\n.m64n128k32\n,\n.m64n144k32\n,\n.m64n160k32\n,\n.m64n176k32\n,\n.m64n192k32\n,\n.m64n208k32\n,\n.m64n224k32\n,\n.m64n240k32\n,\n.m64n256k32\nAlternate floating-point\nformat -\n.e4m3\n/\n.e5m2\nSparse\n.m64n8k64\n,\n.m64n16k64\n,\n.m64n24k64\n,\n.m64n32k64\n,\n.m64n40k64\n,\n.m64n48k64\n,\n.m64n56k64\n,\n.m64n64k64\n,\n.m64n72k64\n,\n.m64n80k64\n,\n.m64n88k64\n,\n.m64n96k64\n,\n.m64n104k64\n,\n.m64n112k64\n,\n.m64n120k64\n,\n.m64n128k64\n,\n.m64n136k64\n,\n.m64n144k64\n,\n.m64n152k64\n,\n.m64n160k64\n,\n.m64n168k64\n,\n.m64n176k64\n,\n.m64n184k64\n,\n.m64n192k64\n,\n.m64n200k64\n,\n.m64n208k64\n,\n.m64n216k64\n,\n.m64n224k64\n,\n.m64n232k64\n,\n.m64n240k64\n,\n.m64n248k64\n,\n.m64n256k64\nInteger -\n.u8\n/\n.s8\nSparse\n.m64n8k64\n,\n.m64n16k64\n,\n.m64n24k64\n,\n.m64n32k64\n,\n.m64n48k64\n,\n.m64n64k64\n,\n.m64n80k64\n,\n.m64n96k64\n,\n.m64n112k64\n,\n.m64n128k64\n,\n.m64n144k64\n,\n.m64n160k64\n,\n.m64n176k64\n,\n.m64n192k64\n,\n.m64n208k64\n,\n.m64n224k64\n,\n.m64n240k64\n,\n.m64n256k64\nSingle-bit -\n.b1\nDense\n.m64n8k256\n,\n.m64n16k256\n,\n.m64n24k256\n,\n.m64n32k256\n,\n.m64n48k256\n,\n.m64n64k256\n,\n.m64n80k256\n,\n.m64n96k256\n,\n.m64n112k256\n,\n.m64n128k256\n,\n.m64n144k256\n,\n.m64n160k256\n,\n.m64n176k256\n,\n.m64n192k256\n,\n.m64n208k256\n,\n.m64n224k256\n,\n.m64n240k256\n,\n.m64n256k256\n9.7.15.3.\nMatrix Data-types\n\nThe matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point.\nFor floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later.\nFor integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have\nelements of the same data-type, e.g. both signed integer or both unsigned integer.\nData-type\nMultiplicands (A or B)\nAccumulator (D)\nInteger\nboth\n.u8\nor both\n.s8\n.s32\nFloating Point\n.f16\n.f16\n,\n.f32\nAlternate floating Point\n.bf16\n.f32\nAlternate floating Point\n.tf32\n.f32\nAlternate floating Point\n.e4m3\n,\n.e5m2\n.f16\n,\n.f32\nSingle-bit integer\n.b1\n.s32\n9.7.15.4.\nAsync Proxy\n\nThe\nwgmma.mma_async\noperations are performed in the asynchronous proxy (or async proxy).\nAccessing the same memory location across multiple proxies needs a cross-proxy fence. For the async\nproxy,\nfence.proxy.async\nshould be used to synchronize memory between generic proxy and the\nasync proxy.\nThe completion of a\nwgmma.mma_async\noperation is followed by an implicit generic-async proxy\nfence. So the result of the asynchronous operation is made visible to the generic proxy as soon as\nits completion is observed.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used\nto wait for the completion of the\nwgmma.mma_async\ninstructions.\n9.7.15.5.\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using\nwgmma.mma_async\ninstruction\n\nThis section describes warpgroup level\nwgmma.mma_async\ninstruction and the organization of\nvarious matrices involved in this instruction.\n9.7.15.5.1.\nRegister Fragments and Shared Memory Matrix Layouts\n\nThe input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared\nmemory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This\nsection describes the layouts of register fragments and shared memory expected by the warpgroup MMA\ninstructions.\nWhen the matrices are in shared memory, their starting addresses must be aligned to 16 bytes.\n9.7.15.5.1.1.\nRegister Fragments\n\nThis section describes the organization of various matrices located in register operands of the\nwgmma.mma_async\ninstruction.\n9.7.15.5.1.1.1.\nMatrix Fragments for\nwgmma.mma_async.m64nNk16\n\nA warpgroup executing\nwgmma.mma_async.m64nNk16\nwill compute an MMA operation of shape\n.m64nNk16\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.f16x2\nregisters, with each\nregister containing two\n.f16\n/\n.bf16\nelements from matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 148\n.\nFigure 148\nWGMMA .m64nNk16 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing N/4 number of\n.f16x2\nregisters, with each register containing two\n.f16\nelements from matrix D.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\n.f32\nA vector expression containing N/2 number of\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown in\nFigure 149\n.\nFigure 149\nWGMMA .m64nNk16 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.2.\nMatrix Fragments for\nwgmma.mma_async.m64nNk8\n\nA warpgroup executing\nwgmma.mma_async.m64nNk8\nwill compute an MMA operation of shape\n.m64nNk8\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters containing\nfour\n.tf32\nelements from matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 150\n.\nFigure 150\nWGMMA .m64nNk8 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.f32\nA vector expression containing N/2 number of\n.f32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\nThe layout of the fragments held by different threads is shown in\nFigure 151\n.\nFigure 151\nWGMMA .m64nNk8 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.3.\nMatrix Fragments for\nwgmma.mma_async.m64nNk32\n\nA warpgroup executing\nwgmma.mma_async.m64nNk32\nwill compute an MMA operation of shape\n.m64nNk32\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.u8\n/\n.s8\nelements from matrix A.\na0, a1, a2, a3, … , a14, a15\n.e4m3\n/\n.e5m2\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\nelements from\nmatrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 152\n.\nFigure 152\nWGMMA .m64nNk32 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\nMiscellaneous Information\n.s32\nA vector expression containing\nN/2 number of\n.s32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\ndepends on .dtype, as\ndescribed in the next column.\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n3,\n4}\n=\n16*i\nwhere\ni\n=\n{3,\n4,\n...,\n15,\n16}\n.f32\nA vector expression containing\nN/2 number of\n.f32\nregisters.\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\n.f16\nA vector expression containing\nN/4 number of\n.f16x2\nregisters, with each register\ncontaining two\n.f16\nelements from matrix D.\nThe layout of the fragments held by different threads is shown in\nFigure 153\n.\nFigure 153\nWGMMA .m64nNk32 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.4.\nMatrix Fragments for\nwgmma.mma_async.m64nNk256\n\nA warpgroup executing\nwgmma.mma_async.m64nNk256\nwill compute an MMA operation of shape\n.m64nNk256\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing thirty two\n.b1\nelement from matrix A.\na0, a1, a2, …, a127\nThe layout of the fragments held by different threads is shown in\nFigure 154\n.\nFigure 154\nWGMMA .m64nNk256 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing N/2 number of\n.s32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n3,\n4}\n=\n16*i\nwhere\ni\n=\n{3,\n4,\n...,\n15,\n16}\nThe layout of the fragments held by different threads is shown in\nFigure 155\n.\nFigure 155\nWGMMA .m64nNk256 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.2.\nShared Memory Matrix Layout\n\nIf the argument\nimm-trans-a\n/\nimm-trans-b\nof the instruction\nwgmma.mma_async{.sp}\nis 0, then\nK-major\nis used for matrix\nA\n/\nB\nrespectively. If the value of argument\nimm-trans-a\nis 1 then\nM-major\nis used for matrix\nA\n. If the value of the argument\nimm-trans-b\nis 1, then\nN-major\nis used for matrix\nB\n.\nIn a column-major default BLAS library such as cuBLAS, the matrices\nA\nand\nB\nwith and\nwithout transpose can be classified as either\nK-Major\nor\nM-or-N-Major\nas shown in the\nfollowing table:\nNon-Transposed\nTransposed\nA\nK-major\nM-major\nB\nK-major\nN-major\nTo avoid confusion with\nA\n,\nB\n,\nrow-major\n,\ncol-major\n,\ntranspose\n, and\nnon-transpose\n, we will use\nMN-Major\nand\nK-Major\nthroughout this section.\nThe matrices in the shared memory are made up of one or more “swizzle layout atom”.\nThe exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,\nand the leading dimension. The layout of the swizzle are shown in\nTable 38\n.\nTable 38\nVarious combinations of swizzling mode, leading dimension and swizzle-atom layout\n\nSwizzling mode\nLeading Dimension\n/ Major-ness\nSwizzle atom layout\n(128b element)\n128B Swizzling Mode\nM/N\n8x8\nK\n8x8\n64B Swizzling Mode\nM/N\n4x8\nK\n8x4\n32B Swizzling Mode\nM/N\n2x8\nK\n8x2\nNone\nM/N\n1x8\nK\n8x1\nThe above shapes are for elements of size 128 bits. For smaller elements sizes, the same\nshapes would get multiplied along the leading dimension by a factor of\n128/sizeof_bits(Element)\n.\nFor example, 128B MN major swizzle atom would have a shape of\n(8*(128/32))x8\n=\n32x8\nfor\ntf32\ntensor core inputs.\nExamples\nThe following are some example layouts of\nMxK\nor\nKxN\nmatrices with various swizzling modes,\nand are in units of 128b elements as shown\nby each colored cell as shown in\nFigure 156\n,\nFigure 157\n,\nFigure 158\n,\nFigure 159\n,\nFigure 160\n,\nFigure 161\n,\nFigure 162\n,\nFigure 163\n.\nFigure 156\nMN major 128B swizzling\n\nFigure 157\nK major 128B swizzling\n\nFigure 158\nMN major 64B swizzling\n\nFigure 159\nK major 64B swizzling\n\nFigure 160\nMN major 32B swizzling\n\nFigure 161\nK major 32B swizzling\n\nFigure 162\nMN major interleaved\n\nFigure 163\nK major interleaved\n\nFollowing are some of the examples of the 128B swizzling layout for\ntf32\nelement type.\nK-Major:\nFigure 164\nFigure 164\nK major\n\nMN-Major:\nFigure 165\nFigure 165\nMN major\n\n9.7.15.5.1.2.1.\nMajor-ness supported by Strides\n\nThere are two strides involved while accessing a matrix from shared memory:\nLeading dimension byte offset\nStride dimension byte offset\n9.7.15.5.1.2.1.1.\nLeading Dimension Byte Offset\n\nThe leading dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The leading byte offset is defined as follows for matrices whose element types are\nnormalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nNo-Swizzling: the offset from the first column to the second columns\nof the 8x2 tile in the 128-bit element type normalized matrix.\nSwizzled layouts: not used, assumed to be 1.\nMN-Major\nInterleave: offset from the first 8 columns to the next 8 columns.\nSwizzled layouts: offset from the first (swizzle-byte-size/16) rows\nto the next (swizzle-byte-size/16) rows.\n9.7.15.5.1.2.1.2.\nStride Dimension Byte Offset\n\nThe stride dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The stride dimension byte offset is defined as follows for matrices whose element\ntypes are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nThe offset from the first 8 rows to the next 8 rows.\nMN-Major\nInterleave: offset from the first row to the next row.\nSwizzled layout: offset from the first 8 columns to the next 8\ncolumns\n9.7.15.5.1.2.1.3.\nCanonical Layouts\n\nIn terms of\nCuTe layouts\nthe canonical layout can be expressed as follows:\nMajor-\nness\nSwizzling mode\nCanonical Layout without swizzling\nSwizzling\non the previous column\nMN-\nmajor\nNo-swizzling or\nInterleaved\n((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nSwizzle<1, 4, 3>\n64B Swizzling\n((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nSwizzle<2, 4, 3>\n128B Swizzling\n((T,8,m),(8,k)):((1,T,LBO),(8T,SBO))\nSwizzle<3, 4, 3>\nK-\nmajor\nNo-swizzling or\nInterleaved\n((8,m),(T,2k)):((1T,SBO),(1,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((8,m),(T,2k)):((2T,SBO),(1,T))\nSwizzle<1, 4, 3>\n64B Swizzling\n((8,m),(T,2k)):((4T,SBO),(1,T))\nSwizzle<2, 4, 3>\n128B Swizzling\n((8,m),(T,2k)):((8T,SBO),(1,T))\nSwizzle<3, 4, 3>\nwhere\nT = 128 / sizeof-elements-in-bits\nT represents scale factor which normalizes matrix element types to 128-bits.\nm represents the number of repeating patterns across rows.\nk represents the number of repeating patterns across columns.\nExamples\nK-Major, no-swizzling and tf32 type:\nFigure 166\nFigure 166\nK major, no-swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))\nCanonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO\n64*sizeof(tf32)\nSBO\n32*sizeof(tf32)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nK-Major, 32B swizzling and tf32 type:\nFigure 167\nFigure 167\nK major, 32B swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))\nCanonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO\nNA\nSBO\n64*sizeof(tf32)\nEncoding of LBO in descriptor\n1 (assumed)\nEncoding of SBO in descriptor\n(SBO) >> 4 = 16\nMN-Major, no-swizzling and bf16 type:\nFigure 168\nFigure 168\nMN major, no-swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))\nCanonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n128*sizeof(bf16)\nSBO\n64*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nMN-Major, 32B swizzling and bf16 type:\nFigure 169\nFigure 169\nMN major, 32B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))\nCanonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n128*sizeof(bf16)\nSBO\n256*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 32\nMN-Major, 64B swizzling and bf16 type:\nFigure 170\nFigure 170\nMN major, 64B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))\nCanonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n256*sizeof(bf16)\nSBO\n512*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 32\nEncoding of SBO in descriptor\n(SBO) >> 4 = 64\n9.7.15.5.1.2.2.\nMatrix Descriptor Format\n\nMatrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in\nthe matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the\nfollowing layout:\nBit-field\nSize in bits\nDescription\n13–0\n14\nmatrix-descriptor-encode(Matrix start address)\n29–16\n14\nmatrix-descriptor-encode\n(\nLeading dimension byte offset\n)\n45–32\n14\nmatrix-descriptor-encode\n(\nStride dimension byte offset\n)\n51–49\n3\nMatrix base offset. This is valid for all swizzling modes except the no-swizzle mode.\n63–62\n2\nSpecifies the swizzling mode to be used:\n0: No swizzle\n1: 128-Byte swizzle\n2: 64-Byte swizzle\n3: 32-Byte swizzle\nwhere\nmatrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4\nThe value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as\nper the below table:\nSwizzling mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the base offset must be a non-zero value, computed using the following formula:\nbase offset = (pattern start addr >> 0x7) & 0x7\n9.7.15.5.2.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async\n\nwgmma.mma_async\nPerform matrix multiply-and-accumulate operation across warpgroup\nSyntax\nHalf precision floating point type:\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype   = {.f16, .f32};\nAlternate floating point type :\n.bf16 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8,\n            .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8,\n            .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8,\n            .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8,\n            .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,\n            .m64n168k8, .m64n176k8, .m64n184k8, .m64n192k8,\n            .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8,\n            .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32};\nInteger type:\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32,\n            .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32,\n            .m64n176k32, .m64n192k32, .m64n208k32, .m64n224k32};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8};\nSingle bit:\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256,\n            .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256,\n            .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256,\n            .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256,\n            .m64n240k256, .m64n256k256};\n.op  = {.and};\nDescription\nInstruction\nwgmma.mma_async\nissues a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the D matrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nscale-d\nis\nfalse.\nwgmma.fence\ninstruction must be used to fence the register accesses of\nwgmma.mma_async\ninstruction from their prior accesses. Otherwise, the behavior is undefined.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed.\nRegister operand\nd\nrepresents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand\na\nrepresents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands\na-desc\nand\nb-desc\nare the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the warps\nin the warpgroup. The format of the matrix descriptor is described in\nMatrix Descriptor Format\n.\nMatrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments\nimm-trans-a\nand\nimm-trans-b\nrespectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of\nimm-trans-a\nand\nimm-trans-b\nare 0\nand 1. The transpose operation is only supported for the\nwgmma.mma_async\nvariants with\n.f16\n/\n.bf16\ntypes on matrices accessed from shared memory using matrix descriptors.\nFor the floating point variants of the\nwgmma.mma_async\noperation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands\nimm-scale-a\nand\nimm-scale-b\nrespectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of\nimm-scale-a\nand\nimm-scale-b\nare -1 and 1.\nThe qualifiers\n.dtype\n,\n.atype\nand\n.btype\nindicate the data type of the elements in\nmatrices D, A and B respectively.\n.atype\nand\n.btype\nmust be the same for all floating point\nwgmma.mma_async\nvariants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the\nwgmma.mma_async\noperation are as follows:\nMatrices A and B have 8-bit data elements when\n.atype\n/\n.btype\nis\n.e4m3\n/\n.e5m2\n.\nMatrices A and B have 16-bit data elements when\n.atype\n/\n.btype\nis\n.bf16\n.\nMatrices A and B have 32-bit data elements when\n.atype\n/\n.btype\nis\n.tf32\n.\nPrecision and rounding:\nFloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with at least single\nprecision. When\n.dtype\nis\n.f16\n, the accumulation is performed with at least half\nprecision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified\nprecision.\nwgmma.mma_async\noperation involving type\n.tf32\nwill truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations:\nThe integer\nwgmma.mma_async\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the\nrange\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.mma_async\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.mma_async\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.mma_async\ninstruction. In conditionally executed code, a\nwgmma.mma_async\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.u8.s8\nand\n.s8.u8\nas .atype.btype introduced in PTX ISA version 8.4.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples of half precision floating point type\n.reg .f16x2 f16a<40>, f16d<40>;\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n.reg .pred  scaleD;\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16\n  {f16d0, f16d1,  f16d2,  f16d3,  f16d4,  f16d5,  f16d6,  f16d7,  f16d8,\n   f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17},\n  descA,\n  descB,\n  scaleD, -1, 1, 1, 0;\nExamples of alternate floating point type\n.reg .f32   f32d<40>;\n.reg .b32   bf16a<40>\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9,\n   f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19,\n   f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29,\n   f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39,\n   f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49,\n   f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59},\n  {bf16a0, bf16a1, bf16a2, bf16a3},\n  descB,\n  scaleD, -1, -1, 0;\n\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7},\n  descA,\n  descB,\n  0, -1, -1;\n\n.reg .b32 f16d<8>, f16a<8>;\n.reg .f32 f32d<8>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2\n  {f16d0, f16d1},\n  descA,\n  descB,\n  scaleD, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1;\nExamples of integer type\n.reg .s32 s32d<8>, s32a<8>;\n.reg .u32 u32a<8>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\nExamples of single bit type\n.reg .s32 s32d<4>;\n.reg .b32 b32a<4>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\n\nwgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc\n  {s32d0, s32d1, s32d2, s32d3},\n  {b32a0, b32a1, b32a2, b32a3},\n  descB,\n  scaleD;\n9.7.15.6.\nAsynchronous Warpgroup Level Multiply-and-Accumulate Operation using\nwgmma.mma_async.sp\ninstruction\n\nThis section describes warp-level\nwgmma.mma_async.sp\ninstruction with sparse matrix A. This\nvariant of the\nwgmma.mma_async\noperation can be used when A is a structured sparse matrix with\n50% zeros in each row distributed in a shape-specific granularity. For an\nMxNxK\nsparse\nwgmma.mma_async.sp\noperation, the\nMxK\nmatrix A is packed into\nMxK/2\nelements. For each\nK-wide row of matrix A, 50% elements are zeros and the remaining\nK/2\nnon-zero elements are\npacked in the operand representing matrix A. The mapping of these\nK/2\nelements to the\ncorresponding K-wide row is provided explicitly as metadata.\n9.7.15.6.1.\nSparse matrix storage\n\nGranularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a\n64x32\nmatrix A used in floating point\nwgmma.mma_async\noperations, sparsity is expected to be at 2:4 granularity, i.e. each 4-element\nvector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each\nnon-zero element in a sub-chunk is stored in the metadata operand. Values\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nare invalid values for metadata and will result in undefined behavior. In a\ngroup of four consecutive threads, one or more threads store the metadata for the whole group\ndepending upon the matrix shape. These threads are specified using an additional sparsity selector operand.\nMatrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in\nFigure 111\n, with an appropriate matrix size.\nGranularities for different matrix shapes and data types are described below.\nSparse\nwgmma.mma_async.sp\nwith half-precision and\n.bf16\ntype\nFor\n.f16\nand\n.bf16\ntypes, for all supported\n64xNx32\nshapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand.\nFigure 171\nSparse WGMMA metadata example for\n.f16\n/\n.bf16\ntype.\n\nThe sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith\n.tf32\ntype\nFor\n.tf32\ntype, for all supported\n64xNx16\nshapes, matrix A is structured sparse at a\ngranularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have\none zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and\nthe 4-bit index in the metadata indicates the position of the non-zero element in the two-wide\nchunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in\nan undefined behavior.\nFigure 172\nSparse WGMMA metadata example for\n.tf32\ntype.\n\nThe sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith\n.e4m3\nand\n.e5m2\nfloating point type\nFor\n.e4m3\nand\n.e5m2\ntypes, for all supported\n64xNx64\nshapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand.\nFigure 173\nSparse WGMMA metadata example for\n.e4m3\n/\n.e5m2\ntype.\n\nAll threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith integer type\nFor the integer type, for all supported\n64xNx64\nshapes, matrix A is structured sparse at a\ngranularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have\ntwo zeroes and two non-zero elements. Only the two non-zero elements are stored in matrix A and two\n2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide\nchunk.\nFigure 174\nSparse WGMMA metadata example for\n.u8\n/\n.s8\ntype.\n\nAll threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior.\n9.7.15.6.2.\nMatrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A\n\nIn this section we describe how the contents of thread registers are associated with fragments of A\nmatrix and the sparsity metadata.\nEach warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following\ntable shows the assignment of warps to rows of matrix A:\nWarp\nSparsity information for rows of matrix A\n%warpid\n% 4 = 3\n48-63\n%warpid\n% 4 = 2\n32-47\n%warpid\n% 4 = 1\n16-31\n%warpid\n% 4 = 0\n0-15\nThe following conventions are used throughout this section:\nFor matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data.\nFor matrix D, since the matrix dimension - data type combination is the same for all supported\nshapes, and is already covered in\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction\n, the pictorial\nrepresentations of matrix fragments are not included in this section.\nFor the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included.\nTk:\n[m..n]\npresent\nin cell\n[x][y..z]\nindicates that bits\nm\nthrough\nn\n(with\nm\nbeing higher) in the\nmetadata operand of thread with\n%laneid=k\ncontains the indices of the non-zero elements from\nthe chunk\n[x][y]..[x][z]\nof matrix A.\n9.7.15.6.2.1.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk32\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk32\nwill compute an MMA operation of shape\n.m64nNk32\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nShared Memory Matrix Layout\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters, with each register containing two\nnon-zero\n.f16\n/\n.bf16\nelements out of 4\nconsecutive elements from matrix A.\nNon-zero elements:\na0, a1, a2, a3, a4, a5, a6, a7\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\nThe layout of the fragments held by different threads is shown in\nFigure 175\n.\nFigure 175\nSparse WGMMA .m64nNk32 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk32\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nShared Memory Matrix Layout\n.\nMetadata operand is a\n.b32\nregister containing 16 2-bit vectors each storing the index of a\nnon-zero element of a 4-wide chunk of matrix A.\nFigure 176\nshows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable\ni\nrepresents the value of the sparsity\nselector operand.\nFigure 176\nSparse WGMMA .m64nNk32 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.15.6.2.2.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk16\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk16\nwill compute an MMA operation of shape\n.m64nNk16\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nShared Memory Matrix Layout\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.tf32\nA vector expression containing four\n.b32\nregisters, containing four non-zero\n.tf32\nelements out of eight consecutive elements\nfrom matrix A.\nNon-zero elements:\na0, a1, a2, a3\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\nThe layout of the fragments held by different threads is shown in\nFigure 177\n.\nFigure 177\nSparse WGMMA .m64nNk16 fragment layout for matrix A with\n.tf32\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk8\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nShared Memory Matrix Layout\n.\nMetadata operand is a\n.b32\nregister containing eight 4-bit vectors each storing the index of a\nnon-zero element of a 2-wide chunk of matrix A.\nFigure 178\nshows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable\ni\nrepresents the value of the sparsity\nselector operand.\nFigure 178\nSparse WGMMA .m64nNk16 metadata layout for\n.tf32\ntype.\n\n9.7.15.6.2.3.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk64\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk64\nwill compute an MMA operation of shape\n.m64nNk64\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nMatrix Fragments for sparse wgmma.mma_async.m64nNk64\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.e4m3\n/\n.e5m2\nA vector expression containing four\n.b32\nregisters, with each register containing four\nnon-zero\n.e4m3\n/\n.e5m2\nelements out of\neight consecutive elements from matrix A.\nNon-zero elements:\na0, a1, a2, … , a15\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each register containing four\nnon-zero\n.s8\n/\n.u8\nelements out of\neight consecutive elements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 179\n.\nFigure 179\nSparse WGMMA .m64nNk64 fragment layout for matrix A with\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk32\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nMatrix Fragments for sparse wgmma.mma_async.m64nNk64\n.\nMetadata operand is a\n.b32\nregister containing 16 4-bit vectors each storing the indices of\ntwo non-zero elements of a 4-wide chunk of matrix A.\nFigure 180\nshows the mapping of the metadata\nbits to the elements of columns 0–31 of matrix A.\nFigure 180\nSparse WGMMA .m64nNk64 metadata layout for\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype for columns 0–31\n\nFigure 181\nshows the mapping of the metadata\nbits to the elements of columns 32–63 of matrix A.\nFigure 181\nSparse WGMMA .m64nNk64 metadata layout for\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype for columns 32–63\n\n9.7.15.6.3.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async.sp\n\nwgmma.mma_async.sp\nPerform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup\nSyntax\nHalf precision floating point type:\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype   = {.f16, .f32};\nAlternate floating point type :\n.bf16 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64,\n            .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64,\n            .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64,\n            .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,\n            .m64n168k64, .m64n176k64, .m64n184k64, .m64n192k64,\n            .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64,\n            .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32};\nInteger type:\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d;\n\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64,\n            .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64,\n            .m64n176k64, .m64n192k64, .m64n208k64, .m64n224k64,\n            .m64n240k64, .m64n256k64};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8};\nDescription\nInstruction\nwgmma.mma_async\nissues a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the D matrix is\nMxN\n.\nThe matrix A is stored in the packed format Mx(K/2) as described in\nSparse matrix storage\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nscale-d\nis\nfalse.\nwgmma.fence\ninstruction must be used to fence the register accesses of\nwgmma.mma_async\ninstruction from their prior accesses. Otherwise, the behavior is undefined.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed.\nRegister operand\nd\nrepresents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand\na\nrepresents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands\na-desc\nand\nb-desc\nare the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the\nwarps in the warpgroup. The format of the matrix descriptor is described in\nMatrix Descriptor Format\n. Matrix A is\nstructured sparse as described in\nSparse matrix storage\n.  Operands\nsp-meta\nand\nsp-sel\nrepresent sparsity metadata and sparsity selector respectively. Operand\nsp-meta\nis a 32-bit\ninteger and operand\nsp-sel\nis a 32-bit integer constant with values in the range 0..3.\nThe valid values of\nsp-meta\nand\nsp-sel\nfor each shape is specified in\nSparse matrix storage\nand are summarized here :\nMatrix shape\n.atype\nValid values of\nsp-meta\nValid values of\nsp-sel\n.m64nNk16\n.tf32\n0b1110 , 0b0100\n0 (threads T0, T1) or 1 (threads T2, T3)\n.m64nNk32\n.f16\n/\n.bf16\n0b00, 0b01, 0b10, 0b11\n0 (threads T0, T1) or 1 (threads T2, T3)\n.m64nNk64\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\n0b00, 0b01, 0b10, 0b11\n0 (all threads contribute)\nMatrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments\nimm-trans-a\nand\nimm-trans-b\nrespectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of\nimm-trans-a\nand\nimm-trans-b\nare 0\nand 1. The transpose operation is only supported for the\nwgmma.mma_async\nvariants with\n.f16\n/\n.bf16\ntypes on matrices accessed from shared memory using matrix descriptors.\nFor the floating point variants of the\nwgmma.mma_async\noperation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands\nimm-scale-a\nand\nimm-scale-b\nrespectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of\nimm-scale-a\nand\nimm-scale-b\nare -1 and 1.\nThe qualifiers\n.dtype\n,\n.atype\nand\n.btype\nindicate the data type of the elements in\nmatrices D, A and B respectively.\n.atype\nand\n.btype\nmust be the same for all floating point\nwgmma.mma_async\nvariants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the\nwgmma.mma_async\noperation are as follows:\nMatrices A and B have 8-bit data elements when\n.atype\n/\n.btype\nis\n.e4m3\n/\n.e5m2\n.\nMatrices A and B have 16-bit data elements when\n.atype\n/\n.btype\nis\n.bf16\n.\nMatrices A and B have 32-bit data elements when\n.atype\n/\n.btype\nis\n.tf32\n.\nPrecision and rounding:\nFloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with at least single\nprecision. When\n.dtype\nis\n.f16\n, the accumulation is performed with at least half\nprecision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified\nprecision.\nwgmma.mma_async\noperation involving type\n.tf32\nwill truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations:\nThe integer\nwgmma.mma_async\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the\nrange\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.mma_async\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.mma_async\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.mma_async\ninstruction. In conditionally executed code, a\nwgmma.mma_async\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.2.\nSupport for\n.u8.s8\nand\n.s8.u8\nas .atype.btype introduced in PTX ISA version 8.4.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples of integer type\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n9.7.15.7.\nAsynchronous\nwgmma\nProxy Operations\n\nThis section describes warpgroup level\nwgmma.fence\n,\nwgmma.commit_group\nand\nwgmma.wait_group\ninstructions.\n9.7.15.7.1.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.fence\n\nwgmma.fence\nEnforce an ordering of register accesses between\nwgmma.mma_async\nand other operations.\nSyntax\nwgmma.fence.sync.aligned;\nDescription\nwgmma.fence\ninstruction establishes an ordering between prior accesses to any warpgroup\nregisters and subsequent accesses to the same registers by a\nwgmma.mma_async\ninstruction. Only\nthe accumulator register and the input registers containing the fragments of matrix A require this\nordering.\nThe\nwgmma.fence\ninstruction must be issued by all warps of the warpgroup at the following\nlocations:\nBefore the first\nwgmma.mma_async\noperation in a warpgroup.\nBetween a register access by a thread in the warpgroup and any\nwgmma.mma_async\ninstruction\nthat accesses the same registers, either as accumulator or input register containing fragments of\nmatrix A, except when these are accumulator register accesses across multiple\nwgmma.mma_async\ninstructions of the same shape. In the latter case, an ordering guarantee is provided by default.\nOtherwise, the behavior is undefined.\nAn async proxy fence must be used to establish an ordering between prior writes to shared memory\nmatrices and subsequent reads of the same matrices in a\nwgmma.mma_async\ninstruction.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.fence\ninstruction causes the executing\nthread to wait until all threads in the warp execute the same\nwgmma.fence\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.fence\ninstruction. In conditionally executed code, an\nwgmma.fence\ninstruction\nshould only be used if it is known that all threads in the warpgroup evaluate the condition\nidentically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\n// Example 1, first use example:\nwgmma.fence.sync.aligned;    // Establishes an ordering w.r.t. prior accesses to the registers s32d<0-3>\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n\n// Example 2, use-case with the input value updated in between:\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\n...\nmov.b32 s32d0, new_val;\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d4, s32d5, s32d6, s32d7},\n                                                 {s32d0, s32d1, s32d2, s32d3},\n                                                  descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n9.7.15.7.2.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.commit_group\n\nwgmma.commit_group\nCommits all prior uncommitted\nwgmma.mma_async\noperations into a\nwgmma-group\n.\nSyntax\nwgmma.commit_group.sync.aligned;\nDescription\nwgmma.commit_group\ninstruction creates a new wgmma-group per warpgroup and batches all prior\nwgmma.mma_async\ninstructions initiated by the executing warp but not committed to any\nwgmma-group into the new wgmma-group. If there are no uncommitted\nwgmma.mma_async\ninstructions\nthen\nwgmma.commit_group\nresults in an empty wgmma-group.\nAn executing thread can wait for the completion of all\nwgmma.mma_async\noperations in a\nwgmma-group by using\nwgmma.wait_group\n.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.commit_group\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.commit_group\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.commit_group\ninstruction. In conditionally executed code, an\nwgmma.commit_group\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\nwgmma.commit_group.sync.aligned;\n9.7.15.7.3.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.wait_group\n\nwgmma.wait_group\nSignal the completion of a preceding warpgroup operation.\nSyntax\nwgmma.wait_group.sync.aligned N;\nDescription\nwgmma.wait_group\ninstruction will cause the executing thread to wait until only N or fewer of\nthe most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing\nthreads are complete. For example, when N is 0, the executing thread waits on all the prior\nwgmma-groups to complete. Operand N is an integer constant.\nAccessing the accumulator register or the input register containing the fragments of matrix A of a\nwgmma.mma_async\ninstruction without first performing a\nwgmma.wait_group\ninstruction that\nwaits on a\nwgmma-group\nincluding that\nwgmma.mma_async\ninstruction is undefined behavior.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.wait_group\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.wait_group\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.wait_group\ninstruction. In conditionally executed code, an\nwgmma.wait_group\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\nwgmma.fence.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3},\n                                                  {f16a0, f16a1, f16a2, f16a3},\n                                                   descB, 1, -1, -1, 1;\nwgmma.commit_group.sync.aligned;\n\nwgmma.wait_group.sync.aligned 0;\n9.7.16.\nTensorCore 5th Generation Family Instructions\n\n9.7.16.1.\nTensor Memory\n\nThe 5\nth\ngeneration TensorCore has dedicated on-chip memory that is specialized for use by\nTensorCore operations. This Tensor Memory is organized as a two-dimensional matrix where\nthe horizontal rows are called lanes and the vertical columns are called columns.\nOn architecture\nsm_100a\n/\nsm_100f\n, the 5\nth\ngeneration TensorCore’s Tensor Memory has a\ntwo-dimensional structure of 512 columns and 128 rows per CTA, with each cell being 32-bits in size.\nRestrictions on threads accessing the Tensor Memory via the load and store operations\nare specified in\nAccess restrictions\n.\n9.7.16.1.1.\nTensor Memory Addressing\n\nTensor Memory addresses are 32-bit wide and specify two components.\nLane index\nColumn index\nThe layout is as follows:\n31                      16\n15                       0\nLane index\nColumn index\nFigure 182\nshows the view of the Tensor Memory Layout within CTA.\nFigure 182\nTensor Memory Layout and Addressing\n\n9.7.16.1.2.\nTensor Memory Allocation\n\nThe Tensor Memory is dynamically allocated. The Tensor Memory must be allocated by a single\nwarp in a CTA using the\nTensor Memory Allocation and Management Instructions\n.\nThe allocation and deallocation of\nTensor Memory\nis performed in terms of\ncolumns. The unit of allocation is 32 columns and the number of columns being allocated must be\na power of 2. When a column is allocated, all 128 lanes of the column are allocated.\nAll of the Tensor Memory that was allocated in a kernel, must be explicitly deallocated\nbefore the kernel exits.\n9.7.16.2.\nMatrix and Data Movement Shape\n\nThere are two kinds of shapes involved.\nShapes in the data movement operations\nShapes in the MMA operations\n9.7.16.2.1.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand matrices\nA\n,\nB\nand\nD\n. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\nwhere\nA\nis\nMxK\nmatrix,\nB\nis a\nKxN\nmatrix, and\nD\nis a\nMxN\nmatrix.\nTable 39\nshows matrix shapes that are supported for the specified types for the\ntcgen05.mma\noperation.\nTable 39\nVarious combinations of .kind and shapes\n\nVarious Combinations\nShapes Supported\n.kind::*\nHas .ws\nCTA Group\nSparsity\ndtype\natype/btype\nkind::f16\nNo\n.ws\n1\nDense\n.f16\n.f16\n64xNxK\n128xNxK\nN = {8, 16, 24, … 256} steps of 8\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nK = 32\n.f32\n.f16\n,\n.bf16\n2\nDense\n.f16\n.f16\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nK = 32\n.f32\n.f16\n,\n.bf16\n.ws\n1\nDense\n.f16\n.f16\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nN = {64, 128}\nK = 32\n.f32\n.f16\n,\n.bf16\n2\nEither\n.f16\n.f16\nInvalid\n.f32\n.f16\n,\n.bf16\n.kind::tf32\nNo\n.ws\n1\nDense\n.f32\n.tf32\n64xNxK\n128xNxK\nN = {8, 16, 24, … 256} steps of 8\nK = 8\nSparse\nK = 16\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 8\nSparse\nK = 16\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 8\nSparse\nN = {64, 128}\nK = 16\n2\nDense\nInvalid\nSparse\n.kind::f8f6f4\nNo\n.ws\n1\nDense\n.f32\n.f16\n.e4m3\n,\n.e5m2\n,\n.e2m3\n,\n.e3m2\n,\n.e2m1\n64xNxK\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 32\nSparse\nK = 64\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 32\nSparse\nN = {64, 128}\nK = 64\n2\nDense\nInvalid\nSparse\n.kind::mxf8f6f4\nNo\n.ws\n1\nDense\n.f32\n.e4m3\n,\n.e5m2\n,\n.e2m3\n,\n.e3m2\n,\n.e2m1\nX\n(Scale)\n.ue8m0\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 32\nSparse\n256xNxK\nK = 64\n.ws\n1\nDense\nInvalid\nSparse\n2\nDense\nSparse\n.kind::i8\nNo\n.ws\n1\nDense\n.s32\n.s8\n,\n.u8\n64xNxK\n128xNxK\nN = {8, 16, 24, 32, 48, … 256}\nsteps of 16 after N > 32\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {32, 64, … 256} steps of 32\nK = 32\nSparse\nK = 64\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 32\nSparse\nN = {64, 128}\nK = 64\n2\nDense\nInvalid\nSparse\n.kind::mxf4\nNo\n.ws\n1\nDense\n.f32\n.e2m1\nX\n(Scale)\n.ue8m0\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 64\nSparse\nK = 128\n2\nDense\n128xNxK\n256xNxK\n256xNxK\n1\nN = {16, 32, … 256} steps of 16\nK = 64\nK\n1\n= 96\nSparse\n256xNxK\nK = 128\n.ws\n1 / 2\nEither\nInvalid\n.kind::mxf4nvf4\nNo\n.ws\n1\nDense\n.f32\n.e2m1\nX\n(Scale)\n.ue8m0\n,\n.ue4m3\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 64\nSparse\nK = 128\n2\nDense\n128xNxK\n256xNxK\n256xNxK\n1\nN = {16, 32, … 256} steps of 16\nK = 64\nK\n1\n= 96\nSparse\n256xNxK\nK = 128\n.ws\n1 / 2\nEither\nInvalid\n9.7.16.2.1.1.\nTarget ISA Note\n\nK = 96 is only supported for target architecture\nsm_103a\n.\n9.7.16.2.2.\nSpecifying Matrix Shape\n\nM\nand\nN\ncan be specified in the\nInstruction descriptor\n.\nK\ncannot be explicitly specified but is implicitly determined by the MMA-kind\nand the sparsity, as shown in the\nTable 39\n.\n9.7.16.2.3.\nData Movement Shape\n\nThe data movement shape indicates the dimension of the data to be moved to or from the\nTensor Memory\n. These shapes are described as a tuple\nlane\nx\nsize\nwhere:\nlane\nindicates the number of rows in the\nTensor Memory\n; and\nsize\nindicates the amount of data, in units of bits (b), across the columns in the\nTensor Memory\n.\nThe following shapes are supported by various tcgen05 operations:\nShape\ntcgen05.<op>\n.16x64b\n,\n.16x128b\n,\n.16x256b\n,\n.16x32bx2\n,\n.32x32b\n.ld\n/\n.st\n.4x256b\n,\n.32x128b\n,\n.64x128b\n,\n.128x256b\n,\n.128x128b\n.cp\n.31x256b\n(implicit)\n.shift\n9.7.16.2.3.1.\nMemory Layout\n\nThe following shows the layout of the matrix fragments across threads of the warp.\n9.7.16.2.3.1.1.\nMatrix fragments for shape .32x32b\n\nA\ntcgen05{.ld,.st}.32x32b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.32x32b\nwill access 32 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (32 * .num)-bits of data as shown in\nFigure 183\n.\nFigure 183\nMatrix Fragment for shape .32x32b\n\n9.7.16.2.3.1.2.\nMatrix fragments for shape .16x64b\n\nA\ntcgen05{.ld,.st}.16x64b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x64b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (64 * .num)-bits of data as shown in\nFigure 184\n.\nFigure 184\nMatrix Fragment for shape .16x64b\n\n9.7.16.2.3.1.3.\nMatrix fragments for shape .16x128b\n\nA\ntcgen05{.ld,.st}.16x128b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x128b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (128 * .num)-bits of data as shown in\nFigure 185\n.\nFigure 185\nMatrix Fragment for shape .16x128b\n\n9.7.16.2.3.1.4.\nMatrix fragments for shape .16x256b\n\nA\ntcgen05{.ld,.st}.16x256b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, r2, r3, …\nA warp executing\ntcgen05{.ld,.st}.16x256b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (256 * .num)-bits of data as shown in\nFigure 186\n.\nFigure 186\nMatrix Fragment for shape .16x256b\n\n9.7.16.2.3.1.5.\nMatrix fragments for shape .16x32bx2\n\nA\ntcgen05{.ld,.st}.16x32bx2\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x32bx2\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (32 * .num)-bits of data as shown in\nFigure 187\n.\nFigure 187\nMatrix Fragment for shape .16x32bx2\n\n9.7.16.3.\nMajor-ness supported by Strides\n\nThere are two strides involved while accessing a matrix from shared memory:\nLeading dimension stride (byte offset or absolute address)\nStride dimension byte offset\n9.7.16.3.1.\nLeading Dimension Stride: relative offset or absolute address\n\nThere are two modes of Leading Dimension Strides as described below.\nBit #52 in the\nShared memory descriptor\nis used to distinguish between two modes.\n9.7.16.3.1.1.\nRelative offset mode\n\nIn this mode, the leading dimension stride is specified as a relative byte offset between the\ncolumns as explained in the below table.\nThe leading dimension stride can either be specified as a relative offset between the columns\nor as an absolute byte address of next buffer. The leading dimension stride is defined\ndifferently for transposed and non-transposed matrices. The leading dimension stride is defined\nas follows for matrices whose element types are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nNo-Swizzling: the stride from the first column to the second column\nof the 8x2 tile in the 128-bit element type normalized matrix.\nSwizzled layouts: not used, assumed to be 1.\nMN-Major\nInterleave: stride from the first 8 columns to the next 8 columns.\nSwizzled layouts: stride from the first (swizzle-byte-size/16) rows\nto the next (swizzle-byte-size/16) rows.\n9.7.16.3.1.2.\nAbsolute address mode for K dimension being 48B\n\nThe\ntcgen05.mma\ninstruction with\nK-dimension\nof 48B would overflow the 128B\nshared memory boundary if the data is packed contiguously.\nIn this case, the absolute address mode can be used to break up the data in the\nshared memory into two chunks such that both these chunks are laid out within\nthe aligned 128-byte address boundary.\nThe leading dimension absolute address can point to the second data chunk in the shared memory.\n9.7.16.3.1.2.1.\nRestrictions on the Leading Dimension Absolute Address Stride\n\nFollowing are the restrictions on the absolute address stride mode:\nOnly 128B swizzle (with 16B atomicity) is supported.\nOnly K-Major mode is supported. That is, the transpose bits(bits #15 and #16) in\nInstruction descriptor\nmust be 0.\nThe matrix base offset must be 0.\n9.7.16.3.2.\nStride Dimension Byte Offset\n\nThe stride dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The stride dimension byte offset is defined as follows for matrices whose element\ntypes are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nThe offset from the first 8 rows to the next 8 rows.\nMN-Major\nInterleave: offset from the first row to the next row.\nSwizzled layout: offset from the first 8 columns to the next 8\ncolumns\n9.7.16.3.3.\nCanonical Layouts\n\nIn terms of\nCuTe layouts\nthe canonical layout can be expressed as follows:\nMajor-\nness\nSwizzling mode\nCanonical Layout without swizzling\nSwizzling\non the previous column\nMN-\nmajor\nNo-swizzling or\nInterleaved\n((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nSwizzle<1, 4, 3>\n64B Swizzling\n((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nSwizzle<2, 4, 3>\n128B Swizzling\n((T,8,m),(8,k)):((1,T,LBO),(8T,SBO))\nSwizzle<3, 4, 3>\nK-\nmajor\nNo-swizzling or\nInterleaved\n((8,m),(T,2k)):((1T,SBO),(1,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((8,m),(T,2k)):((2T,SBO),(1,T))\nSwizzle<1, 4, 3>\n64B Swizzling\n((8,m),(T,2k)):((4T,SBO),(1,T))\nSwizzle<2, 4, 3>\n128B Swizzling\n((8,m),(T,2k)):((8T,SBO),(1,T))\nSwizzle<3, 4, 3>\nwhere\nT = 128 / sizeof-elements-in-bits\nT represents scale factor which normalizes matrix element types to 128-bits.\nm represents the number of repeating patterns across rows.\nk represents the number of repeating patterns across columns.\nExamples\nK-Major, no-swizzling and tf32 type:\nFigure 188\nFigure 188\nK major, no-swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))\nCanonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO (relative offset)\n64*sizeof(tf32)\nSBO\n32*sizeof(tf32)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nK-Major, 32B swizzling and tf32 type:\nFigure 189\nFigure 189\nK major, 32B swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))\nCanonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO (relative offset)\nNA\nSBO\n64*sizeof(tf32)\nEncoding of LBO in descriptor\n1 (assumed)\nEncoding of SBO in descriptor\n(SBO) >> 4 = 16\nMN-Major, no-swizzling and bf16 type:\nFigure 190\nFigure 190\nMN major, no-swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))\nCanonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n128*sizeof(bf16)\nSBO\n64*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nMN-Major, 32B swizzling and bf16 type:\nFigure 191\nFigure 191\nMN major, 32B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))\nCanonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n128*sizeof(bf16)\nSBO\n256*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 32\nMN-Major, 64B swizzling and bf16 type:\nFigure 192\nFigure 192\nMN major, 64B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))\nCanonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n256*sizeof(bf16)\nSBO\n512*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 32\nEncoding of SBO in descriptor\n(SBO) >> 4 = 64\n9.7.16.4.\nMatrix Descriptors\n\nThere are three kinds of matrix descriptors used by the\ntcgen05\nfamily of instructions.\n9.7.16.4.1.\nShared memory descriptor\n\nThe shared memory descriptor describes the properties of multiplicand matrix in shared\nmemory including its location in the shared memory of the current\nCTA\n. It is a 64-bit\nvalue contained in a register with the following layout:\nTable 40\nShared memory descriptor layout\n\nBit-field\nSize in bits\nDescription\n0-13\n14\nmatrix-descriptor-encode (Matrix start address)\n16-29\n14\nmatrix-descriptor-encode\n(\nLeading dimension byte offset relative\n)\nOR\nmatrix-descriptor-encode\n(\nLeading dimension byte address absolute\n)\n32-45\n14\nmatrix-descriptor-encode\n(\nStride dimension byte offset\n)\n46-48\n3\nFixed constant value of 0b001\n49-51\n3\nMatrix base offset\n52\n1\nLeading dimension stride mode:\n- 0: byte offset relative\n- 1: byte address absolute\n53-60\n8\nFixed constant value of 0xb00000000\n61-63\n3\nSpecifies the swizzling mode to be used:\n0. No swizzling\n1. 128-Byte with 32B atomic swizzling\n2. 128-Byte swizzling\n4. 64-Byte swizzling\n6. 32-Byte swizzling\nNote: Values 3, 5 and 7 are invalid\nwhere matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4\nThe value of base offset is 0 when the repeating pattern of the specified swizzling mode\nstarts as per shown in\nTable 41\n.\nTable 41\nStarting address of repeating pattern for various swizzling modes\n\nSwizzling mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the base offset must be a non-zero value, computed using the following formula:\nbase\noffset\n=\n(pattern\nstart\naddr\n>>\n0x7)\n&\n0x7\nThe following must be 16-byte aligned:\nMatrix start address\nLeading dimension byte offset\nStride dimension byte offset\n9.7.16.4.1.1.\nTarget ISA Note\n\nThe byte address mode for the leading dimension stride is supported on\nsm_103a\n.\n9.7.16.4.2.\nInstruction descriptor\n\nThe instruction descriptor describes the shapes, types and other details of all the matrices\nand the matrix-multiplication-and-accumulation operation. It is a 32-bit value in registers\nand the exact layout is dependent on the MMA-Kind:\nTable 42\nInstruction descriptor format for .kind::tf32, .kind::f16, .kind::f8f6f4 and .kind::i8\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::tf32\n.kind::f16\n.kind::f8f6f4\n.kind::i8\n0-1\n2\nSparsity selector\n,\nif Sparsity is enabled\n0-3\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nSaturate for integer types\n0 (NA)\nNo Saturate = 0\nSaturate = 1\n4-5\n2\ndtype (Matrix D type)\nF32 = 1\nF16 = 0\nF32 = 1\nS32 = 2\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nTF32 = 2\nF16 = 0\nBF16 = 1\nE4M3 = 0\nE5M2 = 1\nE2M3 = 3\nE3M2 = 4\nE2M1 = 5\nUnsigned 8b = 0\nSigned 8b = 1\n10-12\n3\nbtype (Matrix B type)\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\nNo Negate = 0\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\nTranspose = 1\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nReserved\n0\n24-28\n5\nM, Dimension of Matrix A\n(4 LSBs not included)\nM >> 4\n29\n1\nReserved\n0\n30-31\n2\nMaximum shift while attempting\nB matrix -reuse in\n.ws\nno shift = 0\nmaximum shift of 8 = 1\nmaximum shift of 16 = 2\nmaximum shift of 32 = 3\nTable 43\nInstruction descriptor format for .kind::mxf8f6f4\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::mxf8f6f4\n0-1\n2\nReserved\n0\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nReserved\n0\n4-5\n2\nMatrix B Scale Factor Data ID\n0-3\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nE4M3 = 0\nE5M2 = 1\nE2M3 = 3\nE3M2 = 4\nE2M1 = 5\n10-12\n3\nbtype (Matrix B type)\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\nTranspose = 1\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nScale Matrix Type, for both scale_A /\nscale_B\nUE8M0 = 1\n24-26\n3\nReserved\n0\n27-28\n2\nM, Dimension of Matrix A\n(7 LSBs not included)\nM >> 7\n29-30\n2\nMatrix A Scale Factor Data ID\n0-3\n31\n1\nReserved\n0\nTable 44\nInstruction descriptor format for .kind::mxf4 and .kind::mxf4nvf4\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::mxf4\n.kind::mxf4nvf4\n0-1\n2\nReserved\n0\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nReserved\n0\n4-5\n2\nMatrix B Scale Factor Data ID\n0 or 2\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nE2M1 = 1\n10-11\n2\nbtype (Matrix B type)\n12\n1\nReserved\n0\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nScale Matrix Type, for both scale_A /\nscale_B\nUE8M0 = 1\nUE4M3 = 0\n24-26\n3\nReserved\n0\n27-28\n2\nM, Dimension of Matrix A\n(7 LSBs not included)\nM >> 7\n29-30\n2\nMatrix A Scale Factor Data ID\n0 or 2\n31\n1\nK Dimension\n(Dense K=64 / Sparse K=128) = 0\n(Dense K=96) = 1\n9.7.16.4.3.\nZero-Column Mask Descriptor\n\nThe zero-column mask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix will have zero value for the MMA operation regardless of the values present in\nthe shared memory. The total size of the generated mask is N-bits.\nA 0-bit in the mask specifies that values of the corresponding column in matrix\nB\nshould\nbe used for the MMA operation. A 1-bit in the mask specifies 0s must be used for the entire\ncolumn for the MMA operation.\nThe zero-column mask descriptor is a 64-bit value in registers with the following layout:\nTable 45\nZero-Column Mask descriptor layout\n\nBits\nSize (bits)\nField Name\nDescription\n0-7\n8\nStart Count 0 (sc0)\nSpecifies the LSBs that must be skipped\nfor sub-mask mask-i\n8-15\n8\nStart Count 1 (sc1)\n16-23\n8\nStart Count 2 (sc2)\n24-31\n8\nStart Count 3 (sc3)\n32\n1\nFirst Span 0 (fs0)\nSpecifies the starting value for\nsub-mask mask-i\n33\n1\nFirst Span 1 (fs1)\n34\n1\nFirst Span 2 (fs2)\n35\n1\nFirst Span 3 (fs3)\n36-38\n3\nReserved\n39\n1\nNon-Zero Mask\nValue 0 indicates generated mask will have all 0s\nValue 1 indicates the mask has to be generated\n40-47\n8\nSkip Span\n(Count of consecutive columns where B matrix is used) - 1\n48-55\n8\nUse Span\n(Count of consecutive columns where 0s ar used) - 1\n56-61\n6\nColumn Shift\nShifts column by specified amount.\nThus allows MMA on non-0 starting column.\nMax shift amount = 16 for M=32\nMax shift amount = 32 otherwise\nThe zero-column mask is made up of one or more sub-mask depending on M, as shown in the table:\nM\nZero-Column Mask breakup\nSub-masks\nFirst Span used\nStart Column used\n128\nSingle sub-mask of size N-bits\nmask0\nfs0\nsc0\n64\nTwo sub-masks, each with size of N/2 bits\nmask0, mask1\nfs0, fs1\nsc0, sc1\n32\nFour sub-masks, each with size of N/4 bits\nmask0, mask1\nmask2, mask3\nfs0, fs1, fs2,\nfs3\nsc0, sc1, sc2,\nsc3\nThe following table shows the coverage of the sub-masks across N-dimension:\nSub-mask\nM\n128\n64\n32\nmask0\nColumns [0, N-1]\nColumns [0, N/2-1]\nColumns [0, N/4-1]\nmask1\n–\nColumns [N/2, N-1]\nColumns [N/4, N/2-1]\nmask2\n–\n–\nColumns [N/2, (N/4*3)-1]\nmask3\n–\n–\nColumns [(N/4*3), N-1]\nThe following examples shows zero-column mask descriptor and their corresponding mask generated:\nExample 1: M = 128\nInput zero-column mask descriptor:\nStart count\nFirst span\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{0, 0, 0, 0}\n{0, 0, 0, 0}\n0\n4\n3\n0\nOutput zero-column mask: 0x0.\nAs Non-Zero Mask field is 0, the mask is 0x0. All the columns of the matrix\nB\nwill be used\nfor the MMA operation.\nExample 2: M = 128\nInput zero-column mask descriptor:\nStart count\nFirst span\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{-, -, -, 0}\n{-, -, -, 0}\n1\n2\n3\n0\nOutput mask0: 0b … 111 0000 111 0000 (size = N)\nExample 3: M = 64\nInput zero-column mask descriptor:\nStart count\n{.., sc1, sc0}\nFirst span\n{.., fs1, fs0}\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{-, -, 0, 0}\n{-, -, 0, 1}\n1\n2\n3\n0\nOutput mask0: 0b …   111 0000 111 0000 111\nOutput masl1: 0b … 0000 111 0000 111 0000\nExample 4: M = 32\nInput zero-column mask descriptor:\nStart count\n{sc3, sc2, sc1, sc0}\nFirst span\n{fs3, fs2, fs1, fs0}\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{1, 2, 1, 0}\n{0, 0, 1, 1}\n1\n2\n3\n2\nOutput mask0: 0b … 0000 111 0000 111\nOutput mask1: 0b … 0000 111 0000 11\nOutput mask2: 0b … 111 0000 111 00\nOutput mask3: 0b … 111 0000 111 000\nIf N = 128 then\nB\nMatrix with columns from 2 to 129 will be used for the MMA operation,\ndue to the shift of 2.\n9.7.16.5.\nIssue Granularity\n\nEach of the\ntcgen05\noperation has different requirements for the number of\nthreads/warps that needs to issue them.\nThe following table lists the execution granularity requirements of each of the\ntcgen05\noperation:\nTable 46\nExecution granularity requirements for tcgen05 operations\n\ntcgen05 operation\n.cta_group\nIssue Granularity\n.\nmma\n,\n.\ncp\n,\n.\nshift\n,\n.\ncommit\n::1\nAn issue from a single thread in the current\nCTA would initiate the base operation.\n::2\nIssue from a single thread from the\nCTA-Pair\nwould initiate\nthe base operation.\nWhen the current CTA issues the operation, the peer\nCTA should be active and should not have exited.\n.\nalloc\n,\n.\ndealloc\n,\n.\nrelinquish_alloc_permit\n::1\nIssue from a single warp in the current CTA\nwould initiate the allocation management instruction.\n::2\nIssue from two warps, one in each of the current CTA\nand its\nPeer CTA\n, in order to\ncollectively perform the operation, i.e., the first\nwarp to perform the operation could block until the\nthe second warp in the\nPeer CTA\nalso performs the operation (see examples below).\n.\nld\n,\n.\nst\n,\n.\nwait\n::\n{\nld\n,\nst\n}\nN/A\nIssue from a warp in the current CTA can access only\n1/4 of the Tensor Memory of the current CTA. So, a\nwarpgroup is needed to access the entire Tensor Memory\nof the current CTA.\n.\nfence\n::*\nN/A\nA thread needs to fence all its accesses to the tensor\nmemory that it wants to order with other accesses to\nthe tensor memory from other threads.\nThe following example shows that:\nBefore attempting to deallocate Tensor Memory, it suffices to ensure that there are no concurrent\nTensor Memory accesses from the\nPeer CTA\n.\nWarps can immediately exit after deallocating Tensor Memory; no extra synchronization required.\nTable 47\nExample of deallocating Tensor Memory before CTA exit.\n\nCTA0 Warp\nCTA1 Warp\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\nThis example uses a cluster barrier for illustration purposes but in practice other\nsynchronization mechanisms are often used.\nThe following example illustrates a scenario in which the program exhibits non-deterministic\nbehavior due to incorrect synchronizaton because\n.dealloc\nmay or may not block:\nTable 48\nExample of non-determinstic hang due to incorrect synchronization\n\nCTA0 Warp\nCTA1 Warp\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\ntcgen05.dealloc.2cta.sync.aligned;\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\nexit;\n9.7.16.5.1.\nCTA Pair\n\nAny 2 CTAs within the cluster whose\n%cluster_ctarank\ndiffers by the last bit only\nis said to form a CTA pair.\nWithin a CTA pair, the CTA whose last bit in the\n%cluster_ctarank\nis:\n0 is termed the even numbered CTA within the CTA pair.\n1 is termed as the odd numbered CTA within the CTA pair.\nMost of the\ntcgen05\noperations can either execute at a single CTA level granularity OR\nat a CTA pair level granularity. When a\ntcgen05\noperation is performed at CTA pair\ngranularity, the Tensor Memory of both the CTAs within the CTA pair are accessed. The set\nof threads that need to issue the\ntcgen05\noperation is listed in the\nIssue Granularity\n.\n9.7.16.5.2.\nPeer CTA\n\nThe peer CTA of the odd CTA within the CTA pair is the even CTA in the same pair.\nSimilarly, the peer CTA of the even CTA within the CTA pair is the odd CTA in the same pair.\n9.7.16.6.\nMemory Consistency Model for 5th generation of TensorCore operations\n\nOrdering of\ntcgen05\ninstructions is described in terms of two key concepts:\nPipelined tcgen05 instructions\nSpecialized tcgen05-specific inter-thread synchronization mechanisms.\nThese concepts combine to form four canonical synchronization patterns, as described further below.\n9.7.16.6.1.\nAsynchronous Operations\n\nThe tcgen05 family of instructions are divided into 2 categories:\nAsynchronous instructions:\nThese\ntcgen05\noperations are not inherently ordered with respect to\nother\ntcgen05\noperations in the same thread (unless pipelined as mentioned below).\nSynchronous instructions:\nThese\ntcgen05\noperations are inherently ordered with respect to other\ntcgen05\noperations in the same order.\nThe Tensor Memory allocation related instructions that access shared memory maintain\nsame-address ordering with respect to non-\ntcgen05\ninstructions.\nThe following table lists the category of each of the\ntcgen05\ninstruction:\ntcgen05.* operation\nCategory\n.alloc\nSynchronous\ninstructions\n.dealloc\n.relinquish_alloc_permit\n.fence::*\n.wait::*\n.commit\n.mma\nAsynchronous\ninstructions\n.cp\n.shift\n.ld\n.st\n9.7.16.6.2.\nPipelined tcgen05 Instructions\n\nThe asynchronous\ntcgen05\noperations may execute and complete in a different order than they\nwere issued. However, some specific pairs of the asynchronous\ntcgen05\ninstructions form\ntcgen05\npipelines, where in the two asynchronous operations are guaranteed to execute in\nthe same order as the instructions that issued them. The specific pairings are as follows:\ntcgen05.mma.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N and accumulator and shape)\ntcgen05.copy.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.cp.4x256b.cta_group::N\n(same N)\ntcgen05.mma.cta_group::N\n->\ntcgen05.shift.cta_group::N\n(same N)\n9.7.16.6.2.1.\nImplicitly pipelined tcgen05 Instructions\n\nInstructions\ntcgen05.commit\nand\ntcgen05.wait\nare implicitly pipelined with respect\nto previously issued\ntcgen05.{mma,cp,shift}\nand\ntcgen05.{ld,st}\ninstructions\nrespectively that they track from the same thread.\n9.7.16.6.2.1.1.\nmbarrier based completion mechanism\n\nCompletion of the following instruction’s asynchronous operations is observed\nthrough the mbarrier based waiting mechanism:\ntcgen05.mma\ntcgen05.cp\ntcgen05.shift\ntcgen05.commit\nis used to track the completion of the above asynchronous instructions.\nFollowing are the implicitly pipelined\ntcgen05\ninstruction pairing that uses mbarrier\nbased completion mechanism:\ntcgen05.mma.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\ntcgen05.cp.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\n9.7.16.6.2.1.2.\ntcgen05.wait\ninstruction based completion mechanism\n\nCompletion of the following instruction’s asynchronous operations is observed through\ntcgen05.wait\nbased waiting mechanism:\ntcgen05.ld\ntcgen05.st\ntcgen05.wait::ld\nand\ntcgen05.wait::st\nis used to track the completion of the\ntcgen05.ld\nand\ntcgen05.st\nasynchronous instructions.\nFollowing are the implicitly pipelined\ntcgen05\ninstruction pairing that uses\ntcgen05.wait\nbased completion mechanism:\ntcgen05.ld\n->\ntcgen05.wait::ld\ntcgen05.st\n->\ntcgen05.wait::st\n9.7.16.6.3.\nSpecialized Inter-thread Synchronization for tcgen05 instructions\n\nThe\ntcgen05\ninstructions support a specialized inter-thread synchronization which are\noptimized for\ntcgen05\nfamily of instructions. The standard memory consistency model\nsynchronization mechanisms also apply to the\ntcgen05\nfamily of instructions.\nThe\nTensorCore 5th Generation Specialized Synchronization Operations\nsection contains the specialized inter-thread\nsynchronization for tcgen05 instructions.\nThe\ntcgen05.fence::before_thread_sync\nand\ntcgen05.fence::after_thread_sync\ncomposes\nwith execution ordering instructions, like morally strong\nld\n/\nst\n/\natom\ninstructions,\nmbarrier\ninstruction,\nbarrier\ninstructions and so on, to establish an ordering between\nthe\ntcgen05\noperations across threads. The asynchronous\ntcgen05\ninstructions that are\nordered across threads also form a\ntcgen05\npipeline.\nAn asynchronous\ntcgen05\noperation prior to a\ntcgen05.fence::before_thread_sync\nis ordered\nbefore all subsequent\ntcgen05\nand the execution ordering operations.\nAn asynchronous\ntcgen05\noperation subsequent to a\ntcgen05.fence::after_thread_sync\nis\nordered after all the prior\ntcgen05\nand the execution ordering operations.\n9.7.16.6.4.\nCanonical synchronization patterns\n\nUsing the above rules, the following are the five canonical synchronization patterns:\n9.7.16.6.4.1.\nPipelined instructions, same thread\n\nIn this pattern, no explicit ordering mechanism is needed and the ordering guarantee is\nprovided by the pipelined instruction pairing.\nExample:\ntcgen05.mma\ntcgen05.mma (same shape and accumulator)\nThe two instructions will be executed in program order.\n9.7.16.6.4.2.\nNon-pipelined instructions, same thread\n\nIn this pattern, explicit waiting mechanisms are used to wait for the completion of the\nasynchronous\ntcgen05\noperations.\nExample 1:\ntcgen05.st\ntcgen05.wait::st\ntcgen05.ld\ntcgen05.wait::st\nis used to wait for the completion of the prior asynchronous\ninstruction\ntcgen05.st\n.\nExample 2:\ntcgen05.mma [d], ...\ntcgen05.commit.mbarrier::arrive::one\nmbarrier.try_wait.relaxed.cluster (loop until successful)\ntcgen05.fence::after_thread_sync\ntcgen05.ld [d], ...\nFor the completion of the asynchronous\ntcgen05.mma\n,\ntcgen05.commit\nis used.\nAs\ntcgen05.ld\nis an asynchronous operation, the instruction\ntcgen05.fence::after_thread_sync\nis needed.\nNo explicit\ntcgen05.fence::before_thread_sync\nis needed as this is implicitly performed by\ntcgen05.commit\n. The combination of\ntcgen05.mma\nand\ntcgen05.commit\nforms a\nconceptual asynchronous pipeline and establishes execution ordering.\ntcgen05.mma [d], ...\ntcgen05.fence::before_thread_sync\nmbarrier::arrive\n9.7.16.6.4.3.\nPipelined instructions, different thread\n\nIn this pattern, no explicit waiting mechanism is needed but proper synchronization between threads is needed.\nExample:\nThread 0\nThread 1\ntcgen05\n.\ncp\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nmma\n9.7.16.6.4.4.\nNon-pipelined instructions, different thread\n\nIn this pattern, the producer threads that issue the asynchronous\ntcgen05\ninstructions\nmust explicitly wait for the instructions’ completion before synchronizing with the consumer threads.\nExample 1:\nThread 0\nThread 1\ntcgen05\n.\nld\ntcgen05\n.\nwait\n::\nld\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nmma\nExample 1:\nThread 0\nThread 1\ntcgen05\n.\nmma\ntcgen05\n.\ncommit\n.\nmbarrier\n::\narrive\n::\none\n[\nmbar\n]\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nmbar\n]\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nld\nThe synchronization mechanisms can also be composed with each other. For example:\nThread 0\nThread 1\ntcgen05\n.\nmma\ntcgen05\n.\ncommit\n.\nmbarrier\n::\narrive\n::\none\n[\nbar1\n]\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nbar1\n]\n// loop\n...\ntcgen05\n.\nfence\n::\nafter_thread_sync\n...\n// completion is guaranteed\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\n[\nbar2\n]\n// loop\n...\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nbar2\n]\n// loop\n...\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nld\n9.7.16.6.4.5.\nRegister dependencies, same thread\n\nFor\ntcgen05.ld\n, an intra-thread ordering through true register dependency will be respected\nregardless of the presence or absence of other forms of synchronization. This form of register\ndependency does not imply any other form of ordering. For example, a register dependency does\nnot imply that a dependee instruction’s memory accesses will be performed before a dependent\ninstruction’s memory accesses. To enforce such memory orderings and avoiding anti-dependency\nhazards around\ntcgen05.ld\n,\ntcgen05.wait::ld\nmust be used.\nExample:\ntcgen05.ld %r1, ...;\ntcgen05.mma ..., %r1, ...;\n9.7.16.6.5.\nShared Memory Accesses\n\nThe shared memory accesses by\ntcgen05.mma\nand\ntcgen05.cp\noperations are performed\nin the asynchronous proxy (async proxy).\nAccessing the same memory location across miltiple proxies needs a cross-proxy fence.\nFor the async proxy,\nfence.proxy.async\nshould be used to synchronize memory between\ngeneric proxy and the async proxy.\n9.7.16.7.\nTensor Memory Allocation and Management Instructions\n\n9.7.16.7.1.\nTensorcore 5th Generation Instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\nDynamic\nTensor Memory\nallocation management instructions\nSyntax\ntcgen05.alloc.cta_group.sync.aligned{.shared::cta}.b32  [dst], nCols;\n\ntcgen05.dealloc.cta_group.sync.aligned.b32              taddr, nCols;\n\ntcgen05.relinquish_alloc_permit.cta_group.sync.aligned;\n\n.cta_group = { .cta_group::1, .cta_group::2 }\nDescription\ntcgen05.alloc\nis a potentially blocking instruction which dynamically allocates\nthe specified number of columns in the\nTensor Memory\nand writes\nthe address of the allocated\nTensor Memory\ninto shared memory\nat the location specified by address operand dst. The\ntcgen05.alloc\nblocks if the\nrequested amount of\nTensor Memory\nis not available and unblocks\nas soon as the requested amount of\nTensor Memory\nbecomes\navailable for allocation.\ntcgen05.dealloc\nis a potentially blocking instruction which deallocates the\nTensor Memory\nspecified by the\nTensor Memory\naddress\ntaddr\n. The operand\ntaddr\nmust point to a previous\nTensor Memory\nallocation.\nIf\n.cta_group::2\nis specified,\nissuing warp and\npeer CTA\nwarp must synchronize\nTensor Memory\naccesses\nbefore attempting to collectively deallocate the\nTensor Memory\n, and\ntcgen05.dealloc\nmay block to collectively performs the deallocation with\nthe other\npeer CTA\n’s warp.\nAll of the Tensor Memory that was allocated using\ntcgen05.alloc\ninstruction in a kernel,\nmust be explicitly deallocated using\ntcgen05.dealloc\nbefore the kernel exits.\nThe unsigned 32-bit operand\nnCols\nspecify the number of columns to be allocated or\nde-allocated. The unit of allocation and de-allocation is 32 columns and all of lanes\nper column. The number of columns must be a power of 2. The operand\nnCols\nmust be\nwithin the range [32, 512]. The number of columns allocated should not increase between\nany two allocations in the execution order within the CTA. Operand\nnCols\nmust be\npower of 2.\nInstruction\ntcgen05.relinquish_alloc_permit\nspecifies that the CTA of the executing\nthread is relinquishing the right to allocate\nTensor Memory\n. So,\nit is illegal for a CTA to perform\ntcgen05.alloc\nafter any of its constituent threads\nexecute\ntcgen05.relinquish_alloc_permit\n.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the address specified by\ndst\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nQualifier\n.cta_group\nspecifies the number of CTAs involved in the allocation and\nde-allocation operation. When\n.cta_group::1\nis specified, one warp from the CTA must\nperform the allocation and de-allocation. When\n.cta_group::2\nis specified, one warp\nfrom each of the\npeer CTAs\nmust collectively perform the allocation and\nde-allocation. Refer to the\nIssue Granularity\nsection.\nWhen\n.cta_group::2\nis specified, the issuing warp must make sure that peer CTA is launched\nand its warps eventually participate in collective operations.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe mandatory\n.sync\nqualifier indicates that the instruction causes the executing thread\nto wait until all threads in the warp execute the same instruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame instruction. In conditionally executed code, the instruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior\nis undefined.\nThe behavior of the instruction is undefined if all the threads in the warp do not use the\nsame values of\nnCols\n, or if any thread in the warp has exited.\nThe store operation in\ntcgen05.alloc\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n// Example 1:\n\ntcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sMemAddr1], 32;\nld.shared.b32 taddr, [sMemAddr1];\n// use taddr ...\n// more allocations and its usages ...\ntcgen05.dealloc.cta_group::1.sync.aligned.b32  taddr, 32;\n// more deallocations ...\ntcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;\n\n// Example 2:\n\n// Following instructions are performed by current warp and the warp in the peer-CTA:\ntcgen05.alloc.cta_group::2.sync.aligned.shared::cta.b32 [sMemAddr2], 32;\nld.shared.b32 taddr, [sMemAddr2];\n// use taddr ...\n// more allocations and its usages ...\ntcgen05.dealloc.cta_group::2.sync.aligned.b32  taddr, 32;\n// more deallocations ...\ntcgen05.relinquish_alloc_permit.cta_group::2.sync.aligned;\n9.7.16.8.\nTensor Memory and Register Load/Store Instructions\n\nThe threads of the CTA can perform the loads and stores to the\nTensor Memory\nof the CTA and move data between registers and Tensor Memory. The loads and stores of data\ncan be performed in certain shapes as specified in the\nMatrix and Data Movement Shape\nsection.\n9.7.16.8.1.\nAccess restrictions\n\nNot all threads of the CTA can access the entire Tensor Memory via the\ntcgen05.ld\nand\ntcgen05.st\noperations.\nThe Tensor Memory of a CTA is divided into 4 equal chunks such that each warp of a warpgroup\nin the CTA can access a chunk of the Tensor Memory. All the columns of the Tensor Memory can\nbe accessed by all the four warps of a warpgroup. A lane of the Tensor Memory can be accessed\nby a single warp in the warpgroup. The following table describes the access restriction.\nID of the warp within the warpgroup\nAccessible Lanes\n0\n0-31\n1\n32-63\n2\n64-95\n3\n96-127\n9.7.16.8.2.\nPacking and Unpacking\n\nOptionally, the following pack and unpack operations can be performed during the load and store:\nPacking: two 16-bit chunks can be packed into a single 32-bit chunk in the register in\ntcgen05.ld\nUnpacking: a single 32-bit chunk in the register can be unpacked into two 16-bit chunks in\ntcgen05.st\nas shown in the\nFigure 193\n.\nFigure 193\nPack/Unpack operations for tcgen05 ld/st\n\n9.7.16.8.3.\nTensorcore 5th Generation Instructions:\ntcgen05.ld\n\ntcgen05.ld\nAsynchronous collective load from tensor memory into registers.\nSyntax\n// Base load instruction:\n\ntcgen05.ld.sync.aligned.shape1.num{.pack}.b32    r, [taddr];\n\ntcgen05.ld.sync.aligned.shape2.num{.pack}.b32    r, [taddr], immHalfSplitoff;\n\n.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }\n.shape2 = { .16x32bx2 }\n.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }\n.pack   = { .pack::16b }\n\n// Floating point type load along with reduction :\n\ntcgen05.ld.red.sync.aligned.shape3.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr];\n\ntcgen05.ld.red.sync.aligned.shape4.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr], immHalfSplitoff;\n\n// Integer type load along with reduction :\n\ntcgen05.ld.red.sync.aligned.shape3.num.redOp.type r, redval, [taddr];\n\ntcgen05.ld.red.sync.aligned.shape4.num.redOp.type r, redval, [taddr], immHalfSplitoff;\n\n.shape3 = { .32x32b   }\n.shape4 = { .16x32bx2 }\n.redOp  = { .min, .max }\n.type   = { .u32, .s32 }\nDescription\nInstruction\ntcgen05.ld\nasynchronously loads data from the\nTensor Memory\nat the location specified by the 32-bit address operand\ntaddr\ninto the destination\nregister\nr\n, collectively across all threads of the warps.\nAll the threads in the warp must specify the same value of\ntaddr\n, which must be the\nbase address of the collective load operation. Otherwise, the behavior is undefined.\nThe\n.shape\nqualifier and the\n.num\nqualifier together determines the total\ndimension of the data which is loaded from the\nTensor Memory\n. The\n.shape\nqualifier indicates the base dimension of data to be accessed as described in the\nData Movement Shape\n. The\n.num\nqualifier indicates\nthe repeat factor on the base dimension resulting in the total dimension of the data that\nis accessed.\nThe shape\n.16x32bx2\nperforms two accesses into Tensor Memory of the shape\n.16x32b\n.\nThe base address of the first access is specified by taddr and the base address of the\nsecond access is specified by\ntaddr+immHalfSplitoff\n, where\nimmHalfSplitoff\nis an\nimmediate argument.\nThe destination operand\nr\nis a brace-enclosed vector expression consisting of one\nor more 32-bit registers as per the value of\n.shape\nand\n.num\n. The size of the\nvector for various combinations of\n.num\nand\n.shape\nis shown in\nTable 49\n.\nTable 49\nVarious-combinations of .num and .shape\n\n.num\n.shape\n.16x32bx2 / .16x64b / .32x32b\n.16x128b\n.16x256b\n.x1\n1\n2\n4\n.x2\n2\n4\n8\n.x4\n4\n8\n16\n.x8\n8\n16\n32\n.x16\n16\n32\n64\n.x32\n32\n64\n128\n.x64\n64\n128\nNA\n.x128\n128\nNA\nNA\nThe qualifier\n.red\nspecifies that the reduction operation specified by\n.redOp\nis\nperformed on the data that is loaded across columns in each lane. The result of the\nreduction operation is written into the corresponding thread’s 32-bit destination register\noperand\nredVal\n. When\n.red\nqualifier is specified,\n.num\nmodifier must be at least\n.x2\n.\nThe optional qualifier\n.pack::16b\ncan be used to pack two 16-bit elements from adjacent\ncolumns into a single 32-bit element during the load as shown in the section\nPacking and Unpacking\n.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.ld\ncauses the executing thread\nto wait until all threads in the warp execute the same\ntcgen05.ld\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame\ntcgen05.ld\ninstruction. In conditionally executed code, a\ntcgen05.ld\ninstruction\nshould only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nThe behavior of\ntcgen05.ld\nis undefined if all threads do not use the same values of\ntaddr\n,\nor if any thread in the warp has exited.\nThe instruction\ntcgen05.ld\nis performed asynchronously and more details are specified in the\nsection\nMemory Consistency Model for 5th generation of TensorCore operations\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\ntcgen05.ld.red\nis introduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\ntcgen05.ld.red\nis supported on following architectures:\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_103f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\ntcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr1];\n\ntcgen05.ld.sync.aligned.16x128b.x4.b32    {r0, r1, r2, r3, r4, r5, r6, r7}, [taddr2];\n\ntcgen05.ld.red.sync.aligned.16x32bx2.x8.u32.max {r0, r1, r2, r3, r4, r5, r6, r7},\n                                                 redVal, [taddr3], 16;\n9.7.16.8.4.\nTensorcore 5th Generation Instructions:\ntcgen05.st\n\ntcgen05.st\nAsynchronous collective store to tensor memory from registers.\nSyntax\ntcgen05.st.sync.aligned.shape1.num{.unpack}.b32    [taddr], r;\n\ntcgen05.st.sync.aligned.shape2.num{.unpack}.b32    [taddr], immHalfSplitoff, r;\n\n.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }\n.shape2 = { .16x32bx2 }\n.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }\n.unpack = { .unpack::16b }\nDescription\nInstruction\ntcgen05.st\nasynchronously stores data from the source register\nr\ninto\nthe\nTensor Memory\nat the location specified by the 32-bit address operand\ntaddr\n,\ncollectively across all threads of the warps.\nAll the threads in the warp must specify the same value of\ntaddr\n, which must be the base\naddress of the collective store operation. Otherwise, the behavior is undefined.\nThe\n.shape\nqualifier and the\n.num\nqualifier together determines the total dimension\nof the data which is stored to the Tensor Memory. The\n.shape\nqualifier indicates the base\ndimension of data to be accessed as described in the\nData Movement Shape\n. The\n.num\nqualifier indicates the repeat factor on the base dimension resulting in the total dimension of\nthe data that is accessed.\nThe shape\n.16x32bx2\nperforms two accesses into Tensor Memory of the shape\n.16x32b\n.\nThe base address of the first access is specified by\ntaddr\nand the base address of the\nsecond access is specified by\ntaddr+immHalfSplitoff\n, where\nimmHalfSplitoff\nis an\nimmediate argument.\nThe source operand\nr\nis a brace-enclosed vector expression consisting of one or more 32-bit\nregisters as per the value of\n.shape\nand\n.num\n. The size of the vector for various\ncombinations of\n.num\nand\n.shape\nis shown in\nTable 50\n.\nTable 50\nVarious-combinations of .num and .shape\n\n.num\n.shape\n.16x32bx2 / .16x64b / .32x32b\n.16x128b\n.16x256b\n.x1\n1\n2\n4\n.x2\n2\n4\n8\n.x4\n4\n8\n16\n.x8\n8\n16\n32\n.x16\n16\n32\n64\n.x32\n32\n64\n128\n.x64\n64\n128\nNA\n.x128\n128\nNA\nNA\nThe optional qualifier\n.unpack::16b\ncan be used to unpack a 32-bit element in the\nregister into two 16-bit elements and store them in adjacent columns as shown in the\nsection\nPacking and Unpacking\n.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.st\ncauses the executing\nthread to wait until all threads in the warp execute the same\ntcgen05.st\ninstruction\nbefore resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute\nthe same\ntcgen05.st\ninstruction. In conditionally executed code, a\ntcgen05.st\ninstruction should only be used if it is known that all threads in the warp evaluate\nthe condition identically, otherwise behavior is undefined.\nThe behavior of\ntcgen05.st\nis undefined if all threads do not use the same values of\ntaddr\n, or if any thread in the warp has exited.\nThe instruction\ntcgen05.st\nis performed asynchronously and more details are specified\nin the section\nMemory Consistency Model for 5th generation of TensorCore operations\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\ntcgen05.st.sync.aligned.16x64b.x4.b32               [taddr0], {r0,  r1,  r2,  r3};\n\ntcgen05.st.sync.aligned.16x128b.x1.unpack::16b.b32  [taddr1], {r0,  r1};\n9.7.16.8.5.\nTensorcore 5th Generation Instructions:\ntcgen05.wait\n\ntcgen05.wait\nWaits for the completion of all prior asynchronous\ntcgen05.ld\n/\ntcgen05.st\ninstructions.\nSyntax\ntcgen05.wait_operation.sync.aligned;\n\n.wait_operation = { .wait::ld, .wait::st }\nDescription\nInstruction\ntcgen05.wait::st\ncauses the executing thread to block until all prior\ntcgen05.st\noperations issued by the executing thread have completed.\nInstruction\ntcgen05.wait::ld\ncauses the executing thread to block until all prior\ntcgen05.ld\noperations issued by the executing thread have completed.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.wait_operation\ncauses the\nexecuting thread to wait until all threads in the warp execute the same\ntcgen05.wait_operation\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame\ntcgen05.wait_operation\ninstruction.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nExample 1:\n\ntcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr0];\n\n// Prevents subsequent tcgen05.mma from racing ahead of the tcgen05.ld\n\ntcgen05.wait::ld.sync.aligned;\n\ntcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;\n\nExample 2:\n\ntcgen05.st.sync.aligned.32x32b.x2.b32     [taddr0], {r0, r1};\n\n// Prevents the write to taddr0 in tcgen05.mma from racing ahead of the tcgen05.st\n\ntcgen05.wait::st.sync.aligned;\n\ntcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;\n9.7.16.9.\nTensor Memory Data Movement Instructions\n\nData from the shared memory can be copied asynchronously to the\nTensor Memory\nusing the\nTensorcore 5th Generation Instructions: tcgen05.cp\noperation.\n9.7.16.9.1.\nOptional Decompression\n\nOptionally, during the copy, a vector of 4-bit and 6-bit\ncustom floating point types can be decompressed into 8-bit types.\n9.7.16.9.1.1.\nDecompression of 4-bit floating point to 8-bit type\n\nA contiguous set of 16 elements of 4-bits each followed by 8 bytes of padding can be converted\ninto 16 elements of 8-bits each as shown in\nFigure 194\n.\nFigure 194\nDecompression from 4-bit to 8-bit\n\nThe individual 4-bit to 8-bit decompression would look like as shown in\nFigure 195\n.\nFigure 195\nIndividual decompression from 4-bit to 8-bit\n\n9.7.16.9.1.2.\nDecompression of 6-bit floating point to 8-bit type\n\nA contiguous set of 16 elements of 6-bits each followed by 4 bytes of padding is\ndecompressed into 16 elements of 8-bits each as shown in\nFigure 196\n.\nFigure 196\nDecompression from 6-bit to 8-bit\n\nThe individual 6-bit to 8-bit decompression for types\nE3M2\nand\nE2M3\nis shown in\nFigure 197\nand\nFigure 198\nrespectively.\nFigure 197\nIndividual decompression from 6-bit to 8-bit for E3M2 type\n\nFigure 198\nIndividual decompression from 6-bit to 8-bit for E2M3 type\n\n9.7.16.9.2.\nTensorcore 5th Generation Instructions:\ntcgen05.cp\n\ntcgen05.cp\nInitiates an asynchronous copy operation from shared memory to the\nTensor Memory\n.\nSyntax\ntcgen05.cp.cta_group.shape{.multicast}{.dst_fmt.src_fmt} [taddr], s-desc;\n\n.cta_group = { .cta_group::1, .cta_group::2 }\n.src_fmt   = { .b6x16_p32 , .b4x16_p64 }\n.dst_fmt   = { .b8x16 }\n.shape     = { .128x256b, .4x256b, .128x128b, .64x128b**, .32x128b*** }\n.multicast = { .warpx2::02_13** , .warpx2::01_23**, .warpx4*** }\nDescription\nInstruction\ntcgen05.cp\ninitiates an asynchronous copy operation from shared memory to the\nlocation specified by the address operand\ntaddr\nin the\nTensor Memory\n.\nThe 64-bit register operand\ns-desc\nis the matrix descriptor which represents the source\nmatrix in the shared memory that needs to be copied. The format of the matrix descriptor is\ndescribed in\nMatrix Descriptors\n.\nThe\n.shape\nqualifier indicates the dimension of data to be copied as described in the\nData Movement Shape\n.\nQualifier\n.cta_group\nspecifies the number of CTAs whose\nTensor Memory\nis\naccessed when a single thread of a single CTA executes the\ntcgen05.cp\ninstruction.\nWhen\n.cta_group::1\nis specified, the data is copied into the\nTensor Memory\nof the current CTA. When\n.cta_group::2\nis specified, the data is copied into the\nTensor Memory\nof both the current and the\npeer CTAs\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nWhen the qualifiers\n.dst_fmt\nand\n.src_fmt\nare specified, the data is decompressed\nfrom the source format\n.src_fmt\nin the shared memory to the destination format\n.dst_fmt\nin\nTensor Memory\nby the copy operation. The details of source\nand the destination formats as specified in the section\nOptional Decompression\n.\nSome of the\n.shape\nqualifiers require certain\n.multicast\nqualifiers.\n.64x128b\nrequires\n.warpx2::02_13\nor\n.warpx2::01_23\n.32x128b\nrequires\n.warpx4\nWhen the\n.multicast\nqualifier is specified as either\n.warpx2::02_13\nor\n.warpx2::01_23\nthen the data being copied is multicasted into warp pairs and each\nwarp in the warp pair receive half of the data. Warp pairs are formed as follows:\n.warpx2::02_13\n: warps 0 and 2 form a pair; warps 1 and 3 form a pair.\n.warpx2::01_23\n: warps 0 and 1 form a pair; warps 2 and 3 form a pair.\nWhen the\n.multicast\nmodifier is specified as\n.warpx4\nthen the data being\ncopied is multicasted into all 4 warps.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\ntcgen05.cp.cta_group::1.128x256b                 [taddr0], sdesc0;\ntcgen05.cp.cta_group::2.128x128b.b8x16.b6x16_p32 [taddr1], sdesc1;\ntcgen05.cp.cta_group::1.64x128b.warpx2::02_13    [taddr2], sdesc2;\n9.7.16.9.3.\nTensorcore 5th Generation Instructions:\ntcgen05.shift\n\ntcgen05.shift\nAsynchronously shift down the rows of the matrix in the\nTensor Memory\nfor a warp.\nSyntax\ntcgen05.shift.cta_group.down  [taddr];\n\n.cta_group = { .cta_group::1, .cta_group::2 }\nDescription\nInstruction\ntcgen05.shift\nis an asynchronous instruction which initiates the shifting of 32-byte\nelements downwards across all the rows, except the last, by one row. The address operand\ntaddr\nspecifies the base address of the matrix in the\nTensor Memory\nwhose rows must\nbe down shifted.\nThe lane of the address operand\ntaddr\nmust be aligned to 32.\nQualifier\n.cta_group\nspecifies the number of CTAs whose\nTensor Memory\nis touched when a single thread of a single CTA executes the\ntcgen05.shift\ninstruction.\nWhen\n.cta_group::1\nis specified, the shift operation is performed in the\nTensor Memory\nof the current CTA. When\n.cta_group::2\nis specified,\nthe shift operation is performed in the\nTensor Memory\nof both the current and the\npeer CTAs\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_103a\nsm_110a\nExamples\ntcgen05.shift.down.cta_group::1 [taddr0];\ntcgen05.shift.down.cta_group::2 [taddr1];\n9.7.16.10.\nTensorCore 5th Generation Matrix Multiply and accumulate Operations\n\nThe 5\nth\ngeneration of TensorCore operations of shape\nMxNxK\nperform matrix\nmultiplication and accumulation of the form:\nD\n=\nA*B+D\nwhere:\nthe\nA\nmatrix has shape\nMxK\n, in either Tensor Memory or Shared Memory\nthe\nB\nmatrix has shape\nKxN\n, in Shared Memory of the current CTA and optionally in peer CTA\nthe\nD\nmatrix is of the shape\nMxN\n, in Tensor Memory\nOptionally an input predicate can be used to disable the input from the accumulator\nmatrix and the following operation can be performed as\nD\n=\nA*B\nThe matrix multiplication and accumulation operations are categorized into various kinds\nbased on input types and the throughput of the multiplication operation. The following shows the\ndifferent kinds of MMA operations that are supported:\nf16\n: supports\nf16\nand\nbf16\ninput types.\ntf32\n: supports\ntf32\ninput types.\nf8f6f4\n: supports all input combinations of\nf8\n,\nf6\nand\nf4\ntypes.\ni8\n: supports signed and unsigned 8-bit integer input types.\nmxf8f6f4\n/\nmxf4\n: supports mx-floating points input types.\nmxf4nvf4\n: supports\nmxf4\ntype and a custom NVIDIA floating-point\ntype for inputs where the type of the vector elements is 4 bits and requires a common\nscaling factor to form the complete floating-point type, similar to other mx-types.\nOptionally, the 5\nth\ngeneration of TensorCore MMAs support dense and sparse matrix\nA\n.\nSparse Matrices\ndescribes the details of the sparse matrices.\nSome of the MMA-kinds requires scaling of input matrices from memory to form the matrix\nA\nand matrix\nB\nbefore performing the MMA operation.\nBlock Scaling for tcgen05.mma.sync\ndescribes the details of the scaling of matrices.\nThe following table show the various matrices involved in the MMA operations and the memory in\nwhich they can reside:\nMatrix Type\nMemory\nA\nTensor Memory OR Shared Memory\nB\nShared Memory\nD\nTensor Memory\nSparse\nMeta\nData\nA-Scale\n/\nB-Scale\nA sequence of MMA instructions may reuse the same\nA\nmatrix with a sequence of\nB\nmatrices or may reuse the same\nB\nmatrix with a sequence of\nA\nmatrices.\nIn these patterns the TensorCore may be able to laod the unchanged matrix once and reuse\nit through the sequence without multiple reloads. The\nA\nor\nB\nmatrices are loaded\ninto a TensorCore collector buffer (i.e., special cache).\nAn MMA instruction has an optional\ncollector\nqualifier to specify when an\nA\nor\nB\nmatrix is new to the sequence and should be loaded, unchanged within the sequence\nand should be reused, or the last use in the sequence and should be discarded.\nThe\ncollector\nqualifier is used to give the TensorCore permission to reuse a previously\nloaded\nA\nor\nB\nmatrix; however reuse is opportunistic in that the TensorCore may\nreload a matrix even when it has permission to reuse that matrix. Thus, the source\nmemory of an\nA\nor\nB\nmatrix must not be modified while the MMA instruction using those\nmatrices has not completed - regardless of\ncollector\nqualifier permissions.\nThe 5\nth\ngeneration of TensorCore MMAs can be used for general matrix multiplication OR for\nconvolution operations. In case of convolutions, the activations can be stored in either\nmatrix\nA\nor matrix\nB\nwhile the weights will be stored in the other matrix.\nActivation\nMatrix\nWeights\nMatrix\nName of the\nop\nInstruction\nName\nCollector Buffer Applicability\nA\nB\nActivation\nStationary\n(default\ntcgen05.mma\n)\nCollector buffer is applicable on matrix\nA\nB\nA\nWeights\nStationary\n.ws\nCollector buffer is applicable on matrix\nB\n9.7.16.10.1.\nTranspose and Negate operations\n\nThe matrices\nA\nand\nB\ncan be transposed by specifying the Tranpose\nA\nMatrix\nand Transpose\nB\nMatrix bits in the instruction descriptor respectively.\nThe elements of the matrices\nA\nand\nB\ncan be negated by specifying the Negate\nA\nMatrix and Negate\nB\nMatrix bits in the instruction descriptor respectively.\nThe support for Transpose and Negate operation for various MMA-Kind are shown in\nTable 51\n.\nTable 51\nTranspose and Negate operation for various MMA-Kind\n\nMMA-Kind\nIs Transpose A/B supported\nIs Negate A/B supported\n.kind::tf32\nYes\nYes\n.kind::f16\nYes\nYes\n.kind::f8f6f4\nYes\nYes\n.kind::mxf8f6f4\nYes\nYes\n.kind::i8\nYes\nNo\n.kind::mxf4\nNo\nYes\n.kind::mxf4nvf4\nNo\nYes\nFor\n.kind::tf32\n, the transpose operations on matrices\nA\nand\nB\nare supported\nonly with 128B swizzling mode with 32B swizzle-atomicity.\nFor all other MMA-Kinds, the transpose operations on matrices\nA\nand\nB\nare not supported\non 128B swizzling mode with 32B swizzle-atomicity.\nTable 52\nshows the valid combinations of N shape with\n.cta_group\nqualifier for 8bit transpose B.\nTable 52\nVarious combinations of N shape with .cta_group qualifier for 8bit transpose B\n\n.cta_group\nN shape\n1\n16 <= N <= 256, step 16\n2\n32 <= N <= 256, step 32\n9.7.16.10.2.\nMatrix Layout Organization\n\nTable 53\ndescribes the major-ness used for different matrices.\nTable 53\nMajor-ness for different matrices\n\nMatrix\nResiding in Memory\nDefault Major-ness\nD\nTensor Memory\nRow-Major\nA\nTensor Memory\nShared Memory\nDepends on swizzling mode.\nRefer\nShared Memory Layout and Swizzling\nB\nShared Memory\n9.7.16.10.3.\nValid Combinations of Type-Size, Major-ness and Swizzling\n\nTable 54\nValid Combinations of Type-Size, Major-ness and Swizzling\n\nType-Size\nMajor-ness\nMatrix\nSupported Swizzle\n4-bit,\n6-bit,\n8-bit,\n16-bit,\n32-bit\nRow\nA\nAll swizzling modes\nColumn\nB\n8-bit\n16-bit\nColumn (transpose)\nA\nAll except 128B\nswizzling with 32B\natomicity\nRow (transpose)\nB\n32-bit\nColumn (transpose)\nA\nOnly 128B swizzling\nwith 32B atomicity\nRow (transpose)\nB\n9.7.16.10.4.\nPacking formats of elements in Tensor and Shared memory\n\n9.7.16.10.4.1.\nPacking format for matrix D in Tensor Memory\n\nThe sub-word elements of matrix\nD\nare expected not to be packed within a 32-bit Tensor Memory word.\nFor example, if the type of elements of the matrix\nD\nis 16 bits then a Tensor Memory word\nwould contain a single 16-bit element in its lower 16 bits.\n9.7.16.10.4.2.\nPacking format for matrix A and B\n\nThe 6-bit and 4-bit floating point types have different packing format requirements for\ndifferent MMA kinds in both Tensor memory and Shared memory. The requirements are as follows.\n9.7.16.10.4.3.\nPacking format used for matrix A by\n.kind::mxf8f6f4\nin Tensor Memory\n\nThe individual 4-bit and the 6-bit floating point type elements must be packed in an 8-bit container\nin Tensor memory as shown below. The 8-bit containers must be contiguously packed in a 32-bit Tensor\nMemory word. For example, if the type of elements of the matrix\nA\nis 6 bits then 4 consecutive\nA\nelements should be packed in one 32-bit Tensor Memory word.\n4-bit packing format as shown in\nFigure 199\nFigure 199\n4-bit packing format with type E2M1\n\n6-bit packing format\nType E3M2 as shown in\nFigure 200\nFigure 200\n6-bit packing format with type E3M2\n\nType E2M3 as shown in\nFigure 201\nFigure 201\n6-bit packing format with type E2M3\n\n9.7.16.10.4.4.\nPacking format used for matrix A and B by\n.kind::mxf8f6f4\nin Shared Memory\n\nThe 4-bit and 6-bit floating point elements in shared memory must be contiguously packed along\nwith padding as follows.\n4-bit packing format as shown in\nFigure 202\nFigure 202\n4-bit packing format\n\n6-bit packing format as shown in\nFigure 203\nFigure 203\n6-bit packing format\n\n9.7.16.10.4.5.\nPacking format used for matrix A by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Tensor Memory\n\nTwo 4-bit floating point type elements must be packed in an 8-bit container in Tensor memory as\nshown in\nFigure 204\nfor\nmxf4\n.\nFigure 204\n4-bit packing format with type E2M1\n\n9.7.16.10.4.6.\nPacking format used for matrix A and B by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Shared Memory\n\nThe packing format for 4-bit floating point elements in shared memory is to pack two 4-bit\nelements in a 8-bit container, with no padding.\n9.7.16.10.5.\nData Path Layout Organization\n\nDifferent MMA variants access the tensor memory with different layout organization.\nThe following table lists the various layouts:\nM\ncta_group\nA-Sparsity\nIs .ws mode\nDatapath organization\nLayout ID\nTensor Memory Datapath Lane Alignment\n32\n::1\nEither\nYes\n1x4\nLayout G\n0\n64\n::1\nEither\nYes\n2x3\nLayout E\n0\n64\n::1\nEither\nNo\n4x1 (1/2 datapath utilized)\nLayout F\n0 or 16\n128\n::1\nEither\nEither\n4x1\nLayout D\n0\n128\n::2\nDense\nN/A\n2x2\nLayout B\n0\n128\n::2\nSparse\nN/A\n4x1 (1/2 datapath utilized)\nLayout C\n0 or 16\n256\n::2\nEither\nN/A\n4x1\nLayout A\n0\nThe layouts which utilize only half the datapath lanes, i.e.,\nLayout F\nand\nLayout C\n, must use the same Tensor Memory\nlane alignment across matrices\nA\n,\nD\nand the sparsity metadata matrix.\nThe following shows the warps that can access the Tensor Memory regions via\ntcgen05.ld\n/\ntcgen05.st\nalong with the addresses for various Tensor Memory Layouts.\n9.7.16.10.5.1.\nLayout A (M = 256)\n\nLayout organization for M = 256 is shown in\nFigure 205\n.\nFigure 205\nLayout organization for M = 256\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 206\nFigure 206\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.2.\nLayout B (M = 128 + cta-group::2 + Dense A matrix)\n\nLayout organization for M = 128 + .cta_group::2 + Dense A matrix is shown in\nFigure 207\n.\nFigure 207\nLayout organization for M = 128 + .cta_group::2 + Dense A matrix\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 208\nFigure 208\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.3.\nLayout C (M = 128 + cta-group::2 + Sparse A matrix)\n\nLayout organization for M = 128 + .cta_group::2 + Sparse A matrix is shown in\nFigure 209\n.\nFigure 209\nLayout organization for M = 128 + .cta_group::2 + Sparse A matrix\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 210\nFigure 210\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.4.\nLayout D (M = 128 + cta-group::1)\n\nLayout organization for M = 128 + .cta_group::1 is shown in\nFigure 211\n.\nFigure 211\nLayout organization for M = 128 + .cta_group::1\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 212\nFigure 212\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.5.\nLayout E (M = 64 + .ws mode)\n\nLayout organization for M = 64 + .ws mode is shown in\nFigure 213\n.\nFigure 213\nLayout organization for M = 64 + .ws mode\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 214\nFigure 214\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.6.\nLayout F (M = 64 + non .ws mode)\n\nLayout organization for M = 64 + non .ws mode is shown in\nFigure 215\n.\nFigure 215\nLayout organization for M = 64 + non .ws mode\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 216\nFigure 216\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.7.\nLayout G (M = 32)\n\nLayout organization for M = 32 is shown in\nFigure 217\n.\nFigure 217\nLayout organization for M = 32\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 218\nFigure 218\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.6.\nShared Memory Layout and Swizzling\n\nIf the bit\nTranspose\nA\nMatrix\n/\nTranspose\nB\nMatrix\nin the\nInstruction descriptor\nis 0, then\nK-major\nis\nused for matrix\nA\n/\nB\nrespectively. If the bit\nTranspose\nA\nMatrix\nin the\nInstruction descriptor\nis 1 then\nM-major\nis\nused for matrix\nA\n. If the bit\nTranspose\nB\nMatrix\nin the\nInstruction descriptor\nis 1, then\nN-major\nis\nused for matrix\nB\n.\nIn a column-major default BLAS library such as cuBLAS, the matrices\nA\nand\nB\nwith and\nwithout transpose can be classified as either\nK-Major\nor\nM-or-N-Major\nas shown in the\nfollowing table:\nNon-Transposed\nTransposed\nA\nK-major\nM-major\nB\nK-major\nN-major\nTo avoid confusion with\nA\n,\nB\n,\nrow-major\n,\ncol-major\n,\ntranspose\n, and\nnon-transpose\n, we will use\nMN-Major\nand\nK-Major\nthroughout this section.\nThe matrices in the shared memory are made up of one or more “swizzle layout atom”.\nThe exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,\nand the leading dimension. The layout of the swizzle are shown in\nTable 55\nTable 55\nLayout for swizzle atoms\n\nSwizzling mode and\nSwizzle-Atomicity\nLeading\nDimension\nSwizzle atom layout\n(128b element)\n128B Swizzling with\n32B atomicity\nM/N\n8x4\n–\n–\n128B Swizzling with\n16B atomicity\nM/N\n8x8\nK\n8x8\n64B Swizzling Mode\nM/N\n4x8\nK\n8x4\n32B Swizzling Mode\nM/N\n2x8\nK\n8x2\nNone\nM/N\n1x8\nK\n8x1\nThe above shapes are for elements of size 128 bits. For smaller element sizes, the same shapes\nwould get multiplied along the leading dimension by a factor of\n128\n/\nsizeof_bits(Element)\n.\nFor example, 128B MN major swizzle atom would have a shape of (8*(128/32))x8 = 32x8 for\ntf32 tensor core inputs.\nSome example Layouts of\nMxK\nor\nKxN\nmatrices with various swizzling modes, and are in units\nof 128b elements as shown by each colored cell as shown in\nFigure 219\n,\nFigure 220\n,\nFigure 221\n,\nFigure 222\n,\nFigure 223\n,\nFigure 224\n,\nFigure 225\n,\nFigure 226\n,\nFigure 227\n.\nFigure 219\nMN major 128B swizzling with 32B atomicity\n\nFigure 220\nMN major 128B swizzling\n\nFigure 221\nK major 128B swizzling\n\nFigure 222\nMN major 64B swizzling\n\nFigure 223\nK major 64B swizzling\n\nFigure 224\nMN major 32B swizzling\n\nFigure 225\nK major 32B swizzling\n\nFigure 226\nMN major no-swizzling mode\n\nFigure 227\nK major no-swizzling mode\n\nFollowing are some of the examples of the 128B swizzling layout for\ntf32\nelement type.\nK-Major:\nFigure 228\nFigure 228\nK major\n\nMN-Major:\nFigure 229\nFigure 229\nMN major\n\n9.7.16.10.7.\nBlock Scaling for\ntcgen05.mma.sync\n\nThe\ntcgen05.mma\ninstructions with the following\n.kind\nqualifier:\n.kind::mxf8f6f4\n.kind::mxf4\n.kind::mxf4nvf4\nperform matrix multiplication with block scaling. This operation has the following form:\n(A\n*\nscale_A)\n*\n(B\n*\nscale_B)\n+\nD\nwhere\nscale_A\nand\nscale_B\nare matrices residing in\nTensor Memory\n.\nFor a\nscale_A\nmatrix of shape\nM x SFA_N\n, each row of matrix\nA\nis divided into\nSFA_N\nnumber of chunks and each chunk of a row is multiplied with the corresponding\nelement in the\nSF_A\nof the same row.\nSimilarly, for a\nscale_B\nmatrix of shape\nSFB_M x N\n, each column of matrix\nB\nis\ndivided into the\nSFB_M\nnumber of chunks and each chunk of a column is multiplied with\nthe corresponding element in the\nSF_B\nof the same column.\nScale factors for\nA\nand\nB\nmatrices need to be duplicated to all 32 lane partitions\nof tensor memory.\nFigure 230\nshows an example of\ntcgen05.mma\nwith block scaling of\nscale_vec::2X\n.\nFigure 230\ntcgen05.mma\nwith block scaling of\nscale_vec::2X\n\n9.7.16.10.7.1.\nValid combinations of scale_vectorsize with types and MMA-Kind\n\nThe shape of\nscale_A\nand\nscale_B\nmatrices depend on the\n.scale_vectorsize\nas shown in\nTable 56\n.\nTable 56\nValid combinations of scale_vectorsize and shapes\n\n.scale_vectorsize\n.kind::*\nK\nShape of scale_A\nShape of scale_B\n.scale_vec::1X\n.kind::mxf8f6f4\nAll supported values of K\nM x 1\n1 x N\n.scale_vec::2X\n.kind::mxf4\n,\n.kind::mxf4nvf4\nAll supported values of K\nM x 2\n2 x N\n.scale_vec::4X\n.kind::mxf4nvf4\nAll supported values of K\nM x 4\n4 x N\n.block16\n.kind::mxf4nvf4\nK = 96\nM x 6\n6 x N\nAll supported values of K\nexcept 96\nM x 4\n4 x N\n.block32\n.kind::mxf4\n,\n.kind::mxf4nvf4\nK = 96\nM x 3\n3 x N\nAll supported values of K\nexcept 96\nM x 2\n2 x N\n.kind::mxf8f6f4\nAll supported values of K\nM x 1\n1 x N\nThe valid combination of the exact element types and the\n.scale_vectorsize\nare listed in\nTable 57\n.\nTable 57\nValid combinations of scale_vectorsize with types and MMA-Kind\n\n.kind::*\nElement Data Type\nScale Data Type\n.scale_vectorsize\n.kind::mxf8f6f4\nE4M3, E5M2, E2M3\nE3M2, E2M1\nUE8M0\n.scale_vec::1X\n/\n.block32\n.kind::mxf4\nE2M1\nUE8M0\n.scale_vec::2X\n/\n.block32\n.kind::mxf4nvf4\nE2M1\nUE8M0\n.scale_vec::2X\n/\n.block32\n,\n.scale_vec::4X\n/\n.block16\nE2M1\nUE4M3\n.scale_vec::4X\n/\n.block16\nNew\n.blockN\nqualifiers are aliases for\n.scale_vec::NX\nqualifiers as:\n.block32\nis alias for\n.scale_vec::1X\nor\n.scale_vec::2X\nbased on\n.kind\nand K dimension\n.block16\nis alias for\n.scale_vec::4X\n9.7.16.10.7.2.\nScale Factor A ID\n\nThe value of the scale factor\nA\nID\nselects the sub-columns in the Tensor Memory to\nform the scale factor\nA\nmatrix, which is used to scale the matrix\nA\n.\nThe following shows the scale factor matrix layout for various scale vector sizes:\n9.7.16.10.7.2.1.\nLayout of the Scale Factor A Matrix for scale_vec::1X/block32 with K=32/K=64\n\nThere is one scale factor per row of the\nA\nmatrix with block size as 32 and the scale factor must be provided in\n1-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies the byte offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 231\nshows which sub-columns get selected for\ndifferent values of\nSFA_ID\n.\nFigure 231\nLayout of scale factor A matrix with scale_vec::1X/block32 with K=32/K=64\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFA_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns,\nrespectively.\n9.7.16.10.7.2.2.\nLayout of the Scale Factor A Matrix for scale_vec::2X/block32 with K=64/K=128\n\nThere are two scale factors per row of the\nA\nmatrix with block size as 32 and the scale factor must be provided in\n2-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies the half word offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 232\nshows which sub-columns gets selected for different\nvalues of\nSFA_ID\n.\nFigure 232\nLayout of scale factor A matrix with scale_vec::2X/block32 with K=64/K=128\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFA_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.2.3.\nLayout of the Scale Factor A Matrix for scale_vec::4X/block16 with K=64/K=128\n\nThere are four scale factors per row of the\nA\nmatrix with block size as 16 and the scale factor must be provided in\n4-byte aligned sub-column of the Tensor Memory. The\nSFA_ID\nvalue must be 0 and this specifies\nthat all of the columns (in green) will be used for the scale factor matrix.\nFigure 233\nshows which sub-columns gets selected for different\nvalues of\nSFA_ID\n.\nFigure 233\nLayout of scale factor A matrix with scale_vec::4X/block16 with K=64/K=128\n\n9.7.16.10.7.2.4.\nLayout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n\nThere are three scale factors per row of the\nA\nmatrix with block size as 32 and the scale\nfactor must be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies\nthe byte offset in the Tensor Memory word that must be used for the scale factor matrix.\nFigure 234\n,\nFigure 235\n,\nFigure 236\nand\nFigure 237\nshow which sub-columns get selected for different values of\nSFA_ID\n.\nFigure 234\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=00\n\nFigure 235\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=01\n\nFigure 236\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=10\n\nFigure 237\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=11\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFA_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns,\nrespectively.\n9.7.16.10.7.2.5.\nLayout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n\nThere are six scale factors per row of the\nA\nmatrix with block size as 16 and the scale\nfactor must be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies\nthe byte offset in the Tensor Memory word that must be used for the scale factor matrix.\nFigure 238\nand\nFigure 239\nshow which sub-columns get selected for different values of\nSFA_ID\n.\nFigure 238\nLayout of scale factor A matrix with block16 with K=96 with SFA_ID=00\n\nFigure 239\nLayout of scale factor A matrix with block16 with K=96 with SFA_ID=10\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFA_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.3.\nScale Factor B ID\n\nThe value of the scale factor\nB\nID\nselects the sub-columns in the Tensor Memory to\nform the scale factor\nB\nmatrix, which is used to scale the matrix\nB\n.\nThe following shows the scale factor matrix layout for various scale vector sizes:\n9.7.16.10.7.3.1.\nLayout of the Scale Factor B Matrix for scale_vec::1X/block32 with K=32/K=64\n\nThere is one scale factor per row of the\nB\nmatrix with block size as 32 and the scale factor must be provided in\n1-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 240\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 240\nLayout of scale factor B matrix with scale_vec::1X/block32 with K=32/K=64\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFB_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.\n9.7.16.10.7.3.2.\nLayout of the Scale Factor B Matrix for scale_vec::2X/block32 with K=64/K=128\n\nThere are two scale factors per row of the\nB\nmatrix with block size as 32 and the scale factor must be provided in\n2-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the half word offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 241\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 241\nLayout of scale factor B matrix with scale_vec::2X/block32 with K=64/K=128\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFB_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.3.3.\nLayout of the Scale Factor B Matrix for scale_vec::4X/block16 with K=64/K=128\n\nThere are four scale factors per row of the\nB\nmatrix with block size as 16 and the scale factor must be provided in\n4-byte aligned sub-column of the Tensor Memory. The\nSFB_ID\nvalue must be 0 and this specifies\nthat all of the columns (in green) will be used for the scale factor matrix.\nFigure 242\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 242\nLayout of scale factor B matrix with scale_vec::4X/block16 with K=64/K=128\n\n9.7.16.10.7.3.4.\nLayout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n\nThere are three scale factors per row of the\nB\nmatrix with block size as 32 and the scale factor\nmust be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte\noffset in the Tensor Memory word that must be used for the scale factor matrix.\nFor N<=128,\nFigure 243\n,\nFigure 244\n,\nFigure 245\nand\nFigure 246\nshow which\nsub-columns get selected for different values of\nSFB_ID\n.\nFigure 243\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=00\n\nFigure 244\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=01\n\nFigure 245\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=10\n\nFigure 246\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=11\n\nFor N>128,\nFigure 247\n,\nFigure 248\n,\nFigure 249\n,\nFigure 250\n,\nFigure 251\nand\nFigure 252\nshow which\nsub-columns get selected for different values of\nSFB_ID\n.\nFigure 247\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=00\n\nFigure 248\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=01\n\nFigure 249\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10\n\nFigure 250\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10\n\nFigure 251\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11\n\nFigure 252\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the\nscale factor matrix. Similarly,\nSFB_ID\nvalues of 1, 2 and 3 would select the blue,\nyellow, and red columns, respectively.\n9.7.16.10.7.3.5.\nLayout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n\nThere are six scale factors per row of the\nB\nmatrix with block size as 16 and the scale factor\nmust be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte\noffset in the Tensor Memory word that must be used for the scale factor matrix.\nFor N<=128,\nFigure 253\nand\nFigure 254\nshow which sub-columns\nget selected for different values of\nSFB_ID\n.\nFigure 253\nLayout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=00\n\nFigure 254\nLayout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=10\n\nFor N>128,\nFigure 255\n,\nFigure 256\n,\nFigure 257\nand\nFigure 258\nshow which sub-columns\nget selected for different values of\nSFB_ID\n.\nFigure 255\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00\n\nFigure 256\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00\n\nFigure 257\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10\n\nFigure 258\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the\nscale factor matrix. Similarly, if\nSFB_ID\nis 2, then all of the blue columns are\nselected to form the scale factor matrix.\n9.7.16.10.8.\nSparse Matrices\n\nThis instruction\ntcgen05.mma.sp\ncan be used when the matrix\nA\nis a structured\nsparse matrix with 50% zeros in each row distributed as per its sparse granularity.\nIn a\nMxNxK\nsparse\ntcgen05.mma.sp\noperation, the matrix\nA\nof shape\nMxK\nis\nstored in a packed form as\nMx(K/2)\nin memory. For each\nK-wide\nrow of matrix\nA\n,\n50% of elements are zeros and the remaining\nK/2\nnon-zero elements are stored in\nmemory. The metadata specifies the mapping of the\nK/2\nnon-zero elements to the\nK\nelements before performing the MMA operation.\nGranularity of sparse matrix\nA\nis defined as the ratio of the number of non-zero\nelements in a sub-chunk of the matrix row to the total number of elements in that\nsub-chunk where the size of the sub-chunk is shape-specific. The following table lists\nthe granularity of different\ntcgen05.mma.sp\nvariants:\n.kind of tcgen05.mma\nSparse Granularity\n.kind::tf32\n1:2\n.kind::f16\n2:4\n.kind::f8f6f4\n.kind::mxf8f6f4\n.kind::i8\n.kind::mxf4\n4:8 (in pairs)\n9.7.16.10.8.1.\nSparse\ntcgen05.mma.sp\nwith\n.kind::tf32\n\nFor\n.kind::tf32\n, matrix\nA\nis structured sparse at a granularity of\n1:2\n.\nIn other words, each chunk of two adjacent elements in a row of matrix\nA\nhas one\nzero and one non-zero element. Only the non-zero element is stored in memory and the\n4-bit index in the metadata indicates the position of the non-zero element in the\ntwo-wide chunk. The only meaningful values of the index are:\n0b1110\n0b0100\nRest of the values result in undefined behavior.\nFigure 259\nSparse tcgen05.mma metadata example for tf32 kind\n\n9.7.16.10.8.2.\nSparse\ntcgen05.mma.sp\nwith\n.kind::f16\n,\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n\nFor the following\n.kind\nvariants of\ntcgen05.mma\n:\n.kind::f16\n.kind::f8f8f4\n.kind::mxf8f6f4\n.kind::i8\nmatrix\nA\nis structured sparse at a granularity of\n2:4\n. In other words, each chunk\nof four adjacent elements in a row of matrix\nA\nhas two zero and two non-zero elements.\nOnly the non-zero elements are stored in memory and the two 2-bit indices in the metadata\nindicates the position of the two non-zero elements in the four-wide chunk. The only\nmeaningful values of the index are:\n0b0100\n0b1000\n0b1100\n0b1001\n0b1101\n0b0110\n0b1110\nFigure 260\nSparse tcgen05.mma metadata example for f16/f8f6f4/mxf8f6f4 kind\n\n9.7.16.10.8.3.\nSparse\ntcgen05.mma.sp\nwith\n.kind::mxf4\nand\n.kind::mxf4nvf4\n\nFor\n.kind::mxf4\nand\n.kind::mxf4nvf4\n, matrix\nA\nis pair-wise structured\nsparse at a granularity of\n4:8\n. In other words, each chunk of eight adjacent\nelements in a row of matrix\nA\nhas four zero and four non-zero elements. The\nzero and non-zero elements are clustered in sub-chunks of two elements each within\nthe eight-wide chunk, so each two-wide sub-chunk within the eight-wide chunk must be\nall zeros or all non-zeros. Only the four non-zero elements are stored in memory and\nthe two 2-bit indices in the metadata indicates the position of the two two-wide\nsub-chunks with non-zero values in the eight-wide chunk of a row of matrix\nA\n.\nThe only meaningful values of the index are:\n0b0100\n0b1000\n0b1100\n0b1001\n0b1101\n0b0110\n0b1110\nRest of the values result in undefined behavior.\nFigure 261\nSparse tcgen05.mma metadata example for mxf4 kind\n\n9.7.16.10.8.4.\nSparsity selector\n\nThe value of the sparsity selector selects the sub-columns in the Tensor Memory\nto form the sparsity metadata matrix, which is used with matrix\nA\nto form the\nmultiplicand matrix.\nThe following shows the sparse metadata matrix layout in Tensor Memory for various MMA variants:\n9.7.16.10.8.4.1.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f16\n\nFigure 262\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 262\nSparsity Metadata Layout for M = 64 for\n.kind::f16\n\n9.7.16.10.8.4.2.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f16\n\nFigure 263\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 263\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::f16\n\n9.7.16.10.8.4.3.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::tf32\n\nFigure 264\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 264\nSparsity Metadata Layout for M = 64 for\n.kind::tf32\n\n9.7.16.10.8.4.4.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::tf32\n\nFigure 265\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 265\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::tf32\n\n9.7.16.10.8.4.5.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\nThe value of the sparsity selector:\nmust be 0 for\n.kind::i8\nand\n.kind::f8f6f4\nis assumed to be 0 for\n.kind::mxf8f6f4\n,\n.kind::mxf4\nand\n.kind::mxf4nvf4\nand all of the columns are selected as\nshown in\nFigure 266\nFigure 266\nSparsity Metadata Layout for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\n9.7.16.10.8.4.6.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\nThe value of the sparsity selector:\nmust be 0 for\n.kind::i8\nand\n.kind::f8f6f4\nis assumed to be 0 for\n.kind::mxf8f6f4\n,\n.kind::mxf4\nand\n.kind::mxf4nvf4\nand all of the columns are selected as\nshown in\nFigure 267\nFigure 267\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\n9.7.16.10.8.5.\nAlignment restriction\n\nThe layouts which utilize only half the datapath lanes as specified in\nData Path Layout Organization\n,\ni.e.\nLayout F\nand\nLayout C\n, must use the same alignment\nacross matrices A, D and the sparsity metadata matrix.\n9.7.16.10.9.\nTensorCore 5th Generation of MMA Instructions\n\n9.7.16.10.9.1.\nTensorCore 5th Generation Instructions:\ntcgen05.mma\n\ntcgen05.mma\nPerform the 5\nth\ngeneration of matrix multiply and accumulate operation.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.cta_group.kind   [d-tmem],  a-desc,  b-desc, idesc,\n                             { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind   [d-tmem], [a-tmem], b-desc, idesc,\n                             { disable-output-lane }, enable-input-d {, scale-input-d};\n\n.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.cta_group = { .cta_group::1, .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 2. Floating-point type with block scaling:\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}\n                                        [d-tmem],  a-desc,  b-desc, idesc,\n                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}\n                                        [d-tmem], [a-tmem], b-desc, idesc,\n                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.cta_group      = { .cta_group::1,   .cta_group::2 }\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n\n----------------------------------------------------------------------------------\n\n// 3. Convolution MMA for floating-point type without block scaling:\n\ntcgen05.mma.cta_group.kind.collector_usage [d-tmem],  a-desc,  b-desc, idesc,\n                                           { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,\n                                                    { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                    { disable-output-lane }, enable-input-d {, scale-input-d};\n\n.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.cta_group = { .cta_group::1,   .cta_group::2 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 4. Activation Stationary MMA for floating-point type with block scaling:\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                            [d-tmem],  a-desc,  b-desc, idesc,\n                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                            [d-tmem], [a-tmem], b-desc, idesc,\n                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.cta_group       = { .cta_group::1,   .cta_group::2 }\n.scale_vectorsize  = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.kind            = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 5. Integer type:\n\ntcgen05.mma.cta_group.kind::i8  [d-tmem],  a-desc,  b-desc, idesc,\n                                { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8  [d-tmem], [a-tmem], b-desc, idesc,\n                                { disable-output-lane }, enable-input-d;\n\n.cta_group = { .cta_group::1,   .cta_group::2  }\n\n----------------------------------------------------------------------------------\n\n// 6. Convolution MMA for integer type:\n\ntcgen05.mma.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\n.cta_group       = { .cta_group::1,   .cta_group::2  }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\nDescription\nInstruction\ntcgen05.mma\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix\nmultiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMxK\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe optional immediate argument\nscale-input-d\ncan be specified to scale the input\nmatrix\nD\nas follows:\nD\n=\nA*B+D\n*\n(2\n^\n-\nscale-input-d)\nThe valid range of values for argument\nscale-input-d\nis [0, 15]. The argument\nscale-input-d\nis only valid for\n.kind::tf32\nand\n.kind::f16\n.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described\nin\nInstruction descriptor\n, specifies\nthe shapes, exact types, sparsity and other details of the input matrices,\noutput matrix and the matrix multiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and\naccumulate operation is performed on the\nTensor Memory\nof the\nexecuting thread’s CTA only. The qualifier\n.cta_group::2\nspecifies that the matrix\nmultiply and accumulate operation is performed on the\nTensor Memory\nof the executing thread’s CTA and its\npeer CTA\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n.\nThe 64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which\nrepresent the matrices\nA\nand\nB\nin shared memory respectively. The format of the\nmatrix descriptor is described in\nMatrix Descriptors\n.\nThe vector operand\ndisable-output-lane\nspecifies the lane(s) in the\nTensor Memory\nthat should be not be updated with the resultant\nmatrix\nD\n. Elements of the vector operand\ndisable-output-lane\nforms a mask where\neach bit corresponds to a lane of the\nTensor Memory\n, with least\nsignificant bit of the first element of the vector (leftmost in syntax) corresponding\nto the lane 0 of the\nTensor Memory\n. If a bit in the mask is 1,\nthen the corresponding lane in the Tensor Memory for the resultant matrix\nD\nwill not\nbe updated. The size of the vector is as follows:\n.cta_group\nSize of the vector disable-output-lane\n::1\n4\n::2\n8\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply\nand accumulate operation as specified in the section\nBlock Scaling for tcgen05.mma.sync\n.\nThe address operand\nscale-A-tmem\nand\nscale-B-tmem\nspecify the base address the\nmatrices\nscale_A\nand\nscale_B\nrespectively in the\nTensor Memory\n.\nFor qualifier\n.scale_vectorsize\n,\nIf\n.scale_vec::NX\nis specified: N specifies the number of columns in\nscale_A\nmatrix and number of rows in\nscale_B\nmatrix.\nIf\n.blockN\nis specified: N specifies the block size for which single scale factor\nwill be applied. In this form, value of N is same as the K-dimension / (N of\n.scale_vec::NX\n).\nAliased\n.scale_vectorsize\nvariants:\n.block16\nis aliased with:\n.scale_vec::4X\nwhen\n.kind\n=\n.kind::mxf4nvf4\nand K = 64 or 128\n.block32\nis aliased with:\n.scale_vec::1X\nwhen\n.kind\n=\n.kind::mxf8f6f4\nfor all supported values of K\n.scale_vec::2X\nwhen\n.kind\n=\n.kind::mxf4\nor\n.kind::mxf4nvf4\nand K = 64 or 128\nThe valid combinations of MMA-kind and\n.scale_vectorsize\nare\ndescribed in\nTable 56\n. For\n.kind::mxf4\nwhen the qualifier\n.scale_vectorsize\nis not specified, then it defaults to\n.block32\n. For\n.kind::mxf4nvf4\n,\nthe qualifier\n.scale_vectorsize\nmust be explicitly specified.\nThe qualifier\n.ashift\nshifts the rows of the\nA\nmatrix down by one row, except for\nthe last row in the\nTensor Memory\n. Qualifier\n.ashift\nis only allowed\nwith\nM\n= 128 or\nM\n= 256.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for matrix\nA\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::a::fill\nSpecifies that the\nA\nmatrix read from the memory\nshould be filled in collector buffer.\n.collector::a::use\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer. This requires a previous fill to\nthe collector buffer to be still valid.\n.collector::a::lastuse\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer and the contents of the collector\nbuffer can be discarded. This requires a previous\nfill to the collector buffer to be valid till the\ncollector buffer is read.\n.collector::a::discard\nSpecifies that the contents of the collector buffer\nfor\nA\ncan be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::a::discard\n.\nIt is illegal to specify either of\n.collector::a::use\nor\n.collector::a::fill\nalong with\n.ashift\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nQualifier\n.kind::mxf4nvf4\nintroduced in PTX ISA version 8.7.\nQualifiers\n.block16\nand\n.block32\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nArgument\nscale-input-d\nrequires\nsm_100a\nand is supported on\nsm_100f\nor higher in the same family from PTX ISA version 8.8.\nFor\n.scale_vectorsize\n,\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\nrequires\nsm_100a\n.\n.block16\n,\n.block32\nrequires\nsm_100f\nor\nsm_110f\n.\nFor Target ISA details on matrix shape, check\nTarget ISA Note\n.\nFor Target ISA details on shared memory descriptor, check\nTarget ISA Note\n.\nExamples\ntcgen05.mma.cta_group::1.kind::tf32      [taddr0],  adesc,  bdesc, idesc, {m0, m1, m2, m3}, p;\ntcgen05.mma.cta_group::1.kind::mxf8f6f4  [taddr2],  [taddr1],  bdesc, idesc,\n                                         [tmem_scaleA], [tmem_scaleB], p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.2.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.sp\n\ntcgen05.mma.sp\nPerform the 5\nth\ngeneration of matrix multiply and accumulate operation with sparse\nA\nmatrix.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.sp.cta_group.kind  [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] ,  idesc,\n                               { disable-output-lane }, enable-input-d{, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind  [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,\n                               { disable-output-lane }, enable-input-d{, scale-input-d};\n\n.kind       = { .kind::f16, , .kind::tf32, .kind::f8f6f4 }\n.cta_group  = { .cta_group::1,  .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 2. Floating-point type with block scaling:\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}\n                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}\n                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.cta_group      = { .cta_group::1,  .cta_group::2 }\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n\n----------------------------------------------------------------------------------\n\n// 3. Convolution MMA with floating-point type without block scaling:\n\ntcgen05.mma.sp.cta_group.kind.collector_usage           [d-tmem],  a-desc,  b-desc,\n                                                        [sp-meta-tmem] ,  idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind.ashift{.collector_usage}  [d-tmem], [a-tmem], b-desc,\n                                                        [sp-meta-tmem] , idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind{.ashift}.collector_usage  [d-tmem], [a-tmem], b-desc,\n                                                        [sp-meta-tmem] , idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\n.kind            = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 4. Activation Stationary MMA with floating-point type with block scaling:\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 5. Integer type:\n\ntcgen05.mma.sp.cta_group.kind::i8 [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] , idesc,\n                                  { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8 [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,\n                                  { disable-output-lane }, enable-input-d;\n\n.cta_group      = { .cta_group::1,  .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 6. Convolution MMA with Integer type:\n\ntcgen05.mma.sp.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc,\n                                                           [sp-meta-tmem] , idesc,\n                                                           { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                           [sp-meta-tmem], idesc ,\n                                                           { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc,\n                                                           [sp-meta-tmem], idesc ,\n                                                           { disable-output-lane }, enable-input-d;\n\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\nDescription\nInstruction\ntcgen05.mma.sp\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix multiply and accumulate operation of the form\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMx(K/2)\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nSparse Matrices\ndescribes the details of the sparsity.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe optional immediate argument\nscale-input-d\ncan be specified to scale the\ninput matrix\nD\nas follows:\nD\n=\nA*B+D\n*\n(2\n^\n-\nscale-input-d)\nThe valid range of values for argument\nscale-input-d\nis [0, 15]. The argument\nscale-input-d\nis only valid for\n.kind::tf32\nand\n.kind::f16\n.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes,\nexact types, sparsity and other details of the input matrices, output matrix and the\nmatrix multiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate\noperation is performed on the\nTensor Memory\nof the executing\nthread’s CTA only. The qualifier\n.cta_group::2\nspecifies that the matrix\nmultiply and accumulate operation is performed on the\nTensor Memory\nof the executing thread’s CTA and its\npeer CTA\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.sp\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.sp\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n. The\n64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which represent\nthe matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor\nis described in\nMatrix Descriptors\n.\nThe vector operand\ndisable-output-lane\nspecifies the lane(s) in the\nTensor Memory\nthat should be not be updated with the resultant matrix\nD\n. Elements of the vector operand\ndisable-output-lane\nforms a mask where each bit corresponds to a lane of the\nTensor Memory\n. with least significant bit of the first element of\nthe vector (leftmost in syntax) corresponding to the lane 0 of the Tensor Memory. If a bit in\nthe mask is 1, then the corresponding lane in the Tensor Memory for the resultant matrix\nD\nwill not be updated. The size of the vector is as follows:\n.cta_group\nSize of the vector disable-output-lane\n::1\n4\n::2\n8\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply\nand accumulate operation as specified in the section\nBlock Scaling for tcgen05.mma.sync\n.\nThe address operand\nscale-A-tmem\nand\nscale-B-tmem\nspecify the base address the\nmatrices\nscale_A\nand\nscale_B\nrespectively in the\nTensor Memory\n.\nFor qualifier\n.scale_vectorsize\n,\nIf\n.scale_vec::NX\nis specified: N specifies the number of columns in\nscale_A\nmatrix and number of rows in\nscale_B\nmatrix.\nIf\n.blockN\nis specified: N specifies the block size for which single scale factor\nwill be applied. In this form, value of N is same as the K-dimension / (N of\n.scale_vec::NX\n).\nAliased\n.scale_vectorsize\nvariants:\n.block16\nis aliased with:\n.scale_vec::4X\nwhen\n.kind\n=\n.kind::mxf4nvf4\nand K = 64 or 128\n.block32\nis aliased with:\n.scale_vec::1X\nwhen\n.kind\n=\n.kind::mxf8f6f4\nfor all supported values of K\n.scale_vec::2X\nwhen\n.kind\n=\n.kind::mxf4\nor\n.kind::mxf4nvf4\nand K = 64 or 128\nThe valid combinations of MMA-kind and\n.scale_vectorsize\nare\ndescribed in\nTable 56\n. For\n.kind::mxf4\nwhen the qualifier\n.scale_vectorsize\nis not specified, then it defaults to\n.block32\n. For\n.kind::mxf4nvf4\n,\nthe qualifier\n.scale_vectorsize\nmust be explicitly specified.\nThe qualifier\n.ashift\nshifts the rows of the\nA\nmatrix down by one row, except for\nthe last row in the\nTensor Memory\n. Qualifier\n.ashift\nis only allowed\nwith\nM\n= 128 or\nM\n= 256.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for matrix\nA\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::a::fill\nSpecifies that the\nA\nmatrix read from the memory\nshould be filled in collector buffer.\n.collector::a::use\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer. This requires a previous fill to\nthe collector buffer to be still valid.\n.collector::a::lastuse\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer and the contents of the collector\nbuffer can be discarded. This requires a previous\nfill to the collector buffer to be valid till the\ncollector buffer is read.\n.collector::a::discard\nSpecifies that the contents of the collector buffer\nfor\nA\ncan be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::a::discard\n.\nIt is illegal to specify either of\n.collector::a::use\nor\n.collector::a::fill\nalong with\n.ashift\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nQualifier\n.kind::mxf4nvf4\nintroduced in PTX ISA version 8.7.\nQualifiers\n.block16\nand\n.block32\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n/\n.kind::mxf4nvf4\n/\n.kind::mxf4\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nQualifiers\n.kind::mxf4nvf4\nand\n.kind::mxf4\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_103a\nsm_110a\nArgument\nscale-input-d\nrequires\nsm_100a\nand is supported on\nsm_100f\nor higher in the same family from PTX ISA version 8.8.\nFor\n.scale_vectorsize\n,\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\nrequires\nsm_100a\n.\n.block16\n,\n.block32\nrequires\nsm_100f\nor\nsm_110f\n.\nFor Target ISA details on matrix shape, check\nTarget ISA Note\n.\nFor Target ISA details on shared memory descriptor, check\nTarget ISA Note\n.\nExamples\ntcgen05.mma.sp.cta_group::1.kind::f16      [taddr0],  adesc,  bdesc, [tmem_spmeta0], idesc, p;\n\ntcgen05.mma.sp.cta_group::1.kind::mxf8f6f4.collector::a:fill\n                                           [taddr2],  [taddr1],  bdesc, [tmem_spmeta1], idesc,\n                                           [tmem_scaleA], [tmem_scaleB], p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.3.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.ws\n\ntcgen05.mma.ws\nPerform the 5\nth\ngeneration of weight stationary convolution matrix multiply and accumulate\noperation.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem],  a-desc,  b-desc,  idesc,\n                                                      enable-input-d {, zero-column-mask-desc };\n\ntcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem], [a-tmem], b-desc, idesc,\n                                                      enable-input-d {, zero-column-mask-desc };\n\n.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n\n----------------------------------------------------------------------------------\n\n// 2. Integer type:\n\ntcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem],  a-desc,  b-desc, idesc,\n                                                       enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                       enable-input-d {, zero-column-mask-desc};\n\n.collector_usage = { .collector::buffer::op }\n::buffer = { ::b0, ::b1, ::b2, ::b3 }\n::op   = { ::fill, ::use, ::lastuse, ::discard}\nDescription\nInstruction\ntcgen05.mma.ws\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMxK\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes, exact\ntypes, sparsity and other details of the input matrices, output matrix and the matrix\nmultiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate operation\nis performed on the\nTensor Memory\nof the executing thread’s CTA only.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.ws\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.ws\nwill result in the initiation of the whole matrix multiply and accumulate\noperation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n. The\n64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which represent\nthe matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor\nis described in\nMatrix Descriptors\n.\nThe optional operand\nzero-column-mask-desc\nis a 64-bit register which specifies the\nZero-Column Mask Descriptor\n. The zero-column\nmask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix\nwill have zero value for the matrix multiply and accumulate operation regardless of the\nvalues present in the shared memory.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for Matrix\nB\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::bN::fill\nSpecifies that the\nB\nmatrix read from the memory\nshould be filled in collector buffer #N.\n.collector::bN::use\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N. This requires a previous fill\nto the collector buffer #N to be still valid.\n.collector::bN::lastuse\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N after which the contents of the\ncollector buffer #N can be discarded. This requires\na previous fill to the collector buffer #N to be\nvalid till the collector buffer #N is read.\n.collector::bN::discard\nSpecifies that the contents of the collector buffer\n#N can be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::b0::discard\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nExamples\ntcgen05.mma.ws.cta_group::1.kind::i8.collector::b2:use [taddr2], [taddr1], bdesc, idesc, p;\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.4.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.ws.sp\n\ntcgen05.mma.ws.sp\nPerform the 5\nth\ngeneration of weight stationary convolution matrix multiply and accumulate\noperation with sparse\nA\nmatrix.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem],  a-desc,  b-desc,\n                                                      [sp-meta-tmem] ,  idesc,\n                                                      enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                      [sp-meta-tmem] , idesc,\n                                                      enable-input-d {, zero-column-mask-desc};\n\n.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n\n----------------------------------------------------------------------------------\n\n// 2. Integer type:\n\ntcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], a-desc, b-desc,\n                                                          [sp-meta-tmem] , idesc,\n                                                          enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                          [sp-meta-tmem] , idesc,\n                                                          enable-input-d {, zero-column-mask-desc};\n\n.collector_usage = { .collector::buffer::op }\n::buffer = { ::b0, ::b1, ::b2, ::b3 }\n::op   = { ::fill, ::use, ::lastuse, ::discard}\nDescription\nInstruction\ntcgen05.mma.ws.sp\nis an asynchronous instruction which initiates\nan\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMx(K/2)\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix\nis\nMxN\n.\nSparse Matrices\ndescribes the details of the\nsparsity.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes, exact\ntypes, sparsity and other details of the input matrices, output matrix and the matrix\nmultiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate\noperation is performed on the Tensor Memory of the executing thread’s CTA only.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.ws.sp\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.ws.sp\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind are\nspecified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies\nthe address of the matrix\nA\nin the\nTensor Memory\n. The 64-bit register\noperand\na-desc\nand\nb-desc\nare the matrix descriptors which represent the matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor is described in\nMatrix Descriptors\n.\nThe optional operand\nzero-column-mask-desc\nis a 64-bit register which specifies the\nZero-Column Mask Descriptor\n. The zero-column\nmask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix\nwill have zero value for the matrix multiply and accumulate operation regardless of the\nvalues present in the shared memory.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for Matrix\nB\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::bN::fill\nSpecifies that the\nB\nmatrix read from the memory\nshould be filled in collector buffer #N.\n.collector::bN::use\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N. This requires a previous fill\nto the collector buffer #N to be still valid.\n.collector::bN::lastuse\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N after which the contents of the\ncollector buffer #N can be discarded. This requires\na previous fill to the collector buffer #N to be\nvalid till the collector buffer #N is read.\n.collector::bN::discard\nSpecifies that the contents of the collector buffer\n#N can be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::b0::discard\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nExamples\ntcgen05.mma.ws.sp.cta_group::1.kind::tf32.collector::b1::fill  [taddr1], [taddr0], bdesc,\n                                                               [tmem_spmeta0], idesc, p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.11.\nTensorCore 5th Generation Specialized Synchronization Operations\n\n9.7.16.11.1.\nTensorCore 5th Generation Instructions:\ntcgen05.fence\n\ntcgen05.fence\nSpecialized fence for the asynchronous tcgen05 operations.\nSyntax\ntcgen05.fence::before_thread_sync ;\ntcgen05.fence::after_thread_sync  ;\nDescription\nThe instruction\ntcgen05.fence::before_thread_sync\norders all the prior asynchronous\ntcgen05\noperations with respect to the subsequent\ntcgen05\nand the execution\nordering operations.\nThe instruction\ntcgen05.fence::after_thread_sync\norders all the subsequent asynchronous\ntcgen05\noperations with respect to the prior\ntcgen05\nand the execution ordering\noperations.\nThe\ntcgen05.fence::*\ninstructions compose with execution ordering instructions across\na thread scope and provide ordering between\ntcgen05\ninstructions across the same scope.\nThe\ntcgen05.fence::before_thread_sync\ninstructions behave as code motion fence for prior\ntcgen05\ninstructions as they cannot be hoisted across. The\ntcgen05.fence::after_thread_sync\ninstructions behave as code motion fence for subsequent\ntcgen05\ninstructions as they cannot\nbe hoisted across.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n// Producer thread:\n\ntcgen05.cp.cta_group::1.128x256b  [taddr0], sdesc0;\n\ntcgen05.fence::before_thread_sync;\nst.relaxed.b32 [flag], 1;\n\n// Consumer thread:\n\nloop:\nld.relaxed.b32 r, [flag];\nsetp.eq.u32 p, r, 1;\n@!p bra loop;\n\ntcgen05.fence::after_thread_sync;\ntcgen05.mma.cta_group.kind   [taddr0], adesc, bdesc, idesc, p;\n9.7.16.12.\nTensorCore 5th Generation Async Synchronization Operations\n\n9.7.16.12.1.\nTensorCore 5th Generation Instructions:\ntcgen05.commit\n\ntcgen05.commit\nMakes the mbarrier object track the completion of all prior async-tcgen05 operations initiated\nby the executing thread.\nSyntax\ntcgen05.commit.cta_group.completion_mechanism{.shared::cluster}{.multicast}.b64\n                                                            [mbar] {, ctaMask};\n\n.completion_mechanism = { .mbarrier::arrive::one }\n.cta_group            = { .cta_group::1, .cta_group::2 }\n.multicast            = { .multicast::cluster }\nDescription\nThe instruction\ntcgen05.commit\nis an asynchronous instruction which makes the mbarrier object,\nspecified by the address operand\nmbar\n, track the completion of all the prior asynchronous\ntcgen05\noperations, as listed in\nmbarrier based completion mechanism\n,\ninitiated by the executing thread. Upon the completion of the tracked asynchronous\ntcgen05\noperations, the signal specified by the\n.completion_mechanism\nis triggered by the system\non the mbarrier object.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe instruction\ntcgen05.commit.cta_group::1\ntracks for the completion of all prior\nasynchronous\ntcgen05\noperations with\n.cta_group::1\nissued by the current thread.\nSimilarly, the instruction\ntcgen05.commit.cta_group::2\ntracks for the completion of all\nprior asynchronous\ntcgen05\noperations with\n.cta_group::2\nissued by the current thread.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe qualifier\n.mbarrier::arrive::one\nindicates that upon the completion of the prior\nasynchronous\ntcgen05\noperation issued by the current thread, an arrive-on operation, with\nthe count argument of 1, is signaled on the mbarrier object. The scope of the arrive-on operation\nis the cluster scope.\nThe optional qualifier\n.multicast::cluster\nallows signaling on the mbarrier objects of multiple\nCTAs in the cluster. Operand\nctaMask\nspecifies the CTAs in the cluster such that each bit\nposition in the 16-bit\nctaMask\noperand corresponds to the\n%cluster_ctarank\nof the destination\nCTA. The mbarrier signal is multicast to the same offset as\nmbar\nin the shared memory of each\ndestination CTA.\nIf no state space is specified then\nGeneric Addressing\nis used. If the\naddress specified by\nmbar\ndoes not fall within the address window of\n.shared::cluster\nstate\nspace then the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nExample 1:\ntcgen05.cp.cta_group::1.128x256b                      [taddr0], sdesc0;\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj1];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj1], 0;\n@!p bra loop;\n\nExample 2:\ntcgen05.mma.cta_group::2.kind::tf32    [taddr0],  adesc,  bdesc, idesc, p;\ntcgen05.commit.cta_group::2.mbarrier::arrive::one.b64 [mbarObj2];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj2], 0;\n@!p bra loop;\n9.7.17.\nStack Manipulation Instructions\n\nThe stack manipulation instructions can be used to dynamically allocate and deallocate memory on the\nstack frame of the current function.\nThe stack manipulation instrucitons are:\nstacksave\nstackrestore\nalloca\n9.7.17.1.\nStack Manipulation Instructions:\nstacksave\n\nstacksave\nSave the value of stack pointer into a register.\nSyntax\nstacksave.type  d;\n\n.type = { .u32, .u64 };\nDescription\nCopies the current value of stack pointer into the destination register\nd\n. Pointer returned by\nstacksave\ncan be used in a subsequent\nstackrestore\ninstruction to restore the stack\npointer. If\nd\nis modified prior to use in\nstackrestore\ninstruction, it may corrupt data in\nthe stack.\nDestination operand\nd\nhas the same type as the instruction type.\nSemantics\nd = stackptr;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nstacksave\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 rd;\nstacksave.u32 rd;\n\n.reg .u64 rd1;\nstacksave.u64 rd1;\n9.7.17.2.\nStack Manipulation Instructions:\nstackrestore\n\nstackrestore\nUpdate the stack pointer with a new value.\nSyntax\nstackrestore.type  a;\n\n.type = { .u32, .u64 };\nDescription\nSets the current stack pointer to source register\na\n.\nWhen\nstackrestore\nis used with operand\na\nwritten by a prior\nstacksave\ninstruction, it\nwill effectively restore the state of stack as it was before\nstacksave\nwas executed. Note that\nif\nstackrestore\nis used with an arbitrary value of\na\n, it may cause corruption of stack\npointer. This implies that the correct use of this feature requires that\nstackrestore.type\na\nis\nused after\nstacksave.type\na\nwithout redefining the value of\na\nbetween them.\nOperand\na\nhas the same type as the instruction type.\nSemantics\nstackptr = a;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nstackrestore\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 ra;\nstacksave.u32 ra;\n// Code that may modify stack pointer\n...\nstackrestore.u32 ra;\n9.7.17.3.\nStack Manipulation Instructions:\nalloca\n\nalloca\nDynamically allocate memory on stack.\nSyntax\nalloca.type  ptr, size{, immAlign};\n\n.type = { .u32, .u64 };\nDescription\nThe\nalloca\ninstruction dynamically allocates memory on the stack frame of the current function\nand updates the stack pointer accordingly. The returned pointer\nptr\npoints to local memory and\ncan be used in the address operand of\nld.local\nand\nst.local\ninstructions.\nIf sufficient memory is unavailable for allocation on the stack, then execution of\nalloca\nmay\nresult in stack overflow. In such cases, attempting to access the allocated memory with\nptr\nwill\nresult in undefined program behavior.\nThe memory allocated by\nalloca\nis deallocated in the following ways:\nIt is automatically deallocated when the function exits.\nIt can be explicitly deallocated using\nstacksave\nand\nstackrestore\ninstructions:\nstacksave\ncan be used to save the value of stack pointer before executing\nalloca\n, and\nstackrestore\ncan be used after\nalloca\nto restore stack pointer to the original value which\nwas previously saved with\nstacksave\n. Note that accessing deallocated memory after executing\nstackrestore\nresults in undefined behavior.\nsize\nis an unsigned value which specifies the amount of memory in number of bytes to be\nallocated on stack.\nsize\n=\n0\nmay not lead to a valid memory allocation.\nBoth\nptr\nand\nsize\nhave the same type as the instruction type.\nimmAlign\nis a 32-bit value which specifies the alignment requirement in number of bytes for the\nmemory allocated by\nalloca\n. It is an integer constant, must be a power of 2 and must not exceed\n2^23.\nimmAlign\nis an optional argument with default value being 8 which is the minimum\nguaranteed alignment.\nSemantics\nalloca.type ptr, size, immAlign:\n\na = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment\n\n// Allocate size bytes of stack memory with alignment a and update the stack pointer.\n// Since the stack grows down, the updated stack pointer contains a lower address.\nstackptr = alloc_stack_mem(size, a);\n\n// Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory\n// allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).\nstacksave ptr;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nalloca\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 ra, stackptr, ptr, size;\n\nstacksave.u32 stackptr;     // Save the current stack pointer\nalloca ptr, size, 8;        // Allocate stack memory\nst.local.u32 [ptr], ra;     // Use the allocated stack memory\nstackrestore.u32 stackptr;  // Deallocate memory by restoring the stack pointer\n9.7.18.\nVideo Instructions\n\nAll video instructions operate on 32-bit register operands. However, the video instructions may be\nclassified as either scalar or SIMD based on whether their core operation applies to one or multiple\nvalues.\nThe video instructions are:\nvadd\n,\nvadd2\n,\nvadd4\nvsub\n,\nvsub2\n,\nvsub4\nvmad\nvavrg2\n,\nvavrg4\nvabsdiff\n,\nvabsdiff2\n,\nvabsdiff4\nvmin\n,\nvmin2\n,\nvmin4\nvmax\n,\nvmax2\n,\nvmax4\nvshl\nvshr\nvset\n,\nvset2\n,\nvset4\n9.7.18.1.\nScalar Video Instructions\n\nAll scalar video instructions operate on 32-bit register operands. The scalar video instructions\nare:\nvadd\nvsub\nvabsdiff\nvmin\nvmax\nvshl\nvshr\nvmad\nvset\nThe scalar video instructions execute the following stages:\nExtract and sign- or zero-extend byte, half-word, or word values from its source operands, to\nproduce signed 33-bit input values.\nPerform a scalar arithmetic operation to produce a signed 34-bit result.\nOptionally clamp the result to the range of the destination type.\nOptionally perform one of the following:\napply a second operation to the intermediate result and a third operand, or\ntruncate the intermediate result to a byte or half-word value and merge into a specified\nposition in the third operand to produce the final result.\nThe general format of scalar video instructions is as follows:\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}        d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.secop  d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}   d.dsel, a{.asel}, b{.bsel}, c;\n\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.secop = { .add, .min, .max };\nThe source and destination operands are all 32-bit registers. The type of each operand (\n.u32\nor\n.s32\n) is specified in the instruction type; all combinations of\ndtype\n,\natype\n, and\nbtype\nare valid. Using the\natype/btype\nand\nasel/bsel\nspecifiers, the input values are\nextracted and sign- or zero-extended internally to\n.s33\nvalues. The primary operation is then\nperformed to produce an\n.s34\nintermediate result. The sign of the intermediate result depends on\ndtype.\nThe intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging.\n.s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) {\n    if ( !sat )  return tmp;\n\n    switch ( dsel ) {\n        case .b0, .b1, .b2, .b3:\n            if ( sign )  return CLAMP( tmp, S8_MAX, S8_MIN );\n            else         return CLAMP( tmp, U8_MAX, U8_MIN );\n        case .h0, .h1:\n            if ( sign )  return CLAMP( tmp, S16_MAX, S16_MIN );\n            else         return CLAMP( tmp, U16_MAX, U16_MIN );\n        default:\n            if ( sign )  return CLAMP( tmp, S32_MAX, S32_MIN );\n            else         return CLAMP( tmp, U32_MAX, U32_MIN );\n    }\n}\nThis intermediate result is then optionally combined with the third source operand using a secondary\narithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the\nthird operand is based on\ndtype\n.\n.s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) {\n    switch ( secop ) {\n        .add:     return tmp + c;\n        .min:     return MIN(tmp, c);\n        .max      return MAX(tmp, c);\n        default:  return tmp;\n    }\n}\n.s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) {\n    switch ( dsel ) {\n        case .h0:  return ((tmp & 0xffff)        | (0xffff0000 & c);\n        case .h1:  return ((tmp & 0xffff) << 16) | (0x0000ffff & c);\n        case .b0:  return ((tmp & 0xff)          | (0xffffff00 & c);\n        case .b1:  return ((tmp & 0xff) <<  8)   | (0xffff00ff & c);\n        case .b2:  return ((tmp & 0xff) << 16)   | (0xff00ffff & c);\n        case .b3:  return ((tmp & 0xff) << 24)   | (0x00ffffff & c);\n        default:   return tmp;\n    }\n}\nThe lower 32-bits are then written to the destination operand.\n9.7.18.1.1.\nScalar Video Instructions:\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\n\nvadd\n,\nvsub\nInteger byte/half-word/word addition/subtraction.\nvabsdiff\nInteger byte/half-word/word absolute value of difference.\nvmin\n,\nvmax\nInteger byte/half-word/word minimum/maximum.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}       d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vadd, vsub, vabsdiff, vmin, vmax };\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nPerform scalar arithmetic operation with optional saturate, and optional secondary arithmetic operation or subword data merge.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\n\nswitch ( vop ) {\n    case vadd:     tmp = ta + tb;\n    case vsub:     tmp = ta - tb;\n    case vabsdiff: tmp = | ta - tb |;\n    case vmin:     tmp = MIN( ta, tb );\n    case vmax:     tmp = MAX( ta, tb );\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\nrequire\nsm_20\nor higher.\nExamples\nvadd.s32.u32.s32.sat      r1, r2.b0, r3.h0;\nvsub.s32.s32.u32.sat      r1, r2.h1, r3.h1;\nvabsdiff.s32.s32.s32.sat  r1.h0, r2.b0, r3.b2, c;\nvmin.s32.s32.s32.sat.add  r1, r2, r3, c;\n9.7.18.1.2.\nScalar Video Instructions:\nvshl\n,\nvshr\n\nvshl\n,\nvshr\nInteger byte/half-word/word left/right shift.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.u32{.sat}.mode       d, a{.asel}, b{.bsel};\nvop.dtype.atype.u32{.sat}.mode.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.u32{.sat}.mode  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vshl, vshr };\n.dtype = .atype = { .u32, .s32 };\n.mode  = { .clamp, .wrap };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nvshl\nShift\na\nleft by unsigned amount in\nb\nwith optional saturate, and optional secondary\narithmetic operation or subword data merge. Left shift fills with zero.\nvshr\nShift\na\nright by unsigned amount in\nb\nwith optional saturate, and optional secondary\narithmetic operation or subword data merge. Signed shift fills with the sign bit, unsigned shift\nfills with zero.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a,atype, asel );\ntb = partSelectSignExtend( b, .u32, bsel );\nif ( mode == .clamp  && tb > 32 )  tb = 32;\nif ( mode == .wrap )                       tb = tb & 0x1f;\nswitch ( vop ){\n   case vshl:  tmp = ta << tb;\n   case vshr:  tmp = ta >> tb;\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvshl\n,\nvshr\nrequire\nsm_20\nor higher.\nExamples\nvshl.s32.u32.u32.clamp  r1, r2, r3;\nvshr.u32.u32.u32.wrap   r1, r2, r3.h1;\n9.7.18.1.3.\nScalar Video Instructions:\nvmad\n\nvmad\nInteger byte/half-word/word multiply-accumulate.\nSyntax\n// 32-bit scalar operation\nvmad.dtype.atype.btype{.sat}{.scale}     d, {-}a{.asel}, {-}b{.bsel},\n                                         {-}c;\nvmad.dtype.atype.btype.po{.sat}{.scale}  d, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.scale = { .shr7, .shr15 };\nDescription\nCalculate\n(a*b)\n+\nc\n, with optional operand negates,\nplus one\nmode, and scaling.\nThe source operands support optional negation with some restrictions. Although PTX syntax allows\nseparate negation of the\na\nand\nb\noperands, internally this is represented as negation of the\nproduct\n(a*b)\n. That is,\n(a*b)\nis negated if and only if exactly one of\na\nor\nb\nis\nnegated. PTX allows negation of either\n(a*b)\nor\nc\n.\nThe plus one mode (\n.po\n) computes\n(a*b)\n+\nc\n+\n1\n, which is used in computing averages. Source\noperands may not be negated in\n.po\nmode.\nThe intermediate result of\n(a*b)\nis unsigned if atype and btype are unsigned and the product\n(a*b)\nis not negated; otherwise, the intermediate result is signed. Input\nc\nhas the same\nsign as the intermediate result.\nThe final result is unsigned if the intermediate result is unsigned and\nc\nis not negated.\nDepending on the sign of the\na\nand\nb\noperands, and the operand negates, the following\ncombinations of operands are supported for VMAD:\n(u32 * u32) + u32  // intermediate unsigned; final unsigned\n-(u32 * u32) + s32  // intermediate   signed; final   signed\n (u32 * u32) - u32  // intermediate unsigned; final   signed\n (u32 * s32) + s32  // intermediate   signed; final   signed\n-(u32 * s32) + s32  // intermediate   signed; final   signed\n (u32 * s32) - s32  // intermediate   signed; final   signed\n (s32 * u32) + s32  // intermediate   signed; final   signed\n-(s32 * u32) + s32  // intermediate   signed; final   signed\n (s32 * u32) - s32  // intermediate   signed; final   signed\n (s32 * s32) + s32  // intermediate   signed; final   signed\n-(s32 * s32) + s32  // intermediate   signed; final   signed\n (s32 * s32) - s32  // intermediate   signed; final   signed\nThe intermediate result is optionally scaled via right-shift; this result is sign-extended if the\nfinal result is signed, and zero-extended otherwise.\nThe final result is optionally saturated to the appropriate 32-bit range based on the type (signed\nor unsigned) of the final result.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\nsignedFinal = isSigned(atype) || isSigned(btype) ||\n                                 (a.negate ^ b.negate) || c.negate;\ntmp[127:0] = ta * tb;\n\nlsb = 0;\nif ( .po )                  {              lsb = 1; } else\nif ( a.negate ^ b.negate )  { tmp = ~tmp;  lsb = 1; } else\nif ( c.negate )             { c   = ~c;    lsb = 1; }\n\nc128[127:0] = (signedFinal) sext32( c ) : zext ( c );\ntmp = tmp + c128 + lsb;\nswitch( scale ) {\n   case .shr7:   result = (tmp >>  7) & 0xffffffffffffffff;\n   case .shr15:  result = (tmp >> 15) & 0xffffffffffffffff;\n}\nif ( .sat ) {\n     if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN);\n     else             result = CLAMP(result, U32_MAX, U32_MIN);\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvmad\nrequires\nsm_20\nor higher.\nExamples\nvmad.s32.s32.u32.sat    r0, r1, r2, -r3;\nvmad.u32.u32.u32.shr15  r0, r1.h0, r2.h0, r3;\n9.7.18.1.4.\nScalar Video Instructions:\nvset\n\nvset\nInteger byte/half-word/word comparison.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvset.atype.btype.cmp       d, a{.asel}, b{.bsel};\nvset.atype.btype.cmp.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvset.atype.btype.cmp  d.dsel, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nCompare input values using specified comparison, with optional secondary arithmetic operation or\nsubword data merge.\nThe intermediate result of the comparison is always unsigned, and therefore destination\nd\nand\noperand\nc\nare also unsigned.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\ntmp = compare( ta, tb, cmp ) ? 1 : 0;\nd = optSecondaryOp( op2, tmp, c );    // optional secondary operation\nd = optMerge( dsel, tmp, c );         // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvset\nrequires\nsm_20\nor higher.\nExamples\nvset.s32.u32.lt    r1, r2, r3;\nvset.u32.u32.ne    r1, r2, r3.h1;\n9.7.18.2.\nSIMD Video Instructions\n\nThe SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values.\nThe SIMD video instructions are:\nvadd2\n,\nvadd4\nvsub2\n,\nvsub4\nvavrg2\n,\nvavrg4\nvabsdiff2\n,\nvabsdiff4\nvmin2\n,\nvmin4\nvmax2\n,\nvmax4\nvset2\n,\nvset4\nPTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit\nvalues. The SIMD video instructions execute the following stages:\nForm input vectors by extracting and sign- or zero-extending byte or half-word values from the\nsource operands, to form pairs of signed 17-bit values.\nPerform a SIMD arithmetic operation on the input pairs.\nOptionally clamp the result to the appropriate signed or unsigned range, as determinted by the\ndestination type.\nOptionally perform one of the following:\nperform a second SIMD merge operation, or\napply a scalar accumulate operation to reduce the intermediate SIMD results to a single\nscalar.\nThe general format of dual half-word SIMD video instructions is as follows:\n// 2-way SIMD operation, with second SIMD merge or accumulate\nvop2.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };\n.asel  = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } };\nThe general format of quad byte SIMD video instructions is as follows:\n// 4-way SIMD operation, with second SIMD merge or accumulate\nvop4.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\nThe source and destination operands are all 32-bit registers. The type of each operand (\n.u32\nor\n.s32\n) is specified in the instruction type; all combinations of\ndtype\n,\natype\n, and\nbtype\nare valid. Using the\natype/btype\nand\nasel/bsel\nspecifiers, the input values are\nextracted and sign- or zero-extended internally to\n.s33\nvalues. The primary operation is then\nperformed to produce an\n.s34\nintermediate result. The sign of the intermediate result depends on\ndtype\n.\nThe intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging.\n9.7.18.2.1.\nSIMD Video Instructions:\nvadd2\n,\nvsub2\n,\nvavrg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\n\nvadd2\n,\nvsub2\nInteger dual half-word SIMD addition/subtraction.\nvavrg2\nInteger dual half-word SIMD average.\nvabsdiff2\nInteger dual half-word SIMD absolute value of difference.\nvmin2\n,\nvmax2\nInteger dual half-word SIMD minimum/maximum.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvop2.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop2.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n vop2  = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 };\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32\nDescription\nTwo-way SIMD parallel arithmetic operation with secondary operation.\nElements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected half-words are then operated on in parallel.\nThe results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.\nFor instructions with a secondary SIMD merge operation:\nFor half-word positions indicated in mask, the selected half-word results are copied into\ndestination\nd\n. For all other positions, the corresponding half-word from source operand\nc\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor half-word positions indicated in mask, the selected half-word results are added to operand\nc\n, producing a result in\nd\n.\nSemantics\n// extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\n\nfor (i=0; i<2; i++) {\n    switch ( vop2 ) {\n       case vadd2:             t[i] = Va[i] + Vb[i];\n       case vsub2:             t[i] = Va[i] - Vb[i];\n       case vavrg2:            if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n       case vabsdiff2:         t[i] = | Va[i] - Vb[i] |;\n       case vmin2:             t[i] = MIN( Va[i], Vb[i] );\n       case vmax2:             t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S16_MAX, S16_MIN );\n        else                   t[i] = CLAMP( t[i], U16_MAX, U16_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvadd2\n,\nvsub2\n,\nvarvg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\nrequire\nsm_30\nor higher.\nExamples\nvadd2.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub2.s32.s32.s32.sat  r1.h0, r2.h10, r3.h32, r1;\nvmin2.s32.u32.u32.add  r1.h10, r2.h00, r3.h22, r1;\n9.7.18.2.2.\nSIMD Video Instructions:\nvset2\n\nvset2\nInteger dual half-word SIMD comparison.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvset2.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset2.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32\nDescription\nTwo-way SIMD parallel comparison with secondary operation.\nElements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected half-words are then compared in parallel.\nThe intermediate result of the comparison is always unsigned, and therefore the half-words of\ndestination\nd\nand operand\nc\nare also unsigned.\nFor instructions with a secondary SIMD merge operation:\nFor half-word positions indicated in mask, the selected half-word results are copied into\ndestination\nd\n. For all other positions, the corresponding half-word from source operand\nb\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor half-word positions indicated in mask, the selected half-word results are added to operand\nc\n, producing\na\nresult in\nd\n.\nSemantics\n// extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\nfor (i=0; i<2; i++) {\n    t[i] = compare( Va[i], Vb[i], .cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvset2\nrequires\nsm_30\nor higher.\nExamples\nvset2.s32.u32.lt      r1, r2, r3, r0;\nvset2.u32.u32.ne.add  r1, r2, r3, r0;\n9.7.18.2.3.\nSIMD Video Instructions:\nvadd4\n,\nvsub4\n,\nvavrg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\n\nvadd4\n,\nvsub4\nInteger quad byte SIMD addition/subtraction.\nvavrg4\nInteger quad byte SIMD average.\nvabsdiff4\nInteger quad byte SIMD absolute value of difference.\nvmin4\n,\nvmax4\nInteger quad byte SIMD minimum/maximum.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvop4.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop4.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\nvop4  = { vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 };\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654\nDescription\nFour-way SIMD parallel arithmetic operation with secondary operation.\nElements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected bytes are then operated on in parallel.\nThe results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.\nFor instructions with a secondary SIMD merge operation:\nFor byte positions indicated in mask, the selected byte results are copied into destination\nd\n. For all other positions, the corresponding byte from source operand\nc\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor byte positions indicated in mask, the selected byte results are added to operand\nc\n,\nproducing a result in\nd\n.\nSemantics\n// extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    switch ( vop4 ) {\n        case vadd4:            t[i] = Va[i] + Vb[i];\n        case vsub4:            t[i] = Va[i] - Vb[i];\n        case vavrg4:           if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n        case vabsdiff4:        t[i] = | Va[i] - Vb[i] |;\n        case vmin4:            t[i] = MIN( Va[i], Vb[i] );\n        case vmax4:            t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S8_MAX, S8_MIN );\n        else                   t[i] = CLAMP( t[i], U8_MAX, U8_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvadd4\n,\nvsub4\n,\nvarvg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\nrequire\nsm_30\nor higher.\nExamples\nvadd4.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub4.s32.s32.s32.sat  r1.b0, r2.b3210, r3.b7654, r1;\nvmin4.s32.u32.u32.add  r1.b00, r2.b0000, r3.b2222, r1;\n9.7.18.2.4.\nSIMD Video Instructions:\nvset4\n\nvset4\nInteger quad byte SIMD comparison.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvset4.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset4.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654\nDescription\nFour-way SIMD parallel comparison with secondary operation.\nElements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected bytes are then compared in parallel.\nThe intermediate result of the comparison is always unsigned, and therefore the bytes of destination\nd\nand operand\nc\nare also unsigned.\nFor instructions with a secondary SIMD merge operation:\nFor byte positions indicated in mask, the selected byte results are copied into destination\nd\n. For all other positions, the corresponding byte from source operand\nb\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor byte positions indicated in mask, the selected byte results are added to operand\nc\n,\nproducing a result in\nd\n.\nSemantics\n// extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    t[i] = compare( Va[i], Vb[i], cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvset4\nrequires\nsm_30\nor higher.\nExamples\nvset4.s32.u32.lt      r1, r2, r3, r0;\nvset4.u32.u32.ne.max  r1, r2, r3, r0;\n9.7.19.\nMiscellaneous Instructions\n\nThe Miscellaneous instructions are:\nbrkpt\nnanosleep\npmevent\ntrap\nsetmaxnreg\n9.7.19.1.\nMiscellaneous Instructions:\nbrkpt\n\nbrkpt\nBreakpoint.\nSyntax\nbrkpt;\nDescription\nSuspends execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nbrkpt\nrequires\nsm_11\nor higher.\nExamples\nbrkpt;\n@p  brkpt;\n9.7.19.2.\nMiscellaneous Instructions:\nnanosleep\n\nnanosleep\nSuspend the thread for an approximate delay given in nanoseconds.\nSyntax\nnanosleep.u32 t;\nDescription\nSuspends the thread for a sleep duration approximately close to the delay\nt\n, specified in\nnanoseconds.\nt\nmay be a register or an immediate value.\nThe sleep duration is approximated, but guaranteed to be in the interval\n[0,\n2*t]\n. The maximum\nsleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual\nthreads within a warp such that all sleeping threads in the warp wake up together.\nPTX ISA Notes\nnanosleep\nintroduced in PTX ISA 6.3.\nTarget ISA Notes\nnanosleep\nrequires\nsm_70\nor higher.\nExamples\n.reg .b32 r;\n.reg .pred p;\n\nnanosleep.u32 r;\nnanosleep.u32 42;\n@p nanosleep.u32 r;\n9.7.19.3.\nMiscellaneous Instructions:\npmevent\n\npmevent\nTrigger one or more Performance Monitor events.\nSyntax\npmevent       a;    // trigger a single performance monitor event\npmevent.mask  a;    // trigger one or more performance monitor events\nDescription\nTriggers one or more of a fixed number of performance monitor events, with event index or mask\nspecified by immediate operand\na\n.\npmevent\n(without modifier\n.mask\n) triggers a single performance monitor event indexed by\nimmediate operand\na\n, in the range\n0..15\n.\npmevent.mask\ntriggers one or more of the performance monitor events. Each bit in the 16-bit\nimmediate operand\na\ncontrols an event.\nProgrammatic performance moniter events may be combined with other hardware events using Boolean\nfunctions to increment one of the four performance counters. The relationship between events and\ncounters is programmed via API calls from the host.\nNotes\nCurrently, there are sixteen performance monitor events, numbered 0 through 15.\nPTX ISA Notes\npmevent\nintroduced in PTX ISA version 1.4.\npmevent.mask\nintroduced in PTX ISA version 3.0.\nTarget ISA Notes\npmevent supported on all target architectures.\npmevent.mask\nrequires\nsm_20\nor higher.\nExamples\npmevent      1;\n@p  pmevent      7;\n@q  pmevent.mask 0xff;\n9.7.19.4.\nMiscellaneous Instructions:\ntrap\n\ntrap\nPerform trap operation.\nSyntax\ntrap;\nDescription\nAbort execution and generate an interrupt to the host CPU.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ntrap;\n@p  trap;\n9.7.19.5.\nMiscellaneous Instructions:\nsetmaxnreg\n\nsetmaxnreg\nHint to change the number of registers owned by the warp.\nSyntax\nsetmaxnreg.action.sync.aligned.u32 imm-reg-count;\n\n.action = { .inc, .dec };\nDescription\nsetmaxnreg\nprovides a hint to the system to update the maximum number of per-thread registers\nowned by the executing warp to the value specified by the\nimm-reg-count\noperand.\nQualifier\n.dec\nis used to release extra registers such that the absolute per-thread maximum\nregister count is reduced from its current value to\nimm-reg-count\n. Qualifier\n.inc\nis used to\nrequest additional registers such that the absolute per-thread maximum register count is increased\nfrom its current value to\nimm-reg-count\n.\nA pool of available registers is maintained per-CTA. Register adjustments requested by the\nsetmaxnreg\ninstructions are handled by supplying extra registers from this pool to the\nrequesting warp or by releasing extra registers from the requesting warp to this pool, depending\nupon the value of the\n.action\nqualifier.\nThe\nsetmaxnreg.inc\ninstruction blocks the execution until enough registers are available in the\nCTA’s register pool. After the instruction\nsetmaxnreg.inc\nobtains new registers from the CTA\npool, the initial contents of the new registers are undefined. The new registers must be initialized\nbefore they are used.\nThe same\nsetmaxnreg\ninstruction must be executed by all warps in a\nwarpgroup\n. After executing a\nsetmaxnreg\ninstruction, all warps in the\nwarpgroup\nmust synchronize explicitly before\nexecuting subsequent setmaxnreg instructions. If a\nsetmaxnreg\ninstruction is not executed by all\nwarps in the\nwarpgroup\n, then the behavior is undefined.\nOperand\nimm-reg-count\nis an integer constant. The value of\nimm-reg-count\nmust be in the\nrange 24 to 256 (both inclusive) and must be a multiple of 8.\nChanges to the register file of the warp always happen at the tail-end of the register file.\nThe\nsetmaxnreg\ninstruction requires that the kernel has been launched with a valid value of\nmaximum number of per-thread registers specified via the appropriate compilation via the appropriate\ncompile-time option or the appropriate performance tuning directive. Otherwise, the\nsetmaxnreg\ninstruction may have no effect.\nWhen qualifier\n.dec\nis specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of\nsetmaxnreg\ninstruction should be greater than or equal to the\nimm-reg-count\n. Otherwise, the behaviour is undefined.\nWhen qualifier\n.inc\nis specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of\nsetmaxnreg\ninstruction should be less than or equal to the\nimm-reg-count\n. Otherwise, the behaviour is undefined.\nThe mandatory\n.sync\nqualifier indicates that\nsetmaxnreg\ninstruction causes the executing\nthread to wait until all threads in the warp execute the same\nsetmaxnreg\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nsetmaxnreg\ninstruction. In conditionally executed code,\nsetmaxnreg\ninstruction should\nonly be used if it is known that all threads in warpgroup evaluate the condition identically,\notherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on following architectures:\nsm_90a\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\nsetmaxnreg.dec.sync.aligned.u32 64;\nsetmaxnreg.inc.sync.aligned.u32 192;\n10.\nSpecial Registers\n\nPTX includes a number of predefined, read-only variables, which are\nvisible as special registers and accessed through\nmov\nor\ncvt\ninstructions.\nThe special registers are:\n%tid\n%ntid\n%laneid\n%warpid\n%nwarpid\n%ctaid\n%nctaid\n%smid\n%nsmid\n%gridid\n%is_explicit_cluster\n%clusterid\n%nclusterid\n%cluster_ctaid\n%cluster_nctaid\n%cluster_ctarank\n%cluster_nctarank\n%lanemask_eq\n,\n%lanemask_le\n,\n%lanemask_lt\n,\n%lanemask_ge\n,\n%lanemask_gt\n%clock\n,\n%clock_hi\n,\n%clock64\n%pm0,\n...,\n%pm7\n%pm0_64,\n...,\n%pm7_64\n%envreg0,\n...,\n%envreg31\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset<2>\n%total_smem_size\n%aggr_smem_size\n%dynamic_smem_size\n%current_graph_exec\n10.1.\nSpecial Registers:\n%tid\n\n%tid\nThread identifier within a CTA.\nSyntax (predefined)\n.sreg .v4 .u32 %tid;                  // thread id vector\n.sreg .u32 %tid.x, %tid.y, %tid.z;    // thread id components\nDescription\nA predefined, read-only, per-thread special register initialized with the thread identifier within\nthe CTA. The\n%tid\nspecial register contains a 1D, 2D, or 3D vector to match the CTA shape; the\n%tid\nvalue in unused dimensions is\n0\n. The fourth element is unused and always returns\nzero. The number of threads in each dimension are specified by the predefined special register\n%ntid\n.\nEvery thread in the CTA has a unique\n%tid\n.\n%tid\ncomponent values range from\n0\nthrough\n%ntid-1\nin each CTA dimension.\n%tid.y\n==\n%tid.z\n==\n0\nin 1D CTAs.\n%tid.z\n==\n0\nin 2D CTAs.\nIt is guaranteed that:\n0  <=  %tid.x <  %ntid.x\n0  <=  %tid.y <  %ntid.y\n0  <=  %tid.z <  %ntid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%tid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32      %r1,%tid.x;  // move tid.x to %rh\n\n// legacy code accessing 16-bit components of %tid\nmov.u16      %rh,%tid.x;\ncvt.u32.u16  %r2,%tid.z;  // zero-extend tid.z to %r2\n10.2.\nSpecial Registers:\n%ntid\n\n%ntid\nNumber of thread IDs per CTA.\nSyntax (predefined)\n.sreg .v4 .u32 %ntid;                   // CTA shape vector\n.sreg .u32 %ntid.x, %ntid.y, %ntid.z;   // CTA dimensions\nDescription\nA predefined, read-only special register initialized with the number of thread ids in each CTA\ndimension. The\n%ntid\nspecial register contains a 3D CTA shape vector that holds the CTA\ndimensions. CTA dimensions are non-zero; the fourth element is unused and always returns zero. The\ntotal number of threads in a CTA is\n(%ntid.x\n*\n%ntid.y\n*\n%ntid.z)\n.\n%ntid.y == %ntid.z == 1 in 1D CTAs.\n%ntid.z ==1 in 2D CTAs.\nMaximum values of %ntid.{x,y,z} are as follows:\n.target architecture\n%ntid.x\n%ntid.y\n%ntid.z\nsm_1x\n512\n512\n64\nsm_20\n,\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n,\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n1024\n1024\n64\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%ntid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n// compute unified thread id for 2D CTA\nmov.u32  %r0,%tid.x;\nmov.u32  %h1,%tid.y;\nmov.u32  %h2,%ntid.x;\nmad.u32  %r0,%h1,%h2,%r0;\n\nmov.u16  %rh,%ntid.x;      // legacy code\n10.3.\nSpecial Registers:\n%laneid\n\n%laneid\nLane Identifier.\nSyntax (predefined)\n.sreg .u32 %laneid;\nDescription\nA predefined, read-only special register that returns the thread’s lane within the warp. The lane\nidentifier ranges from zero to\nWARP_SZ-1\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %laneid;\n10.4.\nSpecial Registers:\n%warpid\n\n%warpid\nWarp identifier.\nSyntax (predefined)\n.sreg .u32 %warpid;\nDescription\nA predefined, read-only special register that returns the thread’s warp identifier. The warp\nidentifier provides a unique warp number within a CTA but not across CTAs within a grid. The warp\nidentifier will be the same for all threads within a single warp.\nNote that\n%warpid\nreturns the location of a thread at the moment when read, but\nits value may change during execution, e.g., due to rescheduling of threads following\npreemption. For this reason,\n%ctaid\nand\n%tid\nshould be used to compute a virtual warp index\nif such a value is needed in kernel code;\n%warpid\nis intended mainly to enable profiling and\ndiagnostic code to sample and log information such as work place mapping and load distribution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %warpid;\n10.5.\nSpecial Registers:\n%nwarpid\n\n%nwarpid\nNumber of warp identifiers.\nSyntax (predefined)\n.sreg .u32 %nwarpid;\nDescription\nA predefined, read-only special register that returns the maximum number of warp identifiers.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%nwarpid\nrequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %nwarpid;\n10.6.\nSpecial Registers:\n%ctaid\n\n%ctaid\nCTA identifier within a grid.\nSyntax (predefined)\n.sreg .v4 .u32 %ctaid;                      // CTA id vector\n.sreg .u32 %ctaid.x, %ctaid.y, %ctaid.z;    // CTA id components\nDescription\nA predefined, read-only special register initialized with the CTA identifier within the CTA\ngrid. The\n%ctaid\nspecial register contains a 1D, 2D, or 3D vector, depending on the shape and\nrank of the CTA grid. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %ctaid.x <  %nctaid.x\n0  <=  %ctaid.y <  %nctaid.y\n0  <=  %ctaid.z <  %nctaid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%ctaid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r0,%ctaid.x;\nmov.u16  %rh,%ctaid.y;   // legacy code\n10.7.\nSpecial Registers:\n%nctaid\n\n%nctaid\nNumber of CTA ids per grid.\nSyntax (predefined)\n.sreg .v4 .u32 %nctaid                      // Grid shape vector\n.sreg .u32 %nctaid.x,%nctaid.y,%nctaid.z;   // Grid dimensions\nDescription\nA predefined, read-only special register initialized with the number of CTAs in each grid\ndimension. The\n%nctaid\nspecial register contains a 3D grid shape vector, with each element\nhaving a value of at least\n1\n. The fourth element is unused and always returns zero.\nMaximum values of %nctaid.{x,y,z} are as follows:\n.target architecture\n%nctaid.x\n%nctaid.y\n%nctaid.z\nsm_1x\n,\nsm_20\n65535\n65535\n65535\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n,\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n2\n31\n-1\n65535\n65535\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%nctaid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r0,%nctaid.x;\nmov.u16  %rh,%nctaid.x;     // legacy code\n10.8.\nSpecial Registers:\n%smid\n\n%smid\nSM identifier.\nSyntax (predefined)\n.sreg .u32 %smid;\nDescription\nA predefined, read-only special register that returns the processor (SM) identifier on which a\nparticular thread is executing. The SM identifier ranges from\n0\nto\n%nsmid-1\n. The SM\nidentifier numbering is not guaranteed to be contiguous.\nNotes\nNote that\n%smid\nreturns the location of a thread at the moment when read, but\nits value may change during execution, e.g. due to rescheduling of threads following\npreemption.\n%smid\nis intended mainly to enable profiling and diagnostic code to sample and log\ninformation such as work place mapping and load distribution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %smid;\n10.9.\nSpecial Registers:\n%nsmid\n\n%nsmid\nNumber of SM identifiers.\nSyntax (predefined)\n.sreg .u32 %nsmid;\nDescription\nA predefined, read-only special register that returns the maximum number of SM identifiers. The SM\nidentifier numbering is not guaranteed to be contiguous, so\n%nsmid\nmay be larger than the\nphysical number of SMs in the device.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%nsmid\nrequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %nsmid;\n10.10.\nSpecial Registers:\n%gridid\n\n%gridid\nGrid identifier.\nSyntax (predefined)\n.sreg .u64 %gridid;\nDescription\nA predefined, read-only special register initialized with the per-grid temporal grid identifier. The\n%gridid\nis used by debuggers to distinguish CTAs and clusters within concurrent (small) grids.\nDuring execution, repeated launches of programs may occur, where each launch starts a\ngrid-of-CTAs. This variable provides the temporal grid launch number for this context.\nFor\nsm_1x\ntargets,\n%gridid\nis limited to the range [0..2\n16\n-1]. For\nsm_20\n,\n%gridid\nis limited to the range [0..2\n32\n-1].\nsm_30\nsupports the entire 64-bit range.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 as type\n.u16\n.\nRedefined as type\n.u32\nin PTX ISA version 1.3.\nRedefined as type\n.u64\nin PTX ISA version 3.0.\nFor compatibility with legacy PTX code, 16-bit and 32-bit\nmov\nand\ncvt\ninstructions may be\nused to read the lower 16-bits or 32-bits of each component of\n%gridid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u64  %s, %gridid;  // 64-bit read of %gridid\nmov.u32  %r, %gridid;  // legacy code with 32-bit %gridid\n10.11.\nSpecial Registers:\n%is_explicit_cluster\n\n%is_explicit_cluster\nChecks if user has explicitly specified cluster launch.\nSyntax (predefined)\n.sreg .pred %is_explicit_cluster;\nDescription\nA predefined, read-only special register initialized with the predicate value of whether the cluster\nlaunch is explicitly specified by user.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .pred p;\n\nmov.pred  p, %is_explicit_cluster;\n10.12.\nSpecial Registers:\n%clusterid\n\n%clusterid\nCluster identifier within a grid.\nSyntax (predefined)\n.sreg .v4 .u32 %clusterid;\n.sreg .u32 %clusterid.x, %clusterid.y, %clusterid.z;\nDescription\nA predefined, read-only special register initialized with the cluster identifier in a grid in each\ndimension. Each cluster in a grid has a unique identifier.\nThe\n%clusterid\nspecial register contains a 1D, 2D, or 3D vector, depending upon the shape and\nrank of the cluster. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %clusterid.x <  %nclusterid.x\n0  <=  %clusterid.y <  %nclusterid.y\n0  <=  %clusterid.z <  %nclusterid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %clusterid.x;\nmov.u32     %r1, %clusterid.z;\nmov.v4.u32  %rx, %clusterid;\n10.13.\nSpecial Registers:\n%nclusterid\n\n%nclusterid\nNumber of cluster identifiers per grid.\nSyntax (predefined)\n.sreg .v4 .u32 %nclusterid;\n.sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z;\nDescription\nA predefined, read-only special register initialized with the number of clusters in each grid\ndimension.\nThe\n%nclusterid\nspecial register contains a 3D grid shape vector that holds the grid dimensions\nin terms of clusters. The fourth element is unused and always returns zero.\nRefer to the\nCuda Programming Guide\nfor details on the maximum values of\n%nclusterid.{x,y,z}\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %nclusterid.x;\nmov.u32     %r1, %nclusterid.z;\nmov.v4.u32  %rx, %nclusterid;\n10.14.\nSpecial Registers:\n%cluster_ctaid\n\n%cluster_ctaid\nCTA identifier within a cluster.\nSyntax (predefined)\n.sreg .v4 .u32 %cluster_ctaid;\n.sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z;\nDescription\nA predefined, read-only special register initialized with the CTA identifier in a cluster in each\ndimension. Each CTA in a cluster has a unique CTA identifier.\nThe\n%cluster_ctaid\nspecial register contains a 1D, 2D, or 3D vector, depending upon the shape of\nthe cluster. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %cluster_ctaid.x <  %cluster_nctaid.x\n0  <=  %cluster_ctaid.y <  %cluster_nctaid.y\n0  <=  %cluster_ctaid.z <  %cluster_nctaid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_ctaid.x;\nmov.u32     %r1, %cluster_ctaid.z;\nmov.v4.u32  %rx, %cluster_ctaid;\n10.15.\nSpecial Registers:\n%cluster_nctaid\n\n%cluster_nctaid\nNumber of CTA identifiers per cluster.\nSyntax (predefined)\n.sreg .v4 .u32 %cluster_nctaid;\n.sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z;\nDescription\nA predefined, read-only special register initialized with the number of CTAs in a cluster in each\ndimension.\nThe\n%cluster_nctaid\nspecial register contains a 3D grid shape vector that holds the cluster\ndimensions in terms of CTAs. The fourth element is unused and always returns zero.\nRefer to the\nCuda Programming Guide\nfor details on the maximum values of\n%cluster_nctaid.{x,y,z}\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_nctaid.x;\nmov.u32     %r1, %cluster_nctaid.z;\nmov.v4.u32  %rx, %cluster_nctaid;\n10.16.\nSpecial Registers:\n%cluster_ctarank\n\n%cluster_ctarank\nCTA identifier in a cluster across all dimensions.\nSyntax (predefined)\n.sreg .u32 %cluster_ctarank;\nDescription\nA predefined, read-only special register initialized with the CTA rank within a cluster across all\ndimensions.\nIt is guaranteed that:\n0  <=  %cluster_ctarank <  %cluster_nctarank\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r;\n\nmov.u32  %r, %cluster_ctarank;\n10.17.\nSpecial Registers:\n%cluster_nctarank\n\n%cluster_nctarank\nNumber of CTA identifiers in a cluster across all dimensions.\nSyntax (predefined)\n.sreg .u32 %cluster_nctarank;\nDescription\nA predefined, read-only special register initialized with the nunber of CTAs within a cluster across\nall dimensions.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r;\n\nmov.u32  %r, %cluster_nctarank;\n10.18.\nSpecial Registers:\n%lanemask_eq\n\n%lanemask_eq\n32-bit mask with bit set in position equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_eq;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with a bit set in the\nposition equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_eq\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_eq;\n10.19.\nSpecial Registers:\n%lanemask_le\n\n%lanemask_le\n32-bit mask with bits set in positions less than or equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_le;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than or equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_le\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_le\n10.20.\nSpecial Registers:\n%lanemask_lt\n\n%lanemask_lt\n32-bit mask with bits set in positions less than the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_lt;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_lt\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_lt;\n10.21.\nSpecial Registers:\n%lanemask_ge\n\n%lanemask_ge\n32-bit mask with bits set in positions greater than or equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_ge;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than or equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_ge\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_ge;\n10.22.\nSpecial Registers:\n%lanemask_gt\n\n%lanemask_gt\n32-bit mask with bits set in positions greater than the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_gt;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_gt\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_gt;\n10.23.\nSpecial Registers:\n%clock\n,\n%clock_hi\n\n%clock\n,\n%clock_hi\n%clock\nA predefined, read-only 32-bit unsigned cycle counter.\n%clock_hi\nThe upper 32-bits of\n%clock64\nspecial register.\nSyntax (predefined)\n.sreg .u32 %clock;\n.sreg .u32 %clock_hi;\nDescription\nSpecial register\n%clock\nand\n%clock_hi\nare unsigned 32-bit read-only cycle counters that wrap\nsilently.\nPTX ISA Notes\n%clock\nintroduced in PTX ISA version 1.0.\n%clock_hi\nintroduced in PTX ISA version 5.0.\nTarget ISA Notes\n%clock\nsupported on all target architectures.\n%clock_hi\nrequires\nsm_20\nor higher.\nExamples\nmov.u32 r1,%clock;\nmov.u32 r2, %clock_hi;\n10.24.\nSpecial Registers:\n%clock64\n\n%clock64\nA predefined, read-only 64-bit unsigned cycle counter.\nSyntax (predefined)\n.sreg .u64 %clock64;\nDescription\nSpecial register\n%clock64\nis an unsigned 64-bit read-only cycle counter that wraps silently.\nNotes\nThe lower 32-bits of\n%clock64\nare identical to\n%clock\n.\nThe upper 32-bits of\n%clock64\nare identical to\n%clock_hi\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%clock64\nrequires\nsm_20\nor higher.\nExamples\nmov.u64  r1,%clock64;\n10.25.\nSpecial Registers:\n%pm0\n…\n%pm7\n\n%pm0\n…\n%pm7\nPerformance monitoring counters.\nSyntax (predefined)\n.sreg .u32 %pm<8>;\nDescription\nSpecial registers\n%pm0\n…\n%pm7\nare unsigned 32-bit read-only performance monitor counters. Their\nbehavior is currently undefined.\nPTX ISA Notes\n%pm0\n…\n%pm3\nintroduced in PTX ISA version 1.3.\n%pm4\n…\n%pm7\nintroduced in PTX ISA version 3.0.\nTarget ISA Notes\n%pm0\n…\n%pm3\nsupported on all target architectures.\n%pm4\n…\n%pm7\nrequire\nsm_20\nor higher.\nExamples\nmov.u32  r1,%pm0;\nmov.u32  r1,%pm7;\n10.26.\nSpecial Registers:\n%pm0_64\n…\n%pm7_64\n\n%pm0_64\n…\n%pm7_64\n64 bit Performance monitoring counters.\nSyntax (predefined)\n.sreg .u64 %pm0_64;\n.sreg .u64 %pm1_64;\n.sreg .u64 %pm2_64;\n.sreg .u64 %pm3_64;\n.sreg .u64 %pm4_64;\n.sreg .u64 %pm5_64;\n.sreg .u64 %pm6_64;\n.sreg .u64 %pm7_64;\nDescription\nSpecial registers\n%pm0_64\n…\n%pm7_64\nare unsigned 64-bit read-only performance monitor\ncounters. Their behavior is currently undefined.\nNotes\nThe lower 32bits of\n%pm0_64\n…\n%pm7_64\nare identical to\n%pm0\n…\n%pm7\n.\nPTX ISA Notes\n%pm0_64\n…\n%pm7_64\nintroduced in PTX ISA version 4.0.\nTarget ISA Notes\n%pm0_64\n…\n%pm7_64\nrequire\nsm_50\nor higher.\nExamples\nmov.u32  r1,%pm0_64;\nmov.u32  r1,%pm7_64;\n10.27.\nSpecial Registers:\n%envreg<32>\n\n%envreg<32>\nDriver-defined read-only registers.\nSyntax (predefined)\n.sreg .b32 %envreg<32>;\nDescription\nA set of 32 pre-defined read-only registers used to capture execution environment of PTX program\noutside of PTX virtual machine. These registers are initialized by the driver prior to kernel launch\nand can contain cta-wide or grid-wide values.\nPrecise semantics of these registers is defined in the driver documentation.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.b32      %r1,%envreg0;  // move envreg0 to %r1\n10.28.\nSpecial Registers:\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n%globaltimer\nA predefined, 64-bit global nanosecond timer.\n%globaltimer_lo\nThe lower 32-bits of %globaltimer.\n%globaltimer_hi\nThe upper 32-bits of %globaltimer.\nSyntax (predefined)\n.sreg .u64 %globaltimer;\n.sreg .u32 %globaltimer_lo, %globaltimer_hi;\nDescription\nSpecial registers intended for use by NVIDIA tools. The behavior is target-specific and may change\nor be removed in future GPUs. When JIT-compiled to other targets, the value of these registers is\nunspecified.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nRequires target\nsm_30\nor higher.\nExamples\nmov.u64  r1,%globaltimer;\n10.29.\nSpecial Registers:\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n%reserved_smem_offset_begin\nStart of the reserved shared memory region.\n%reserved_smem_offset_end\nEnd of the reserved shared memory region.\n%reserved_smem_offset_cap\nTotal size of the reserved shared memory region.\n%reserved_smem_offset_<2>\nOffsets in the reserved shared memory region.\nSyntax (predefined)\n.sreg .b32 %reserved_smem_offset_begin;\n.sreg .b32 %reserved_smem_offset_end;\n.sreg .b32 %reserved_smem_offset_cap;\n.sreg .b32 %reserved_smem_offset_<2>;\nDescription\nThese are predefined, read-only special registers containing information about the shared memory\nregion which is reserved for the NVIDIA system software use. This region of shared memory is not\navailable to users, and accessing this region from user code results in undefined behavior. Refer to\nCUDA Programming Guide\nfor details.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nRequire\nsm_80\nor higher.\nExamples\n.reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1;\n\nmov.b32 %reg_begin,   %reserved_smem_offset_begin;\nmov.b32 %reg_end,     %reserved_smem_offset_end;\nmov.b32 %reg_cap,     %reserved_smem_offset_cap;\nmov.b32 %reg_offset0, %reserved_smem_offset_0;\nmov.b32 %reg_offset1, %reserved_smem_offset_1;\n10.30.\nSpecial Registers:\n%total_smem_size\n\n%total_smem_size\nTotal size of shared memory used by a CTA of a kernel.\nSyntax (predefined)\n.sreg .u32 %total_smem_size;\nDescription\nA predefined, read-only special register initialized with total size of shared memory allocated\n(statically and dynamically, excluding the shared memory reserved for the NVIDIA system software\nuse) for the CTA of a kernel at launch time.\nSize is returned in multiples of shared memory allocation unit size supported by target\narchitecture.\nAllocation unit values are as follows:\nTarget architecture\nShared memory allocation unit size\nsm_2x\n128 bytes\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n256 bytes\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n128 bytes\nPTX ISA Notes\nIntroduced in PTX ISA version 4.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %total_smem_size;\n10.31.\nSpecial Registers:\n%aggr_smem_size\n\n%aggr_smem_size\nTotal size of shared memory used by a CTA of a kernel.\nSyntax (predefined)\n.sreg .u32 %aggr_smem_size;\nDescription\nA predefined, read-only special register initialized with total aggregated size of shared memory\nconsisting of the size of user shared memory allocated (statically and dynamically) at launch time\nand the size of shared memory region which is reserved for the NVIDIA system software use.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmov.u32  %r, %aggr_smem_size;\n10.32.\nSpecial Registers:\n%dynamic_smem_size\n\n%dynamic_smem_size\nSize of shared memory allocated dynamically at kernel launch.\nSyntax (predefined)\n.sreg .u32 %dynamic_smem_size;\nDescription\nSize of shared memory allocated dynamically at kernel launch.\nA predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %dynamic_smem_size;\n10.33.\nSpecial Registers:\n%current_graph_exec\n\n%current_graph_exec\nAn Identifier for currently executing CUDA device graph.\nSyntax (predefined)\n.sreg .u64 %current_graph_exec;\nDescription\nA predefined, read-only special register initialized with the identifier referring to the CUDA\ndevice graph being currently executed. This register is 0 if the executing kernel is not part of a\nCUDA device graph.\nRefer to the\nCUDA Programming Guide\nfor more details on CUDA device graphs.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nExamples\nmov.u64  r1, %current_graph_exec;\n11.\nDirectives\n\n11.1.\nPTX Module Directives\n\nThe following directives declare the PTX ISA version of the code in the module, the target\narchitecture for which the code was generated, and the size of addresses within the PTX module.\n.version\n.target\n.address_size\n11.1.1.\nPTX Module Directives:\n.version\n\n.version\nPTX ISA version number.\nSyntax\n.version  major.minor    // major, minor are integers\nDescription\nSpecifies the PTX language version number.\nThe\nmajor\nnumber is incremented when there are incompatible changes to the PTX language, such as\nchanges to the syntax or semantics. The version major number is used by the PTX compiler to ensure\ncorrect execution of legacy PTX code.\nThe\nminor\nnumber is incremented when new features are added to PTX.\nSemantics\nIndicates that this module must be compiled with tools that support an equal or greater version\nnumber.\nEach PTX module must begin with a\n.version\ndirective, and no other\n.version\ndirective is\nallowed anywhere else within the module.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.version 3.1\n.version 3.0\n.version 2.3\n11.1.2.\nPTX Module Directives:\n.target\n\n.target\nArchitecture and Platform target.\nSyntax\n.target stringlist         // comma separated list of target specifiers\nstring = { sm_120a, sm_120f, sm_120,          // sm_12x target architectures\n           sm_121a, sm_121f, sm_121,          // sm_12x target architectures\n           sm_110a, sm_110f, sm_110,          // sm_11x target architectures\n           sm_100a, sm_100f, sm_100,          // sm_10x target architectures\n           sm_101a, sm_101f, sm_101,          // sm_10x target architectures\n           sm_103a, sm_103f, sm_103           // sm_10x target architectures\n           sm_90a, sm_90,                     // sm_9x target architectures\n           sm_80, sm_86, sm_87, sm_88, sm_89, // sm_8x target architectures\n           sm_70, sm_72, sm_75,               // sm_7x target architectures\n           sm_60, sm_61, sm_62,               // sm_6x target architectures\n           sm_50, sm_52, sm_53,               // sm_5x target architectures\n           sm_30, sm_32, sm_35, sm_37,        // sm_3x target architectures\n           sm_20,                             // sm_2x target architectures\n           sm_10, sm_11, sm_12, sm_13,        // sm_1x target architectures\n           texmode_unified, texmode_independent,   // texturing mode\n           debug,                                  // platform option\n           map_f64_to_f32 };                       // platform option\nDescription\nSpecifies the set of features in the target architecture for which the current PTX code was\ngenerated. In general, generations of SM architectures follow an\nonion layer\nmodel, where each\ngeneration adds new features and retains all features of previous generations. The onion layer model\nallows the PTX code generated for a given target to be run on later generation devices.\nTarget architectures with suffix “\na\n”, such as\nsm_90a\n, include architecture-specific\nfeatures that are supported on the specified architecture only, hence such targets do not follow the\nonion layer model. Therefore, PTX code generated for such targets cannot be run on later generation\ndevices. Architecture-specific features can only be used with targets that support these\nfeatures.\nTarget architectures with suffix “\nf\n”, such as\nsm_100f\n, include family-specific features that\nare supported only within the same architecture family. Therefore, PTX code generated for such\ntargets can run only on later generation devices in the same family. Family-specific features can be\nused with f-targets as well as a-targets of later generation devices in the same family.\nTable 58\ndefines the architecture families.\nTable 58\nArchitecture Families\n\nFamily\nTarget SM architectures included\nsm_10x family\nsm_100f, sm_103f, future targets\nin sm_10x family\nsm_11x family\nsm_110f, sm_101f, future targets\nin sm_11x family\nsm_12x family\nsm_120f, sm_121f, future targets\nin sm_12x family\nSemantics\nEach PTX module must begin with a\n.version\ndirective, immediately followed by a\n.target\ndirective containing a target architecture and optional platform options. A\n.target\ndirective\nspecifies a single target architecture, but subsequent\n.target\ndirectives can be used to change\nthe set of target features allowed during parsing. A program with multiple\n.target\ndirectives\nwill compile and run only on devices that support all features of the highest-numbered architecture\nlisted in the program.\nPTX features are checked against the specified target architecture, and an error is generated if an\nunsupported feature is used. The following table summarizes the features in PTX that vary according\nto target architecture.\nTarget\nDescription\nsm_120\nBaseline feature set for\nsm_120\narchitecture.\nsm_120f\nAdds support for\nsm_120f\nfamily specific features.\nsm_120a\nAdds support for\nsm_120a\narchitecture-specific features.\nsm_121\nBaseline feature set for\nsm_121\narchitecture.\nsm_121f\nAdds support for\nsm_121f\nfamily specific features.\nsm_121a\nAdds support for\nsm_121a\narchitecture-specific features.\nTarget\nDescription\nsm_110\nBaseline feature set for\nsm_110\narchitecture.\nsm_110f\nAdds support for\nsm_110f\nfamily specific features.\nsm_110a\nAdds support for\nsm_110a\narchitecture-specific features.\nTarget\nDescription\nsm_100\nBaseline feature set for\nsm_100\narchitecture.\nsm_100f\nAdds support for\nsm_100f\nfamily specific features.\nsm_100a\nAdds support for\nsm_100a\narchitecture-specific features.\nsm_101\nBaseline feature set for\nsm_101\narchitecture. (Renamed to\nsm_110\n)\nsm_101f\nAdds support for\nsm_101f\nfamily specific features. (Renamed to\nsm_110f\n)\nsm_101a\nAdds support for\nsm_101a\narchitecture-specific features. (Renamed to\nsm_110a\n)\nsm_103\nBaseline feature set for\nsm_103\narchitecture.\nsm_103f\nAdds support for\nsm_103f\nfamily specific features.\nsm_103a\nAdds support for\nsm_103a\narchitecture-specific features.\nTarget\nDescription\nsm_90\nBaseline feature set for\nsm_90\narchitecture.\nsm_90a\nAdds support for\nsm_90a\narchitecture-specific features.\nTarget\nDescription\nsm_80\nBaseline feature set for\nsm_80\narchitecture.\nsm_86\nAdds support for\n.xorsign\nmodifier on\nmin\nand\nmax\ninstructions.\nsm_87\nBaseline feature set for\nsm_87\narchitecture.\nsm_88\nBaseline feature set for\nsm_88\narchitecture.\nsm_89\nBaseline feature set for\nsm_89\narchitecture.\nTarget\nDescription\nsm_70\nBaseline feature set for\nsm_70\narchitecture.\nsm_72\nAdds support for integer multiplicand and accumulator matrices in\nwmma\ninstructions.\nAdds support for\ncvt.pack\ninstruction.\nsm_75\nAdds support for sub-byte integer and single-bit multiplicant matrices in\nwmma\ninstructions.\nAdds support for\nldmatrix\ninstruction.\nAdds support for\nmovmatrix\ninstruction.\nAdds support for\ntanh\ninstruction.\nTarget\nDescription\nsm_60\nBaseline feature set for\nsm_60\narchitecture.\nsm_61\nAdds support for\ndp2a\nand\ndp4a\ninstructions.\nsm_62\nBaseline feature set for\nsm_61\narchitecture.\nTarget\nDescription\nsm_50\nBaseline feature set for\nsm_50\narchitecture.\nsm_52\nBaseline feature set for\nsm_50\narchitecture.\nsm_53\nAdds support for arithmetic, comparsion and texture instructions for\n.f16\nand\n.f16x2\ntypes.\nTarget\nDescription\nsm_30\nBaseline feature set for\nsm_30\narchitecture.\nsm_32\nAdds 64-bit\n{atom,red}.{and,or,xor,min,max}\ninstructions.\nAdds\nshf\ninstruction.\nAdds\nld.global.nc\ninstruction.\nsm_35\nAdds support for CUDA Dynamic Parallelism.\nsm_37\nBaseline feature set for\nsm_35\narchitecture.\nTarget\nDescription\nsm_20\nBaseline feature set for\nsm_20\narchitecture.\nTarget\nDescription\nsm_10\nBaseline feature set for\nsm_10\narchitecture.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_11\nAdds 64-bit\n{atom,red}.{and,or,xor,min,max}\ninstructions.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_12\nAdds\n{atom,red}.shared\n, 64-bit\n{atom,red}.global\n,\nvote\ninstructions.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_13\nAdds double-precision support, including expanded rounding modifiers.\nDisallows use of\nmap_f64_to_f32\n.\nThe texturing mode is specified for an entire module and cannot be changed within the module.\nThe\n.target\ndebug option declares that the PTX file contains DWARF debug information, and\nsubsequent compilation of PTX will retain information needed for source-level debugging. If the\ndebug option is declared, an error message is generated if no DWARF information is found in the\nfile. The debug option requires PTX ISA version 3.0 or later.\nmap_f64_to_f32\nindicates that all double-precision instructions map to single-precision\nregardless of the target architecture. This enables high-level language compilers to compile\nprograms containing type double to target device that do not support double-precision\noperations. Note that\n.f64\nstorage remains as 64-bits, with only half being used by instructions\nconverted from\n.f64\nto\n.f32\n.\nNotes\nTargets of the form\ncompute_xx\nare also accepted as synonyms for\nsm_xx\ntargets.\nTargets\nsm_{101,101f,101a}\nare renamed to targets\nsm_{110,110f,110a}\nfrom PTX ISA version 9.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget strings\nsm_10\nand\nsm_11\nintroduced in PTX ISA version 1.0.\nTarget strings\nsm_12\nand\nsm_13\nintroduced in PTX ISA version 1.2.\nTexturing mode introduced in PTX ISA version 1.5.\nTarget string\nsm_20\nintroduced in PTX ISA version 2.0.\nTarget string\nsm_30\nintroduced in PTX ISA version 3.0.\nPlatform option\ndebug\nintroduced in PTX ISA version 3.0.\nTarget string\nsm_35\nintroduced in PTX ISA version 3.1.\nTarget strings\nsm_32\nand\nsm_50\nintroduced in PTX ISA version 4.0.\nTarget strings\nsm_37\nand\nsm_52\nintroduced in PTX ISA version 4.1.\nTarget string\nsm_53\nintroduced in PTX ISA version 4.2.\nTarget string\nsm_60\n,\nsm_61\n,\nsm_62\nintroduced in PTX ISA version 5.0.\nTarget string\nsm_70\nintroduced in PTX ISA version 6.0.\nTarget string\nsm_72\nintroduced in PTX ISA version 6.1.\nTarget string\nsm_75\nintroduced in PTX ISA version 6.3.\nTarget string\nsm_80\nintroduced in PTX ISA version 7.0.\nTarget string\nsm_86\nintroduced in PTX ISA version 7.1.\nTarget string\nsm_87\nintroduced in PTX ISA version 7.4.\nTarget string\nsm_88\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_89\nintroduced in PTX ISA version 7.8.\nTarget string\nsm_90\nintroduced in PTX ISA version 7.8.\nTarget string\nsm_90a\nintroduced in PTX ISA version 8.0.\nTarget string\nsm_100\nintroduced in PTX ISA version 8.6.\nTarget string\nsm_100f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_100a\nintroduced in PTX ISA version 8.6.\nTarget string\nsm_101\nintroduced in PTX ISA version 8.6. (Renamed to\nsm_110\n)\nTarget string\nsm_101f\nintroduced in PTX ISA version 8.8. (Renamed to\nsm_110f\n)\nTarget string\nsm_101a\nintroduced in PTX ISA version 8.6. (Renamed to\nsm_110a\n)\nTarget string\nsm_103\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_103f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_103a\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_110\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_110f\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_110a\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_120\nintroduced in PTX ISA version 8.7.\nTarget string\nsm_120f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_120a\nintroduced in PTX ISA version 8.7.\nTarget string\nsm_121\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_121f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_121a\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nThe\n.target\ndirective is supported on all target architectures.\nExamples\n.target sm_10       // baseline target architecture\n.target sm_13       // supports double-precision\n.target sm_20, texmode_independent\n.target sm_90       // baseline target architecture\n.target sm_90a      // PTX using architecture-specific features\n.target sm_100f     // PTX using family-specific features\n11.1.3.\nPTX Module Directives:\n.address_size\n\n.address_size\nAddress size used throughout PTX module.\nSyntax\n.address_size  address-size\naddress-size = { 32, 64 };\nDescription\nSpecifies the address size assumed throughout the module by the PTX code and the binary DWARF\ninformation in PTX.\nRedefinition of this directive within a module is not allowed. In the presence of separate\ncompilation all modules must specify (or default to) the same address size.\nThe\n.address_size\ndirective is optional, but it must immediately follow the\n.target\ndirective if present within a module.\nSemantics\nIf the\n.address_size\ndirective is omitted, the address size defaults to 32.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n// example directives\n   .address_size 32       // addresses are 32 bit\n   .address_size 64       // addresses are 64 bit\n\n// example of directive placement within a module\n   .version 2.3\n   .target sm_20\n   .address_size 64\n...\n.entry foo () {\n...\n}\n11.2.\nSpecifying Kernel Entry Points and Functions\n\nThe following directives specify kernel entry points and functions.\n.entry\n.func\n11.2.1.\nKernel and Function Directives:\n.entry\n\n.entry\nKernel entry point and body, with optional parameters.\nSyntax\n.entry kernel-name ( param-list )  kernel-body\n.entry kernel-name  kernel-body\nDescription\nDefines a kernel entry point name, parameters, and body for the kernel function.\nParameters are passed via\n.param\nspace memory and are listed within an optional parenthesized\nparameter list. Parameters may be referenced by name within the kernel body and loaded into\nregisters using\nld.param{::entry}\ninstructions.\nIn addition to normal parameters, opaque\n.texref\n,\n.samplerref\n, and\n.surfref\nvariables\nmay be passed as parameters. These parameters can only be referenced by name within texture and\nsurface load, store, and query instructions and cannot be accessed via\nld.param\ninstructions.\nThe shape and size of the CTA executing the kernel are available in special registers.\nSemantics\nSpecify the entry point for a kernel program.\nAt kernel launch, the kernel dimensions and properties are established and made available via\nspecial registers, e.g.,\n%ntid\n,\n%nctaid\n, etc.\nPTX ISA Notes\nFor PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter\nlist. For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body.\nThe maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764\nbytes. Depending upon the PTX ISA version, the parameter size limit varies. The following table\nshows the allowed parameter size for a PTX ISA version:\nPTX ISA Version\nMaximum parameter size (In bytes)\nPTX ISA version 8.1 and above\n32764\nPTX ISA version 1.5 and above\n4352\nPTX ISA version 1.4 and above\n256\nThe CUDA and OpenCL drivers support the following limits for parameter memory:\nDriver\nParameter memory size\nCUDA\n256 bytes for\nsm_1x\n, 4096 bytes for\nsm_2x\nand\nhigher\n,\n32764 bytes fo\nsm_70\nand higher\nOpenCL\n32764 bytes for\nsm_70\nand higher, 4352 bytes on\nsm_6x\nand lower\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry cta_fft\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r<99>;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n\n.entry prefix_sum ( .param .align 4 .s32 pitch[8000] )\n{\n    .reg .s32 %t;\n    ld.param::entry.s32  %t, [pitch];\n    ...\n}\n11.2.2.\nKernel and Function Directives:\n.func\n\n.func\nFunction definition.\nSyntax\n.func {.attribute(attr-list)} fname {.noreturn} {.abi_preserve N} {.abi_preserve_control N} function-body\n.func {.attribute(attr-list)} fname (param-list) {.noreturn} {.abi_preserve N} {.abi_preserve_control N} function-body\n.func {.attribute(attr-list)} (ret-param) fname (param-list) {.abi_preserve N} {.abi_preserve_control N} function-body\nDescription\nDefines a function, including input and return parameters and optional function body.\nAn optional\n.noreturn\ndirective indicates that the function does not return to the caller\nfunction.\n.noreturn\ndirective cannot be specified on functions which have return parameters. See\nthe description of\n.noreturn\ndirective in\nPerformance-Tuning Directives: .noreturn\n.\nAn optional\n.attribute\ndirective specifies additional information associated with the\nfunction. See the description of\nVariable and Function Attribute Directive: .attribute\nfor allowed attributes.\nOptional\n.abi_preserve\nand\n.abi_preserve_control\ndirectives are used to specify the number\nof general purpose registers and control registers. See description of\nPerformance-Tuning Directives: .abi_preserve\nand\nPerformance-Tuning Directives: .abi_preserve_control\nfor more details.\nDirectives, if any specified, for a function, e.g.\n.noreturn\n, must be specified consistently\nbetween function declaration and definition.\nA\n.func\ndefinition with no body provides a function prototype.\nThe parameter lists define locally-scoped variables in the function body. Parameters must be base\ntypes in either the register or parameter state space. Parameters in register state space may be\nreferenced directly within instructions in the function body. Parameters in\n.param\nspace are\naccessed using\nld.param{::func}\nand\nst.param{::func}\ninstructions in the body. Parameter\npassing is call-by-value.\nThe last parameter in the parameter list may be a\n.param\narray of type\n.b8\nwith no size\nspecified. It is used to pass an arbitrary number of parameters to the function packed into a single\narray object.\nWhen calling a function with such an unsized last argument, the last argument may be omitted from\nthe\ncall\ninstruction if no parameter is passed through it. Accesses to this array parameter must\nbe within the bounds of the array. The result of an access is undefined if no array was passed, or\nif the access was outside the bounds of the actual array being passed.\nSemantics\nThe PTX syntax hides all details of the underlying calling convention and ABI.\nThe implementation of parameter passing is left to the optimizing translator, which may use a\ncombination of registers and stack locations to pass parameters.\nRelease Notes\nFor PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and\nrecursion is illegal.\nPTX ISA versions 2.0 and later with target\nsm_20\nor higher allow parameters in the\n.param\nstate space, implements an ABI with stack, and supports recursion.\nPTX ISA versions 2.0 and later with target\nsm_20\nor higher support at most one return value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nSupport for unsized array parameter introduced in PTX ISA version 6.0.\nSupport for\n.noreturn\ndirective introduced in PTX ISA version 6.4.\nSupport for\n.attribute\ndirective introduced in PTX ISA version 8.0.\nSupport for\n.abi_preserve\nand\n.abi_preserve_control\ndirectives introduced in PTX ISA version 9.0.\nTarget ISA Notes\nFunctions without unsized array parameter supported on all target architectures.\nUnsized array parameter requires\nsm_30\nor higher.\n.noreturn\ndirective requires\nsm_30\nor higher.\n.attribute\ndirective requires\nsm_90\nor higher.\n.abi_preserve\nand\n.abi_preserve_control\ndirectives require\nsm_80\nor higher.\nExamples\n.func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl)\n{\n.reg .b32 localVar;\n\n... use N, dbl;\nother code;\n\nmov.b32 rval,result;\nret;\n}\n\n...\ncall (fooval), foo, (val0, val1);  // return value in fooval\n...\n\n.func foo (.reg .b32 N, .reg .f64 dbl) .noreturn\n{\n.reg .b32 localVar;\n... use N, dbl;\nother code;\nmov.b32 rval, result;\nret;\n}\n...\ncall foo, (val0, val1);\n...\n\n.func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[])\n{\n    .reg .b32 input0, input1;\n    ld.param.b32   input0, [numbers + 0];\n    ld.param.b32   input1, [numbers + 4];\n    ...\n    other code;\n    ret;\n}\n...\n\n.param .u32 N;\n.param .align 4 .b8 numbers[8];\nst.param.u32    [N], 2;\nst.param.b32    [numbers + 0], 5;\nst.param.b32    [numbers + 4], 10;\ncall (rval), bar, (N, numbers);\n...\n11.2.3.\nKernel and Function Directives:\n.alias\n\n.alias\nDefine an alias to existing function symbol.\nSyntax\n.alias fAlias, fAliasee;\nDescription\n.alias\nis a module scope directive that defines identifier\nfAlias\nto be an alias to function\nspecified by\nfAliasee\n.\nBoth\nfAlias\nand\nfAliasee\nare non-entry function symbols.\nIdentifier\nfAlias\nis a function declaration without body.\nIdentifier\nfAliasee\nis a function symbol which must be defined in the same module as\n.alias\ndeclaration. Function\nfAliasee\ncannot have\n.weak\nlinkage.\nPrototype of\nfAlias\nand\nfAliasee\nmust match.\nProgram can use either\nfAlias\nor\nfAlisee\nidentifiers to reference function defined with\nfAliasee\n.\nPTX ISA Notes\n.alias\ndirective introduced in PTX ISA 6.3.\nTarget ISA Notes\n.alias\ndirective requires\nsm_30\nor higher.\nExamples\n.visible .func foo(.param .u32 p) {\n   ...\n}\n.visible .func bar(.param .u32 p);\n.alias bar, foo;\n.entry test()\n{\n      .param .u32 p;\n      ...\n      call foo, (p);       // call foo directly\n       ...\n       .param .u32 p;\n       call bar, (p);        // call foo through alias\n}\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r1, %r2, %r3;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n11.3.\nControl Flow Directives\n\nPTX provides directives for specifying potential targets for\nbrx.idx\nand\ncall\ninstructions. See the descriptions of\nbrx.idx\nand\ncall\nfor more information.\n.branchtargets\n.calltargets\n.callprototype\n11.3.1.\nControl Flow Directives:\n.branchtargets\n\n.branchtargets\nDeclare a list of potential branch targets.\nSyntax\nLabel:   .branchtargets  list-of-labels ;\nDescription\nDeclares a list of potential branch targets for a subsequent\nbrx.idx\n, and associates the list\nwith the label at the start of the line.\nAll control flow labels in the list must occur within the same function as the declaration.\nThe list of labels may use the compact, shorthand syntax for enumerating a range of labels having a\ncommon prefix, similar to the syntax described in\nParameterized Variable Names\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\n.function foo () {\n      .reg .u32 %r0;\n      ...\n      L1:\n      ...\n      L2:\n      ...\n      L3:\n      ...\n      ts: .branchtargets L1, L2, L3;\n      @p brx.idx %r0, ts;\n      ...\n\n.function bar() {\n      .reg .u32 %r0;\n      ...\n      N0:\n      ...\n      N1:\n      ...\n      N2:\n      ...\n      N3:\n      ...\n      N4:\n      ...\n      ts: .branchtargets N<5>;\n      @p brx.idx %r0, ts;\n      ...\n11.3.2.\nControl Flow Directives:\n.calltargets\n\n.calltargets\nDeclare a list of potential call targets.\nSyntax\nLabel:   .calltargets  list-of-functions ;\nDescription\nDeclares a list of potential call targets for a subsequent indirect call, and associates the list\nwith the label at the start of the line.\nAll functions named in the list must be declared prior to the\n.calltargets\ndirective, and all\nfunctions must have the same type signature.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ncalltgt:  .calltargets  fastsin, fastcos;\n...\n@p   call  (%f1), %r0, (%x), calltgt;\n...\n11.3.3.\nControl Flow Directives:\n.callprototype\n\n.callprototype\nDeclare a prototype for use in an indirect call.\nSyntax\n// no input or return parameters\nlabel: .callprototype _ .noreturn {.abi_preserve N} {.abi_preserve_control N};\n// input params, no return params\nlabel: .callprototype _ (param-list) .noreturn {.abi_preserve N} {.abi_preserve_control N};\n// no input params, // return params\nlabel: .callprototype (ret-param) _ {.abi_preserve N} {.abi_preserve_control N};\n// input, return parameters\nlabel: .callprototype (ret-param) _ (param-list) {.abi_preserve N} {.abi_preserve_control N};\nDescription\nDefines a prototype with no specific function name, and associates the prototype with a label. The\nprototype may then be used in indirect call instructions where there is incomplete knowledge of the\npossible call targets.\nParameters may have either base types in the register or parameter state spaces, or array types in\nparameter state space. The sink symbol\n'_'\nmay be used to avoid dummy parameter names.\nAn optional\n.noreturn\ndirective indicates that the function does not return to the caller\nfunction.\n.noreturn\ndirective cannot be specified on functions which have return parameters. See\nthe description of .noreturn directive in\nPerformance-Tuning Directives: .noreturn\n.\nOptional\n.abi_preserve\nand\n.abi_preserve_control\ndirectives are used to specify the number\nof general purpose registers and control registers. See description of\nPerformance-Tuning Directives: .abi_preserve\nand\nPerformance-Tuning Directives: .abi_preserve_control\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nSupport for\n.noreturn\ndirective introduced in PTX ISA version 6.4.\nSupport for\n.abi_preserve\nand\n.abi_preserve_control\ndirectives introduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\n.noreturn\ndirective requires\nsm_30\nor higher.\n.abi_preserve\nand\n.abi_preserve_control\ndirectives require\nsm_80\nor higher.\nExamples\nFproto1: .callprototype  _ ;\nFproto2: .callprototype  _ (.param .f32 _);\nFproto3: .callprototype  (.param .u32 _) _ ;\nFproto4: .callprototype  (.param .u32 _) _ (.param .f32 _);\n...\n@p   call  (%val), %r0, (%f1), Fproto4;\n...\n\n// example of array parameter\nFproto5: .callprototype _ (.param .b8 _[12]);\n\nFproto6: .callprototype  _ (.param .f32 _) .noreturn;\n...\n@p   call  %r0, (%f1), Fproto6;\n...\n\n// example of .abi_preserve\nFproto7: .callprototype _ (.param .b32 _) .abi_preserve 10;\n...\n@p   call %r0, (%r1), Fproto7;\n...\n11.4.\nPerformance-Tuning Directives\n\nTo provide a mechanism for low-level performance tuning, PTX supports the following directives,\nwhich pass information to the optimizing backend compiler.\n.maxnreg\n.maxntid\n.reqntid\n.minnctapersm\n.maxnctapersm\n(deprecated)\n.pragma\n.abi_preserve\n.abi_preserve_control\nThe\n.maxnreg\ndirective specifies the maximum number of registers to be allocated to a single\nthread;\nthe\n.maxntid\ndirective specifies the maximum number of threads\nin a thread block (CTA); the\n.reqntid\ndirective specifies the required number of threads in a\nthread block (CTA); and the\n.minnctapersm\ndirective specifies a minimum number of thread blocks\nto be scheduled on a single multiprocessor (SM). These can be used, for example, to throttle the\nresource requirements (e.g., registers) to increase total thread count and provide a greater\nopportunity to hide memory latency. The\n.minnctapersm\ndirective can be used together with either\nthe\n.maxntid\nor\n.reqntid\ndirective to trade-off registers-per-thread against multiprocessor\nutilization without needed to directly specify a maximum number of registers. This may achieve better\nperformance when compiling PTX for multiple devices having different numbers of registers per SM.\nDevice function directives\n.abi_preserve\nand\n.abi_preserve_control\nspecify number of data\nand control registers from callee save registers that a function must preserve for its caller. This\ncan be considered to be the number of general purpose and control registers live in the caller when function\nis called. Control registers refer to the number of divergent program points that happen in the calltree\nleading to current function call.\nCurrently, the\n.maxnreg\n,\n.maxntid\n,\n.reqntid\n, and\n.minnctapersm\ndirectives may be applied per-entry and must appear between an\n.entry\ndirective and its body.\nThe directives take precedence over any module-level constraints passed to the optimizing backend.\nA warning message is generated if the directives’ constraints are inconsistent or cannot be met for\nthe specified target device.\nA general\n.pragma\ndirective is supported for passing information to the PTX backend. The\ndirective passes a list of strings to the backend, and the strings have no semantics within the PTX\nvirtual machine model. The interpretation of\n.pragma\nvalues is determined by the backend\nimplementation and is beyond the scope of the PTX ISA. Note that\n.pragma\ndirectives may appear\nat module (file) scope, at entry-scope, or as statements within a kernel or device function body.\n11.4.1.\nPerformance-Tuning Directives:\n.maxnreg\n\n.maxnreg\nMaximum number of registers that can be allocated per thread.\nSyntax\n.maxnreg n\nDescription\nDeclare the maximum number of registers per thread in a CTA.\nSemantics\nThe compiler guarantees that this limit will not be exceeded. The actual number of registers used\nmay be less; for example, the backend may be able to compile to fewer registers, or the maximum\nnumber of registers may be further constrained by\n.maxntid\nand\n.maxctapersm\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxnreg 16 { ... }  // max regs per thread = 16\n11.4.2.\nPerformance-Tuning Directives:\n.maxntid\n\n.maxntid\nMaximum number of threads in the thread block (CTA).\nSyntax\n.maxntid nx\n.maxntid nx, ny\n.maxntid nx, ny, nz\nDescription\nDeclare the maximum number of threads in the thread block (CTA). This maximum is specified by giving\nthe maximum extent of each dimension of the 1D, 2D, or 3D CTA. The maximum number of threads is the\nproduct of the maximum extent in each dimension.\nSemantics\nThe maximum number of threads in the thread block, computed as the product of the maximum extent\nspecified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in\nwhich this directive appears. Exceeding the maximum number of threads results in a runtime error or\nkernel launch failure.\nNote that this directive guarantees that the\ntotal\nnumber of threads does not exceed the maximum,\nbut does not guarantee that the limit in any particular dimension is not exceeded.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256       { ... }  // max threads = 256\n.entry bar .maxntid 16,16,4   { ... }  // max threads = 1024\n11.4.3.\nPerformance-Tuning Directives:\n.reqntid\n\n.reqntid\nNumber of threads in the thread block (CTA).\nSyntax\n.reqntid nx\n.reqntid nx, ny\n.reqntid nx, ny, nz\nDescription\nDeclare the number of threads in the thread block (CTA) by specifying the extent of each dimension\nof the 1D, 2D, or 3D CTA. The total number of threads is the product of the number of threads in\neach dimension.\nSemantics\nThe size of each CTA dimension specified in any invocation of the kernel is required to be equal to\nthat specified in this directive. Specifying a different CTA dimension at launch will result in a\nruntime error or kernel launch failure.\nNotes\nThe\n.reqntid\ndirective cannot be used in conjunction with the\n.maxntid\ndirective.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .reqntid 256       { ... }  // num threads = 256\n.entry bar .reqntid 16,16,4   { ... }  // num threads = 1024\n11.4.4.\nPerformance-Tuning Directives:\n.minnctapersm\n\n.minnctapersm\nMinimum number of CTAs per SM.\nSyntax\n.minnctapersm ncta\nDescription\nDeclare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor\n(SM).\nNotes\nOptimizations based on\n.minnctapersm\nneed either\n.maxntid\nor\n.reqntid\nto be specified as\nwell.\nIf the total number of threads on a single SM resulting from\n.minnctapersm\nand\n.maxntid\n/\n.reqntid\nexceed maximum number of threads supported by an SM then directive\n.minnctapersm\nwill be ignored.\nIn PTX ISA version 2.1 or higher, a warning is generated if\n.minnctapersm\nis specified without\nspecifying either\n.maxntid\nor\n.reqntid\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0 as a replacement for\n.maxnctapersm\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256 .minnctapersm 4 { ... }\n11.4.5.\nPerformance-Tuning Directives:\n.maxnctapersm\n(deprecated)\n\n.maxnctapersm\nMaximum number of CTAs per SM.\nSyntax\n.maxnctapersm ncta\nDescription\nDeclare the maximum number of CTAs from the kernel’s grid that may be mapped to a single\nmultiprocessor (SM).\nNotes\nOptimizations based on .maxnctapersm generally need\n.maxntid\nto be specified as well. The\noptimizing backend compiler uses\n.maxntid\nand\n.maxnctapersm\nto compute an upper-bound on\nper-thread register usage so that the specified number of CTAs can be mapped to a single\nmultiprocessor. However, if the number of registers used by the backend is sufficiently lower than\nthis bound, additional CTAs may be mapped to a single multiprocessor. For this reason,\n.maxnctapersm\nhas been renamed to .minnctapersm in PTX ISA version 2.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3. Deprecated in PTX ISA version 2.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256 .maxnctapersm 4 { ... }\n11.4.6.\nPerformance-Tuning Directives:\n.noreturn\n\n.noreturn\nIndicate that the function does not return to its caller function.\nSyntax\n.noreturn\nDescription\nIndicate that the function does not return to its caller function.\nSemantics\nAn optional\n.noreturn\ndirective indicates that the function does not return to caller\nfunction.\n.noreturn\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nThe directive cannot be specified on functions which have return parameters.\nIf a function with\n.noreturn\ndirective returns to the caller function at runtime, then the\nbehavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.4.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\n.func foo .noreturn { ... }\n11.4.7.\nPerformance-Tuning Directives:\n.pragma\n\n.pragma\nPass directives to PTX backend compiler.\nSyntax\n.pragma list-of-strings ;\nDescription\nPass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler.\nThe\n.pragma\ndirective may occur at module-scope, at entry-scope, or at statement-level.\nSemantics\nThe interpretation of\n.pragma\ndirective strings is implementation-specific and has no impact on\nPTX semantics. See\nDescriptions of .pragma Strings\nfor\ndescriptions of the pragma strings defined in\nptxas\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.pragma \"nounroll\";    // disable unrolling in backend\n\n// disable unrolling for current kernel\n.entry foo .pragma \"nounroll\"; { ... }\n11.4.8.\nPerformance-Tuning Directives:\n.abi_preserve\n\n.abi_preserve\nSpecify number of general purpose registers that should be preserved by the callers of this function.\nSyntax\n.abi_preserve N\nDescription\nIt is an architecture agnostic value specifying actual number of general purpose registers.\nInternally ABI defines some general purpose registers as preserved (callee save) registers.\nInteger N specifies the actual number of general purpose registers that should be preserved by\nthe function.\n.abi_preserve\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nSemantics\nWhen this directive is specified compiler backend modifies low level ABI components to ensure that\nnumber of live data variables in the callers of this function that are stored in the callee save\nregisters are less than specified value.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.func bar() .abi_preserve 8\n\n// Indirect call via call prototype\n.func (.param .b32 out[30]) foo (.param .b32 in[30]) .abi_preserve 10 { ... }\n...\nmov.b64 lpfoo, foo;\nprot: .callprototype (.param .b32 out[30]) _ (.param .b32 in[30]) .abi_preserve 10;\ncall (out), lpfoo, (in), prot;\n11.4.9.\nPerformance-Tuning Directives:\n.abi_preserve_control\n\n.abi_preserve_control\nSpecify number of control registers that should be preserved by the callers of this function.\nSyntax\n.abi_preserve_control N\nDescription\nIt is an architecture agnostic value specifying the number of divergent program points that happen\nin the calltree leading to current function call.\nInternally ABI defines some control registers as preserved (callee save) registers.\nInteger N specifies the actual number of control registers that should be preserved by the function.\n.abi_preserve_control\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nSemantics\nWhen this directive is specified compiler backend modifies low level ABI components to ensure that\nnumber of live control variables in the callers of this function that are stored in the callee save\ncontrol registers are less than specified value.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.func foo() .abi_preserve_control 14\n\n// Indirect call via call prototype\n.func (.param .b32 out[30]) bar (.param .b32 in[30]) .abi_preserve_control 10 { ... }\n...\nmov.b64 lpbar, bar;\nprot: .callprototype (.param .b32 out[30]) _ (.param .b32 in[30]) .abi_preserve_control 10;\ncall (out), lpbar, (in), prot;\n11.5.\nDebugging Directives\n\nDWARF-format debug information is passed through PTX modules using the following directives:\n@@DWARF\n.section\n.file\n.loc\nThe\n.section\ndirective was introduced in PTX ISA version 2.0 and replaces the\n@@DWARF\nsyntax. The\n@@DWARF\nsyntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX\nISA version 1.x code.\nBeginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the\n.target\ndebug\nplatform option. This forward declaration directs PTX compilation to retain\nmappings for source-level debugging.\n11.5.1.\nDebugging Directives:\n@@dwarf\n\n@@dwarf\nDWARF-format information.\nSyntax\n@@DWARF dwarf-string\n\ndwarf-string may have one of the\n.byte   byte-list   // comma-separated hexadecimal byte values\n.4byte  int32-list  // comma-separated hexadecimal integers in range [0..2^32-1]\n.quad   int64-list  // comma-separated hexadecimal integers in range [0..2^64-1]\n.4byte  label\n.quad   label\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2. Deprecated as of PTX ISA version 2.0, replaced by\n.section\ndirective.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n@@DWARF .section .debug_pubnames, \"\", @progbits\n@@DWARF .byte   0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n@@DWARF .4byte  .debug_info\n@@DWARF .4byte  0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n@@DWARF .4byte  0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n@@DWARF .byte   0x00, 0x00, 0x00, 0x00, 0x00\n11.5.2.\nDebugging Directives:\n.section\n\n.section\nPTX section definition.\nSyntax\n.section section_name { dwarf-lines }\n\ndwarf-lines have the following formats:\n  .b8    byte-list       // comma-separated list of integers\n                         // in range [-128..255]\n  .b16   int16-list      // comma-separated list of integers\n                         // in range [-2^15..2^16-1]\n  .b32   int32-list      // comma-separated list of integers\n                         // in range [-2^31..2^32-1]\n  label:                 // Define label inside the debug section\n  .b64   int64-list      // comma-separated list of integers\n                         // in range [-2^63..2^64-1]\n  .b32   label\n  .b64   label\n  .b32   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 32bit)\n  .b64   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 64bit)\n  .b32   label1-label2   // a difference in label addresses between labels in\n                         // the same dwarf section (32bit)\n  .b64   label3-label4   // a difference in label addresses between labels in\n                         // the same dwarf section (64bit)\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0, replaces\n@@DWARF\nsyntax.\nlabel+imm expression introduced in PTX ISA version 3.2.\nSupport for\n.b16\nintegers in dwarf-lines introduced in PTX ISA version 6.0.\nSupport for defining\nlabel\ninside the DWARF section is introduced in PTX ISA version 7.2.\nlabel1-label2\nexpression introduced in PTX ISA version 7.5.\nNegative numbers in dwarf lines introduced in PTX ISA version 7.5.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.section .debug_pubnames\n{\n    .b32    LpubNames_end0-LpubNames_begin0\n  LpubNames_begin0:\n    .b8     0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n    .b32    .debug_info\n  info_label1:\n    .b32    0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n    .b32    0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n    .b8     0x00, 0x00, 0x00, 0x00, 0x00\n  LpubNames_end0:\n}\n\n.section .debug_info\n{\n    .b32 11430\n    .b8 2, 0\n    .b32 .debug_abbrev\n    .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49\n    .b8 0\n    .b32 3, 37, 176, -99\n    .b32 info_label1\n    .b32 .debug_loc+0x4\n    .b8 -11, 11, 112, 97\n    .b32 info_label1+12\n    .b64 -1\n    .b16 -5, -65535\n}\n11.5.3.\nDebugging Directives:\n.file\n\n.file\nSource file name.\nSyntax\n.file file_index \"filename\" {, timestamp, file_size}\nDescription\nAssociates a source filename with an integer index.\n.loc\ndirectives reference source files by\nindex.\n.file\ndirective allows optionally specifying an unsigned number representing time of last\nmodification and an unsigned integer representing size in bytes of source file.\ntimestamp\nand\nfile_size\nvalue can be 0 to indicate this information is not available.\ntimestamp\nvalue is in format of C and C++ data type\ntime_t\n.\nfile_size\nis an unsigned 64-bit integer.\nThe\n.file\ndirective is allowed only in the outermost scope, i.e., at the same level as kernel\nand device function declarations.\nSemantics\nIf timestamp and file size are not specified, they default to 0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTimestamp and file size introduced in PTX ISA version 3.2.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.file 1 \"example.cu\"\n.file 2 \"kernel.cu\"\n.file 1 \"kernel.cu\", 1339013327, 64118\n11.5.4.\nDebugging Directives:\n.loc\n\n.loc\nSource file location.\nSyntax\n.loc file_index line_number column_position\n.loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2\nDescription\nDeclares the source file location (source file, line number, and column position) to be associated\nwith lexically subsequent PTX instructions.\n.loc\nrefers to\nfile_index\nwhich is defined by a\n.file\ndirective.\nTo indicate PTX instructions that are generated from a function that got inlined, additional\nattribute\n.inlined_at\ncan be specified as part of the\n.loc\ndirective.\n.inlined_at\nattribute specifies source location at which the specified function is inlined.\nfile_index2\n,\nline_number2\n, and\ncolumn_position2\nspecify the location at which function is inlined. Source\nlocation specified as part of\n.inlined_at\ndirective must lexically precede as source location in\n.loc\ndirective.\nThe\nfunction_name\nattribute specifies an offset in the DWARF section named\n.debug_str\n. Offset is specified as\nlabel\nexpression or\nlabel\n+\nimmediate\nexpression\nwhere\nlabel\nis defined in\n.debug_str\nsection. DWARF section\n.debug_str\ncontains ASCII\nnull-terminated strings that specify the name of the function that is inlined.\nNote that a PTX instruction may have a single associated source location, determined by the nearest\nlexically preceding .loc directive, or no associated source location if there is no preceding .loc\ndirective. Labels in PTX inherit the location of the closest lexically following instruction. A\nlabel with no following PTX instruction has no associated source location.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nfunction_name\nand\ninlined_at\nattributes are introduced in PTX ISA version 7.2.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.loc 2 4237 0\nL1:                        // line 4237, col 0 of file #2,\n                           // inherited from mov\n    mov.u32  %r1,%r2;      // line 4237, col 0 of file #2\n    add.u32  %r2,%r1,%r3;  // line 4237, col 0 of file #2\n...\nL2:                        // line 4239, col 5 of file #2,\n                           // inherited from sub\n    .loc 2 4239 5\n    sub.u32  %r2,%r1,%r3;  // line 4239, col 5 of file #2\n    .loc 1 21 3\n    .loc 1 9 3, function_name info_string0, inlined_at 1 21 3\n    ld.global.u32   %r1, [gg]; // Function at line 9\n    setp.lt.s32 %p1, %r1, 8;   // inlined at line 21\n    .loc 1 27 3\n    .loc 1 10 5, function_name info_string1, inlined_at 1 27 3\n    .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5\n    setp.ne.s32 %p2, %r1, 18;\n    @%p2 bra    BB2_3;\n\n    .section .debug_str {\n    info_string0:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 102 // f\n     .b8 111 // o\n     .b8 111 // o\n     .b8 118 // v\n     .b8 0\n\n    info_string1:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 98  // b\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 99  // c\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n    }\n11.6.\nLinking Directives\n\n.extern\n.visible\n.weak\n11.6.1.\nLinking Directives:\n.extern\n\n.extern\nExternal symbol declaration.\nSyntax\n.extern identifier\nDescription\nDeclares identifier to be defined external to the current module. The module defining such\nidentifier must define it as\n.weak\nor\n.visible\nonly once in a single object file. Extern\ndeclaration of symbol may appear multiple times and references to that get resolved against the\nsingle definition of that symbol.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.extern .global .b32 foo;  // foo is defined in another module\n11.6.2.\nLinking Directives:\n.visible\n\n.visible\nVisible (externally) symbol declaration.\nSyntax\n.visible identifier\nDescription\nDeclares identifier to be globally visible. Unlike C, where identifiers are globally visible unless\ndeclared static, PTX identifiers are visible only within the current module unless declared\n.visible\noutside the current.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.visible .global .b32 foo;  // foo will be externally visible\n11.6.3.\nLinking Directives:\n.weak\n\n.weak\nVisible (externally) symbol declaration.\nSyntax\n.weak identifier\nDescription\nDeclares identifier to be globally visible but\nweak\n. Weak symbols are similar to globally visible\nsymbols, except during linking, weak symbols are only chosen after globally visible symbols during\nsymbol resolution. Unlike globally visible symbols, multiple object files may declare the same weak\nsymbol, and references to a symbol get resolved against a weak symbol only if no global symbols have\nthe same name.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.weak .func (.reg .b32 val) foo;  // foo will be externally visible\n11.6.4.\nLinking Directives:\n.common\n\n.common\nVisible (externally) symbol declaration.\nSyntax\n.common identifier\nDescription\nDeclares identifier to be globally visible but “common”.\nCommon symbols are similar to globally visible symbols. However multiple object files may declare\nthe same common symbol and they may have different types and sizes and references to a symbol get\nresolved against a common symbol with the largest size.\nOnly one object file can initialize a common symbol and that must have the largest size among all\nother definitions of that common symbol from different object files.\n.common\nlinking directive can be used only on variables with\n.global\nstorage. It cannot be\nused on function symbols or on symbols with opaque type.\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\n.common\ndirective requires\nsm_20\nor higher.\nExamples\n.common .global .u32 gbl;\n11.7.\nCluster Dimension Directives\n\nThe following directives specify information about clusters:\n.reqnctapercluster\n.explicitcluster\n.maxclusterrank\nThe\n.reqnctapercluster\ndirective specifies the number of CTAs in the cluster. The\n.explicitcluster\ndirective specifies that the kernel should be launched with explicit cluster\ndetails. The\n.maxclusterrank\ndirective specifies the maximum number of CTAs in the cluster.\nThe cluster dimension directives can be applied only on kernel functions.\n11.7.1.\nCluster Dimension Directives:\n.reqnctapercluster\n\n.reqnctapercluster\nDeclare the number of CTAs in the cluster.\nSyntax\n.reqnctapercluster nx\n.reqnctapercluster nx, ny\n.reqnctapercluster nx, ny, nz\nDescription\nSet the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of\nthe 1D, 2D, or 3D cluster. The total number of CTAs is the product of the number of CTAs in each\ndimension. For kernels with\n.reqnctapercluster\ndirective specified, runtime will use the\nspecified values for configuring the launch if the same are not specified at launch time.\nSemantics\nIf cluster dimension is explicitly specified at launch time, it should be equal to the values\nspecified in this directive. Specifying a different cluster dimension at launch will result in a\nruntime error or kernel launch failure.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .reqnctapercluster 2         { . . . }\n.entry bar .reqnctapercluster 2, 2, 1   { . . . }\n.entry ker .reqnctapercluster 3, 2      { . . . }\n11.7.2.\nCluster Dimension Directives:\n.explicitcluster\n\n.explicitcluster\nDeclare that Kernel must be launched with cluster dimensions explicitly specified.\nSyntax\n.explicitcluster\nDescription\nDeclares that this Kernel should be launched with cluster dimension explicitly specified.\nSemantics\nKernels with\n.explicitcluster\ndirective must be launched with cluster dimension explicitly\nspecified (either at launch time or via\n.reqnctapercluster\n), otherwise program will fail with\nruntime error or kernel launch failure.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .explicitcluster         { . . . }\n11.7.3.\nCluster Dimension Directives:\n.maxclusterrank\n\n.maxclusterrank\nDeclare the maximum number of CTAs that can be part of the cluster.\nSyntax\n.maxclusterrank n\nDescription\nDeclare the maximum number of thread blocks (CTAs) allowed to be part of the cluster.\nSemantics\nProduct of the number of CTAs in each cluster dimension specified in any invocation of the kernel is\nrequired to be less or equal to that specified in this directive. Otherwise invocation will result\nin a runtime error or kernel launch failure.\nThe\n.maxclusterrank\ndirective cannot be used in conjunction with the\n.reqnctapercluster\ndirective.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo ..maxclusterrank 8         { . . . }\n11.8.\nMiscellaneous Directives\n\nPTX provides the following miscellaneous directives:\n.blocksareclusters\n11.8.1.\nMiscellaneous Directives:\n.blocksareclusters\n\n.blocksareclusters\nSpecify that CUDA thread blocks are mapped to clusters.\nSyntax\n.blocksareclusters\nDescription\nDefault behavior of CUDA API is to specify the grid launch configuration by specifying the number of\nthread blocks and the number of threads per block.\nWhen\n.blocksareclusters\ndirective is specified, it implies that the grid launch configuration\nfor the corresponding\n.entry\nfunction is specifying the number of clusters, i.e. the launch\nconfiguration is specifying number of clusters instead of the number of thread blocks. In this case,\nthe number of thread blocks per cluster is specified by\n.reqnctapercluster\ndirective and the\nthread block size is specified with the\n.reqntid\ndirective.\n.blocksareclusters\ndirective is only allowed for\n.entry\nfunctions and also needs\n.reqntid\nand\n.reqnctapercluster\ndirectives to be specified.\nRefer to\nCUDA Programming Guide\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .reqntid 32, 32, 1 .reqnctapercluster 32, 32, 1 .blocksareclusters { ... }\n12.\nDescriptions of\n.pragma\nStrings\n\nThis section describes the\n.pragma\nstrings defined by ptxas.\n12.1.\nPragma Strings:\n\"nounroll\"\n\n\"nounroll\"\nDisable loop unrolling in optimizing the backend compiler.\nSyntax\n.pragma \"nounroll\";\nDescription\nThe\n\"nounroll\"\npragma\nis a directive to disable loop unrolling in the optimizing backend\ncompiler.\nThe\n\"nounroll\"\npragma\nis allowed at module, entry-function, and statement levels, with the\nfollowing meanings:\nmodule scope\ndisables unrolling for all loops in module, including loops preceding the\n.pragma\n.\nentry-function scope\ndisables unrolling for all loops in the entry function body.\nstatement-level pragma\ndisables unrolling of the loop for which the current block is the loop header.\nNote that in order to have the desired effect at statement level, the\n\"nounroll\"\ndirective must\nappear before any instruction statements in the loop header basic block for the desired loop. The\nloop header block is defined as the block that dominates all blocks in the loop body and is the\ntarget of the loop backedge. Statement-level\n\"nounroll\"\ndirectives appearing outside of loop\nheader blocks are silently ignored.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher. Ignored for\nsm_1x\ntargets.\nExamples\n.entry foo (...)\n.pragma \"nounroll\";  // do not unroll any loop in this function\n{\n...\n}\n\n.func bar (...)\n{\n...\nL1_head:\n     .pragma \"nounroll\";  // do not unroll this loop\n     ...\n@p   bra L1_end;\nL1_body:\n     ...\nL1_continue:\n     bra L1_head;\nL1_end:\n     ...\n}\n12.2.\nPragma Strings:\n\"used_bytes_mask\"\n\n\"used_bytes_mask\"\nMask for indicating used bytes in data of ld operation.\nSyntax\n.pragma \"used_bytes_mask mask\";\nDescription\nThe\n\"used_bytes_mask\"\npragma\nis a directive that specifies used bytes in a load\noperation based on the mask provided.\n\"used_bytes_mask\"\npragma\nneeds to be specified prior to a load instruction for which\ninformation about bytes used from the load operation is needed.\nPragma is ignored if instruction following it is not a load instruction.\nFor a load instruction without this pragma, all bytes from the load operation are assumed\nto be used.\nOperand\nmask\nis a 32-bit integer with set bits indicating the used bytes in data of\nload operation.\nSemantics\nEach bit in mask operand corresponds to a byte data where each set bit represents the used byte.\nMost-significant bit corresponds to most-significant byte of data.\n\n// For 4 bytes load with only lower 3 bytes used\n.pragma \"used_bytes_mask 0x7\";\nld.global.u32 %r0, [gbl];     // Higher 1 byte from %r0 is unused\n\n// For vector load of 16 bytes with lower 12 bytes used\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];  // %r3 unused\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nExamples\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // Only lower 12 bytes used\n12.3.\nPragma Strings:\n\"enable_smem_spilling\"\n\n\"enable_smem_spilling\"\nEnable shared memory spilling for CUDA kernels.\nSyntax\n.pragma \"enable_smem_spilling\";\nDescription\nThe\n\"enable_smem_spilling\"\npragma\nis a directive that enables register spilling into shared memory.\nDuring the spilling process, registers are first spilled into shared memory, and once the allocated\nshared memory is full, any additional spills are redirected to local memory. This can enhance\nperformance by reducing memory access latency since shared memory accesses are faster than local memory.\nThe\n\"enable_smem_spilling\"\npragma\nis only allowed within the function scope. When applied, it enables\nshared memory spilling for the specified function.\nThe usage of pragma is valid only in certain scenarios and specific compilation modes. The usage of\npragma is disallowed under following cases and may result in an error:\nPer-function compilation mode: e.g., Separate Compilation, Device-debug, Whole program with recursive\nfunction calls, Extensible-whole-program\nKernels utilizing dynamically allocated shared memory\nKernels using\nsetmaxnreg\ninstruction\nNote\nIf launch bounds are not explicitly specified, the compiler assumes the maximum possible number of\nthreads per CTA to estimate shared memory allocated per CTA and corresponding spill size. However,\nif the kernel is launched with fewer threads per CTA than estimated, the shared memory allocated\nper CTA may exceed the compiler estimated size, thereby potentially limiting the number of CTAs\nthat can be launched on an SM. Due to this, using the pragma without launch bounds may lead to\nperformance regressions. Hence it is recommended to use this pragma only when launch bounds are\nexplicitly specified.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\n.entry foo (...)\n{\n    ...\n    .pragma \"enable_smem_spilling\";   // Enable shared memory spilling for this function\n    ...\n}\n12.4.\nPragma Strings:\n\"frequency\"\n\n\"frequency\"\nSpecify frequency for basic block execution.\nSyntax\n.pragma \"frequency n\";\nDescription\nThe\n\"frequency\"\npragma\nis a directive that specifies the number of times a basic block is\nexecuted by an executing thread. The optimizing compiler backend treats this pragma as a hint\nwhich will be used for optimizations.\nOperand\nn\nis a 64-bit non-negative integer constant that specifies the execution frequency.\nNote that in order to have the desired effect of this pragma, it should be specified at the start of\nthe basic block. Basic block is defined as a straight-line sequence of instructions with only one\nentry point and one exit point.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo (...)\n{\n    .pragma \"frequency 32\";\n    ...\n}\n13.\nRelease Notes\n\nThis section describes the history of change in the PTX ISA and implementation. The first section\ndescribes ISA and implementation changes in the current release of PTX ISA version 9.1, and the\nremaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX\nISA version 2.0.\nTable 59\nshows the PTX release history.\nTable 59\nPTX Release History\n\nPTX ISA Version\nCUDA Release\nSupported Targets\nPTX ISA 1.0\nCUDA 1.0\nsm_{10,11}\nPTX ISA 1.1\nCUDA 1.1\nsm_{10,11}\nPTX ISA 1.2\nCUDA 2.0\nsm_{10,11,12,13}\nPTX ISA 1.3\nCUDA 2.1\nsm_{10,11,12,13}\nPTX ISA 1.4\nCUDA 2.2\nsm_{10,11,12,13}\nPTX ISA 1.5\ndriver r190\nsm_{10,11,12,13}\nPTX ISA 2.0\nCUDA 3.0, driver r195\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.1\nCUDA 3.1, driver r256\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.2\nCUDA 3.2, driver r260\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.3\nCUDA 4.0, driver r270\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 3.0\nCUDA 4.1, driver r285\nsm_{10,11,12,13}\n,\nsm_20\nCUDA 4.2, driver r295\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_30\nPTX ISA 3.1\nCUDA 5.0, driver r302\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,35}\nPTX ISA 3.2\nCUDA 5.5, driver r319\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,35}\nPTX ISA 4.0\nCUDA 6.0, driver r331\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35}\n,\nsm_50\nPTX ISA 4.1\nCUDA 6.5, driver r340\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52}\nPTX ISA 4.2\nCUDA 7.0, driver r346\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\nPTX ISA 4.3\nCUDA 7.5, driver r352\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\nPTX ISA 5.0\nCUDA 8.0, driver r361\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\nPTX ISA 6.0\nCUDA 9.0, driver r384\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\nPTX ISA 6.1\nCUDA 9.1, driver r387\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\nPTX ISA 6.2\nCUDA 9.2, driver r396\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\nPTX ISA 6.3\nCUDA 10.0, driver r400\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 6.4\nCUDA 10.1, driver r418\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 6.5\nCUDA 10.2, driver r440\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 7.0\nCUDA 11.0, driver r445\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_80\nPTX ISA 7.1\nCUDA 11.1, driver r455\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.2\nCUDA 11.2, driver r460\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.3\nCUDA 11.3, driver r465\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.4\nCUDA 11.4, driver r470\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.5\nCUDA 11.5, driver r495\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.6\nCUDA 11.6, driver r510\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.7\nCUDA 11.7, driver r515\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.8\nCUDA 11.8, driver r520\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_90\nPTX ISA 8.0\nCUDA 12.0, driver r525\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.1\nCUDA 12.1, driver r530\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.2\nCUDA 12.2, driver r535\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.3\nCUDA 12.3, driver r545\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.4\nCUDA 12.4, driver r550\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.5\nCUDA 12.5, driver r555\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nCUDA 12.6, driver r560\nPTX ISA 8.6\nCUDA 12.7, driver r565\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100a,101,101a}\nPTX ISA 8.7\nCUDA 12.8, driver r570\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100,101,101a}\n,\nsm_{120,120a}\nPTX ISA 8.8\nCUDA 12.9, driver r575\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,101,101f,101a,103,103f,103a}\n,\nsm_{120,120f,120a,121,121f,121a}\nPTX ISA 9.0\nCUDA 13.0, driver r580\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,88,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,103,103f,103a}\n,\nsm_{110,110f,110a}\n,\nsm_{120,120f,120a,121,121f,121a}\nPTX ISA 9.1\nCUDA 13.1, driver r590\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,88,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,103,103f,103a}\n,\nsm_{110,110f,110a}\n,\nsm_{120,120f,120a,121,121f,121a}\nTable 60\nArch-specific/ Family-specific PTX Features Release History\n\nInstruction\nVariant\nPTX ISA Version\nSupported Targets\ntensormap.replace\nBase variant\n8.3\nsm_90a\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ntensormap.replace.swizzle_atomicity\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\n.elemtype\nfor\n.field3\nwith values\n13\n,\n14\n,\n15\nfor\nnew_val\n8.7\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\n.swizzle_mode\nfor\n.field3\nwith value\n4\nfor\nnew_val\n8.8\nsm_103a\nwgmma.mma_async\n,\nwgmma.mma_async.sp\n,\nwgmma.fence\n,\nwgmma.commit_group\n,\nwgmma.wait_group\nBase variant\n8.0\nsm_90a\nsetmaxnreg\nBase variant\n8.0\nsm_90a\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nTypes\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\n,\n.e4m3x4\n,\n.e5m2x4\n8.6\nsm_100a\n,\nsm_120a\n,\nsm_121a\n8.8\nsm_100f\n9.0\nsm_110f\n.acc::f16\nqualifier\n8.6\nsm_100a\n,\nsm_120a\n,\nsm_121a\n8.8\nsm_100f\n9.0\nsm_110f\ncvt\ncvt\nfor\n.f32\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n/\n.ue8m0x2\nAND\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\nto\n.f16x2\nAND\n.ue8m0x2\nto\n.bf16x2\nAND\n.bf16x2\nto\n.ue8m0x2\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ncvt\nwith\n.rs\nrounding mode\n8.7\nsm_100a\n8.8\nsm_103a\ncvt\nwith\n.s2f6x2\ntype\n9.1\nsm_100a\n,\nsm_103a\n,\nsm_110a\n,\nsm_120a\n,\nsm_121a\ncvt\nfor\n.f16x2\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n9.1\nsm_100f\n,\nsm_110f\n,\nsm_120f\ncvt\nfor\n.bf16x2\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n/\n.e4m3x2\n/\n.e5m2x2\n9.1\nsm_100f\n,\nsm_110f\n,\nsm_120f\ncp.async.bulk.tensor\n.tile::gather4\nand\n.im2col::w\nwith\n.shared::cluster\nas destination state\nspace\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\n.tile::scatter4\nand\n.im2col::w::128\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\n.cta_group\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ncp.async.bulk.prefetch.tensor\n.tile::gather4\n,\n.im2col::w\n,\n.im2col::w::128\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nredux.sync\nType\n.f32\nand\n.abs\n,\n.NaN\nqualifiers\n8.6\nsm_100a\n8.8\nsm_100f\nclusterlaunchcontrol.try_cancel\n.multicast::cluster::all\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nldmatrix\nShapes\n.m16n16\n,\n.m8n16\nAND\nType\n.b8\nAND\nQualifiers\n.src_fmt\n,\n.dst_fmt\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nstmatrix\nShapes\n.m16n8\nAND\nType\n.b8\nAND\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ntcgen05.ld\n,\ntcgen05.st\n,\ntcgen05.wait\n,\ntcgen05.cp\n,\ntcgen05.fence\n,\ntcgen05.commit\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ntcgen05.ld.red\nBase variant\n8.8\nsm_103f\n9.0\nsm_110f\ntcgen05.shift\nBase variant\n8.6\nsm_100a\n8.8\nsm_103a\n9.0\nsm_110a\ntcgen05.mma\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nArgument\nscale-input-d\n8.6\nsm_100a\n8.8\nsm_100f\nQualifiers\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\n8.6\nsm_100a\nQualifiers\n.block16\n,\n.block32\n8.8\nsm_100f\n,\nsm_110f\nK shape value\n96\n8.8\nsm_103a\ntcgen05.mma.sp\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nKind\n.kind::mxf4nvf4\nand\n.kind::mxf4\n8.6\nsm_100a\n8.8\nsm_103a\n9.0\nsm_110a\nArgument\nscale-input-d\n8.6\nsm_100a\n8.8\nsm_100f\nQualifiers\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\n8.6\nsm_100a\nQualifiers\n.block16\n,\n.block32\n8.8\nsm_100f\n,\nsm_110f\ntcgen05.mma.ws\n,\ntcgen05.mma.ws.sp\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nmma\nTypes\n.e3m2\n,\n.e2m3\n,\n.e2m1\nAND Qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n8.7\nsm_120a\n8.8\nsm_120f\nmma.sp\nTypes\n.e3m2\n,\n.e2m3\n,\n.e2m1\nAND Qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n8.7\nsm_120a\n8.8\nsm_120f\nKind\n.kind::mxf4nvf4\nand\n.kind::mxf4\n8.7\nsm_120a\n,\nsm_121a\n13.1.\nChanges in PTX ISA Version 9.1\n\nNew Features\nPTX ISA version 9.1 introduces the following new features:\nAdds support for\n.volatile\nqualifier with\n.local\nstate space for\nld\nand\nst\ninstructions.\nAdds support for\n.f16x2\nand\n.bf16x2\nsource types for\ncvt\ninstruction\nwith destination types\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x2\n,\n.e5m2x2\n.\nAdds support for\n.scale_vec::4X\nwith\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nfor\nmma\n/\nmma.sp\ninstructions.\nAdds support for\n.s2f6x2\ninstruction type for\ncvt\ninstruction.\nAdds support for\nmultimem.cp.async.bulk\nand\nmultimem.cp.reduce.async.bulk\ninstructions.\nSemantic Changes and Clarifications\nNone.\n13.2.\nChanges in PTX ISA Version 9.0\n\nNew Features\nPTX ISA version 9.0 introduces the following new features:\nAdds support for\nsm_88\ntarget architecture.\nAdds support for\nsm_110\ntarget architecture.\nAdds support for target\nsm_110f\nthat supports family-specific features.\nAdds support for target\nsm_110a\nthat supports architecture-specific features.\nAdds support for pragma\nenable_smem_spilling\nthat is used to enable shared\nmemory spilling for a function.\nAdds support for pragma\nfrequency\nthat is used to specify the execution frequency of a basic\nblock.\nAdds support for directive\n.blocksareclusters\nthat is used to specify that CUDA thread blocks\nare mapped to clusters.\nExtends\nsize\noperand of\nst.bulk\ninstruction to support 32-bit length.\nAdds support for performance-tuning directives\n.abi_preserve\nand\n.abi_preserve_control\nthat are used to specify the number of data and control registers that should be preserved by the\ncallers of a function.\nNotes\nTargets\nsm_{101,101f,101a}\nare renamed to targets\nsm_{110,110f,110a}\nfrom PTX ISA version 9.0.\nSemantic Changes and Clarifications\nAll\ntcgen05\ninstructions(\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n,\ntcgen05.cp\n,\ntcgen05.shift\n,\ntcgen05.mma\n,\ntcgen05.mma.sp\n,\ntcgen05.mma.ws,\ntcgen05.mma.ws.sp\n,\ntcgen05.commit\n) within a kernel must specify the same value for the\n.cta_group\nqualifier.\n13.3.\nChanges in PTX ISA Version 8.8\n\nNew Features\nPTX ISA version 8.8 introduces the following new features:\nAdds support for\nsm_103\ntarget architecture.\nAdds support for target\nsm_103a\nthat supports architecture-specific features.\nAdds support for\nsm_121\ntarget architecture.\nAdds support for target\nsm_121a\nthat supports architecture-specific features.\nIntroduces family-specific target architectures that are represented with “f” suffix.\nPTX for family-specific targets is compatible with all subsequent targets in same family.\nAdds support for\nsm_100f\n,\nsm_101f\n,\nsm_103f\n,\nsm_120f\n,\nsm_121f\n.\nExtends\nmin\nand\nmax\ninstructions to support three input arguments.\nExtends\ntcgen05.mma\ninstruction to add support for new\nscale_vectorsize\nqualifiers\n.block16\nand\n.block32\nand K dimension 96.\nExtends\n.field3\nof\ntensormap.replace\ninstruction to support 96B swizzle mode.\nAdds support for\ntcgen05.ld.red\ninstruction.\nExtends\nld\n,\nld.global.nc\nand\nst\ninstructions to support 256b load/store operations.\nTable 61\nshows the list of features that are\nsupported on family-specific targets:\nTable 61\nList of features promoted to family-specific architecture\n\nFeature\nSupported targets\n.m16n8\n,\n.m16n16\n,\n.m8n16\nshapes and\n.b8\ntype for\nldmatrix\n/\nstmatrix\nsm_100f\n,\nsm_101f\n,\nsm_120f\nShapes for\ntcgen05\n.16x64b\n.16x128b\n,\n.16x256b\n,\n.16x32bx2\n,\n.32x32b\n,\n.4x256b\n,\n.32x128b\n,\n.64x128b\n,\n.128x256b\n,\n.128x128b\n,\n.31x256b\nsm_100f\n,\nsm_101f\nsetmaxnreg\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.cta_group\nmodifier\nsm_100f\n,\nsm_101f\ncvt\nwith\n.e2m1x2\n,\n.e3m2x2\n,\n.e2m3x2\n,\n.ue8m0x2\nsm_100f\n,\nsm_101f\n,\nsm_120f\nmultimem\nwith\n.acc::f16\nand\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\ntypes\nsm_100f\n,\nsm_101f\ntensormap.replace\nsm_100f\n,\nsm_101f\n,\nsm_120f\ntcgen05.ld.red\nsm_101f\n,\nsm_103f\ntcgen05.ld\n/\nst\n/\nfence\n/\nwait\n/\ncommit\n/\ncp\n/\nalloc\n/\ndealloc\n/\nrelinquish_alloc_permit\nsm_100f\n,\nsm_101f\ntcgen05.mma{.ws}{.sp}\n(except\nkind::mxf4\n/\nkind::mxf4nvf4\nfor\n.sp\n)\nsm_100f\n,\nsm_101f\ntcgen05\n.kind::mxf4nvf4\n,\n.kind::mxf4\n,\n.kind::mxf8f6f4\n,\n.kind::f16\n,\n.kind::tf32\n,\n.kind::f8f6f4\nsm_100f\n,\nsm_101f\n.ashift\n,\n.collector_usage\nmodifiers for\ntcgen05\nsm_100f\n,\nsm_101f\nModifiers\n.b8x16\n,\n.b6x16_p32\n,\n.b4x16_p64\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.block_scale\nmodifier\nsm_100f\n,\nsm_101f\n,\nsm_120f\nmma{.sp}\nwith\n.e3m2\n,\n.e2m3\n,\n.e2m1\ntypes and\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nmodifiers\n(except\n.sp\nwith\nmxf4\n/\nmxf4nvf4\n)\nsm_120f\n.scale_vec::1X\n/\n2X\n/\n4X\nmodifiers\nsm_120f\n.block16\n/\n.block32\nmodifiers (alias to\nscale_vec\n)\nsm_100f\n,\nsm_101f\n.warpx2::02_13\n,\n.warpx2::01_23\n,\n.warpx4\n,\n.pack::16b\n,\n.unpack::16b\nmodifiers for\ntcgen05\nsm_100f\n,\nsm_101f\nclusterlaunchcontrol.try_cancel\nmulticast::cluster::all\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.tile::scatter4\n,\n.tile::gather4\n,\n.im2col::w\n,\n.im2col::w::128\nsm_100f\n,\nsm_101f\nredux.f32\nsm_100f\nscale-input-d\nfor\ntcgen05\nsm_100f\nSemantic Changes and Clarifications\nClarified the behavior of float-to-integer conversions for\nNaN\ninput.\n13.4.\nChanges in PTX ISA Version 8.7\n\nNew Features\nPTX ISA version 8.7 introduces the following new features:\nAdds support for\nsm_120\ntarget architecture.\nAdds support for target\nsm_120a\nthat supports architecture-specific features.\nExtends\ntcgen05.mma\ninstruction to add support for\n.kind::mxf4nvf4\nand\n.scale_vec::4X\nqualifiers.\nExtends\nmma\ninstructions to support\n.f16\ntype accumulator and shape\n.m16n8k16\nwith\nFP8 types\n.e4m3\nand\n.e5m2\n.\nExtends\ncvt\ninstruction to add support for\n.rs\nrounding mode and destination types\n.e2m1x4\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e3m2x4\n,\n.e2m3x4\n.\nExtends support for\nst.async\nand\nred.async\ninstructions to add support for\n.mmio\n,\n.release\n,\n.global\nand\n.scope\nqualifiers.\nExtends\ntensormap.replace\ninstruction to add support for values\n13\nto\n15\nfor\n.elemtype\nqualifier.\nExtends\nmma\nand\nmma.sp::ordered_metadata\ninstructions to add support for types\n.e3m2\n/\n.e2m3\n/\n.e2m1\nand qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n.\nSemantic Changes and Clarifications\nClarified that in\n.tile::gather4\n,\n.tile::scatter4\nmodes, tensor coordinates need to be\nspecified as {col_idx, row_idx0, row_idx1, row_idx2, row_idx3} i.e. {x, y0, y1, y2, y3} instead\nof {x0, x1, x2, x3, y}.\nUpdated\nInstruction descriptor\nof\ntcgen05.mma\ninstruction\nto clarify the bits that are reserved for future use.\n13.5.\nChanges in PTX ISA Version 8.6\n\nNew Features\nPTX ISA version 8.6 introduces the following new features:\nAdds support for\nsm_100\ntarget architecture.\nAdds support for target\nsm_100a\nthat supports architecture-specific features.\nAdds support for\nsm_101\ntarget architecture.\nAdds support for target\nsm_101a\nthat supports architecture-specific features.\nExtends\ncp.async.bulk\nand\ncp.async.bulk.tensor\ninstructions to add\n.shared::cta\nas destination state space.\nExtends\nfence\ninstruction to add support for\n.acquire\nand\n.release\nqualifiers.\nExtends\nfence\nand\nfence.proxy\ninstructions to add support for\n.sync_restrict\nqualifier.\nExtends\nldmatrix\ninstruction to support\n.m16n16\n,\n.m8n16\nshapes and\n.b8\ntype.\nExtends\nldmatrix\ninstruction to support\n.src_fmt\n,\n.dst_fmt\nqualifiers.\nExtends\nstmatrix\ninstruction to support\n.m16n8\nshape and\n.b8\ntype.\nAdds support for\nclusterlaunchcontrol\ninstruction.\nExtends\nadd\n,\nsub\nand\nfma\ninstructions to support mixed precision floating point\noperations with\n.f32\nas destaination operand type and\n.f16\n/\n.bf16\nas source operand\ntypes.\nExtends\nadd\n,\nsub\n,\nmul\nand\nfma\ninstructions to support\n.f32x2\ntype.\nExtends\ncvt\ninstruction with\n.tf32\ntype to support\n.satfinite\nqualifier\nfor\n.rn\n/\n.rz\nrounding modes.\nExtends\ncp.async.bulk\ninstruction to support\n.cp_mask\nqualifier and\nbyteMask\noperand.\nExtends\nmultimem.ld_reduce\nand\nmultimem.st\ninstructions to support\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\nand\n.e4m3x4\ntypes.\nExtends\ncvt\ninstruction to support conversions to/from\n.e2m1x2\n,\n.e3m2x2\n,\n.e2m3x2\nand\n.ue8m0x2\ntypes.\nExtends\ncp.async.bulk.tensor\nand\ncp.async.bulk.prefetch.tensor\ninstructions to\nsupport new load_mode qualifiers\n.tile::scatter4\nand\n.tile::gather4\n.\nExtends\ntensormap.replace\ninstruction to add support for new qualifier\n.swizzle_atomicity\nfor supporting new swizzle modes.\nExtends\nmbarrier.arrive\n,\nmbarrier.arrive_drop\n,\n.mbarrier.test_wait\nand\n.mbarrier.try_wait\ninstructions to support\n.relaxed\nqualifier.\nExtends\ncp.async.bulk.tensor\nand\ncp.async.bulk.prefetch.tensor\ninstructions to\nsupport new load_mode qualifiers\n.im2col::w\nand\n.im2col::w::128\n.\nExtends\ncp.async.bulk.tensor\ninstruction to support new qualifier\n.cta_group\n.\nAdd support for\nst.bulk\ninstruction.\nAdds support for tcgen05 features and related instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n,\ntcgen05.ld\n,\ntcgen05.st\n,\ntcgen05.wait\n,\ntcgen05.cp\n,\ntcgen05.shift\n,\ntcgen05.mma\n,\ntcgen05.mma.sp\n,\ntcgen05.mma.ws\n,\ntcgen05.mma.ws.sp\n,\ntcgen05.fence\nand\ntcgen05.commit\n.\nExtends\nredux.sync\ninstruction to add support for\n.f32\ntype with qualifiers\n.abs\nand\n.NaN\n.\nSemantic Changes and Clarifications\nNone.\n13.6.\nChanges in PTX ISA Version 8.5\n\nNew Features\nPTX ISA version 8.5 introduces the following new features:\nAdds support for\nmma.sp::ordered_metadata\ninstruction.\nSemantic Changes and Clarifications\nValues\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nfor sparsity metadata (operand\ne\n)\nof instruction\nmma.sp\nare invalid and their usage results in undefined behavior.\n13.7.\nChanges in PTX ISA Version 8.4\n\nNew Features\nPTX ISA version 8.4 introduces the following new features:\nExtends\nld\n,\nst\nand\natom\ninstructions with\n.b128\ntype to support\n.sys\nscope.\nExtends integer\nwgmma.mma_async\ninstruction to support\n.u8.s8\nand\n.s8.u8\nas\n.atype\nand\n.btype\nrespectively.\nExtends\nmma\n,\nmma.sp\ninstructions to support FP8 types\n.e4m3\nand\n.e5m2\n.\nSemantic Changes and Clarifications\nNone.\n13.8.\nChanges in PTX ISA Version 8.3\n\nNew Features\nPTX ISA version 8.3 introduces the following new features:\nAdds support for pragma\nused_bytes_mask\nthat is used to specify mask for used bytes for a load operation.\nExtends\nisspacep\n,\ncvta.to\n,\nld\nand\nst\ninstructions to accept\n::entry\nand\n::func\nsub-qualifiers with\n.param\nstate space qualifier.\nAdds support for\n.b128\ntype on instructions\nld\n,\nld.global.nc\n,\nldu\n,\nst\n,\nmov\nand\natom\n.\nAdd support for instructions\ntensormap.replace\n,\ntensormap.cp_fenceproxy\nand support for qualifier\n.to_proxykind::from_proxykind\non instruction\nfence.proxy\nto support modifying\ntensor-map\n.\nSemantic Changes and Clarifications\nNone.\n13.9.\nChanges in PTX ISA Version 8.2\n\nNew Features\nPTX ISA version 8.2 introduces the following new features:\nAdds support for\n.mmio\nqualifier on\nld\nand\nst\ninstructions.\nExtends\nlop3\ninstruction to allow predicate destination.\nExtends\nmultimem.ld_reduce\ninstruction to support\n.acc::f32\nqualifer to allow\n.f32\nprecision of the intermediate accumulation.\nExtends the asynchronous warpgroup-level matrix multiply-and-accumulate operation\nwgmma.mma_async\nto support\n.sp\nmodifier that allows matrix multiply-accumulate operation\nwhen input matrix A is sparse.\nSemantic Changes and Clarifications\nThe\n.multicast::cluster\nqualifier on\ncp.async.bulk\nand\ncp.async.bulk.tensor\ninstructions\nis optimized for target architecture\nsm_90a\nand may have substantially reduced performance on\nother targets and hence\n.multicast::cluster\nis advised to be used with\nsm_90a\n.\n13.10.\nChanges in PTX ISA Version 8.1\n\nNew Features\nPTX ISA version 8.1 introduces the following new features:\nAdds support for\nst.async\nand\nred.async\ninstructions for asynchronous store and\nasynchronous reduction operations respectively on shared memory.\nAdds support for\n.oob\nmodifier on half-precision\nfma\ninstruction.\nAdds support for\n.satfinite\nsaturation modifer on\ncvt\ninstruction for\n.f16\n,\n.bf16\nand\n.tf32\nformats.\nExtends support for\ncvt\nwith\n.e4m3\n/\n.e5m2\nto\nsm_89\n.\nExtends\natom\nand\nred\ninstructions to support vector types.\nAdds support for special register\n%aggr_smem_size\n.\nExtends\nsured\ninstruction with 64-bit\nmin\n/\nmax\noperations.\nAdds support for increased kernel parameter size of 32764 bytes.\nAdds support for multimem addresses in memory consistency model.\nAdds support for\nmultimem.ld_reduce\n,\nmultimem.st\nand\nmultimem.red\ninstructions to\nperform memory operations on multimem addresses.\nSemantic Changes and Clarifications\nNone.\n13.11.\nChanges in PTX ISA Version 8.0\n\nNew Features\nPTX ISA version 8.0 introduces the following new features:\nAdds support for target\nsm_90a\nthat supports architecture-specific features.\nAdds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation\nwgmma\n.\nExtends the asynchronous copy operations with bulk operations that operate on large data,\nincluding tensor data.\nIntroduces packed integer types\n.u16x2\nand\n.s16x2\n.\nExtends integer arithmetic instruction\nadd\nto allow packed integer types\n.u16x2\nand\n.s16x2\n.\nExtends integer arithmetic instructions\nmin\nand\nmax\nto allow packed integer types\n.u16x2\nand\n.s16x2\n, as well as saturation modifier\n.relu\non\n.s16x2\nand\n.s32\ntypes.\nAdds support for special register\n%current_graph_exec\nthat identifies the currently executing\nCUDA device graph.\nAdds support for\nelect.sync\ninstruction.\nAdds support for\n.unified\nattribute on functions and variables.\nAdds support for\nsetmaxnreg\ninstruction.\nAdds support for\n.sem\nqualifier on\nbarrier.cluster\ninstruction.\nExtends the\nfence\ninstruction to allow opcode-specific synchronizaion using\nop_restrict\nqualifier.\nAdds support for\n.cluster\nscope on\nmbarrier.arrive\n,\nmbarrier.arrive_drop\n,\nmbarrier.test_wait\nand\nmbarrier.try_wait\noperations.\nAdds support for transaction count operations on\nmbarrier\nobjects, specified with\n.expect_tx\nand\n.complete_tx\nqualifiers.\nSemantic Changes and Clarifications\nNone.\n13.12.\nChanges in PTX ISA Version 7.8\n\nNew Features\nPTX ISA version 7.8 introduces the following new features:\nAdds support for\nsm_89\ntarget architecture.\nAdds support for\nsm_90\ntarget architecture.\nExtends\nbar\nand\nbarrier\ninstructions to accept optional scope qualifier\n.cta\n.\nExtends\n.shared\nstate space qualifier with optional sub-qualifier\n::cta\n.\nAdds support for\nmovmatrix\ninstruction which transposes a matrix in registers across a warp.\nAdds support for\nstmatrix\ninstruction which stores one or more matrices to shared memory.\nExtends the\n.f64\nfloating point type\nmma\noperation with shapes\n.m16n8k4\n,\n.m16n8k8\n,\nand\n.m16n8k16\n.\nExtends\nadd\n,\nsub\n,\nmul\n,\nset\n,\nsetp\n,\ncvt\n,\ntanh\n,\nex2\n,\natom\nand\nred\ninstructions with\nbf16\nalternate floating point data format.\nAdds support for new alternate floating-point data formats\n.e4m3\nand\n.e5m2\n.\nExtends\ncvt\ninstruction to convert\n.e4m3\nand\n.e5m2\nalternate floating point data formats.\nAdds support for\ngriddepcontrol\ninstruction as a communication mechanism to control the\nexecution of dependent grids.\nExtends\nmbarrier\ninstruction to allow a new phase completion check operation\ntry_wait\n.\nAdds support for new thread scope\n.cluster\nwhich is a set of Cooperative Thread Arrays (CTAs).\nExtends\nfence\n/\nmembar\n,\nld\n,\nst\n,\natom\n, and\nred\ninstructions to accept\n.cluster\nscope.\nAdds support for extended visibility of shared state space to all threads within a cluster.\nExtends\n.shared\nstate space qualifier with\n::cluster\nsub-qualifier for cluster-level\nvisibility of shared memory.\nExtends\nisspacep\n,\ncvta\n,\nld\n,\nst\n,\natom\n, and\nred\ninstructions to accept\n::cluster\nsub-qualifier with\n.shared\nstate space qualifier.\nAdds support for\nmapa\ninstruction to map a shared memory address to the corresponding address\nin a different CTA within the cluster.\nAdds support for\ngetctarank\ninstruction to query the rank of the CTA that contains a given\naddress.\nAdds support for new barrier synchronization instruction\nbarrier.cluster\n.\nExtends the memory consistency model to include the new cluster scope.\nAdds support for special registers related to cluster information:\n%is_explicit_cluster\n,\n%clusterid\n,\n%nclusterid\n,\n%cluster_ctaid\n,\n%cluster_nctaid\n,\n%cluster_ctarank\n,\n%cluster_nctarank\n.\nAdds support for cluster dimension directives\n.reqnctapercluster\n,\n.explicitcluster\n, and\n.maxclusterrank\n.\nSemantic Changes and Clarifications\nNone.\n13.13.\nChanges in PTX ISA Version 7.7\n\nNew Features\nPTX ISA version 7.7 introduces the following new features:\nExtends\nisspacep\nand\ncvta\ninstructions to include the\n.param\nstate space for kernel\nfunction parameters.\nSemantic Changes and Clarifications\nNone.\n13.14.\nChanges in PTX ISA Version 7.6\n\nNew Features\nPTX ISA version 7.6 introduces the following new features:\nSupport for\nszext\ninstruction which performs sign-extension or zero-extension on a specified\nvalue.\nSupport for\nbmsk\ninstruction which creates a bitmask of the specified width starting at the\nspecified bit position.\nSupport for special registers\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset<2>\n.\nSemantic Changes and Clarifications\nNone.\n13.15.\nChanges in PTX ISA Version 7.5\n\nNew Features\nPTX ISA version 7.5 introduces the following new features:\nDebug information enhancements to support label difference and negative values in the\n.section\ndebugging directive.\nSupport for\nignore-src\noperand on\ncp.async\ninstruction.\nExtensions to the memory consistency model to introduce the following new concepts:\nA\nmemory proxy\nas an abstract label for different methods of memory access.\nVirtual aliases as distinct memory addresses accessing the same physical memory location.\nSupport for new\nfence.proxy\nand\nmembar.proxy\ninstructions to allow synchronization of\nmemory accesses performed via virtual aliases.\nSemantic Changes and Clarifications\nNone.\n13.16.\nChanges in PTX ISA Version 7.4\n\nNew Features\nPTX ISA version 7.4 introduces the following new features:\nSupport for\nsm_87\ntarget architecture.\nSupport for\n.level::eviction_priority\nqualifier which allows specifying cache eviction\npriority hints on\nld\n,\nld.global.nc\n,\nst\n, and\nprefetch\ninstructions.\nSupport for\n.level::prefetch_size\nqualifier which allows specifying data prefetch hints on\nld\nand\ncp.async\ninstructions.\nSupport for\ncreatepolicy\ninstruction which allows construction of different types of cache\neviction policies.\nSupport for\n.level::cache_hint\nqualifier which allows the use of cache eviction policies with\nld\n,\nld.global.nc\n,\nst\n,\natom\n,\nred\nand\ncp.async\ninstructions.\nSupport for\napplypriority\nand\ndiscard\noperations on cached data.\nSemantic Changes and Clarifications\nNone.\n13.17.\nChanges in PTX ISA Version 7.3\n\nNew Features\nPTX ISA version 7.3 introduces the following new features:\nExtends\nmask()\noperator used in initializers to also support integer constant expression.\nAdds support for stack manpulation instructions that allow manipulating stack using\nstacksave\nand\nstackrestore\ninstructions and allocation of per-thread stack using\nalloca\ninstruction.\nSemantic Changes and Clarifications\nThe unimplemented version of\nalloca\nfrom the older PTX ISA specification has been replaced with\nnew stack manipulation instructions in PTX ISA version 7.3.\n13.18.\nChanges in PTX ISA Version 7.2\n\nNew Features\nPTX ISA version 7.2 introduces the following new features:\nEnhances\n.loc\ndirective to represent inline function information.\nAdds support to define labels inside the debug sections.\nExtends\nmin\nand\nmax\ninstructions to support\n.xorsign\nand\n.abs\nmodifiers.\nSemantic Changes and Clarifications\nNone.\n13.19.\nChanges in PTX ISA Version 7.1\n\nNew Features\nPTX ISA version 7.1 introduces the following new features:\nSupport for\nsm_86\ntarget architecture.\nAdds a new operator,\nmask()\n, to extract a specific byte from variable’s address used in\ninitializers.\nExtends\ntex\nand\ntld4\ninstructions to return an optional predicate that indicates if data\nat specified coordinates is resident in memory.\nExtends single-bit\nwmma\nand\nmma\ninstructions to support\n.and\noperation.\nExtends\nmma\ninstruction to support\n.sp\nmodifier that allows matrix multiply-accumulate\noperation when input matrix A is sparse.\nExtends\nmbarrier.test_wait\ninstruction to test the completion of specific phase parity.\nSemantic Changes and Clarifications\nNone.\n13.20.\nChanges in PTX ISA Version 7.0\n\nNew Features\nPTX ISA version 7.0 introduces the following new features:\nSupport for\nsm_80\ntarget architecture.\nAdds support for asynchronous copy instructions that allow copying of data asynchronously from one\nstate space to another.\nAdds support for\nmbarrier\ninstructions that allow creation of\nmbarrier objects\nin memory and\nuse of these objects to synchronize threads and asynchronous copy operations initiated by threads.\nAdds support for\nredux.sync\ninstruction which allows reduction operation across threads in a\nwarp.\nAdds support for new alternate floating-point data formats\n.bf16\nand\n.tf32\n.\nExtends\nwmma\ninstruction to support\n.f64\ntype with shape\n.m8n8k4\n.\nExtends\nwmma\ninstruction to support\n.bf16\ndata format.\nExtends\nwmma\ninstruction to support\n.tf32\ndata format with shape\n.m16n16k8\n.\nExtends\nmma\ninstruction to support\n.f64\ntype with shape\n.m8n8k4\n.\nExtends\nmma\ninstruction to support\n.bf16\nand\n.tf32\ndata formats with shape\n.m16n8k8\n.\nExtends\nmma\ninstruction to support new shapes\n.m8n8k128\n,\n.m16n8k4\n,\n.m16n8k16\n,\n.m16n8k32\n,\n.m16n8k64\n,\n.m16n8k128\nand\n.m16n8k256\n.\nExtends\nabs\nand\nneg\ninstructions to support\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\nmin\nand\nmax\ninstructions to support\n.NaN\nmodifier and\n.f16\n,\n.f16x2\n,\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\nfma\ninstruction to support\n.relu\nsaturation mode and\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\ncvt\ninstruction to support\n.relu\nsaturation mode and\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\nand\n.tf32\ndestination formats.\nAdds support for\ntanh\ninstruction that computes hyperbolic-tangent.\nExtends\nex2\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nSemantic Changes and Clarifications\nNone.\n13.21.\nChanges in PTX ISA Version 6.5\n\nNew Features\nPTX ISA version 6.5 introduces the following new features:\nAdds support for integer destination types for half precision comparison instruction\nset\n.\nExtends\nabs\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nAdds support for\ncvt.pack\ninstruction which allows converting two integer values and packing\nthe results together.\nAdds new shapes\n.m16n8k8\n,\n.m8n8k16\nand\n.m8n8k32\non the\nmma\ninstruction.\nAdds support for\nldmatrix\ninstruction which loads one or more matrices from shared memory for\nmma\ninstruction.\nRemoved Features\nPTX ISA version 6.5 removes the following features:\nSupport for\n.satfinite\nqualifier on floating point\nwmma.mma\ninstruction has been\nremoved. This support was deprecated since PTX ISA version 6.4.\nSemantic Changes and Clarifications\nNone.\n13.22.\nChanges in PTX ISA Version 6.4\n\nNew Features\nPTX ISA version 6.4 introduces the following new features:\nAdds support for\n.noreturn\ndirective which can be used to indicate a function does not return\nto it’s caller function.\nAdds support for\nmma\ninstruction which allows performing matrix multiply-and-accumulate\noperation.\nDeprecated Features\nPTX ISA version 6.4 deprecates the following features:\nSupport for\n.satfinite\nqualifier on floating point\nwmma.mma\ninstruction.\nRemoved Features\nPTX ISA version 6.4 removes the following features:\nSupport for\nshfl\nand\nvote\ninstructions without the\n.sync\nqualifier has been removed\nfor\n.target\nsm_70\nand higher. This support was deprecated since PTX ISA version 6.0 as\ndocumented in PTX ISA version 6.2.\nSemantic Changes and Clarifications\nClarified that resolving references of a\n.weak\nsymbol considers only\n.weak\nor\n.visible\nsymbols with the same name and does not consider local symbols with the same name.\nClarified that in\ncvt\ninstruction, modifier\n.ftz\ncan only be specified when either\n.atype\nor\n.dtype\nis\n.f32\n.\n13.23.\nChanges in PTX ISA Version 6.3\n\nNew Features\nPTX ISA version 6.3 introduces the following new features:\nSupport for\nsm_75\ntarget architecture.\nAdds support for a new instruction\nnanosleep\nthat suspends a thread for a specified duration.\nAdds support for\n.alias\ndirective which allows definining alias to function symbol.\nExtends\natom\ninstruction to perform\n.f16\naddition operation and\n.cas.b16\noperation.\nExtends\nred\ninstruction to perform\n.f16\naddition operation.\nThe\nwmma\ninstructions are extended to support multiplicand matrices of type\n.s8\n,\n.u8\n,\n.s4\n,\n.u4\n,\n.b1\nand accumulator matrices of type\n.s32\n.\nSemantic Changes and Clarifications\nIntroduced the mandatory\n.aligned\nqualifier for all\nwmma\ninstructions.\nSpecified the alignment required for the base address and stride parameters passed to\nwmma.load\nand\nwmma.store\n.\nClarified that layout of fragment returned by\nwmma\noperation is architecture dependent and\npassing\nwmma\nfragments around functions compiled for different link compatible SM\narchitectures may not work as expected.\nClarified that atomicity for\n{atom/red}.f16x2}\noperations is guranteed separately for each of\nthe two\n.f16\nelements but not guranteed to be atomic as single 32-bit access.\n13.24.\nChanges in PTX ISA Version 6.2\n\nNew Features\nPTX ISA version 6.2 introduces the following new features:\nA new instruction\nactivemask\nfor querying active threads in a warp.\nExtends atomic and reduction instructions to perform\n.f16x2\naddition operation with mandatory\n.noftz\nqualifier.\nDeprecated Features\nPTX ISA version 6.2 deprecates the following features:\nThe use of\nshfl\nand\nvote\ninstructions without the\n.sync\nis deprecated retrospectively\nfrom PTX ISA version 6.0, which introduced the\nsm_70\narchitecture that implements\nIndependent Thread Scheduling\n.\nSemantic Changes and Clarifications\nClarified that\nwmma\ninstructions can be used in conditionally executed code only if it is\nknown that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined.\nIn the memory consistency model, the definition of\nmorally strong operations\nwas updated to\nexclude fences from the requirement of\ncomplete overlap\nsince fences do not access memory.\n13.25.\nChanges in PTX ISA Version 6.1\n\nNew Features\nPTX ISA version 6.1 introduces the following new features:\nSupport for\nsm_72\ntarget architecture.\nSupport for new matrix shapes\n32x8x16\nand\n8x32x16\nin\nwmma\ninstruction.\nSemantic Changes and Clarifications\nNone.\n13.26.\nChanges in PTX ISA Version 6.0\n\nNew Features\nPTX ISA version 6.0 introduces the following new features:\nSupport for\nsm_70\ntarget architecture.\nSpecifies the memory consistency model for programs running on\nsm_70\nand later architectures.\nVarious extensions to memory instructions to specify memory synchronization semantics and scopes\nat which such synchronization can be observed.\nNew instruction\nwmma\nfor matrix operations which allows loading matrices from memory,\nperforming multiply-and-accumulate on them and storing result in memory.\nSupport for new\nbarrier\ninstruction.\nExtends\nneg\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nA new instruction\nfns\nwhich allows finding n-th set bit in integer.\nA new instruction\nbar.warp.sync\nwhich allows synchronizing threads in warp.\nExtends\nvote\nand\nshfl\ninstructions with\n.sync\nmodifier which waits for specified\nthreads before executing the\nvote\nand\nshfl\noperation respectively.\nA new instruction\nmatch.sync\nwhich allows broadcasting and comparing a value across threads in\nwarp.\nA new instruction\nbrx.idx\nwhich allows branching to a label indexed from list of potential\ntargets.\nSupport for unsized array parameter for\n.func\nwhich can be used to implement variadic\nfunctions.\nSupport for\n.b16\ninteger type in dwarf-lines.\nSupport for taking address of device function return parameters using\nmov\ninstruction.\nSemantic Changes and Clarifications\nSemantics of\nbar\ninstruction were updated to indicate that executing thread waits for other\nnon-exited threads from it’s warp.\nSupport for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from\nthe spec.\nSupport for taking address of labels, using labels in initializers which was unimplemented has\nbeen removed from the spec.\nSupport for variadic functions which was unimplemented has been removed from the spec.\n13.27.\nChanges in PTX ISA Version 5.0\n\nNew Features\nPTX ISA version 5.0 introduces the following new features:\nSupport for\nsm_60\n,\nsm_61\n,\nsm_62\ntarget architecture.\nExtends atomic and reduction instructions to perform double-precision add operation.\nExtends atomic and reduction instructions to specify\nscope\nmodifier.\nA new\n.common\ndirective to permit linking multiple object files containing declarations of the\nsame symbol with different size.\nA new\ndp4a\ninstruction which allows 4-way dot product with accumulate operation.\nA new\ndp2a\ninstruction which allows 2-way dot product with accumulate operation.\nSupport for special register\n%clock_hi\n.\nSemantic Changes and Clarifications\nSemantics of cache modifiers on\nld\nand\nst\ninstructions were clarified to reflect cache\noperations are treated as performance hint only and do not change memory consistency behavior of the\nprogram.\nSemantics of\nvolatile\noperations on\nld\nand\nst\ninstructions were clarified to reflect how\nvolatile\noperations are handled by optimizing compiler.\n13.28.\nChanges in PTX ISA Version 4.3\n\nNew Features\nPTX ISA version 4.3 introduces the following new features:\nA new\nlop3\ninstruction which allows arbitrary logical operation on 3 inputs.\nAdds support for 64-bit computations in extended precision arithmetic instructions.\nExtends\ntex.grad\ninstruction to support\ncube\nand\nacube\ngeometries.\nExtends\ntld4\ninstruction to support\na2d\n,\ncube\nand\nacube\ngeometries.\nExtends\ntex\nand\ntld4\ninstructions to support optional operands for offset vector and depth\ncompare.\nExtends\ntxq\ninstruction to support querying texture fields from specific LOD.\nSemantic Changes and Clarifications\nNone.\n13.29.\nChanges in PTX ISA Version 4.2\n\nNew Features\nPTX ISA version 4.2 introduces the following new features:\nSupport for\nsm_53\ntarget architecture.\nSupport for arithmetic, comparsion and texture instructions for\n.f16\nand\n.f16x2\ntypes.\nSupport for\nmemory_layout\nfield for surfaces and\nsuq\ninstruction support for querying this\nfield.\nSemantic Changes and Clarifications\nSemantics for parameter passing under ABI were updated to indicate\nld.param\nand\nst.param\ninstructions used for argument passing cannot be predicated.\nSemantics of\n{atom/red}.add.f32\nwere updated to indicate subnormal inputs and results are\nflushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on\nshared memory preserve subnormal inputs and results and don’t flush them to zero.\n13.30.\nChanges in PTX ISA Version 4.1\n\nNew Features\nPTX ISA version 4.1 introduces the following new features:\nSupport for\nsm_37\nand\nsm_52\ntarget architectures.\nSupport for new fields\narray_size\n,\nnum_mipmap_levels\nand\nnum_samples\nfor Textures, and\nthe\ntxq\ninstruction support for querying these fields.\nSupport for new field\narray_size\nfor Surfaces, and the\nsuq\ninstruction support for\nquerying this field.\nSupport for special registers\n%total_smem_size\nand\n%dynamic_smem_size\n.\nSemantic Changes and Clarifications\nNone.\n13.31.\nChanges in PTX ISA Version 4.0\n\nNew Features\nPTX ISA version 4.0 introduces the following new features:\nSupport for\nsm_32\nand\nsm_50\ntarget architectures.\nSupport for 64bit performance counter special registers\n%pm0_64,..,%pm7_64\n.\nA new\nistypep\ninstruction.\nA new instruction,\nrsqrt.approx.ftz.f64\nhas been added to compute a fast approximation of the\nsquare root reciprocal of a value.\nSupport for a new directive\n.attribute\nfor specifying special attributes of a variable.\nSupport for\n.managed\nvariable attribute.\nSemantic Changes and Clarifications\nThe\nvote\ninstruction semantics were updated to clearly indicate that an inactive thread in a\nwarp contributes a 0 for its entry when participating in\nvote.ballot.b32\n.\n13.32.\nChanges in PTX ISA Version 3.2\n\nNew Features\nPTX ISA version 3.2 introduces the following new features:\nThe texture instruction supports reads from multi-sample and multisample array textures.\nExtends\n.section\ndebugging directive to include label + immediate expressions.\nExtends\n.file\ndirective to include timestamp and file size information.\nSemantic Changes and Clarifications\nThe\nvavrg2\nand\nvavrg4\ninstruction semantics were updated to indicate that instruction adds 1\nonly if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than\nbeing divided by 2).\n13.33.\nChanges in PTX ISA Version 3.1\n\nNew Features\nPTX ISA version 3.1 introduces the following new features:\nSupport for\nsm_35\ntarget architecture.\nSupport for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work.\nld.global.nc\nfor loading read-only global data though the non-coherent texture cache.\nA new funnel shift instruction,\nshf\n.\nExtends atomic and reduction instructions to perform 64-bit\n{and,\nor,\nxor}\noperations, and\n64-bit integer\n{min,\nmax}\noperations.\nAdds support for\nmipmaps\n.\nAdds support for indirect access to textures and surfaces.\nExtends support for generic addressing to include the\n.const\nstate space, and adds a new\noperator,\ngeneric()\n, to form a generic address for\n.global\nor\n.const\nvariables used in\ninitializers.\nA new\n.weak\ndirective to permit linking multiple object files containing declarations of the\nsame symbol.\nSemantic Changes and Clarifications\nPTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit\ngeneric()\noperator for each global variable used in an initializer. PTX 3.1 code should either\ninclude explicit\ngeneric()\noperators in initializers, use\ncvta.global\nto form generic\naddresses at runtime, or load from the non-generic address using\nld.global\n.\nInstruction\nmad.f32\nrequires a rounding modifier for\nsm_20\nand higher targets. However for\nPTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and\nmad.f32\nsilently\ndefaults to\nmad.rn.f32\n. For PTX ISA version 3.1, ptxas generates a warning and defaults to\nmad.rn.f32\n, and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later.\n13.34.\nChanges in PTX ISA Version 3.0\n\nNew Features\nPTX ISA version 3.0 introduces the following new features:\nSupport for\nsm_30\ntarget architectures.\nSIMD video instructions.\nA new warp shuffle instruction.\nInstructions\nmad.cc\nand\nmadc\nfor efficient, extended-precision integer multiplication.\nSurface instructions with 3D and array geometries.\nThe texture instruction supports reads from cubemap and cubemap array textures.\nPlatform option\n.target\ndebug to declare that a PTX module contains\nDWARF\ndebug information.\npmevent.mask\n, for triggering multiple performance monitor events.\nPerformance monitor counter special registers\n%pm4..%pm7\n.\nSemantic Changes and Clarifications\nSpecial register\n%gridid\nhas been extended from 32-bits to 64-bits.\nPTX ISA version 3.0 deprecates module-scoped\n.reg\nand\n.local\nvariables when compiling to the\nApplication Binary Interface (ABI). When compiling without use of the ABI, module-scoped\n.reg\nand\n.local\nvariables are supported as before. When compiling legacy PTX code (ISA versions prior\nto 3.0) containing module-scoped\n.reg\nor\n.local\nvariables, the compiler silently disables\nuse of the ABI.\nThe\nshfl\ninstruction semantics were updated to clearly indicate that value of source operand\na\nis unpredictable for inactive and predicated-off threads within the warp.\nPTX modules no longer allow duplicate\n.version\ndirectives. This feature was unimplemented, so\nthere is no semantic change.\nUnimplemented instructions\nsuld.p\nand\nsust.p.{u32,s32,f32}\nhave been removed.\n13.35.\nChanges in PTX ISA Version 2.3\n\nNew Features\nPTX 2.3 adds support for texture arrays. The texture array feature supports access to an array of 1D\nor 2D textures, where an integer indexes into the array of textures, and then one or two\nsingle-precision floating point coordinates are used to address within the selected 1D or 2D\ntexture.\nPTX 2.3 adds a new directive,\n.address_size\n, for specifying the size of addresses.\nVariables in\n.const\nand\n.global\nstate spaces are initialized to zero by default.\nSemantic Changes and Clarifications\nThe semantics of the\n.maxntid\ndirective have been updated to match the current\nimplementation. Specifically,\n.maxntid\nonly guarantees that the total number of threads in a\nthread block does not exceed the maximum. Previously, the semantics indicated that the maximum was\nenforced separately in each dimension, which is not the case.\nBit field extract and insert instructions BFE and BFI now indicate that the\nlen\nand\npos\noperands are restricted to the value range\n0..255\n.\nUnimplemented instructions\n{atom,red}.{min,max}.f32\nhave been removed.\n13.36.\nChanges in PTX ISA Version 2.2\n\nNew Features\nPTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a\nnew directives for specifying that a kernel parameter is a pointer, for specifying to which state\nspace the parameter points, and for optionally specifying the alignment of the memory to which the\nparameter points.\nPTX 2.2 adds a new field named\nforce_unnormalized_coords\nto the\n.samplerref\nopaque\ntype. This field is used in the independent texturing mode to override the\nnormalized_coords\nfield in the texture header. This field is needed to support languages such as OpenCL, which\nrepresent the property of normalized/unnormalized coordinates in the sampler header rather than in\nthe texture header.\nPTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the\n.const\nstate space. Legacy PTX that uses explicit constant banks is still supported.\nPTX 2.2 adds a new\ntld4\ninstruction for loading a component (\nr\n,\ng\n,\nb\n, or\na\n) from\nthe four texels compising the bilinear interpolation footprint of a given texture location. This\ninstruction may be used to compute higher-precision bilerp results in software, or for performing\nhigher-bandwidth texture loads.\nSemantic Changes and Clarifications\nNone.\n13.37.\nChanges in PTX ISA Version 2.1\n\nNew Features\nThe underlying, stack-based ABI is supported in PTX ISA version 2.1 for\nsm_2x\ntargets.\nSupport for indirect calls has been implemented for\nsm_2x\ntargets.\nNew directives,\n.branchtargets\nand\n.calltargets\n, have been added for specifying potential\ntargets for indirect branches and indirect function calls. A\n.callprototype\ndirective has been\nadded for declaring the type signatures for indirect function calls.\nThe names of\n.global\nand\n.const\nvariables can now be specified in variable initializers to\nrepresent their addresses.\nA set of thirty-two driver-specific execution environment special registers has been added. These\nare named\n%envreg0..%envreg31\n.\nTextures and surfaces have new fields for channel data type and channel order, and the\ntxq\nand\nsuq\ninstructions support queries for these fields.\nDirective\n.minnctapersm\nhas replaced the\n.maxnctapersm\ndirective.\nDirective\n.reqntid\nhas been added to allow specification of exact CTA dimensions.\nA new instruction,\nrcp.approx.ftz.f64\n, has been added to compute a fast, gross approximate\nreciprocal.\nSemantic Changes and Clarifications\nA warning is emitted if\n.minnctapersm\nis specified without also specifying\n.maxntid\n.\n13.38.\nChanges in PTX ISA Version 2.0\n\nNew Features\nFloating Point Extensions\nThis section describes the floating-point changes in PTX ISA version 2.0 for\nsm_20\ntargets. The\ngoal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility\nwith legacy PTX ISA version 1.x code and\nsm_1x\ntargets.\nThe changes from PTX ISA version 1.x are as follows:\nSingle-precision instructions support subnormal numbers by default for\nsm_20\ntargets. The\n.ftz\nmodifier may be used to enforce backward compatibility with\nsm_1x\n.\nSingle-precision\nadd\n,\nsub\n, and\nmul\nnow support\n.rm\nand\n.rp\nrounding modifiers\nfor\nsm_20\ntargets.\nA single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754\ncompliant rounding modifiers and support for subnormal numbers. The\nfma.f32\ninstruction also\nsupports\n.ftz\nand\n.sat\nmodifiers.\nfma.f32\nrequires\nsm_20\n. The\nmad.f32\ninstruction has been extended with rounding modifiers so that it’s synonymous with\nfma.f32\nfor\nsm_20\ntargets. Both\nfma.f32\nand\nmad.f32\nrequire a rounding modifier for\nsm_20\ntargets.\nThe\nmad.f32\ninstruction\nwithout rounding\nis retained so that compilers can generate code for\nsm_1x\ntargets. When code compiled for\nsm_1x\nis executed on\nsm_20\ndevices,\nmad.f32\nmaps to\nfma.rn.f32\n.\nSingle- and double-precision\ndiv\n,\nrcp\n, and\nsqrt\nwith IEEE 754 compliant rounding have\nbeen added. These are indicated by the use of a rounding modifier and require\nsm_20\n.\nInstructions\ntestp\nand\ncopysign\nhave been added.\nNew Instructions\nA\nload uniform\ninstruction,\nldu\n, has been added.\nSurface instructions support additional\n.clamp\nmodifiers,\n.clamp\nand\n.zero\n.\nInstruction\nsust\nnow supports formatted surface stores.\nA\ncount leading zeros\ninstruction,\nclz\n, has been added.\nA\nfind leading non-sign bit instruction\n,\nbfind\n, has been added.\nA\nbit reversal\ninstruction,\nbrev\n, has been added.\nBit field extract and insert instructions,\nbfe\nand\nbfi\n, have been added.\nA\npopulation count\ninstruction,\npopc\n, has been added.\nA\nvote ballot\ninstruction,\nvote.ballot.b32\n, has been added.\nInstructions\n{atom,red}.add.f32\nhave been implemented.\nInstructions\n{atom,red}\n.shared have been extended to handle 64-bit data types for\nsm_20\ntargets.\nA system-level membar instruction,\nmembar.sys\n, has been added.\nThe\nbar\ninstruction has been extended as follows:\nA\nbar.arrive\ninstruction has been added.\nInstructions\nbar.red.popc.u32\nand\nbar.red.{and,or}.pred\nhave been added.\nbar\nnow supports optional thread count and register operands.\nScalar video instructions (includes\nprmt\n) have been added.\nInstruction\nisspacep\nfor querying whether a generic address falls within a specified state space\nwindow has been added.\nInstruction\ncvta\nfor converting global, local, and shared addresses to generic address and\nvice-versa has been added.\nOther New Features\nInstructions\nld\n,\nldu\n,\nst\n,\nprefetch\n,\nprefetchu\n,\nisspacep\n,\ncvta\n,\natom\n,\nand\nred\nnow support generic addressing.\nNew special registers\n%nwarpid\n,\n%nsmid\n,\n%clock64\n,\n%lanemask_{eq,le,lt,ge,gt}\nhave\nbeen added.\nCache operations have been added to instructions\nld\n,\nst\n,\nsuld\n, and\nsust\n, e.g., for\nprefetching\nto specified level of memory hierarchy. Instructions\nprefetch\nand\nprefetchu\nhave also been added.\nThe\n.maxnctapersm\ndirective was deprecated and replaced with\n.minnctapersm\nto better match\nits behavior and usage.\nA new directive,\n.section\n, has been added to replace the\n@@DWARF\nsyntax for passing\nDWARF-format debugging information through PTX.\nA new directive,\n.pragma\nnounroll\n, has been added to allow users to disable loop unrolling.\nSemantic Changes and Clarifications\nThe errata in\ncvt.ftz\nfor PTX ISA versions 1.4 and earlier, where single-precision subnormal\ninputs and results were not flushed to zero if either source or destination type size was 64-bits,\nhas been fixed. In PTX ISA version 1.5 and later,\ncvt.ftz\n(and\ncvt\nfor\n.target\nsm_1x\n,\nwhere\n.ftz\nis implied) instructions flush single-precision subnormal inputs and results to\nsign-preserving zero for all combinations of floating-point instruction types. To maintain\ncompatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs\nand results are flushed to sign-preserving zero only when neither source nor destination type size\nis 64-bits.\nComponents of special registers\n%tid\n,\n%ntid\n,\n%ctaid\n, and\n%nctaid\nhave been extended\nfrom 16-bits to 32-bits. These registers now have type\n.v4.u32\n.\nThe number of samplers available in independent texturing mode was incorrectly listed as thirty-two\nin PTX ISA version 1.5; the correct number is sixteen.\n14.\nNotices\n\n14.1.\nNotice\n\nThis document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\n14.2.\nOpenCL\n\nOpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.\n14.3.\nTrademarks\n\nNVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.\nPrivacy Policy\n|\nManage My Privacy\n|\nDo Not Sell or Share My Data\n|\nTerms of Service\n|\nAccessibility\n|\nCorporate Policies\n|\nProduct Security\n|\nContact\nCopyright © 2007-2026, NVIDIA Corporation & affiliates. All rights reserved.\nLast updated on Jan 08, 2026."}], "system": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished.", "user": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n5 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n4. [Tick 4] Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.\n\nNOTES:\nNext: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] CVE-2022-40982 public disclosure was August 10, 2023 per NVD. Research by Daniel Moghimi (downfall.page) with PoC code linked from the project site to GitHub. Intel advisory is INTEL-SA-00828 naming it Gather Data Sampling (GDS).\n[3] Downfall (CVE-2022-40982) exploits Intel's vector 'Gather' instruction (vgather* variants) allowing transient/execution leakage of the internal vector register file. Intel advisory INTEL-SA-00828 names this Gather Data Sampling (GDS) issue and credits researcher Daniel Moghimi.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] Linux kernel added documentation and mitigation support for Gather Data Sampling under Documentation/admin-guide/hw-vuln/gather_data_sampling.rst and related x86 mitigation code; distributions offer kernel/microcode updates and a gather_data_sampling= kernel parameter to control mitigation behavior.\n[6] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[9] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] OpenSSH release-9.8 / 9.8p1 advisory notes the regreSSHion (CVE-2024-6387) fix: Portable OpenSSH 8.5p1–9.7p1 were affected and the 9.8/9.8p1 release contains the vendor fix; advisory lists affected versions and mitigation details.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n\nOBSERVATIONS:\nVisited https://docs.nvidia.com/cuda/parallel-thread-execution/index.html:\n1. Introduction — PTX ISA 9.1 documentation\n1. Introduction\n1.1. Scalable Data-Parallel Computing using GPUs\n1.2. Goals of PTX\n1.3. PTX ISA Version 9.1\n1.4. Document Structure\n2. Programming Model\n2.1. A Highly Multithreaded Coprocessor\n2.2. Thread Hierarchy\n2.2.1. Cooperative Thread Arrays\n2.2.2. Cluster of Cooperative Thread Arrays\n2.2.3. Grid of Clusters\n2.3. Memory Hierarchy\n3. PTX Machine Model\n3.1. A Set of SIMT Multiprocessors\n3.2. Independent Thread Scheduling\n3.3. On-chip Shared Memory\n4. Syntax\n4.1. Source Format\n4.2. Comments\n4.3. Statements\n4.3.1. Directive Statements\n4.3.2. Instruction Statements\n4.4. Identifiers\n4.5. Constants\n4.5.1. Integer Constants\n4.5.2. Floating-Point Constants\n4.5.3. Predicate Constants\n4.5.4. Constant Expressions\n4.5.5. Integer Constant Expression Evaluation\n4.5.6. Summary of Constant Expression Evaluation Rules\n5. State Spaces, Types, and Variables\n5.1. State Spaces\n5.1.1. Register State Space\n5.1.2. Special Register State Space\n5.1.3. Constant State Space\n5.1.3.1. Banked Constant State Space (deprecated)\n5.1.4. Global State Space\n5.1.5. Local State Space\n5.1.6. Parameter State Space\n5.1.6.1. Kernel Function Parameters\n5.1.6.2. Kernel Function Parameter Attributes\n5.1.6.3. Kernel Parameter Attribute:\n.ptr\n5.1.6.4. Device Function Parameters\n5.1.7. Shared State Space\n5.1.8. Texture State Space (deprecated)\n5.2. Types\n5.2.1. Fundamental Types\n5.2.2. Restricted Use of Sub-Word Sizes\n5.2.3. Alternate Floating-Point Data Formats\n5.2.4. Fixed-point Data format\n5.2.5. Packed Data Types\n5.2.5.1. Packed Floating Point Data Types\n5.2.5.2. Packed Integer Data Types\n5.2.5.3. Packed Fixed-Point Data Types\n5.3. Texture Sampler and Surface Types\n5.3.1. Texture and Surface Properties\n5.3.2. Sampler Properties\n5.3.3. Channel Data Type and Channel Order Fields\n5.4. Variables\n5.4.1. Variable Declarations\n5.4.2. Vectors\n5.4.3. Array Declarations\n5.4.4. Initializers\n5.4.5. Alignment\n5.4.6. Parameterized Variable Names\n5.4.7. Variable Attributes\n5.4.8. Variable and Function Attribute Directive:\n.attribute\n5.5. Tensors\n5.5.1. Tensor Dimension, size and format\n5.5.1.1. Sub-byte Types\n5.5.1.1.1. Padding and alignment of the sub-byte types\n5.5.2. Tensor Access Modes\n5.5.3. Tiled Mode\n5.5.3.1. Bounding Box\n5.5.3.2. Traversal-Stride\n5.5.3.3. Out of Boundary Access\n5.5.3.4.\n.tile::scatter4\nand\n.tile::gather4\nmodes\n5.5.3.4.1. Bounding Box\n5.5.4.\nim2col\nmode\n5.5.4.1. Bounding Box\n5.5.4.2. Traversal Stride\n5.5.4.3. Out of Boundary Access\n5.5.5.\nim2col::w\nand\nim2col::w::128\nmodes\n5.5.5.1. Bounding Box\n5.5.5.2. Traversal Stride\n5.5.5.3.\nwHalo\n5.5.5.4.\nwOffset\n5.5.6. Interleave layout\n5.5.7. Swizzling Modes\n5.5.8. Tensor-map\n6. Instruction Operands\n6.1. Operand Type Information\n6.2. Source Operands\n6.3. Destination Operands\n6.4. Using Addresses, Arrays, and Vectors\n6.4.1. Addresses as Operands\n6.4.1.1. Generic Addressing\n6.4.2. Arrays as Operands\n6.4.3. Vectors as Operands\n6.4.4. Labels and Function Names as Operands\n6.5. Type Conversion\n6.5.1. Scalar Conversions\n6.5.2. Rounding Modifiers\n6.6. Operand Costs\n7. Abstracting the ABI\n7.1. Function Declarations and Definitions\n7.1.1. Changes from PTX ISA Version 1.x\n7.2. Variadic Functions\n7.3. Alloca\n8. Memory Consistency Model\n8.1. Scope and applicability of the model\n8.1.1. Limitations on atomicity at system scope\n8.2. Memory operations\n8.2.1. Overlap\n8.2.2. Aliases\n8.2.3. Multimem Addresses\n8.2.4. Memory Operations on Vector Data Types\n8.2.5. Memory Operations on Packed Data Types\n8.2.6. Initialization\n8.3. State spaces\n8.4. Operation types\n8.4.1. mmio Operation\n8.4.2. volatile Operation\n8.5. Scope\n8.6. Proxies\n8.7. Morally strong operations\n8.7.1. Conflict and Data-races\n8.7.2. Limitations on Mixed-size Data-races\n8.8. Release and Acquire Patterns\n8.9. Ordering of memory operations\n8.9.1. Program Order\n8.9.1.1. Asynchronous Operations\n8.9.2. Observation Order\n8.9.3. Fence-SC Order\n8.9.4. Memory synchronization\n8.9.5. Causality Order\n8.9.6. Coherence Order\n8.9.7. Communication Order\n8.10. Axioms\n8.10.1. Coherence\n8.10.2. Fence-SC\n8.10.3. Atomicity\n8.10.4. No Thin Air\n8.10.5. Sequential Consistency Per Location\n8.10.6. Causality\n8.11. Special Cases\n8.11.1. Reductions do not form Acquire Patterns\n9. Instruction Set\n9.1. Format and Semantics of Instruction Descriptions\n9.2. PTX Instructions\n9.3. Predicated Execution\n9.3.1. Comparisons\n9.3.1.1. Integer and Bit-Size Comparisons\n9.3.1.2. Floating Point Comparisons\n9.3.2. Manipulating Predicates\n9.4. Type Information for Instructions and Operands\n9.4.1. Operand Size Exceeding Instruction-Type Size\n9.5. Divergence of Threads in Control Constructs\n9.6. Semantics\n9.6.1. Machine-Specific Semantics of 16-bit Code\n9.7. Instructions\n9.7.1. Integer Arithmetic Instructions\n9.7.1.1. Integer Arithmetic Instructions:\nadd\n9.7.1.2. Integer Arithmetic Instructions:\nsub\n9.7.1.3. Integer Arithmetic Instructions:\nmul\n9.7.1.4. Integer Arithmetic Instructions:\nmad\n9.7.1.5. Integer Arithmetic Instructions:\nmul24\n9.7.1.6. Integer Arithmetic Instructions:\nmad24\n9.7.1.7. Integer Arithmetic Instructions:\nsad\n9.7.1.8. Integer Arithmetic Instructions:\ndiv\n9.7.1.9. Integer Arithmetic Instructions:\nrem\n9.7.1.10. Integer Arithmetic Instructions:\nabs\n9.7.1.11. Integer Arithmetic Instructions:\nneg\n9.7.1.12. Integer Arithmetic Instructions:\nmin\n9.7.1.13. Integer Arithmetic Instructions:\nmax\n9.7.1.14. Integer Arithmetic Instructions:\npopc\n9.7.1.15. Integer Arithmetic Instructions:\nclz\n9.7.1.16. Integer Arithmetic Instructions:\nbfind\n9.7.1.17. Integer Arithmetic Instructions:\nfns\n9.7.1.18. Integer Arithmetic Instructions:\nbrev\n9.7.1.19. Integer Arithmetic Instructions:\nbfe\n9.7.1.20. Integer Arithmetic Instructions:\nbfi\n9.7.1.21. Integer Arithmetic Instructions:\nszext\n9.7.1.22. Integer Arithmetic Instructions:\nbmsk\n9.7.1.23. Integer Arithmetic Instructions:\ndp4a\n9.7.1.24. Integer Arithmetic Instructions:\ndp2a\n9.7.2. Extended-Precision Integer Arithmetic Instructions\n9.7.2.1. Extended-Precision Arithmetic Instructions:\nadd.cc\n9.7.2.2. Extended-Precision Arithmetic Instructions:\naddc\n9.7.2.3. Extended-Precision Arithmetic Instructions:\nsub.cc\n9.7.2.4. Extended-Precision Arithmetic Instructions:\nsubc\n9.7.2.5. Extended-Precision Arithmetic Instructions:\nmad.cc\n9.7.2.6. Extended-Precision Arithmetic Instructions:\nmadc\n9.7.3. Floating-Point Instructions\n9.7.3.1. Floating Point Instructions:\ntestp\n9.7.3.2. Floating Point Instructions:\ncopysign\n9.7.3.3. Floating Point Instructions:\nadd\n9.7.3.4. Floating Point Instructions:\nsub\n9.7.3.5. Floating Point Instructions:\nmul\n9.7.3.6. Floating Point Instructions:\nfma\n9.7.3.7. Floating Point Instructions:\nmad\n9.7.3.8. Floating Point Instructions:\ndiv\n9.7.3.9. Floating Point Instructions:\nabs\n9.7.3.10. Floating Point Instructions:\nneg\n9.7.3.11. Floating Point Instructions:\nmin\n9.7.3.12. Floating Point Instructions:\nmax\n9.7.3.13. Floating Point Instructions:\nrcp\n9.7.3.14. Floating Point Instructions:\nrcp.approx.ftz.f64\n9.7.3.15. Floating Point Instructions:\nsqrt\n9.7.3.16. Floating Point Instructions:\nrsqrt\n9.7.3.17. Floating Point Instructions:\nrsqrt.approx.ftz.f64\n9.7.3.18. Floating Point Instructions:\nsin\n9.7.3.19. Floating Point Instructions:\ncos\n9.7.3.20. Floating Point Instructions:\nlg2\n9.7.3.21. Floating Point Instructions:\nex2\n9.7.3.22. Floating Point Instructions:\ntanh\n9.7.4. Half Precision Floating-Point Instructions\n9.7.4.1. Half Precision Floating Point Instructions:\nadd\n9.7.4.2. Half Precision Floating Point Instructions:\nsub\n9.7.4.3. Half Precision Floating Point Instructions:\nmul\n9.7.4.4. Half Precision Floating Point Instructions:\nfma\n9.7.4.5. Half Precision Floating Point Instructions:\nneg\n9.7.4.6. Half Precision Floating Point Instructions:\nabs\n9.7.4.7. Half Precision Floating Point Instructions:\nmin\n9.7.4.8. Half Precision Floating Point Instructions:\nmax\n9.7.4.9. Half Precision Floating Point Instructions:\ntanh\n9.7.4.10. Half Precision Floating Point Instructions:\nex2\n9.7.5. Mixed Precision Floating-Point Instructions\n9.7.5.1. Mixed Precision Floating Point Instructions:\nadd\n9.7.5.2. Mixed Precision Floating Point Instructions:\nsub\n9.7.5.3. Mixed Precision Floating Point Instructions:\nfma\n9.7.6. Comparison and Selection Instructions\n9.7.6.1. Comparison and Selection Instructions:\nset\n9.7.6.2. Comparison and Selection Instructions:\nsetp\n9.7.6.3. Comparison and Selection Instructions:\nselp\n9.7.6.4. Comparison and Selection Instructions:\nslct\n9.7.7. Half Precision Comparison Instructions\n9.7.7.1. Half Precision Comparison Instructions:\nset\n9.7.7.2. Half Precision Comparison Instructions:\nsetp\n9.7.8. Logic and Shift Instructions\n9.7.8.1. Logic and Shift Instructions:\nand\n9.7.8.2. Logic and Shift Instructions:\nor\n9.7.8.3. Logic and Shift Instructions:\nxor\n9.7.8.4. Logic and Shift Instructions:\nnot\n9.7.8.5. Logic and Shift Instructions:\ncnot\n9.7.8.6. Logic and Shift Instructions:\nlop3\n9.7.8.7. Logic and Shift Instructions:\nshf\n9.7.8.8. Logic and Shift Instructions:\nshl\n9.7.8.9. Logic and Shift Instructions:\nshr\n9.7.9. Data Movement and Conversion Instructions\n9.7.9.1. Cache Operators\n9.7.9.2. Cache Eviction Priority Hints\n9.7.9.3. Data Movement and Conversion Instructions:\nmov\n9.7.9.4. Data Movement and Conversion Instructions:\nmov\n9.7.9.5. Data Movement and Conversion Instructions:\nshfl\n(deprecated)\n9.7.9.6. Data Movement and Conversion Instructions:\nshfl.sync\n9.7.9.7. Data Movement and Conversion Instructions:\nprmt\n9.7.9.8. Data Movement and Conversion Instructions:\nld\n9.7.9.9. Data Movement and Conversion Instructions:\nld.global.nc\n9.7.9.10. Data Movement and Conversion Instructions:\nldu\n9.7.9.11. Data Movement and Conversion Instructions:\nst\n9.7.9.12. Data Movement and Conversion Instructions:\nst.async\n9.7.9.13. Data Movement and Conversion Instructions:\nst.bulk\n9.7.9.14. Data Movement and Conversion Instructions:\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\n9.7.9.15. Data Movement and Conversion Instructions:\nprefetch\n,\nprefetchu\n9.7.9.16. Data Movement and Conversion Instructions:\napplypriority\n9.7.9.17. Data Movement and Conversion Instructions:\ndiscard\n9.7.9.18. Data Movement and Conversion Instructions:\ncreatepolicy\n9.7.9.19. Data Movement and Conversion Instructions:\nisspacep\n9.7.9.20. Data Movement and Conversion Instructions:\ncvta\n9.7.9.21. Data Movement and Conversion Instructions:\ncvt\n9.7.9.22. Data Movement and Conversion Instructions:\ncvt.pack\n9.7.9.23. Data Movement and Conversion Instructions:\nmapa\n9.7.9.24. Data Movement and Conversion Instructions:\ngetctarank\n9.7.9.25. Data Movement and Conversion Instructions: Asynchronous copy\n9.7.9.25.1. Completion Mechanisms for Asynchronous Copy Operations\n9.7.9.25.1.1. Async-group mechanism\n9.7.9.25.1.2. Mbarrier-based mechanism\n9.7.9.25.2. Async Proxy\n9.7.9.25.3. Data Movement and Conversion Instructions: Non-bulk copy\n9.7.9.25.3.1. Data Movement and Conversion Instructions:\ncp.async\n9.7.9.25.3.2. Data Movement and Conversion Instructions:\ncp.async.commit_group\n9.7.9.25.3.3. Data Movement and Conversion Instructions:\ncp.async.wait_group\n/\ncp.async.wait_all\n9.7.9.25.4. Data Movement and Conversion Instructions: Bulk copy\n9.7.9.25.4.1. Data Movement and Conversion Instructions:\ncp.async.bulk\n9.7.9.25.4.2. Data Movement and Conversion Instructions:\ncp.reduce.async.bulk\n9.7.9.25.4.3. Data Movement and Conversion Instructions:\ncp.async.bulk.prefetch\n9.7.9.26. Data Movement and Conversion Instructions:\nmultimem.cp.async.bulk\n9.7.9.27. Data Movement and Conversion Instructions:\nmultimem.cp.reduce.async.bulk\n9.7.9.27.1. Data Movement and Conversion Instructions: Tensor copy\n9.7.9.27.1.1. Restriction on Tensor Copy instructions\n9.7.9.27.1.2. Data Movement and Conversion Instructions:\ncp.async.bulk.tensor\n9.7.9.27.1.3. Data Movement and Conversion Instructions:\ncp.reduce.async.bulk.tensor\n9.7.9.27.1.4. Data Movement and Conversion Instructions:\ncp.async.bulk.prefetch.tensor\n9.7.9.27.2. Data Movement and Conversion Instructions: Bulk and Tensor copy completion instructions\n9.7.9.27.2.1. Data Movement and Conversion Instructions:\ncp.async.bulk.commit_group\n9.7.9.27.2.2. Data Movement and Conversion Instructions:\ncp.async.bulk.wait_group\n9.7.9.28. Data Movement and Conversion Instructions:\ntensormap.replace\n9.7.10. Texture Instructions\n9.7.10.1. Texturing Modes\n9.7.10.2. Mipmaps\n9.7.10.3. Texture Instructions:\ntex\n9.7.10.4. Texture Instructions:\ntld4\n9.7.10.5. Texture Instructions:\ntxq\n9.7.10.6. Texture Instructions:\nistypep\n9.7.11. Surface Instructions\n9.7.11.1. Surface Instructions:\nsuld\n9.7.11.2. Surface Instructions:\nsust\n9.7.11.3. Surface Instructions:\nsured\n9.7.11.4. Surface Instructions:\nsuq\n9.7.12. Control Flow Instructions\n9.7.12.1. Control Flow Instructions:\n{}\n9.7.12.2. Control Flow Instructions:\n@\n9.7.12.3. Control Flow Instructions:\nbra\n9.7.12.4. Control Flow Instructions:\nbrx.idx\n9.7.12.5. Control Flow Instructions:\ncall\n9.7.12.6. Control Flow Instructions:\nret\n9.7.12.7. Control Flow Instructions:\nexit\n9.7.13. Parallel Synchronization and Communication Instructions\n9.7.13.1. Parallel Synchronization and Communication Instructions:\nbar\n,\nbarrier\n9.7.13.2. Parallel Synchronization and Communication Instructions:\nbar.warp.sync\n9.7.13.3. Parallel Synchronization and Communication Instructions:\nbarrier.cluster\n9.7.13.4. Parallel Synchronization and Communication Instructions:\nmembar\n/\nfence\n9.7.13.5. Parallel Synchronization and Communication Instructions:\natom\n9.7.13.6. Parallel Synchronization and Communication Instructions:\nred\n9.7.13.7. Parallel Synchronization and Communication Instructions:\nred.async\n9.7.13.8. Parallel Synchronization and Communication Instructions:\nvote\n(deprecated)\n9.7.13.9. Parallel Synchronization and Communication Instructions:\nvote.sync\n9.7.13.10. Parallel Synchronization and Communication Instructions:\nmatch.sync\n9.7.13.11. Parallel Synchronization and Communication Instructions:\nactivemask\n9.7.13.12. Parallel Synchronization and Communication Instructions:\nredux.sync\n9.7.13.13. Parallel Synchronization and Communication Instructions:\ngriddepcontrol\n9.7.13.14. Parallel Synchronization and Communication Instructions:\nelect.sync\n9.7.13.15. Parallel Synchronization and Communication Instructions:\nmbarrier\n9.7.13.15.1. Size and alignment of mbarrier object\n9.7.13.15.2. Contents of the mbarrier object\n9.7.13.15.3. Lifecycle of the mbarrier object\n9.7.13.15.4. Phase of the mbarrier object\n9.7.13.15.5. Tracking asynchronous operations by the mbarrier object\n9.7.13.15.5.1. expect-tx operation\n9.7.13.15.5.2. complete-tx operation\n9.7.13.15.6. Phase Completion of the mbarrier object\n9.7.13.15.7. Arrive-on operation on mbarrier object\n9.7.13.15.8. mbarrier support with shared memory\n9.7.13.15.9. Parallel Synchronization and Communication Instructions:\nmbarrier.init\n9.7.13.15.10. Parallel Synchronization and Communication Instructions:\nmbarrier.inval\n9.7.13.15.11. Parallel Synchronization and Communication Instructions:\nmbarrier.expect_tx\n9.7.13.15.12. Parallel Synchronization and Communication Instructions:\nmbarrier.complete_tx\n9.7.13.15.13. Parallel Synchronization and Communication Instructions:\nmbarrier.arrive\n9.7.13.15.14. Parallel Synchronization and Communication Instructions:\nmbarrier.arrive_drop\n9.7.13.15.15. Parallel Synchronization and Communication Instructions:\ncp.async.mbarrier.arrive\n9.7.13.15.16. Parallel Synchronization and Communication Instructions:\nmbarrier.test_wait\n/\nmbarrier.try_wait\n9.7.13.15.17. Parallel Synchronization and Communication Instructions:\nmbarrier.pending_count\n9.7.13.16. Parallel Synchronization and Communication Instructions:\ntensormap.cp_fenceproxy\n9.7.13.17. Parallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.try_cancel\n9.7.13.18. Parallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.query_cancel\n9.7.14. Warp Level Matrix Multiply-Accumulate Instructions\n9.7.14.1. Matrix Shape\n9.7.14.2. Matrix Data-types\n9.7.14.3. Block Scaling for\nmma.sync\n9.7.14.4. Matrix multiply-accumulate operation using\nwmma\ninstructions\n9.7.14.4.1. Matrix Fragments for WMMA\n9.7.14.4.2. Matrix Storage for WMMA\n9.7.14.4.3. Warp-level Matrix Load Instruction:\nwmma.load\n9.7.14.4.4. Warp-level Matrix Store Instruction:\nwmma.store\n9.7.14.4.5. Warp-level Matrix Multiply-and-Accumulate Instruction:\nwmma.mma\n9.7.14.5. Matrix multiply-accumulate operation using\nmma\ninstruction\n9.7.14.5.1. Matrix Fragments for\nmma.m8n8k4\nwith\n.f16\nfloating point type\n9.7.14.5.2. Matrix Fragments for\nmma.m8n8k4\nwith\n.f64\nfloating point type\n9.7.14.5.3. Matrix Fragments for\nmma.m8n8k16\n9.7.14.5.4. Matrix Fragments for\nmma.m8n8k32\n9.7.14.5.5. Matrix Fragments for\nmma.m8n8k128\n9.7.14.5.6. Matrix Fragments for\nmma.m16n8k4\n9.7.14.5.7. Matrix Fragments for\nmma.m16n8k8\n9.7.14.5.8. Matrix Fragments for\nmma.m16n8k16\nwith floating point type\n9.7.14.5.9. Matrix Fragments for\nmma.m16n8k16\nwith integer type\n9.7.14.5.10. Matrix Fragments for\nmma.m16n8k32\n9.7.14.5.11. Matrix Fragments for\nmma.m16n8k64\n9.7.14.5.12. Matrix Fragments for\nmma.m16n8k128\n9.7.14.5.13. Matrix Fragments for\nmma.m16n8k256\n9.7.14.5.14. Multiply-and-Accumulate Instruction:\nmma\n9.7.14.5.15. Warp-level matrix load instruction:\nldmatrix\n9.7.14.5.16. Warp-level matrix store instruction:\nstmatrix\n9.7.14.5.17. Warp-level matrix transpose instruction:\nmovmatrix\n9.7.14.6. Matrix multiply-accumulate operation using\nmma.sp\ninstruction with sparse matrix A\n9.7.14.6.1. Sparse matrix storage\n9.7.14.6.2. Matrix fragments for multiply-accumulate operation with sparse matrix A\n9.7.14.6.2.1. Matrix Fragments for sparse\nmma.m16n8k16\nwith\n.f16\nand\n.bf16\ntypes\n9.7.14.6.2.2. Matrix Fragments for sparse\nmma.m16n8k32\nwith\n.f16\nand\n.bf16\ntypes\n9.7.14.6.2.3. Matrix Fragments for sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type\n9.7.14.6.2.4. Matrix Fragments for sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type\n9.7.14.6.2.5. Matrix Fragments for sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type\n9.7.14.6.2.6. Matrix Fragments for sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\ntype\n9.7.14.6.2.7. Matrix Fragments for sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type\n9.7.14.6.2.8. Matrix Fragments for sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\ninteger type\n9.7.14.6.3. Multiply-and-Accumulate Instruction:\nmma.sp\n/\nmma.sp::ordered_metadata\n9.7.15. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions\n9.7.15.1. Warpgroup\n9.7.15.2. Matrix Shape\n9.7.15.3. Matrix Data-types\n9.7.15.4. Async Proxy\n9.7.15.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using\nwgmma.mma_async\ninstruction\n9.7.15.5.1. Register Fragments and Shared Memory Matrix Layouts\n9.7.15.5.1.1. Register Fragments\n9.7.15.5.1.1.1. Matrix Fragments for\nwgmma.mma_async.m64nNk16\n9.7.15.5.1.1.2. Matrix Fragments for\nwgmma.mma_async.m64nNk8\n9.7.15.5.1.1.3. Matrix Fragments for\nwgmma.mma_async.m64nNk32\n9.7.15.5.1.1.4. Matrix Fragments for\nwgmma.mma_async.m64nNk256\n9.7.15.5.1.2. Shared Memory Matrix Layout\n9.7.15.5.1.2.1. Major-ness supported by Strides\n9.7.15.5.1.2.1.1. Leading Dimension Byte Offset\n9.7.15.5.1.2.1.2. Stride Dimension Byte Offset\n9.7.15.5.1.2.1.3. Canonical Layouts\n9.7.15.5.1.2.2. Matrix Descriptor Format\n9.7.15.5.2. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async\n9.7.15.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using\nwgmma.mma_async.sp\ninstruction\n9.7.15.6.1. Sparse matrix storage\n9.7.15.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A\n9.7.15.6.2.1. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk32\n9.7.15.6.2.2. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk16\n9.7.15.6.2.3. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk64\n9.7.15.6.3. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async.sp\n9.7.15.7. Asynchronous\nwgmma\nProxy Operations\n9.7.15.7.1. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.fence\n9.7.15.7.2. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.commit_group\n9.7.15.7.3. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.wait_group\n9.7.16. TensorCore 5th Generation Family Instructions\n9.7.16.1. Tensor Memory\n9.7.16.1.1. Tensor Memory Addressing\n9.7.16.1.2. Tensor Memory Allocation\n9.7.16.2. Matrix and Data Movement Shape\n9.7.16.2.1. Matrix Shape\n9.7.16.2.1.1. Target ISA Note\n9.7.16.2.2. Specifying Matrix Shape\n9.7.16.2.3. Data Movement Shape\n9.7.16.2.3.1. Memory Layout\n9.7.16.2.3.1.1. Matrix fragments for shape .32x32b\n9.7.16.2.3.1.2. Matrix fragments for shape .16x64b\n9.7.16.2.3.1.3. Matrix fragments for shape .16x128b\n9.7.16.2.3.1.4. Matrix fragments for shape .16x256b\n9.7.16.2.3.1.5. Matrix fragments for shape .16x32bx2\n9.7.16.3. Major-ness supported by Strides\n9.7.16.3.1. Leading Dimension Stride: relative offset or absolute address\n9.7.16.3.1.1. Relative offset mode\n9.7.16.3.1.2. Absolute address mode for K dimension being 48B\n9.7.16.3.1.2.1. Restrictions on the Leading Dimension Absolute Address Stride\n9.7.16.3.2. Stride Dimension Byte Offset\n9.7.16.3.3. Canonical Layouts\n9.7.16.4. Matrix Descriptors\n9.7.16.4.1. Shared memory descriptor\n9.7.16.4.1.1. Target ISA Note\n9.7.16.4.2. Instruction descriptor\n9.7.16.4.3. Zero-Column Mask Descriptor\n9.7.16.5. Issue Granularity\n9.7.16.5.1. CTA Pair\n9.7.16.5.2. Peer CTA\n9.7.16.6. Memory Consistency Model for 5th generation of TensorCore operations\n9.7.16.6.1. Asynchronous Operations\n9.7.16.6.2. Pipelined tcgen05 Instructions\n9.7.16.6.2.1. Implicitly pipelined tcgen05 Instructions\n9.7.16.6.2.1.1. mbarrier based completion mechanism\n9.7.16.6.2.1.2.\ntcgen05.wait\ninstruction based completion mechanism\n9.7.16.6.3. Specialized Inter-thread Synchronization for tcgen05 instructions\n9.7.16.6.4. Canonical synchronization patterns\n9.7.16.6.4.1. Pipelined instructions, same thread\n9.7.16.6.4.2. Non-pipelined instructions, same thread\n9.7.16.6.4.3. Pipelined instructions, different thread\n9.7.16.6.4.4. Non-pipelined instructions, different thread\n9.7.16.6.4.5. Register dependencies, same thread\n9.7.16.6.5. Shared Memory Accesses\n9.7.16.7. Tensor Memory Allocation and Management Instructions\n9.7.16.7.1. Tensorcore 5th Generation Instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n9.7.16.8. Tensor Memory and Register Load/Store Instructions\n9.7.16.8.1. Access restrictions\n9.7.16.8.2. Packing and Unpacking\n9.7.16.8.3. Tensorcore 5th Generation Instructions:\ntcgen05.ld\n9.7.16.8.4. Tensorcore 5th Generation Instructions:\ntcgen05.st\n9.7.16.8.5. Tensorcore 5th Generation Instructions:\ntcgen05.wait\n9.7.16.9. Tensor Memory Data Movement Instructions\n9.7.16.9.1. Optional Decompression\n9.7.16.9.1.1. Decompression of 4-bit floating point to 8-bit type\n9.7.16.9.1.2. Decompression of 6-bit floating point to 8-bit type\n9.7.16.9.2. Tensorcore 5th Generation Instructions:\ntcgen05.cp\n9.7.16.9.3. Tensorcore 5th Generation Instructions:\ntcgen05.shift\n9.7.16.10. TensorCore 5th Generation Matrix Multiply and accumulate Operations\n9.7.16.10.1. Transpose and Negate operations\n9.7.16.10.2. Matrix Layout Organization\n9.7.16.10.3. Valid Combinations of Type-Size, Major-ness and Swizzling\n9.7.16.10.4. Packing formats of elements in Tensor and Shared memory\n9.7.16.10.4.1. Packing format for matrix D in Tensor Memory\n9.7.16.10.4.2. Packing format for matrix A and B\n9.7.16.10.4.3. Packing format used for matrix A by\n.kind::mxf8f6f4\nin Tensor Memory\n9.7.16.10.4.4. Packing format used for matrix A and B by\n.kind::mxf8f6f4\nin Shared Memory\n9.7.16.10.4.5. Packing format used for matrix A by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Tensor Memory\n9.7.16.10.4.6. Packing format used for matrix A and B by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Shared Memory\n9.7.16.10.5. Data Path Layout Organization\n9.7.16.10.5.1. Layout A (M = 256)\n9.7.16.10.5.2. Layout B (M = 128 + cta-group::2 + Dense A matrix)\n9.7.16.10.5.3. Layout C (M = 128 + cta-group::2 + Sparse A matrix)\n9.7.16.10.5.4. Layout D (M = 128 + cta-group::1)\n9.7.16.10.5.5. Layout E (M = 64 + .ws mode)\n9.7.16.10.5.6. Layout F (M = 64 + non .ws mode)\n9.7.16.10.5.7. Layout G (M = 32)\n9.7.16.10.6. Shared Memory Layout and Swizzling\n9.7.16.10.7. Block Scaling for\ntcgen05.mma.sync\n9.7.16.10.7.1. Valid combinations of scale_vectorsize with types and MMA-Kind\n9.7.16.10.7.2. Scale Factor A ID\n9.7.16.10.7.2.1. Layout of the Scale Factor A Matrix for scale_vec::1X/block32 with K=32/K=64\n9.7.16.10.7.2.2. Layout of the Scale Factor A Matrix for scale_vec::2X/block32 with K=64/K=128\n9.7.16.10.7.2.3. Layout of the Scale Factor A Matrix for scale_vec::4X/block16 with K=64/K=128\n9.7.16.10.7.2.4. Layout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n9.7.16.10.7.2.5. Layout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n9.7.16.10.7.3. Scale Factor B ID\n9.7.16.10.7.3.1. Layout of the Scale Factor B Matrix for scale_vec::1X/block32 with K=32/K=64\n9.7.16.10.7.3.2. Layout of the Scale Factor B Matrix for scale_vec::2X/block32 with K=64/K=128\n9.7.16.10.7.3.3. Layout of the Scale Factor B Matrix for scale_vec::4X/block16 with K=64/K=128\n9.7.16.10.7.3.4. Layout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n9.7.16.10.7.3.5. Layout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n9.7.16.10.8. Sparse Matrices\n9.7.16.10.8.1. Sparse\ntcgen05.mma.sp\nwith\n.kind::tf32\n9.7.16.10.8.2. Sparse\ntcgen05.mma.sp\nwith\n.kind::f16\n,\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n9.7.16.10.8.3. Sparse\ntcgen05.mma.sp\nwith\n.kind::mxf4\nand\n.kind::mxf4nvf4\n9.7.16.10.8.4. Sparsity selector\n9.7.16.10.8.4.1. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f16\n9.7.16.10.8.4.2. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f16\n9.7.16.10.8.4.3. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::tf32\n9.7.16.10.8.4.4. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::tf32\n9.7.16.10.8.4.5. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n9.7.16.10.8.4.6. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n9.7.16.10.8.5. Alignment restriction\n9.7.16.10.9. TensorCore 5th Generation of MMA Instructions\n9.7.16.10.9.1. TensorCore 5th Generation Instructions:\ntcgen05.mma\n9.7.16.10.9.2. TensorCore 5th Generation Instructions:\ntcgen05.mma.sp\n9.7.16.10.9.3. TensorCore 5th Generation Instructions:\ntcgen05.mma.ws\n9.7.16.10.9.4. TensorCore 5th Generation Instructions:\ntcgen05.mma.ws.sp\n9.7.16.11. TensorCore 5th Generation Specialized Synchronization Operations\n9.7.16.11.1. TensorCore 5th Generation Instructions:\ntcgen05.fence\n9.7.16.12. TensorCore 5th Generation Async Synchronization Operations\n9.7.16.12.1. TensorCore 5th Generation Instructions:\ntcgen05.commit\n9.7.17. Stack Manipulation Instructions\n9.7.17.1. Stack Manipulation Instructions:\nstacksave\n9.7.17.2. Stack Manipulation Instructions:\nstackrestore\n9.7.17.3. Stack Manipulation Instructions:\nalloca\n9.7.18. Video Instructions\n9.7.18.1. Scalar Video Instructions\n9.7.18.1.1. Scalar Video Instructions:\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\n9.7.18.1.2. Scalar Video Instructions:\nvshl\n,\nvshr\n9.7.18.1.3. Scalar Video Instructions:\nvmad\n9.7.18.1.4. Scalar Video Instructions:\nvset\n9.7.18.2. SIMD Video Instructions\n9.7.18.2.1. SIMD Video Instructions:\nvadd2\n,\nvsub2\n,\nvavrg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\n9.7.18.2.2. SIMD Video Instructions:\nvset2\n9.7.18.2.3. SIMD Video Instructions:\nvadd4\n,\nvsub4\n,\nvavrg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\n9.7.18.2.4. SIMD Video Instructions:\nvset4\n9.7.19. Miscellaneous Instructions\n9.7.19.1. Miscellaneous Instructions:\nbrkpt\n9.7.19.2. Miscellaneous Instructions:\nnanosleep\n9.7.19.3. Miscellaneous Instructions:\npmevent\n9.7.19.4. Miscellaneous Instructions:\ntrap\n9.7.19.5. Miscellaneous Instructions:\nsetmaxnreg\n10. Special Registers\n10.1. Special Registers:\n%tid\n10.2. Special Registers:\n%ntid\n10.3. Special Registers:\n%laneid\n10.4. Special Registers:\n%warpid\n10.5. Special Registers:\n%nwarpid\n10.6. Special Registers:\n%ctaid\n10.7. Special Registers:\n%nctaid\n10.8. Special Registers:\n%smid\n10.9. Special Registers:\n%nsmid\n10.10. Special Registers:\n%gridid\n10.11. Special Registers:\n%is_explicit_cluster\n10.12. Special Registers:\n%clusterid\n10.13. Special Registers:\n%nclusterid\n10.14. Special Registers:\n%cluster_ctaid\n10.15. Special Registers:\n%cluster_nctaid\n10.16. Special Registers:\n%cluster_ctarank\n10.17. Special Registers:\n%cluster_nctarank\n10.18. Special Registers:\n%lanemask_eq\n10.19. Special Registers:\n%lanemask_le\n10.20. Special Registers:\n%lanemask_lt\n10.21. Special Registers:\n%lanemask_ge\n10.22. Special Registers:\n%lanemask_gt\n10.23. Special Registers:\n%clock\n,\n%clock_hi\n10.24. Special Registers:\n%clock64\n10.25. Special Registers:\n%pm0\n…\n%pm7\n10.26. Special Registers:\n%pm0_64\n…\n%pm7_64\n10.27. Special Registers:\n%envreg<32>\n10.28. Special Registers:\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n10.29. Special Registers:\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n10.30. Special Registers:\n%total_smem_size\n10.31. Special Registers:\n%aggr_smem_size\n10.32. Special Registers:\n%dynamic_smem_size\n10.33. Special Registers:\n%current_graph_exec\n11. Directives\n11.1. PTX Module Directives\n11.1.1. PTX Module Directives:\n.version\n11.1.2. PTX Module Directives:\n.target\n11.1.3. PTX Module Directives:\n.address_size\n11.2. Specifying Kernel Entry Points and Functions\n11.2.1. Kernel and Function Directives:\n.entry\n11.2.2. Kernel and Function Directives:\n.func\n11.2.3. Kernel and Function Directives:\n.alias\n11.3. Control Flow Directives\n11.3.1. Control Flow Directives:\n.branchtargets\n11.3.2. Control Flow Directives:\n.calltargets\n11.3.3. Control Flow Directives:\n.callprototype\n11.4. Performance-Tuning Directives\n11.4.1. Performance-Tuning Directives:\n.maxnreg\n11.4.2. Performance-Tuning Directives:\n.maxntid\n11.4.3. Performance-Tuning Directives:\n.reqntid\n11.4.4. Performance-Tuning Directives:\n.minnctapersm\n11.4.5. Performance-Tuning Directives:\n.maxnctapersm\n(deprecated)\n11.4.6. Performance-Tuning Directives:\n.noreturn\n11.4.7. Performance-Tuning Directives:\n.pragma\n11.4.8. Performance-Tuning Directives:\n.abi_preserve\n11.4.9. Performance-Tuning Directives:\n.abi_preserve_control\n11.5. Debugging Directives\n11.5.1. Debugging Directives:\n@@dwarf\n11.5.2. Debugging Directives:\n.section\n11.5.3. Debugging Directives:\n.file\n11.5.4. Debugging Directives:\n.loc\n11.6. Linking Directives\n11.6.1. Linking Directives:\n.extern\n11.6.2. Linking Directives:\n.visible\n11.6.3. Linking Directives:\n.weak\n11.6.4. Linking Directives:\n.common\n11.7. Cluster Dimension Directives\n11.7.1. Cluster Dimension Directives:\n.reqnctapercluster\n11.7.2. Cluster Dimension Directives:\n.explicitcluster\n11.7.3. Cluster Dimension Directives:\n.maxclusterrank\n11.8. Miscellaneous Directives\n11.8.1. Miscellaneous Directives:\n.blocksareclusters\n12. Descriptions of\n.pragma\nStrings\n12.1. Pragma Strings:\n\"nounroll\"\n12.2. Pragma Strings:\n\"used_bytes_mask\"\n12.3. Pragma Strings:\n\"enable_smem_spilling\"\n12.4. Pragma Strings:\n\"frequency\"\n13. Release Notes\n13.1. Changes in PTX ISA Version 9.1\n13.2. Changes in PTX ISA Version 9.0\n13.3. Changes in PTX ISA Version 8.8\n13.4. Changes in PTX ISA Version 8.7\n13.5. Changes in PTX ISA Version 8.6\n13.6. Changes in PTX ISA Version 8.5\n13.7. Changes in PTX ISA Version 8.4\n13.8. Changes in PTX ISA Version 8.3\n13.9. Changes in PTX ISA Version 8.2\n13.10. Changes in PTX ISA Version 8.1\n13.11. Changes in PTX ISA Version 8.0\n13.12. Changes in PTX ISA Version 7.8\n13.13. Changes in PTX ISA Version 7.7\n13.14. Changes in PTX ISA Version 7.6\n13.15. Changes in PTX ISA Version 7.5\n13.16. Changes in PTX ISA Version 7.4\n13.17. Changes in PTX ISA Version 7.3\n13.18. Changes in PTX ISA Version 7.2\n13.19. Changes in PTX ISA Version 7.1\n13.20. Changes in PTX ISA Version 7.0\n13.21. Changes in PTX ISA Version 6.5\n13.22. Changes in PTX ISA Version 6.4\n13.23. Changes in PTX ISA Version 6.3\n13.24. Changes in PTX ISA Version 6.2\n13.25. Changes in PTX ISA Version 6.1\n13.26. Changes in PTX ISA Version 6.0\n13.27. Changes in PTX ISA Version 5.0\n13.28. Changes in PTX ISA Version 4.3\n13.29. Changes in PTX ISA Version 4.2\n13.30. Changes in PTX ISA Version 4.1\n13.31. Changes in PTX ISA Version 4.0\n13.32. Changes in PTX ISA Version 3.2\n13.33. Changes in PTX ISA Version 3.1\n13.34. Changes in PTX ISA Version 3.0\n13.35. Changes in PTX ISA Version 2.3\n13.36. Changes in PTX ISA Version 2.2\n13.37. Changes in PTX ISA Version 2.1\n13.38. Changes in PTX ISA Version 2.0\n14. Notices\n14.1. Notice\n14.2. OpenCL\n14.3. Trademarks\nPTX ISA\n»\n1.\nIntroduction\nv9.1 |\nPDF\n|\nArchive\nParallel Thread Execution ISA Version 9.1\nThe programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture).\n1.\nIntroduction\n\nThis document describes PTX, a low-level\nparallel thread execution\nvirtual machine and instruction\nset architecture (ISA). PTX exposes the GPU as a data-parallel computing\ndevice\n.\n1.1.\nScalable Data-Parallel Computing using GPUs\n\nDriven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable\nGPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous\ncomputational horsepower and very high memory bandwidth. The GPU is especially well-suited to\naddress problems that can be expressed as data-parallel computations - the same program is executed\non many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic\noperations to memory operations. Because the same program is executed for each data element, there\nis a lower requirement for sophisticated flow control; and because it is executed on many data\nelements and has high arithmetic intensity, the memory access latency can be hidden with\ncalculations instead of big data caches.\nData-parallel processing maps data elements to parallel processing threads. Many applications that\nprocess large data sets can use a data-parallel programming model to speed up the computations. In\n3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and\nmedia processing applications such as post-processing of rendered images, video encoding and\ndecoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to\nparallel processing threads. In fact, many algorithms outside the field of image rendering and\nprocessing are accelerated by data-parallel processing, from general signal processing or physics\nsimulation to computational finance or computational biology.\nPTX\ndefines a virtual machine and ISA for general purpose parallel thread execution. PTX programs\nare translated at install time to the target hardware instruction set. The PTX-to-GPU translator\nand driver enable NVIDIA GPUs to be used as programmable parallel computers.\n1.2.\nGoals of PTX\n\nPTX\nprovides a stable programming model and instruction set for general purpose parallel\nprogramming. It is designed to be efficient on NVIDIA GPUs supporting the computation features\ndefined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA\nand C/C++ generate PTX instructions, which are optimized for and translated to native\ntarget-architecture instructions.\nThe goals for PTX include the following:\nProvide a stable ISA that spans multiple GPU generations.\nAchieve performance in compiled applications comparable to native GPU performance.\nProvide a machine-independent ISA for C/C++ and other compilers to target.\nProvide a code distribution ISA for application and middleware developers.\nProvide a common source-level ISA for optimizing code generators and translators, which map PTX to\nspecific target machines.\nFacilitate hand-coding of libraries, performance kernels, and architecture tests.\nProvide a scalable programming model that spans GPU sizes from a single unit to many parallel units.\n1.3.\nPTX ISA Version 9.1\n\nPTX ISA version 9.1 introduces the following new features:\nAdds support for\n.volatile\nqualifier with\n.local\nstate space for\nld\nand\nst\ninstructions.\nAdds support for\n.f16x2\nand\n.bf16x2\nsource types for\ncvt\ninstruction\nwith destination types\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x2\n,\n.e5m2x2\n.\nAdds support for\n.scale_vec::4X\nwith\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nfor\nmma\n/\nmma.sp\ninstructions.\nAdds support for\n.s2f6x2\ninstruction type for\ncvt\ninstruction.\nAdds support for\nmultimem.cp.async.bulk\nand\nmultimem.cp.reduce.async.bulk\ninstructions.\n1.4.\nDocument Structure\n\nThe information in this document is organized into the following Chapters:\nProgramming Model\noutlines the programming model.\nPTX Machine Model\ngives an overview of the PTX virtual machine model.\nSyntax\ndescribes the basic syntax of the PTX language.\nState Spaces, Types, and Variables\ndescribes\nstate spaces, types, and variable declarations.\nInstruction Operands\ndescribes instruction operands.\nAbstracting the ABI\ndescribes the function and call syntax,\ncalling convention, and PTX support for abstracting the\nApplication Binary Interface (ABI)\n.\nInstruction Set\ndescribes the instruction set.\nSpecial Registers\nlists special registers.\nDirectives\nlists the assembly directives supported in PTX.\nRelease Notes\nprovides release notes for PTX ISA versions 2.x and\nbeyond.\nReferences\n754-2008 IEEE Standard for Floating-Point Arithmetic. ISBN 978-0-7381-5752-8, 2008.\nhttp://ieeexplore.ieee.org/servlet/opac?punumber=4610933\nThe OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011.\nhttp://www.khronos.org/registry/cl/specs/opencl-1.1.pdf\nCUDA Programming Guide.\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\nCUDA Dynamic Parallelism Programming Guide.\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism\nCUDA Atomicity Requirements.\nhttps://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity\nPTX Writers Guide to Interoperability.\nhttps://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html\n2.\nProgramming Model\n\n2.1.\nA Highly Multithreaded Coprocessor\n\nThe GPU is a compute device capable of executing a very large number of threads in parallel. It\noperates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive\nportions of applications running on the host are off-loaded onto the device.\nMore precisely, a portion of an application that is executed many times, but independently on\ndifferent data, can be isolated into a kernel function that is executed on the GPU as many different\nthreads. To that effect, such a function is compiled to the PTX instruction set and the resulting\nkernel is translated at install time to the target GPU instruction set.\n2.2.\nThread Hierarchy\n\nThe batch of threads that executes a kernel is organized as a grid. A grid consists of either\ncooperative thread arrays or clusters of cooperative thread arrays as described in this section and\nillustrated in\nFigure 1\nand\nFigure 2\n.\nCooperative thread arrays (CTAs)\nimplement CUDA\nthread blocks and clusters implement CUDA thread block clusters.\n2.2.1.\nCooperative Thread Arrays\n\nThe\nParallel Thread Execution (PTX)\nprogramming model is explicitly parallel: a PTX program\nspecifies the execution of a given thread of a parallel thread array. A\ncooperative thread array\n,\nor CTA, is an array of threads that execute a kernel concurrently or in parallel.\nThreads within a CTA can communicate with each other. To coordinate the communication of the threads\nwithin the CTA, one can specify synchronization points where threads wait until all threads in the\nCTA have arrived.\nEach thread has a unique thread identifier within the CTA. Programs use a data parallel\ndecomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread\nuses its thread identifier to determine its assigned role, assign specific input and output\npositions, compute addresses, and select work to perform. The thread identifier is a three-element\nvector\ntid\n, (with elements\ntid.x\n,\ntid.y\n, and\ntid.z\n) that specifies the thread’s\nposition within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the\nnumber of thread ids in that CTA dimension.\nEach CTA has a 1D, 2D, or 3D shape specified by a three-element vector\nntid\n(with elements\nntid.x\n,\nntid.y\n, and\nntid.z\n). The vector\nntid\nspecifies the number of threads in each\nCTA dimension.\nThreads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called\nwarps\n. A\nwarp\nis a maximal subset of threads from a single CTA, such that the threads execute\nthe same instructions at the same time. Threads within a warp are sequentially numbered. The warp\nsize is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be\nable to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate\nconstant,\nWARP_SZ\n, which may be used in any instruction where an immediate operand is allowed.\n2.2.2.\nCluster of Cooperative Thread Arrays\n\nCluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate\nwith each other via shared memory. The executing CTA has to make sure that the shared memory of the\npeer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before\ncompleting the shared memory operation.\nThreads within the different CTAs in a cluster can synchronize and communicate with each other via\nshared memory. Cluster-wide barriers can be used to synchronize all the threads within the\ncluster. Each CTA in a cluster has a unique CTA identifier within its cluster\n(\ncluster_ctaid\n). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter\ncluster_nctaid\n. Each CTA in the cluster also has a unique CTA identifier (\ncluster_ctarank\n)\nacross all dimensions. The total number of CTAs across all the dimensions in the cluster is\nspecified by\ncluster_nctarank\n. Threads may read and use these values through predefined, read-only\nspecial registers\n%cluster_ctaid\n,\n%cluster_nctaid\n,\n%cluster_ctarank\n,\n%cluster_nctarank\n.\nCluster level is applicable only on target architecture\nsm_90\nor higher. Specifying cluster\nlevel during launch time is optional. If the user specifies the cluster dimensions at launch time\nthen it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster\nlaunch with default dimension 1x1x1. PTX provides read-only special register\n%is_explicit_cluster\nto differentiate between explicit and implicit cluster launch.\n2.2.3.\nGrid of Clusters\n\nThere is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a\ncluster can contain. However, clusters with CTAs that execute the same kernel can be batched\ntogether into a grid of clusters, so that the total number of threads that can be launched in a\nsingle kernel invocation is very large. This comes at the expense of reduced thread communication\nand synchronization, because threads in different clusters cannot communicate and synchronize with\neach other.\nEach cluster has a unique cluster identifier (\nclusterid\n) within a grid of clusters. Each grid of\nclusters has a 1D, 2D , or 3D shape specified by the parameter\nnclusterid\n. Each grid also has a\nunique temporal grid identifier (\ngridid\n). Threads may read and use these values through\npredefined, read-only special registers\n%tid\n,\n%ntid\n,\n%clusterid\n,\n%nclusterid\n, and\n%gridid\n.\nEach CTA has a unique identifier (\nctaid\n) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape\nspecified by the parameter\nnctaid\n. Thread may use and read these values through predefined,\nread-only special registers\n%ctaid\nand\n%nctaid\n.\nEach kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs\nwhere cluster is optional level and is applicable only for target architectures\nsm_90\nand\nhigher.\nFigure 1\nshows a grid consisting of CTAs and\nFigure 2\nshows a grid consisting of clusters.\nGrids may be launched with dependencies between one another - a grid may be a dependent grid and/or\na prerequisite grid. To understand how grid dependencies may be defined, refer to the section on\nCUDA Graphs\nin the\nCuda Programming Guide\n.\nFigure 1\nGrid with CTAs\n\nFigure 2\nGrid with clusters\n\nA cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads\nthat execute the same kernel program. A grid is a set of clusters consisting of CTAs that\nexecute independently.\n2.3.\nMemory Hierarchy\n\nPTX threads may access data from multiple state spaces during their execution as illustrated by\nFigure 3\nwhere cluster level is introduced from\ntarget architecture\nsm_90\nonwards. Each thread has a private local memory. Each thread block\n(CTA) has a shared memory visible to all threads of the block and to all active blocks in the\ncluster and with the same lifetime as the block. Finally, all threads have access to the same global\nmemory.\nThere are additional state spaces accessible by all threads: the constant, param, texture, and\nsurface state spaces.  Constant and texture memory are read-only; surface memory is readable and\nwritable. The global, constant, param, texture, and surface state spaces are optimized for different\nmemory usages. For example, texture memory offers different addressing modes as well as data\nfiltering for specific data formats. Note that texture and surface memory is cached, and within the\nsame kernel call, the cache is not kept coherent with respect to global memory writes and surface\nmemory writes, so any texture fetch or surface read to an address that has been written to via a\nglobal or a surface write in the same kernel call returns undefined data. In other words, a thread\ncan safely read some texture or surface memory location only if this memory location has been\nupdated by a previous kernel call or memory copy, but not if it has been previously updated by the\nsame thread or another thread from the same kernel call.\nThe global, constant, and texture state spaces are persistent across kernel launches by the same\napplication.\nBoth the host and the device maintain their own local memory, referred to as\nhost memory\nand\ndevice memory\n, respectively. The device memory may be mapped and read or written by the host, or,\nfor more efficient transfer, copied from the host memory through optimized API calls that utilize\nthe device’s high-performance\nDirect Memory Access (DMA)\nengine.\nFigure 3\nMemory Hierarchy\n\n3.\nPTX Machine Model\n\n3.1.\nA Set of SIMT Multiprocessors\n\nThe NVIDIA GPU architecture is built around a scalable array of multithreaded\nStreaming\nMultiprocessors (SMs)\n. When a host program invokes a kernel grid, the blocks of the grid are\nenumerated and distributed to multiprocessors with available execution capacity. The threads of a\nthread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are\nlaunched on the vacated multiprocessors.\nA multiprocessor consists of multiple\nScalar Processor (SP)\ncores, a multithreaded instruction\nunit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent\nthreads in hardware with zero scheduling overhead. It implements a single-instruction barrier\nsynchronization. Fast barrier synchronization together with lightweight thread creation and\nzero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for\nexample, a low granularity decomposition of problems by assigning one thread to each data element\n(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation).\nTo manage hundreds of threads running several different programs, the multiprocessor employs an\narchitecture we call\nSIMT (single-instruction, multiple-thread)\n. The multiprocessor maps each\nthread to one scalar processor core, and each scalar thread executes independently with its own\ninstruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,\nand executes threads in groups of parallel threads called\nwarps\n. (This term originates from\nweaving, the first parallel thread technology.) Individual threads composing a SIMT warp start\ntogether at the same program address but are otherwise free to branch and execute independently.\nWhen a multiprocessor is given one or more thread blocks to execute, it splits them into warps that\nget scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp\ncontains threads of consecutive, increasing thread IDs with the first warp containing thread 0.\nAt every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues\nthe next instruction to the active threads of the warp. A warp executes one common instruction at a\ntime, so full efficiency is realized when all threads of a warp agree on their execution path. If\nthreads of a warp diverge via a data-dependent conditional branch, the warp serially executes each\nbranch path taken, disabling threads that are not on that path, and when all paths complete, the\nthreads converge back to the same execution path. Branch divergence occurs only within a warp;\ndifferent warps execute independently regardless of whether they are executing common or disjointed\ncode paths.\nSIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a\nsingle instruction controls multiple processing elements. A key difference is that SIMD vector\norganizations expose the SIMD width to the software, whereas SIMT instructions specify the execution\nand branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables\nprogrammers to write thread-level parallel code for independent, scalar threads, as well as\ndata-parallel code for coordinated threads. For the purposes of correctness, the programmer can\nessentially ignore the SIMT behavior; however, substantial performance improvements can be realized\nby taking care that the code seldom requires threads in a warp to diverge. In practice, this is\nanalogous to the role of cache lines in traditional code: Cache line size can be safely ignored when\ndesigning for correctness but must be considered in the code structure when designing for peak\nperformance. Vector architectures, on the other hand, require the software to coalesce loads into\nvectors and manage divergence manually.\nHow many blocks a multiprocessor can process at once depends on how many registers per thread and\nhow much shared memory per block are required for a given kernel since the multiprocessor’s\nregisters and shared memory are split among all the threads of the batch of blocks. If there are not\nenough registers or shared memory available per multiprocessor to process at least one block, the\nkernel will fail to launch.\nFigure 4\nHardware Model\n\nA set of SIMT multiprocessors with on-chip shared memory.\n3.2.\nIndependent Thread Scheduling\n\nOn architectures prior to Volta, warps used a single program counter shared amongst all 32 threads\nin the warp together with an active mask specifying the active threads of the warp. As a result,\nthreads from the same warp in divergent regions or different states of execution cannot signal each\nother or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or\nmutexes can easily lead to deadlock, depending on which warp the contending threads come from.\nStarting with the Volta architecture,\nIndependent Thread Scheduling\nallows full concurrency\nbetween threads, regardless of warp. With\nIndependent Thread Scheduling\n, the GPU maintains\nexecution state per thread, including a program counter and call stack, and can yield execution at a\nper-thread granularity, either to make better use of execution resources or to allow one thread to\nwait for data to be produced by another. A schedule optimizer determines how to group active threads\nfrom the same warp together into SIMT units. This retains the high throughput of SIMT execution as\nin prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at\nsub-warp granularity.\nIndependent Thread Scheduling\ncan lead to a rather different set of threads participating in the\nexecuted code than intended if the developer made assumptions about warp-synchronicity of previous\nhardware architectures. In particular, any warp-synchronous code (such as synchronization-free,\nintra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the\nsection on Compute Capability 7.x in the\nCuda Programming Guide\nfor further details.\n3.3.\nOn-chip Shared Memory\n\nAs illustrated by\nFigure 4\n, each multiprocessor has\non-chip memory of the four following types:\nOne set of local 32-bit\nregisters\nper processor,\nA parallel data cache or\nshared memory\nthat is shared by all scalar processor cores and is where\nthe shared memory space resides,\nA read-only\nconstant cache\nthat is shared by all scalar processor cores and speeds up reads from\nthe constant memory space, which is a read-only region of device memory,\nA read-only\ntexture cache\nthat is shared by all scalar processor cores and speeds up reads from\nthe texture memory space, which is a read-only region of device memory; each multiprocessor\naccesses the texture cache via a\ntexture unit\nthat implements the various addressing modes and\ndata filtering.\nThe local and global memory spaces are read-write regions of device memory.\n4.\nSyntax\n\nPTX programs are a collection of text source modules (files). PTX source modules have an\nassembly-language style syntax with instruction operation codes and operands. Pseudo-operations\nspecify symbol and addressing management. The ptxas optimizing backend compiler optimizes and\nassembles PTX source modules to produce corresponding binary object files.\n4.1.\nSource Format\n\nSource modules are ASCII text. Lines are separated by the newline character (\n\\n\n).\nAll whitespace characters are equivalent; whitespace is ignored except for its use in separating\ntokens in the language.\nThe C preprocessor cpp may be used to process PTX source modules. Lines beginning with\n#\nare\npreprocessor directives. The following are common preprocessor directives:\n#include\n,\n#define\n,\n#if\n,\n#ifdef\n,\n#else\n,\n#endif\n,\n#line\n,\n#file\nC: A Reference Manual\nby Harbison and Steele provides a good description of the C preprocessor.\nPTX is case sensitive and uses lowercase for keywords.\nEach PTX module must begin with a\n.version\ndirective specifying the PTX language version,\nfollowed by a\n.target\ndirective specifying the target architecture assumed. See\nPTX Module Directives\nfor a more information on these directives.\n4.2.\nComments\n\nComments in PTX follow C/C++ syntax, using non-nested\n/*\nand\n*/\nfor comments that may span\nmultiple lines, and using\n//\nto begin a comment that extends up to the next newline character,\nwhich terminates the current line. Comments cannot occur within character constants, string\nliterals, or within other comments.\nComments in PTX are treated as whitespace.\n4.3.\nStatements\n\nA PTX statement is either a directive or an instruction. Statements begin with an optional label and\nend with a semicolon.\nExamples\n.reg     .b32 r1, r2;\n        .global  .f32  array[N];\n\nstart:  mov.b32   r1, %tid.x;\n        shl.b32   r1, r1, 2;          // shift thread id by 2 bits\n        ld.global.b32 r2, array[r1];  // thread[tid] gets array[tid]\n        add.f32   r2, r2, 0.5;        // add 1/2\n4.3.1.\nDirective Statements\n\nDirective keywords begin with a dot, so no conflict is possible with user-defined identifiers. The\ndirectives in PTX are listed in\nTable 1\nand\ndescribed in\nState Spaces, Types, and Variables\nand\nDirectives\n.\nTable 1\nPTX Directives\n\n.address_size\n.explicitcluster\n.maxnreg\n.section\n.alias\n.extern\n.maxntid\n.shared\n.align\n.file\n.minnctapersm\n.sreg\n.branchtargets\n.func\n.noreturn\n.target\n.callprototype\n.global\n.param\n.tex\n.calltargets\n.loc\n.pragma\n.version\n.common\n.local\n.reg\n.visible\n.const\n.maxclusterrank\n.reqnctapercluster\n.weak\n.entry\n.maxnctapersm\n.reqntid\n4.3.2.\nInstruction Statements\n\nInstructions are formed from an instruction opcode followed by a comma-separated list of zero or\nmore operands, and terminated with a semicolon. Operands may be register variables, constant\nexpressions, address expressions, or label names. Instructions have an optional guard predicate\nwhich controls conditional execution. The guard predicate follows the optional label and precedes\nthe opcode, and is written as\n@p\n, where\np\nis a predicate register. The guard predicate may\nbe optionally negated, written as\n@!p\n.\nThe destination operand is first, followed by source operands.\nInstruction keywords are listed in\nTable 2\n. All instruction keywords are\nreserved tokens in PTX.\nTable 2\nReserved Instruction Keywords\n\nabs\ncvta\nmembar\nsetp\nvabsdiff\nactivemask\ndiscard\nmin\nshf\nvabsdiff2\nadd\ndiv\nmma\nshfl\nvabsdiff4\naddc\ndp2a\nmov\nshl\nvadd\nalloca\ndp4a\nmovmatrix\nshr\nvadd2\nand\nelect\nmul\nsin\nvadd4\napplypriority\nex2\nmul24\nslct\nvavrg2\natom\nexit\nmultimem\nsqrt\nvavrg4\nbar\nfence\nnanosleep\nst\nvmad\nbarrier\nfma\nneg\nstackrestore\nvmax\nbfe\nfns\nnot\nstacksave\nvmax2\nbfi\ngetctarank\nor\nstmatrix\nvmax4\nbfind\ngriddepcontrol\npmevent\nsub\nvmin\nbmsk\nisspacep\npopc\nsubc\nvmin2\nbra\nistypep\nprefetch\nsuld\nvmin4\nbrev\nld\nprefetchu\nsuq\nvote\nbrkpt\nldmatrix\nprmt\nsured\nvset\nbrx\nldu\nrcp\nsust\nvset2\ncall\nlg2\nred\nszext\nvset4\nclz\nlop3\nredux\ntanh\nvshl\ncnot\nmad\nrem\ntcgen05\nvshr\ncopysign\nmad24\nret\ntensormap\nvsub\ncos\nmadc\nrsqrt\ntestp\nvsub2\nclusterlaunchcontrol\nmapa\nsad\ntex\nvsub4\ncp\nmatch\nselp\ntld4\nwgmma\ncreatepolicy\nmax\nset\ntrap\nwmma\ncvt\nmbarrier\nsetmaxnreg\ntxq\nxor\n4.4.\nIdentifiers\n\nUser-defined identifiers follow extended C++ rules: they either start with a letter followed by zero\nor more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar,\nor percentage character followed by one or more letters, digits, underscore, or dollar characters:\nfollowsym:   [a-zA-Z0-9_$]\nidentifier:  [a-zA-Z]{followsym}* | {[_$%]{followsym}+\nPTX does not specify a maximum length for identifiers and suggests that all implementations support\na minimum length of at least 1024 characters.\nMany high-level languages such as C and C++ follow similar rules for identifier names, except that\nthe percentage sign is not allowed. PTX allows the percentage sign as the first character of an\nidentifier. The percentage sign can be used to avoid name conflicts, e.g., between user-defined\nvariable names and compiler-generated names.\nPTX predefines one constant and a small number of special registers that begin with the percentage\nsign, listed in\nTable 3\n.\nTable 3\nPredefined Identifiers\n\n%aggr_smem_size\n%dynamic_smem_size\n%lanemask_gt\n%reserved_smem_offset_begin\n%clock\n%envreg<32>\n%lanemask_le\n%reserved_smem_offset_cap\n%clock64\n%globaltimer\n%lanemask_lt\n%reserved_smem_offset_end\n%cluster_ctaid\n%globaltimer_hi\n%nclusterid\n%smid\n%cluster_ctarank\n%globaltimer_lo\n%nctaid\n%tid\n%cluster_nctaid\n%gridid\n%nsmid\n%total_smem_size\n%cluster_nctarank\n%is_explicit_cluster\n%ntid\n%warpid\n%clusterid\n%laneid\n%nwarpid\nWARP_SZ\n%ctaid\n%lanemask_eq\n%pm0,\n...,\n%pm7\n%current_graph_exec\n%lanemask_ge\n%reserved_smem_offset_<2>\n4.5.\nConstants\n\nPTX supports integer and floating-point constants and constant expressions. These constants may be\nused in data initialization and as operands to instructions. Type checking rules remain the same for\ninteger, floating-point, and bit-size types. For predicate-type data and instructions, integer\nconstants are allowed and are interpreted as in C, i.e., zero values are\nFalse\nand non-zero\nvalues are\nTrue\n.\n4.5.1.\nInteger Constants\n\nInteger constants are 64-bits in size and are either signed or unsigned, i.e., every integer\nconstant has type\n.s64\nor\n.u64\n. The signed/unsigned nature of an integer constant is needed\nto correctly evaluate constant expressions containing operations such as division and ordered\ncomparisons, where the behavior of the operation depends on the operand types. When used in an\ninstruction or data initialization, each integer constant is converted to the appropriate size based\non the data or instruction type at its use.\nInteger literals may be written in decimal, hexadecimal, octal, or binary notation. The syntax\nfollows that of C. Integer literals may be followed immediately by the letter\nU\nto indicate that\nthe literal is unsigned.\nhexadecimal literal:  0[xX]{hexdigit}+U?\noctal literal:        0{octal digit}+U?\nbinary literal:       0[bB]{bit}+U?\ndecimal literal       {nonzero-digit}{digit}*U?\nInteger literals are non-negative and have a type determined by their magnitude and optional type\nsuffix as follows: literals are signed (\n.s64\n) unless the value cannot be fully represented in\n.s64\nor the unsigned suffix is specified, in which case the literal is unsigned (\n.u64\n).\nThe predefined integer constant\nWARP_SZ\nspecifies the number of threads per warp for the target\nplatform; to date, all target architectures have a\nWARP_SZ\nvalue of 32.\n4.5.2.\nFloating-Point Constants\n\nFloating-point constants are represented as 64-bit double-precision values, and all floating-point\nconstant expressions are evaluated using 64-bit double precision arithmetic. The only exception is\nthe 32-bit hex notation for expressing an exact single-precision floating-point value; such values\nretain their exact 32-bit single-precision value and may not be used in constant expressions. Each\n64-bit floating-point constant is converted to the appropriate floating-point size based on the data\nor instruction type at its use.\nFloating-point literals may be written with an optional decimal point and an optional signed\nexponent. Unlike C and C++, there is no suffix letter to specify size; literals are always\nrepresented in 64-bit double-precision format.\nPTX includes a second representation of floating-point constants for specifying the exact machine\nrepresentation using a hexadecimal constant. To specify IEEE 754 double-precision floating point\nvalues, the constant begins with\n0d\nor\n0D\nfollowed by 16 hex digits. To specify IEEE 754\nsingle-precision floating point values, the constant begins with\n0f\nor\n0F\nfollowed by 8 hex\ndigits.\n0[fF]{hexdigit}{8}      // single-precision floating point\n0[dD]{hexdigit}{16}     // double-precision floating point\nExample\nmov.f32  $f3, 0F3f800000;       //  1.0\n4.5.3.\nPredicate Constants\n\nIn PTX, integer constants may be used as predicates. For predicate-type data initializers and\ninstruction operands, integer constants are interpreted as in C, i.e., zero values are\nFalse\nand\nnon-zero values are\nTrue\n.\n4.5.4.\nConstant Expressions\n\nIn PTX, constant expressions are formed using operators as in C and are evaluated using rules\nsimilar to those in C, but simplified by restricting types and sizes, removing most casts, and\ndefining full semantics to eliminate cases where expression evaluation in C is implementation\ndependent.\nConstant expressions are formed from constant literals, unary plus and minus, basic arithmetic\noperators (addition, subtraction, multiplication, division), comparison operators, the conditional\nternary operator (\n?:\n), and parentheses. Integer constant expressions also allow unary logical\nnegation (\n!\n), bitwise complement (\n~\n), remainder (\n%\n), shift operators (\n<<\nand\n>>\n), bit-type operators (\n&\n,\n|\n, and\n^\n), and logical operators (\n&&\n,\n||\n).\nConstant expressions in PTX do not support casts between integer and floating-point.\nConstant expressions are evaluated using the same operator precedence as\nin C.\nTable 4\ngives operator precedence and\nassociativity. Operator precedence is highest for unary operators and decreases with each line in\nthe chart. Operators on the same line have the same precedence and are evaluated right-to-left for\nunary operators and left-to-right for binary operators.\nTable 4\nOperator Precedence\n\nKind\nOperator Symbols\nOperator Names\nAssociates\nPrimary\n()\nparenthesis\nn/a\nUnary\n+-\n!\n~\nplus, minus, negation, complement\nright\n(.s64)\n(.u64)\ncasts\nright\nBinary\n*/\n%\nmultiplication, division, remainder\nleft\n+-\naddition, subtraction\n>>\n<<\nshifts\n<\n>\n<=\n>=\nordered comparisons\n==\n!=\nequal, not equal\n&\nbitwise AND\n^\nbitwise XOR\n|\nbitwise OR\n&&\nlogical AND\n||\nlogical OR\nTernary\n?:\nconditional\nright\n4.5.5.\nInteger Constant Expression Evaluation\n\nInteger constant expressions are evaluated at compile time according to a set of rules that\ndetermine the type (signed\n.s64\nversus unsigned\n.u64\n) of each sub-expression. These rules\nare based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and\nbehavior is fully defined in all cases (specifically, for remainder and shift operators).\nLiterals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a\nU\nsuffix. For example:\n42\n,\n0x1234\n,\n0123\nare signed.\n0xfabc123400000000\n,\n42U\n,\n0x1234U\nare unsigned.\nUnary plus and minus preserve the type of the input operand. For example:\n+123\n,\n-1\n,\n-(-42)\nare signed.\n-1U\n,\n-0xfabc123400000000\nare unsigned.\nUnary logical negation (\n!\n) produces a signed result with value\n0\nor\n1\n.\nUnary bitwise complement (\n~\n) interprets the source operand as unsigned and produces an\nunsigned result.\nSome binary operators require normalization of source operands. This normalization is known as\nthe usual arithmetic conversions\nand simply converts both operands to unsigned type if either\noperand is unsigned.\nAddition, subtraction, multiplication, and division perform the usual arithmetic conversions and\nproduce a result with the same type as the converted operands. That is, the operands and result\nare unsigned if either source operand is unsigned, and is otherwise signed.\nRemainder (\n%\n) interprets the operands as unsigned. Note that this differs from C, which allows\na negative divisor but defines the behavior to be implementation dependent.\nLeft and right shift interpret the second operand as unsigned and produce a result with the same\ntype as the first operand. Note that the behavior of right-shift is determined by the type of the\nfirst operand: right shift of a signed value is arithmetic and preserves the sign, and right shift\nof an unsigned value is logical and shifts in a zero bit.\nAND (\n&\n), OR (\n|\n), and XOR (\n^\n) perform the usual arithmetic conversions and produce a\nresult with the same type as the converted operands.\nAND_OP (\n&&\n), OR_OP (\n||\n), Equal (\n==\n), and Not_Equal (\n!=\n) produce a signed\nresult. The result value is 0 or 1.\nOrdered comparisons (\n<\n,\n<=\n,\n>\n,\n>=\n) perform the usual arithmetic conversions on\nsource operands and produce a signed result. The result value is\n0\nor\n1\n.\nCasting of expressions to signed or unsigned is supported using (\n.s64\n) and (\n.u64\n) casts.\nFor the conditional operator (\n?\n:\n) , the first operand must be an integer, and the second\nand third operands are either both integers or both floating-point. The usual arithmetic\nconversions are performed on the second and third operands, and the result type is the same as the\nconverted type.\n4.5.6.\nSummary of Constant Expression Evaluation Rules\n\nTable 5\ncontains a summary of the constant expression evaluation rules.\nTable 5\nConstant Expression Evaluation Rules\n\nKind\nOperator\nOperand Types\nOperand Interpretation\nResult Type\nPrimary\n()\nany type\nsame as source\nsame as source\nconstant literal\nn/a\nn/a\n.u64\n,\n.s64\n, or\n.f64\nUnary\n+-\nany type\nsame as source\nsame as source\n!\ninteger\nzero or non-zero\n.s64\n~\ninteger\n.u64\n.u64\nCast\n(.u64)\ninteger\n.u64\n.u64\n(.s64)\ninteger\n.s64\n.s64\nBinary\n+-\n*\n/\n.f64\n.f64\n.f64\ninteger\nuse usual conversions\nconverted type\n<\n>\n<=\n>=\n.f64\n.f64\n.s64\ninteger\nuse usual conversions\n.s64\n==\n!=\n.f64\n.f64\n.s64\ninteger\nuse usual conversions\n.s64\n%\ninteger\n.u64\n.s64\n>>\n<<\ninteger\n1st unchanged, 2nd is\n.u64\nsame as 1st operand\n&\n|\n^\ninteger\n.u64\n.u64\n&&\n||\ninteger\nzero or non-zero\n.s64\nTernary\n?:\nint\n?\n.f64\n:\n.f64\nsame as sources\n.f64\nint\n?\nint\n:\nint\nuse usual conversions\nconverted type\n5.\nState Spaces, Types, and Variables\n\nWhile the specific resources available in a given target GPU will vary, the kinds of resources will\nbe common across platforms, and these resources are abstracted in PTX through state spaces and data\ntypes.\n5.1.\nState Spaces\n\nA state space is a storage area with particular characteristics. All variables reside in some state\nspace. The characteristics of a state space include its size, addressability, access speed, access\nrights, and level of sharing between threads.\nThe state spaces defined in PTX are a byproduct of parallel programming and graphics\nprogramming. The list of state spaces is shown in\nTable 6\n,and\nproperties of state spaces are shown in\nTable 7\n.\nTable 6\nState Spaces\n\nName\nDescription\n.reg\nRegisters, fast.\n.sreg\nSpecial registers. Read-only; pre-defined; platform-specific.\n.const\nShared, read-only memory.\n.global\nGlobal memory, shared by all threads.\n.local\nLocal memory, private to each thread.\n.param\nKernel parameters, defined per-grid; or\nFunction or local parameters, defined per-thread.\n.shared\nAddressable memory, defined per CTA, accessible to all threads in the cluster\nthroughout the lifetime of the CTA that defines it.\n.tex\nGlobal texture memory (deprecated).\nTable 7\nProperties of State Spaces\n\nName\nAddressable\nInitializable\nAccess\nSharing\n.reg\nNo\nNo\nR/W\nper-thread\n.sreg\nNo\nNo\nRO\nper-CTA\n.const\nYes\nYes\n1\nRO\nper-grid\n.global\nYes\nYes\n1\nR/W\nContext\n.local\nYes\nNo\nR/W\nper-thread\n.param\n(as input to kernel)\nYes\n2\nNo\nRO\nper-grid\n.param\n(used in functions)\nRestricted\n3\nNo\nR/W\nper-thread\n.shared\nYes\nNo\nR/W\nper-cluster\n5\n.tex\nNo\n4\nYes, via driver\nRO\nContext\nNotes:\n1\nVariables in\n.const\nand\n.global\nstate spaces are initialized to zero by default.\n2\nAccessible only via the\nld.param{::entry}\ninstruction. Address may be taken via\nmov\ninstruction.\n3\nAccessible via\nld.param{::func}\nand\nst.param{::func}\ninstructions. Device function\ninput and return parameters may have their address taken via\nmov\n; the parameter is then located\non the stack frame and its address is in the\n.local\nstate space.\n4\nAccessible only via the\ntex\ninstruction.\n5\nVisible to the owning CTA and other active CTAs in the cluster.\n5.1.1.\nRegister State Space\n\nRegisters (\n.reg\nstate space) are fast storage locations. The number of registers is limited, and\nwill vary from platform to platform. When the limit is exceeded, register variables will be spilled\nto memory, causing changes in performance. For each architecture, there is a recommended maximum\nnumber of registers to use (see the\nCUDA Programming Guide\nfor details).\nRegisters may be typed (signed integer, unsigned integer, floating point, predicate) or\nuntyped. Register size is restricted; aside from predicate registers which are 1-bit, scalar\nregisters have a width of 8-, 16-, 32-, 64-, or 128-bits, and vector registers have a width of\n16-, 32-, 64-, or 128-bits. The most common use of 8-bit registers is with\nld\n,\nst\n, and\ncvt\ninstructions, or as elements of vector tuples.\nRegisters differ from the other state spaces in that they are not fully addressable, i.e., it is not\npossible to refer to the address of a register. When compiling to use the Application Binary\nInterface (ABI), register variables are restricted to function scope and may not be declared at\nmodule scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped\n.reg\nvariables, the compiler silently disables use of the ABI. Registers may have alignment\nboundaries required by multi-word loads and stores.\n5.1.2.\nSpecial Register State Space\n\nThe special register (\n.sreg\n) state space holds predefined, platform-specific registers, such as\ngrid, cluster, CTA, and thread parameters, clock counters, and performance monitoring registers. All\nspecial registers are predefined.\n5.1.3.\nConstant State Space\n\nThe constant (\n.const\n) state space is a read-only memory initialized by the host. Constant memory\nis accessed with a\nld.const\ninstruction. Constant memory is restricted in size, currently\nlimited to 64 KB which can be used to hold statically-sized constant variables. There is an\nadditional 640 KB of constant memory, organized as ten independent 64 KB regions. The driver may\nallocate and initialize constant buffers in these regions and pass pointers to the buffers as kernel\nfunction parameters. Since the ten regions are not contiguous, the driver must ensure that constant\nbuffers are allocated so that each buffer fits entirely within a 64 KB region and does not span a\nregion boundary.\nStatically-sized constant variables have an optional variable initializer; constant variables with\nno explicit initializer are initialized to zero by default. Constant buffers allocated by the driver\nare initialized by the host, and pointers to such buffers are passed to the kernel as\nparameters. See the description of kernel parameter attributes in\nKernel Function Parameter Attributes\nfor more details on passing pointers\nto constant buffers as kernel parameters.\n5.1.3.1.\nBanked Constant State Space (deprecated)\n\nPrevious versions of PTX exposed constant memory as a set of eleven 64 KB banks, with explicit bank\nnumbers required for variable declaration and during access.\nPrior to PTX ISA version 2.2, the constant memory was organized into fixed size banks. There were\neleven 64 KB banks, and banks were specified using the\n.const[bank]\nmodifier, where\nbank\nranged from 0 to 10. If no bank number was given, bank zero was assumed.\nBy convention, bank zero was used for all statically-sized constant variables. The remaining banks\nwere used to declare\nincomplete\nconstant arrays (as in C, for example), where the size is not\nknown at compile time. For example, the declaration\n.extern .const[2] .b32 const_buffer[];\nresulted in\nconst_buffer\npointing to the start of constant bank two. This pointer could then be\nused to access the entire 64 KB constant bank. Multiple incomplete array variables declared in the\nsame bank were aliased, with each pointing to the start address of the specified constant bank.\nTo access data in contant banks 1 through 10, the bank number was required in the state space of the\nload instruction. For example, an incomplete array in bank 2 was accessed as follows:\n.extern .const[2] .b32 const_buffer[];\nld.const[2].b32  %r1, [const_buffer+4]; // load second word\nIn PTX ISA version 2.2, we eliminated explicit banks and replaced the incomplete array\nrepresentation of driver-allocated constant buffers with kernel parameter attributes that allow\npointers to constant buffers to be passed as kernel parameters.\n5.1.4.\nGlobal State Space\n\nThe global (\n.global\n) state space is memory that is accessible by all threads in a context. It is\nthe mechanism by which threads in different CTAs, clusters, and grids can communicate. Use\nld.global\n,\nst.global\n, and\natom.global\nto access global variables.\nGlobal variables have an optional variable initializer; global variables with no explicit\ninitializer are initialized to zero by default.\n5.1.5.\nLocal State Space\n\nThe local state space (\n.local\n) is private memory for each thread to keep its own data. It is\ntypically standard memory with cache. The size is limited, as it must be allocated on a per-thread\nbasis. Use\nld.local\nand\nst.local\nto access local variables.\nWhen compiling to use the\nApplication Binary Interface (ABI)\n,\n.local\nstate-space variables\nmust be declared within function scope and are allocated on the stack. In implementations that do\nnot support a stack, all local memory variables are stored at fixed addresses, recursive function\ncalls are not supported, and\n.local\nvariables may be declared at module scope. When compiling\nlegacy PTX code (ISA versions prior to 3.0) containing module-scoped\n.local\nvariables, the\ncompiler silently disables use of the ABI.\n5.1.6.\nParameter State Space\n\nThe parameter (\n.param\n) state space is used (1) to pass input arguments from the host to the\nkernel, (2a) to declare formal input and return parameters for device functions called from within\nkernel execution, and (2b) to declare locally-scoped byte array variables that serve as function\ncall arguments, typically for passing large structures by value to a function. Kernel function\nparameters differ from device function parameters in terms of access and sharing (read-only versus\nread-write, per-kernel versus per-thread). Note that PTX ISA versions 1.x supports only kernel\nfunction parameters in .param space; device function parameters were previously restricted to the\nregister state space. The use of parameter state space for device function parameters was introduced\nin PTX ISA version 2.0 and requires target architecture\nsm_20\nor higher. Additional sub-qualifiers\n::entry\nor\n::func\ncan be specified on instructions with\n.param\nstate space to indicate\nwhether the address refers to kernel function parameter or device function parameter. If no\nsub-qualifier is specified with the\n.param\nstate space, then the default sub-qualifier is specific\nto and dependent on the exact instruction. For example,\nst.param\nis equivalent to\nst.param::func\nwhereas\nisspacep.param\nis equivalent to\nisspacep.param::entry\n. Refer to the instruction\ndescription for more details on default sub-qualifier assumption.\nNote\nThe location of parameter space is implementation specific. For example, in some implementations\nkernel parameters reside in global memory. No access protection is provided between parameter and\nglobal space in this case. Though the exact location of the kernel parameter space is\nimplementation specific, the kernel parameter space window is always contained within the global\nspace window. Similarly, function parameters are mapped to parameter passing registers and/or\nstack locations based on the function calling conventions of the\nApplication Binary Interface\n(ABI)\n. Therefore, PTX code should make no assumptions about the relative locations or ordering\nof\n.param\nspace variables.\n5.1.6.1.\nKernel Function Parameters\n\nEach kernel function definition includes an optional list of parameters. These parameters are\naddressable, read-only variables declared in the\n.param\nstate space. Values passed from the host\nto the kernel are accessed through these parameter variables using\nld.param\ninstructions. The\nkernel parameter variables are shared across all CTAs from all clusters within a grid.\nThe address of a kernel parameter may be moved into a register using the\nmov\ninstruction. The\nresulting address is in the\n.param\nstate space and is accessed using\nld.param\ninstructions.\nExample\n.entry foo ( .param .b32 N, .param .align 8 .b8 buffer[64] )\n{\n    .reg .u32 %n;\n    .reg .f64 %d;\n\n    ld.param.u32 %n, [N];\n    ld.param.f64 %d, [buffer];\n    ...\nExample\n.entry bar ( .param .b32 len )\n{\n    .reg .u32 %ptr, %n;\n\n    mov.u32      %ptr, len;\n    ld.param.u32 %n, [%ptr];\n    ...\nKernel function parameters may represent normal data values, or they may hold addresses to objects\nin constant, global, local, or shared state spaces. In the case of pointers, the compiler and\nruntime system need information about which parameters are pointers, and to which state space they\npoint. Kernel parameter attribute directives are used to provide this information at the PTX\nlevel. See\nKernel Function Parameter Attributes\nfor a description of kernel parameter attribute\ndirectives.\nNote\nThe current implementation does not allow creation of generic pointers to constant variables\n(\ncvta.const\n) in programs that have pointers to constant buffers passed as kernel parameters.\n5.1.6.2.\nKernel Function Parameter Attributes\n\nKernel function parameters may be declared with an optional .ptr attribute to indicate that a\nparameter is a pointer to memory, and also indicate the state space and alignment of the memory\nbeing pointed to.\nKernel Parameter Attribute: .ptr\ndescribes the\n.ptr\nkernel parameter attribute.\n5.1.6.3.\nKernel Parameter Attribute:\n.ptr\n\n.ptr\nKernel parameter alignment attribute.\nSyntax\n.param .type .ptr .space .align N  varname\n.param .type .ptr        .align N  varname\n\n.space = { .const, .global, .local, .shared };\nDescription\nUsed to specify the state space and, optionally, the alignment of memory pointed to by a pointer\ntype kernel parameter. The alignment value\nN\n, if present, must be a power of two. If no state\nspace is specified, the pointer is assumed to be a generic address pointing to one of const, global,\nlocal, or shared memory. If no alignment is specified, the memory pointed to is assumed to be\naligned to a 4 byte boundary.\nSpaces between\n.ptr\n,\n.space\n, and\n.align\nmay be eliminated to improve readability.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.2.\nSupport for generic addressing of .const space added in PTX ISA version 3.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo ( .param .u32 param1,\n             .param .u32 .ptr.global.align 16 param2,\n             .param .u32 .ptr.const.align 8 param3,\n             .param .u32 .ptr.align 16 param4  // generic address\n                                               // pointer\n) { .. }\n5.1.6.4.\nDevice Function Parameters\n\nPTX ISA version 2.0 extended the use of parameter space to device function parameters. The most\ncommon use is for passing objects by value that do not fit within a PTX register, such as C\nstructures larger than 8 bytes. In this case, a byte array in parameter space is used. Typically,\nthe caller will declare a locally-scoped\n.param\nbyte array variable that represents a flattened\nC structure or union. This will be passed by value to a callee, which declares a\n.param\nformal\nparameter having the same size and alignment as the passed argument.\nExample\n// pass object of type struct { double d; int y; };\n.func foo ( .reg .b32 N, .param .align 8 .b8 buffer[12] )\n{\n    .reg .f64 %d;\n    .reg .s32 %y;\n\n    ld.param.f64 %d, [buffer];\n    ld.param.s32 %y, [buffer+8];\n    ...\n}\n\n// code snippet from the caller\n// struct { double d; int y; } mystruct; is flattened, passed to foo\n    ...\n    .reg .f64 dbl;\n    .reg .s32 x;\n    .param .align 8 .b8 mystruct;\n    ...\n    st.param.f64 [mystruct+0], dbl;\n    st.param.s32 [mystruct+8], x;\n    call foo, (4, mystruct);\n    ...\nSee the section on function call syntax for more details.\nFunction input parameters may be read via\nld.param\nand function return parameters may be written\nusing\nst.param\n; it is illegal to write to an input parameter or read from a return parameter.\nAside from passing structures by value,\n.param\nspace is also required whenever a formal\nparameter has its address taken within the called function. In PTX, the address of a function input\nparameter may be moved into a register using the\nmov\ninstruction. Note that the parameter will\nbe copied to the stack if necessary, and so the address will be in the\n.local\nstate space and is\naccessed via\nld.local\nand\nst.local\ninstructions. It is not possible to use\nmov\nto get\nthe address of or a locally-scoped\n.param\nspace variable. Starting PTX ISA version 6.0, it is\npossible to use\nmov\ninstruction to get address of return parameter of device function.\nExample\n// pass array of up to eight floating-point values in buffer\n.func foo ( .param .b32 N, .param .b32 buffer[32] )\n{\n    .reg .u32  %n, %r;\n    .reg .f32  %f;\n    .reg .pred %p;\n\n    ld.param.u32 %n, [N];\n    mov.u32      %r, buffer;  // forces buffer to .local state space\nLoop:\n    setp.eq.u32  %p, %n, 0;\n@%p bra         Done;\n    ld.local.f32 %f, [%r];\n    ...\n    add.u32      %r, %r, 4;\n    sub.u32      %n, %n, 1;\n    bra          Loop;\nDone:\n    ...\n}\n5.1.7.\nShared State Space\n\nThe shared (\n.shared\n) state space is a memory that is owned by an executing CTA and is accessible\nto the threads of all the CTAs within a cluster. An address in shared memory can be read and written\nby any thread in a CTA cluster.\nAdditional sub-qualifiers\n::cta\nor\n::cluster\ncan be specified on instructions with\n.shared\nstate space to indicate whether the address belongs to the shared memory window of the\nexecuting CTA or of any CTA in the cluster respectively. The addresses in the\n.shared::cta\nwindow also fall within the\n.shared::cluster\nwindow. If no sub-qualifier is specified with the\n.shared\nstate space, then it defaults to\n::cta\n. For example,\nld.shared\nis equivalent to\nld.shared::cta\n.\nVariables declared in\n.shared\nstate space refer to the memory addresses in the current\nCTA. Instruction\nmapa\ngives the\n.shared::cluster\naddress of the corresponding variable in\nanother CTA in the cluster.\nShared memory typically has some optimizations to support the sharing. One example is broadcast;\nwhere all threads read from the same address. Another is sequential access from sequential threads.\n5.1.8.\nTexture State Space (deprecated)\n\nThe texture (\n.tex\n) state space is global memory accessed via the texture instruction. It is\nshared by all threads in a context. Texture memory is read-only and cached, so accesses to texture\nmemory are not coherent with global memory stores to the texture image.\nThe GPU hardware has a fixed number of texture bindings that can be accessed within a single kernel\n(typically 128). The .tex directive will bind the named texture memory variable to a hardware\ntexture identifier, where texture identifiers are allocated sequentially beginning with\nzero. Multiple names may be bound to the same physical texture identifier. An error is generated if\nthe maximum number of physical resources is exceeded. The texture name must be of type\n.u32\nor\n.u64\n.\nPhysical texture resources are allocated on a per-kernel granularity, and\n.tex\nvariables are\nrequired to be defined in the global scope.\nTexture memory is read-only. A texture’s base address is assumed to be aligned to a 16 byte\nboundary.\nExample\n.tex .u32 tex_a;         // bound to physical texture 0\n.tex .u32 tex_c, tex_d;  // both bound to physical texture 1\n.tex .u32 tex_d;         // bound to physical texture 2\n.tex .u32 tex_f;         // bound to physical texture 3\nNote\nExplicit declarations of variables in the texture state space is deprecated, and programs should\ninstead reference texture memory through variables of type\n.texref\n. The\n.tex\ndirective is\nretained for backward compatibility, and variables declared in the\n.tex\nstate space are\nequivalent to module-scoped\n.texref\nvariables in the\n.global\nstate space.\nFor example, a legacy PTX definitions such as\n.tex .u32 tex_a;\nis equivalent to:\n.global .texref tex_a;\nSee\nTexture Sampler and Surface Types\nfor the\ndescription of the\n.texref\ntype and\nTexture Instructions\nfor its use in texture instructions.\n5.2.\nTypes\n\n5.2.1.\nFundamental Types\n\nIn PTX, the fundamental types reflect the native data types supported by the target architectures. A\nfundamental type specifies both a basic type and a size. Register variables are always of a\nfundamental type, and instructions operate on these types. The same type-size specifiers are used\nfor both variable definitions and for typing instructions, so their names are intentionally short.\nTable 8\nlists the fundamental type specifiers for\neach basic type:\nTable 8\nFundamental Type Specifiers\n\nBasic Type\nFundamental Type Specifiers\nSigned integer\n.s8\n,\n.s16\n,\n.s32\n,\n.s64\nUnsigned integer\n.u8\n,\n.u16\n,\n.u32\n,\n.u64\nFloating-point\n.f16\n,\n.f16x2\n,\n.f32\n,\n.f64\nBits (untyped)\n.b8\n,\n.b16\n,\n.b32\n,\n.b64\n,\n.b128\nPredicate\n.pred\nMost instructions have one or more type specifiers, needed to fully specify instruction\nbehavior. Operand types and sizes are checked against instruction types for compatibility.\nTwo fundamental types are compatible if they have the same basic type and are the same size. Signed\nand unsigned integer types are compatible if they have the same size. The bit-size type is\ncompatible with any fundamental type having the same size.\nIn principle, all variables (aside from predicates) could be declared using only bit-size types, but\ntyped variables enhance program readability and allow for better operand type checking.\n5.2.2.\nRestricted Use of Sub-Word Sizes\n\nThe\n.u8\n,\n.s8\n, and\n.b8\ninstruction types are restricted to\nld\n,\nst\n, and\ncvt\ninstructions. The\n.f16\nfloating-point type is allowed only in conversions to and from\n.f32\n,\n.f64\ntypes, in half precision floating point instructions and texture fetch instructions. The\n.f16x2\nfloating point type is allowed only in half precision floating point arithmetic\ninstructions and texture fetch instructions.\nFor convenience,\nld\n,\nst\n, and\ncvt\ninstructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes.\n5.2.3.\nAlternate Floating-Point Data Formats\n\nThe fundamental floating-point types supported in PTX have implicit bit representations that\nindicate the number of bits used to store exponent and mantissa. For example, the\n.f16\ntype\nindicates 5 bits reserved for exponent and 10 bits reserved for mantissa. In addition to the\nfloating-point representations assumed by the fundamental types, PTX allows the following alternate\nfloating-point data formats:\nbf16\ndata format:\nThis data format is a 16-bit floating point format with 8 bits for exponent and 7 bits for\nmantissa. A register variable containing\nbf16\ndata must be declared with\n.b16\ntype.\ne4m3\ndata format:\nThis data format is an 8-bit floating point format with 4 bits for exponent and 3 bits for\nmantissa. The\ne4m3\nencoding does not support infinity and\nNaN\nvalues are limited to\n0x7f\nand\n0xff\n. A register variable containing\ne4m3\nvalue must be declared using\nbit-size type.\ne5m2\ndata format:\nThis data format is an 8-bit floating point format with 5 bits for exponent and 2 bits for\nmantissa. A register variable containing\ne5m2\nvalue must be declared using bit-size type.\ntf32\ndata format:\nThis data format is a special 32-bit floating point format supported by the matrix\nmultiply-and-accumulate instructions, with the same range as\n.f32\nand reduced precision (>=10\nbits). The internal layout of\ntf32\nformat is implementation defined. PTX facilitates\nconversion from single precision\n.f32\ntype to\ntf32\nformat. A register variable containing\ntf32\ndata must be declared with\n.b32\ntype.\ne2m1\ndata format:\nThis data format is a 4-bit floating point format with 2 bits for exponent and 1 bit for mantissa.\nThe\ne2m1\nencoding does not support infinity and\nNaN\n.\ne2m1\nvalues must be used in a\npacked format specified as\ne2m1x2\n. A register variable containing two\ne2m1\nvalues must be\ndeclared with\n.b8\ntype.\ne2m3\ndata format:\nThis data format is a 6-bit floating point format with 2 bits for exponent and 3 bits for mantissa.\nThe\ne2m3\nencoding does not support infinity and\nNaN\n.\ne2m3\nvalues must be used in a\npacked format specified as\ne2m3x2\n. A register variable containing two\ne2m3\nvalues must be\ndeclared with\n.b16\ntype where each\n.b8\nelement has 6-bit floating point value and 2 MSB\nbits padded with zeros.\ne3m2\ndata format:\nThis data format is a 6-bit floating point format with 3 bits for exponent and 2 bits for mantissa.\nThe\ne3m2\nencoding does not support infinity and\nNaN\n.\ne3m2\nvalues must be used in a\npacked format specified as\ne3m2x2\n. A register variable containing two\ne3m2\nvalues must be\ndeclared with\n.b16\ntype where each\n.b8\nelement has 6-bit floating point value and 2 MSB\nbits padded with zeros.\nue8m0\ndata format:\nThis data format is an 8-bit unsigned floating-point format with 8 bits for exponent and 0 bits for\nmantissa. The\nue8m0\nencoding does not support infinity.\nNaN\nvalue is limited to\n0xff\n.\nue8m0\nvalues must be used in a packed format specified as\nue8m0x2\n. A register variable\ncontaining two\nue8m0\nvalues must be declared with\n.b16\ntype.\nue4m3\ndata format:\nThis data format is a 7-bit unsigned floating-point format with 4 bits for exponent and 3 bits for\nmantissa. The\nue4m3\nencoding does not support infinity.\nNaN\nvalue is limited to\n0x7f\n.\nA register variable containing single\nue4m3\nvalue must be declared with\n.b8\ntype having\nMSB bit padded with zero.\nAlternate data formats cannot be used as fundamental types. They are supported as source or\ndestination formats by certain instructions.\n5.2.4.\nFixed-point Data format\n\nPTX supports following fixed-point data formats:\ns2f6\ndata format:\nThis data format is 8-bit signed 2’s complement integer with 2 sign-integer bits and\n6 fractional bits with form\nxx.xxxxxx\n. The\ns2f6\nencoding does not support infinity\nand\nNaN\n.\ns2f6\nvalue = s8 value * 2^(-6)\nPositive max representation = 01.111111 = 127 * 2^(-6) = 1.984375\nNegative max representation = 10.000000 = -128 * 2^(-6) = -2.0\n5.2.5.\nPacked Data Types\n\nCertain PTX instructions operate on two or more sets of inputs in parallel, and produce two or more\noutputs. Such instructions can use the data stored in a packed format. PTX supports packing two or\nfour values of the same scalar data type into a single, larger value. The packed value is considered\nas a value of a\npacked data type\n. In this section we describe the packed data types supported in PTX.\n5.2.5.1.\nPacked Floating Point Data Types\n\nPTX supports various variants of packed floating point data types. Out of them, only\n.f16x2\nis\nsupported as a fundamental type, while others cannot be used as fundamental types - they are\nsupported as instruction types on certain instructions. When using an instruction with such\nnon-fundamental types, the operand data variables must be of bit type of appropriate size.\nFor example, all of the operand variables must be of type\n.b32\nfor an instruction with\ninstruction type as\n.bf16x2\n.\nTable 9\ndescribed various variants\nof packed floating point data types in PTX.\nTable 9\nOperand types for packed floating point instruction type.\n\nPacked floating\npoint type\nNumber of elements\ncontained in a\npacked format\nType of each\nelement\nRegister variable type\nto be used in the\ndeclaration\n.f16x2\nTwo\n.f16\n.f16x2\nor\n.b32\n.f32x2\n.f32\n.b64\n.bf16x2\n.bf16\n.b32\n.e4m3x2\n.e4m3\n.b16\n.e5m2x2\n.e5m2\n.e2m3x2\n.e2m3\n.e3m2x2\n.e3m2\n.ue8m0x2\n.ue8m0\n.s2f6x2\n.s2f6\n.e2m1x2\n.e2m1\n.b8\n.e4m3x4\nFour\n.e4m3\n.b32\n.e5m2x4\n.e5m2\n.e2m3x4\n.e2m3\n.e3m2x4\n.e3m2\n.e2m1x4\n.e2m1\n.b16\n5.2.5.2.\nPacked Integer Data Types\n\nPTX supports two variants of packed integer data types:\n.u16x2\nand\n.s16x2\n. The packed data\ntype consists of two\n.u16\nor\n.s16\nvalues. A register variable containing\n.u16x2\nor\n.s16x2\ndata must be declared with\n.b32\ntype. Packed integer data types cannot be used as\nfundamental types. They are supported as instruction types on certain instructions.\n5.2.5.3.\nPacked Fixed-Point Data Types\n\nPTX supports\n.s2f6x2\npacked fixed-point data type consisting of two\n.s2f6\npacked\nfixed-point values. A register variable containing\n.s2f6x2\nvalue must be declared with\n.b16\ntype. Packed fixed-point data type cannot be used as fundamental type and is only\nsupported as instruction type.\n5.3.\nTexture Sampler and Surface Types\n\nPTX includes built-in\nopaque\ntypes for defining texture, sampler, and surface descriptor\nvariables. These types have named fields similar to structures, but all information about layout,\nfield ordering, base address, and overall size is hidden to a PTX program, hence the term\nopaque\n. The use of these opaque types is limited to:\nVariable definition within global (module) scope and in kernel entry parameter lists.\nStatic initialization of module-scope variables using comma-delimited static assignment\nexpressions for the named members of the type.\nReferencing textures, samplers, or surfaces via texture and surface load/store instructions\n(\ntex\n,\nsuld\n,\nsust\n,\nsured\n).\nRetrieving the value of a named member via query instructions (\ntxq\n,\nsuq\n).\nCreating pointers to opaque variables using\nmov\n, e.g.,\nmov.u64\nreg,\nopaque_var;\n. The\nresulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and\nde-referenced by texture and surface load, store, and query instructions, but the pointer cannot\notherwise be treated as an address, i.e., accessing the pointer with\nld\nand\nst\ninstructions, or performing pointer arithmetic will result in undefined results.\nOpaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque\nvariable.\nNote\nIndirect access to textures and surfaces using pointers to opaque variables is supported\nbeginning with PTX ISA version 3.1 and requires target\nsm_20\nor later.\nIndirect access to textures is supported only in unified texture mode (see below).\nThe three built-in types are\n.texref\n,\n.samplerref\n, and\n.surfref\n. For working with\ntextures and samplers, PTX has two modes of operation. In the\nunified mode,\ntexture and sampler\ninformation is accessed through a single\n.texref\nhandle. In the\nindependent mode\n, texture and\nsampler information each have their own handle, allowing them to be defined separately and combined\nat the site of usage in the program. In independent mode, the fields of the\n.texref\ntype that\ndescribe sampler properties are ignored, since these properties are defined by\n.samplerref\nvariables.\nTable 10\nand\nTable 11\nlist the named members\nof each type for unified and independent texture modes. These members and their values have\nprecise mappings to methods and values defined in the texture\nHW\nclass as well as\nexposed values via the API.\nTable 10\nOpaque Type Fields in Unified Texture Mode\n\nMember\n.texref values\n.surfref values\nwidth\nin elements\nheight\nin elements\ndepth\nin elements\nchannel_data_type\nenum\ntype corresponding to source language API\nchannel_order\nenum\ntype corresponding to source language API\nnormalized_coords\n0\n,\n1\nN/A\nfilter_mode\nnearest\n,\nlinear\nN/A\naddr_mode_0\n,\naddr_mode_1\n,\naddr_mode_2\nwrap\n,\nmirror\n,\nclamp_ogl\n,\nclamp_to_edge\n,\nclamp_to_border\nN/A\narray_size\nas number of textures in a texture\narray\nas number of surfaces in a surface array\nnum_mipmap_levels\nas number of levels in a mipmapped\ntexture\nN/A\nnum_samples\nas number of samples in a multi-sample\ntexture\nN/A\nmemory_layout\nN/A\n1\nfor linear memory layout;\n0\notherwise\n5.3.1.\nTexture and Surface Properties\n\nFields\nwidth\n,\nheight\n, and\ndepth\nspecify the size of the texture or surface in number of\nelements in each dimension.\nThe\nchannel_data_type\nand\nchannel_order\nfields specify these properties of the texture or\nsurface using enumeration types corresponding to the source language API. For example, see\nChannel Data Type and Channel Order Fields\nfor\nthe OpenCL enumeration types currently supported in PTX.\n5.3.2.\nSampler Properties\n\nThe\nnormalized_coords\nfield indicates whether the texture or surface uses normalized coordinates\nin the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N). If no value is\nspecified, the default is set by the runtime system based on the source language.\nThe\nfilter_mode\nfield specifies how the values returned by texture reads are computed based on\nthe input texture coordinates.\nThe\naddr_mode_{0,1,2}\nfields define the addressing mode in each dimension, which determine how\nout-of-range coordinates are handled.\nSee the\nCUDA C++ Programming Guide\nfor more details of these properties.\nTable 11\nOpaque Type Fields in Independent Texture Mode\n\nMember\n.samplerref values\n.texref values\n.surfref values\nwidth\nN/A\nin elements\nheight\nN/A\nin elements\ndepth\nN/A\nin elements\nchannel_data_type\nN/A\nenum\ntype corresponding to source\nlanguage API\nchannel_order\nN/A\nenum\ntype corresponding to source\nlanguage AP\nnormalized_coords\nN/A\n0\n,\n1\nN/A\nforce_unnormalized_coords\n0\n,\n1\nN/A\nN/A\nfilter_mode\nnearest\n,\nlinear\nignored\nN/A\naddr_mode_0\n,\naddr_mode_1\n,\naddr_mode_2\nwrap\n,\nmirror\n,\nclamp_ogl\n,\nclamp_to_edge\n,\nclamp_to_border\nN/A\nN/A\narray_size\nN/A\nas number of textures\nin a texture array\nas number of surfaces in\na surface array\nnum_mipmap_levels\nN/A\nas number of levels\nin a mipmapped\ntexture\nN/A\nnum_samples\nN/A\nas number of samples\nin a multi-sample\ntexture\nN/A\nmemory_layout\nN/A\nN/A\n1\nfor linear memory\nlayout;\n0\notherwise\nIn independent texture mode, the sampler properties are carried in an independent\n.samplerref\nvariable, and these fields are disabled in the\n.texref\nvariables. One additional sampler\nproperty,\nforce_unnormalized_coords\n, is available in independent texture mode.\nThe\nforce_unnormalized_coords\nfield is a property of\n.samplerref\nvariables that allows the\nsampler to override the texture header\nnormalized_coords\nproperty. This field is defined only in\nindependent texture mode. When\nTrue\n, the texture header setting is overridden and unnormalized\ncoordinates are used; when\nFalse\n, the texture header setting is used.\nThe\nforce_unnormalized_coords\nproperty is used in compiling OpenCL; in OpenCL, the property of\nnormalized coordinates is carried in sampler headers. To compile OpenCL to PTX, texture headers are\nalways initialized with\nnormalized_coords\nset to True, and the OpenCL sampler-based\nnormalized_coords\nflag maps (negated) to the PTX-level\nforce_unnormalized_coords\nflag.\nVariables using these types may be declared at module scope or within kernel entry parameter\nlists. At module scope, these variables must be in the\n.global\nstate space. As kernel\nparameters, these variables are declared in the\n.param\nstate space.\nExample\n.global .texref     my_texture_name;\n.global .samplerref my_sampler_name;\n.global .surfref    my_surface_name;\nWhen declared at module scope, the types may be initialized using a list of static expressions\nassigning values to the named members.\nExample\n.global .texref tex1;\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n5.3.3.\nChannel Data Type and Channel Order Fields\n\nThe\nchannel_data_type\nand\nchannel_order\nfields have enumeration types corresponding to the\nsource language API. Currently, OpenCL is the only source language that defines these\nfields.\nTable 13\nand\nTable 12\nshow the\nenumeration values defined in OpenCL version 1.0 for channel data type and channel order.\nTable 12\nOpenCL 1.0 Channel Data Type Definition\n\nCL_SNORM_INT8\n0x10D0\nCL_SNORM_INT16\n0x10D1\nCL_UNORM_INT8\n0x10D2\nCL_UNORM_INT16\n0x10D3\nCL_UNORM_SHORT_565\n0x10D4\nCL_UNORM_SHORT_555\n0x10D5\nCL_UNORM_INT_101010\n0x10D6\nCL_SIGNED_INT8\n0x10D7\nCL_SIGNED_INT16\n0x10D8\nCL_SIGNED_INT32\n0x10D9\nCL_UNSIGNED_INT8\n0x10DA\nCL_UNSIGNED_INT16\n0x10DB\nCL_UNSIGNED_INT32\n0x10DC\nCL_HALF_FLOAT\n0x10DD\nCL_FLOAT\n0x10DE\nTable 13\nOpenCL 1.0 Channel Order Definition\n\nCL_R\n0x10B0\nCL_A\n0x10B1\nCL_RG\n0x10B2\nCL_RA\n0x10B3\nCL_RGB\n0x10B4\nCL_RGBA\n0x10B5\nCL_BGRA\n0x10B6\nCL_ARGB\n0x10B7\nCL_INTENSITY\n0x10B8\nCL_LUMINANCE\n0x10B9\n5.4.\nVariables\n\nIn PTX, a variable declaration describes both the variable’s type and its state space. In addition\nto fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays.\n5.4.1.\nVariable Declarations\n\nAll storage for data is specified with variable declarations. Every variable must reside in one of\nthe state spaces enumerated in the previous section.\nA variable declaration names the space in which the variable resides, its type and size, its name,\nan optional array size, an optional initializer, and an optional fixed address for the variable.\nPredicate variables may only be declared in the register state space.\nExamples\n.global .u32 loc;\n.reg    .s32 i;\n.const  .f32 bias[] = {-1.0, 1.0};\n.global .u8  bg[4] = {0, 0, 0, 0};\n.reg    .v4 .f32 accel;\n.reg    .pred p, q, r;\n5.4.2.\nVectors\n\nLimited-length vector types are supported. Vectors of length 2 and 4 of any non-predicate\nfundamental type can be declared by prefixing the type with\n.v2\nor\n.v4\n. Vectors must be\nbased on a fundamental type, and they may reside in the register space. Vectors cannot exceed\n128-bits in length; for example,\n.v4\n.f64\nis not allowed. Three-element vectors may be\nhandled by using a\n.v4\nvector, where the fourth element provides padding. This is a common case\nfor three-dimensional grids, textures, etc.\nExamples\n.global .v4 .f32 V;   // a length-4 vector of floats\n.shared .v2 .u16 uv;  // a length-2 vector of unsigned ints\n.global .v4 .b8  v;   // a length-4 vector of bytes\nBy default, vector variables are aligned to a multiple of their overall size (vector length times\nbase-type size), to enable vector load and store instructions which require addresses aligned to a\nmultiple of the access size.\n5.4.3.\nArray Declarations\n\nArray declarations are provided to allow the programmer to reserve space. To declare an array, the\nvariable name is followed with dimensional declarations similar to fixed-size array declarations\nin C. The size of each dimension is a constant expression.\nExamples\n.local  .u16 kernel[19][19];\n.shared .u8  mailbox[128];\nThe size of the array specifies how many elements should be reserved. For the declaration of array\nkernel\nabove, 19*19 = 361 halfwords are reserved, for a total of 722 bytes.\nWhen declared with an initializer, the first dimension of the array may be omitted. The size of the\nfirst array dimension is determined by the number of elements in the array initializer.\nExamples\n.global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n.global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} };\nArray\nindex\nhas eight elements, and array\noffset\nis a 4x2 array.\n5.4.4.\nInitializers\n\nDeclared variables may specify an initial value using a syntax similar to C/C++, where the variable\nname is followed by an equals sign and the initial value or values for the variable. A scalar takes\na single value, while vectors and arrays take nested lists of values inside of curly braces (the\nnesting matches the dimensionality of the declaration).\nAs in C, array initializers may be incomplete, i.e., the number of initializer elements may be less\nthan the extent of the corresponding array dimension, with remaining array locations initialized to\nthe default value for the specified array type.\nExamples\n.const  .f32 vals[8] = { 0.33, 0.25, 0.125 };\n.global .s32 x[3][2] = { {1,2}, {3} };\nis equivalent to\n.const  .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 };\n.global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} };\nCurrently, variable initialization is supported only for constant and global state spaces. Variables\nin constant and global state spaces with no explicit initializer are initialized to zero by\ndefault. Initializers are not allowed in external variable declarations.\nVariable names appearing in initializers represent the address of the variable; this can be used to\nstatically initialize a pointer to a variable. Initializers may also contain\nvar+offset\nexpressions, where\noffset\nis a byte offset added to the address of\nvar\n. Only variables in\n.global\nor\n.const\nstate spaces may be used in initializers. By default, the resulting\naddress is the offset in the variable’s state space (as is the case when taking the address of a\nvariable with a\nmov\ninstruction). An operator,\ngeneric()\n, is provided to create a generic\naddress for variables used in initializers.\nStarting PTX ISA version 7.1, an operator\nmask()\nis provided, where\nmask\nis an integer\nimmediate. The only allowed expressions in the\nmask()\noperator are integer constant expression\nand symbol expression representing address of variable. The\nmask()\noperator extracts\nn\nconsecutive bits from the expression used in initializers and inserts these bits at the lowest\nposition of the initialized variable. The number\nn\nand the starting position of the bits to be\nextracted is specified by the integer immediate\nmask\n. PTX ISA version 7.1 only supports\nextracting a single byte starting at byte boundary from the address of the variable. PTX ISA version\n7.3 supports Integer constant expression as an operand in the\nmask()\noperator.\nSupported values for\nmask\nare: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000,\n0xFF000000000000, 0xFF00000000000000.\nExamples\n.const  .u32 foo = 42;\n.global .u32 bar[] = { 2, 3, 5 };\n.global .u32 p1 = foo;          // offset of foo in .const space\n.global .u32 p2 = generic(foo); // generic address of foo\n\n// array of generic-address pointers to elements of bar\n.global .u32 parr[] = { generic(bar), generic(bar)+4,\ngeneric(bar)+8 };\n\n// examples using mask() operator are pruned for brevity\n.global .u8 addr[] = {0xff(foo), 0xff00(foo), 0xff0000(foo), ...};\n\n.global .u8 addr2[] = {0xff(foo+4), 0xff00(foo+4), 0xff0000(foo+4),...}\n\n.global .u8 addr3[] = {0xff(generic(foo)), 0xff00(generic(foo)),...}\n\n.global .u8 addr4[] = {0xff(generic(foo)+4), 0xff00(generic(foo)+4),...}\n\n// mask() operator with integer const expression\n.global .u8 addr5[] = { 0xFF(1000 + 546), 0xFF00(131187), ...};\nNote\nPTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit\ngeneric()\noperator for each global variable used in an initializer. PTX 3.1 code should\neither include explicit\ngeneric()\noperators in initializers, use\ncvta.global\nto form\ngeneric addresses at runtime, or load from the non-generic address using\nld.global\n.\nDevice function names appearing in initializers represent the address of the first instruction in\nthe function; this can be used to initialize a table of function pointers to be used with indirect\ncalls. Beginning in PTX ISA version 3.1, kernel function names can be used as initializers e.g. to\ninitialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch\nkernels from GPU. See the\nCUDA Dynamic Parallelism Programming Guide\nfor details.\nLabels cannot be used in initializers.\nVariables that hold addresses of variables or functions should be of type\n.u8\nor\n.u32\nor\n.u64\n.\nType\n.u8\nis allowed only if the\nmask()\noperator is used.\nInitializers are allowed for all types except\n.f16\n,\n.f16x2\nand\n.pred\n.\nExamples\n.global .s32 n = 10;\n.global .f32 blur_kernel[][3]\n               = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}};\n\n.global .u32 foo[] = { 2, 3, 5, 7, 9, 11 };\n.global .u64 ptr = generic(foo);   // generic address of foo[0]\n.global .u64 ptr = generic(foo)+8; // generic address of foo[2]\n5.4.5.\nAlignment\n\nByte alignment of storage for all addressable variables can be specified in the variable\ndeclaration. Alignment is specified using an optional\n.align\nbyte-count\nspecifier immediately\nfollowing the state-space specifier. The variable will be aligned to an address which is an integer\nmultiple of byte-count. The alignment value byte-count must be a power of two. For arrays, alignment\nspecifies the address alignment for the starting address of the entire array, not for individual\nelements.\nThe default alignment for scalar and array variables is to a multiple of the base-type size. The\ndefault alignment for vector variables is to a multiple of the overall vector size.\nExamples\n// allocate array at 4-byte aligned address.  Elements are bytes.\n.const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0};\nNote that all PTX instructions that access memory require that the address be aligned to a multiple\nof the access size. The access size of a memory instruction is the total number of bytes accessed in\nmemory. For example, the access size of\nld.v4.b32\nis 16 bytes, while the access size of\natom.f16x2\nis 4 bytes.\n5.4.6.\nParameterized Variable Names\n\nSince PTX supports virtual registers, it is quite common for a compiler frontend to generate a large\nnumber of register names. Rather than require explicit declaration of every name, PTX supports a\nsyntax for creating a set of variables having a common prefix string appended with integer suffixes.\nFor example, suppose a program uses a large number, say one hundred, of\n.b32\nvariables, named\n%r0\n,\n%r1\n, …,\n%r99\n. These 100 register variables can be declared as follows:\n.reg .b32 %r<100>;    // declare %r0, %r1, ..., %r99\nThis shorthand syntax may be used with any of the fundamental types and with any state space, and\nmay be preceded by an alignment specifier. Array variables cannot be declared this way, nor are\ninitializers permitted.\n5.4.7.\nVariable Attributes\n\nVariables may be declared with an optional\n.attribute\ndirective which allows specifying special\nattributes of variables. Keyword\n.attribute\nis followed by attribute specification inside\nparenthesis. Multiple attributes are separated by comma.\nVariable and Function Attribute Directive: .attribute\ndescribes the\n.attribute\ndirective.\n5.4.8.\nVariable and Function Attribute Directive:\n.attribute\n\n.attribute\nVariable and function attributes\nDescription\nUsed to specify special attributes of a variable or a function.\nThe following attributes are supported.\n.managed\n.managed\nattribute specifies that variable will be allocated at a location in unified virtual\nmemory environment where host and other devices in the system can reference the variable\ndirectly. This attribute can only be used with variables in .global state space. See the\nCUDA\nUVM-Lite Programming Guide\nfor details.\n.unified\n.unified\nattribute specifies that function has the same memory address on the host and on\nother devices in the system. Integer constants\nuuid1\nand\nuuid2\nrespectively specify upper\nand lower 64 bits of the unique identifier associated with the function or the variable. This\nattribute can only be used on device functions or on variables in the\n.global\nstate\nspace. Variables with\n.unified\nattribute are read-only and must be loaded by specifying\n.unified\nqualifier on the address operand of\nld\ninstruction, otherwise the behavior is\nundefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.0.\nSupport for function attributes introduced in PTX ISA version 8.0.\nTarget ISA Notes\n.managed\nattribute requires\nsm_30\nor higher.\n.unified\nattribute requires\nsm_90\nor higher.\nExamples\n.global .attribute(.managed) .s32 g;\n.global .attribute(.managed) .u64 x;\n\n.global .attribute(.unified(19,95)) .f32 f;\n\n.func .attribute(.unified(0xAB, 0xCD)) bar() { ... }\n5.5.\nTensors\n\nA tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following\nproperties:\nDimensionality\nDimension sizes across each dimension\nIndividual element types\nTensor stride across each dimension\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include:\nCopying data between global and shared memories\nReducing the destination tensor data with the source.\nThe Tensor data can be operated on by various\nwmma.mma\n,\nmma\nand\nwgmma.mma_async\ninstructions.\nPTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure\nand treat the data in the shared memory as a linear data.\n5.5.1.\nTensor Dimension, size and format\n\nTensors can have dimensions: 1D, 2D, 3D, 4D or 5D.\nEach dimension has a size which represents the number of elements along the dimension. The elements\ncan have one the following types:\nBit-sized type:\n.b32\n,\n.b64\nSub-byte types:\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\n,\n.b6p2x16\nInteger:\n.u8\n,\n.u16\n,\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\nFloating point and alternate floating point:\n.f16\n,\n.bf16\n,\n.tf32\n,\n.f32\n,\n.f64\n(rounded to nearest even).\nTensor can have padding at the end in each of the dimensions to provide alignment for the data in\nthe subsequent dimensions. Tensor stride can be used to specify the amount of padding in each\ndimension.\n5.5.1.1.\nSub-byte Types\n\n5.5.1.1.1.\nPadding and alignment of the sub-byte types\n\nThe sub-byte types are expected to packed contiguously in the global memory and\nthe Tensor copy instruction will expand them by appending empty spaces as shown below:\nType\n.b4x16\n:\nWith this type, there is no padding involved and the packed sixteen\n.b4\nelements\nin a 64-bits container is copied as is between the shared memory and the global memory.\nType\n.b4x16_p64\n:\nWith this type, sixteen contiguous 4-bits of data is copied from global memory to the\nshared memory with the append of 64-bits of padding as shown in\nFigure 5\nFigure 5\nLayout for .b4x16_p64\n\nThe padded region that gets added is un-initialized.\nType\n.b6x16_p32\n:\nWith this type, sixteen 6-bits of data is copied from global memory to the shared memory\nwith an append of 32-bits of padding as shown in\nFigure 6\nFigure 6\nLayout for .b6x16_p32\n\nThe padded region that gets added is un-initialized.\nType\n.b6p2x16\n:\nWith this type, sixteen elements, each containing 6-bits of data at the LSB and 2-bits\nof padding at the MSB, are copied from shared memory into the global memory by discarding\nthe 2-bits of padding data and packing the 6-bits data contiguously as shown in\nFigure 7\nFigure 7\nLayout for .b6p2x16\n\nIn case of\n.b6x16_p32\nand\n.b4x16_p64\n, the padded region that gets added is\nun-initialized.\nThe types\n.b6x16_p32\nand\n.b6p2x16\nshare the same encoding value in the\ndescriptor (value 15) as the two types are applicable for different types of\ntensor copy operations:\nType\nValid Tensor Copy Direction\n.b6x16_p32\n.shared::cluster.global\n,\n.shared::cta.global\n.b6p2x16\n.global.shared::cta\n5.5.2.\nTensor Access Modes\n\nTensor data can be accessed in two modes:\nTiled mode:\nIn tiled mode, the source multi-dimensional tensor layout is preserved at the destination.\nIm2col mode:\nIn im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns\nat the destination. Refer\nhere\nfor more details.\n5.5.3.\nTiled Mode\n\nThis section talks about how Tensor and Tensor access work in tiled mode.\n5.5.3.1.\nBounding Box\n\nA tensor can be accessed in chunks known as\nBounding Box\n. The Bounding Box has the same\ndimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple\nof 16 bytes. The address of the bounding Box must also be aligned to 16 bytes.\nBounding Box has the following access properties:\nBounding Box dimension sizes\nOut of boundary access mode\nTraversal strides\nThe tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the\nbounding box. Starting offset of the bounding box along with the rest of the bounding box\ninformation together are used to determine the elements which are to be accessed.\n5.5.3.2.\nTraversal-Stride\n\nWhile the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies\nthe exact number of elements to be skipped. If no jump over is required, default value of 1 must be\nspecified.\nThe traversal stride in dimension 0 can be used for the\nInterleave layout\n.\nFor non-interleaved layout, the traversal stride in\ndimension 0 must always be 1.\nFigure 8\nillustrates tensor, tensor size, tensor stride,\nBounding Box size and traversal stride.\nFigure 8\nTiled mode bounding box, tensor size and traversal stride\n\n5.5.3.3.\nOut of Boundary Access\n\nPTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor\nboundary in any dimension. There are 2 modes:\nZero fill mode:\nElements in the Bounding Box which fall outside of the tensor boundary are set to 0.\nOOB-NaN\nfill mode:\nElements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN\ncalled\nOOB-NaN\n.\nFigure 9\nshows an example of the out of boundary access.\nFigure 9\nOut of boundary access\n\n5.5.3.4.\n.tile::scatter4\nand\n.tile::gather4\nmodes\n\nThese modes are similar to the tiled mode with restriction that these modes work only on 2D tensor data.\nTile::scatter4\nand\nTile::gather4\nmodes are used to access multiple non-contiguous rows of tensor data.\nIn\nTile::scatter4\nmode single 2D source tensor is divided into four rows in the 2D destination tensor.\nIn\nTile::gather4\nmode four rows in the source 2D tensor are combined to form single 2D destination tensor.\nThese modes work on four rows and hence the instruction will take:\nfour tensor coordinates across the dimension 0\none tensor coordinate across the dimension 1\nThe interleave layout is not supported for\n.tile::scatter4\nand\n.tile::gather4\nmodes.\nAll other constraints and rules of the tile mode apply to these modes as well.\n5.5.3.4.1.\nBounding Box\n\nFor\nTile::scatter4\nand\nTile::gather4\nmodes, four request coordinates will form four Bounding\nBoxes in the tensor space.\nFigure 10\nshows an example of the same with start\ncoordinates (1, 2), (1, 5), (1, 0) and (1, 9).\nThe size of the bounding box in the dimension 0 represents the length of the rows.\nThe size of the bounding box in the dimension 1 must be one.\nFigure 10\ntiled::scatter4/tiled::gather4 mode bounding box example\n\n5.5.4.\nim2col\nmode\n\nIm2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data\nis treated as a batch of images with the following properties:\nN : number of images in the batch\nD, H, W : size of a 3D image (depth, height and width)\nC: channels per image element\nThe above properties are associated with 3D, 4D and 5D tensors as follows:\nDimension\nN/D/H/W/C applicability\n3D\nNWC\n4D\nNHWC\n5D\nNDHWC\n5.5.4.1.\nBounding Box\n\nIn im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are\nspecified by Pixels-per-Column and Channels-per-Pixel parameters as described below.\nThe dimensionality of the Bounding Box is two less than the tensor dimensionality.\nThe following properties describe how to access of the elements in im2col mode:\nBounding-Box Lower-Corner\nBounding-Box Upper-Corner\nPixels-per-Column\nChannels-per-Pixel\nBounding-box Lower-Corner\nand\nBounding-box Upper-Corner\nspecify the two opposite corners of the\nBounding Box in the DHW space.\nBounding-box Lower-Corner\nspecifies the corner with the smallest\ncoordinate and\nBounding-box Upper-Corner\nspecifies the corner with the largest coordinate.\nBounding-box Upper-\nand\nLower-Corners\nare 16-bit signed values whose limits varies across the\ndimensions and are as shown below:\n3D\n4D\n5D\nUpper- / Lower- Corner sizes\n[-2\n15\n, 2\n15\n-1]\n[-2\n7\n, 2\n7\n-1]\n[-2\n4\n, 2\n4\n-1]\nFigure 11\nand\nFigure 12\nshow the Upper-Corners and Lower-Corners.\nFigure 11\nim2col mode bounding box example 1\n\nFigure 12\nim2col mode bounding box example 2\n\nThe\nBounding-box Upper-\nand\nLower- Corners\nspecify only the boundaries and not the number of\nelements to be accessed.\nPixels-per-Column\nspecifies the number of elements to be accessed in the\nNDHW space.\nChannels-per-Pixel\nspecifies the number of elements to access across the C dimension.\nThe tensor coordinates, specified in the PTX tensor instructions, behaves differently in different\ndimensions:\nAcross N and C dimensions: specify the starting offsets along the dimension, similar to the tiled\nmode.\nAcross DHW dimensions: specify the location of the convolution filter base in the tensor\nspace. The filter corner location must be within the bounding box.\nThe im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter\nbase coordinates to determine the starting location in the tensor space from where the elements are\naccessed.\nThe size of the im2col offsets varies across the dimensions and their valid ranges are as shown\nbelow:\n3D\n4D\n5D\nim2col offsets range\n[0, 2\n16\n-1]\n[0, 2\n8\n-1]\n[0, 2\n5\n-1]\nFollowing are some examples of the im2col mode accesses:\nExample 1 (\nFigure 13\n):\nTensor\nSize\n[\n0\n]\n=\n64\nTensor\nSize\n[\n1\n]\n=\n9\nTensor\nSize\n[\n2\n]\n=\n14\nTensor\nSize\n[\n3\n]\n=\n64\nPixels\n-\nper\n-\nColumn\n=\n64\nchannels\n-\nper\n-\npixel\n=\n8\nBounding\n-\nBox\nLower\n-\nCorner\nW\n=\n-1\nBounding\n-\nBox\nLower\n-\nCorner\nH\n=\n-1\nBounding\n-\nBox\nUpper\n-\nCorner\nW\n=\n-1\nBounding\n-\nBox\nUpper\n-\nCorner\nH\n=\n-1.\ntensor\ncoordinates\n=\n(\n7\n,\n7\n,\n4\n,\n0\n)\nim2col\noffsets\n:\n(\n0\n,\n0\n)\nFigure 13\nim2col mode example 1\n\nExample 2 (\nFigure 14\n):\nTensor\nSize\n[\n0\n]\n=\n64\nTensor\nSize\n[\n1\n]\n=\n9\nTensor\nSize\n[\n2\n]\n=\n14\nTensor\nSize\n[\n3\n]\n=\n64\nPixels\n-\nper\n-\nColumn\n=\n64\nchannels\n-\nper\n-\npixel\n=\n8\nBounding\n-\nBox\nLower\n-\nCorner\nW\n=\n0\nBounding\n-\nBox\nLower\n-\nCorner\nH\n=\n0\nBounding\n-\nBox\nUpper\n-\nCorner\nW\n=\n-2\nBounding\n-\nBox\nUpper\n-\nCorner\nH\n=\n-2\ntensor\ncoordinates\n=\n(\n7\n,\n7\n,\n4\n,\n0\n)\nim2col\noffsets\n:\n(\n2\n,\n2\n)\nFigure 14\nim2col mode example 2\n\n5.5.4.2.\nTraversal Stride\n\nThe traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being\naccessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being\naccessed, in im2col mode.\nThe number of elements traversed along the D, H and W dimensions is strided by the traversal stride\nfor that dimension.\nThe following example with\nFigure 15\nillustrates accesse with traversal-strides:\nTensor Size[0] = 64\nTensor Size[1] = 8\nTensor Size[2] = 14\nTensor Size[3] = 64\nTraversal Stride = 2\nPixels-per-Column = 32\nchannels-per-pixel = 16\nBounding-Box Lower-Corner W = -1\nBounding-Box Lower-Corner H = -1\nBounding-Box Upper-Corner W = -1\nBounding-Box Upper-Corner H = -1.\nTensor coordinates in the instruction = (7, 7, 5, 0)\nIm2col offsets in the instruction : (1, 1)\nFigure 15\nim2col mode traversal stride example\n\n5.5.4.3.\nOut of Boundary Access\n\nIn im2col mode, when the number of requested pixels in NDHW space specified by\nPixels-per-Column\nexceeds the number of available pixels in the image batch then out-of-bounds access is performed.\nSimilar to tiled mode, zero fill or\nOOB-NaN\nfill can be performed based on the Fill-Mode\nspecified.\n5.5.5.\nim2col::w\nand\nim2col::w::128\nmodes\n\nThese modes are similar to the im2col mode with the restriction that elements are accessed across\nthe\nW\ndimension only while keeping the\nH\nand\nD\ndimension constant.\nAll the constraints and rules of the im2col mode apply to these modes as well.\nThe number of elements accessed in the\nim2col::w::128\nmode is fixed and is equal to 128.\nThe number of elements accessed in the\nim2col::w\nmode depends on the field Pixels-per-Column\nfield in the TensorMap.\n5.5.5.1.\nBounding Box\n\nIn these modes, the size of the bounding box in\nD\nand\nH\ndimensions are 1.\nThe\nD\nand\nH\ndimensions in the tensor coordinates argument in the PTX instruction specify\nthe position of the bounding box in the tensor space.\nThe Bounding-Box\nLower-Corner-W\nand Bounding-Box\nUpper-Corner-W\nspecify the two opposite\ncorners of the Bounding Box in the\nW\ndimension.\nThe\nW\ndimension in the tensor coordinates argument in the PTX instruction specify the location\nof the first element that is to be accessed in the bounding box.\nNumber of pixels loaded in\nim2col::w\nmode is as specified by Pixels-per-Column in the TensorMap.\nNumber of pixels loaded in\nim2col::w::128\nmode is always 128. So, Pixels-per-Column is ignored\nin\nim2col::w::128\nmode.\nFigure 16\nshows an example of the\nim2col::w\nand\nim2col::w:128\nmodes.\nFigure 16\nim2col::w and im2col::w::128 modes example\n\nThe first element can lie outside of the Bounding Box in the W-dimension only and only on the left\nside of the Bounding Box.\nFigure 17\nshows of an example of this.\nFigure 17\nim2col::w and im2col::w::128 modes first element outside Bounding Box example\n\n5.5.5.2.\nTraversal Stride\n\nThis is similar to im2col mode with the exception of that the number of elements traversed\nalong only the\nW\ndimension is strided by the traversal stride as specified in the TensorMap.\n5.5.5.3.\nwHalo\n\nIn\nim2col::w\nmode, the\nwHalo\nargument in the PTX instruction specifies how many filter\nhalo elements must be loaded at the end of the image.\nIn\nim2col::w::128\nmode, the halo elements are loaded after every 32 elements in the bounding\nbox along the\nW\ndimension. The\nwHalo\nargument in the PTX instruction specifies how many\nhalo elements must be loaded after every 32 elements.\nFollowing is an example of\n.im2col::w\nmode access:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 7\nTensor Size [3] = 64\nPixels-per-column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = 0\nBounding Box Upper Corner W = 0\n\nTensor Coordinates in the instruction = (7, 2, 3, 0)\nwHalo in the instruction = 2 (as 3x3 convolution filter is used)\nA tensor copy operation with the above parameters loads 128 pixels and the two halo pixels as shown in\nFigure 18\n.\nFigure 18\ntensor copy operation with im2col::w mode example\n\nThe halo pixels are always loaded in the shared memory next to the main row pixels as shown in\nFigure 18\n.\nFollowing is an example of\n.im2col::w::128\nmode access:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 7\nTensor Size [3] = 64\nChannels-per-pixel = 64\nBounding Box Lower Corner W = 0\nBounding Box Upper Corner W = 0\n\nTensor Coordinates in the instruction = (7, 2, 3, 0)\nwHalo in the instruction = 2 (as 3x3 convolution filter is used)\nA tensor copy operation with the above parameters loads 128 elements such that after every 32 elements,\nwHalo number of elements are loaded as shown in\nFigure 19\n.\nFigure 19\ntensor copy operation with im2col::w::128 mode example\n\n5.5.5.4.\nwOffset\n\nIn the convolution calculations, the same elements along the\nW\ndimension are reused for different\nlocations within the convolution filter footprint. Based on the number of times a pixel is used, the\npixels may be loaded into different shared memory buffers. Each buffer can be loaded by a separate\ntensor copy operation.\nThe\nwOffset\nargument in the tensor copy and prefetch instruction adjusts the source pixel location\nfor each buffer. The exact position of the buffer is adjusted along the\nW\ndimension using the\nfollowing formula:\nBounding Box Lower Corner W += wOffset\nBounding Box Upper Corner W += wOffset\nW += wOffset\nFollowing are examples of tensor copy to multiple buffers with various\nwHalo\nand\nwOffset\nvalues:\nExample 1:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 67\nTensor Size [3] = 64\nPixels-per-Column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = -1\nBounding Box Upper Corner W = 0\nTraversal Stride = 2\n\nTensor Coordinates in the instruction = (7, 2, -1, 0)\n\nShared memory buffer 1:\n   wHalo = 1\n   wOffset = 0\n\nShared memory buffer 2:\n   wHalo = 0\n   wOffset = 1\nFigure 20\ntensor copy operation to buffer 1 of Example 1\n\nFigure 21\ntensor copy operation to buffer 2 of Example 1\n\nExample 2:\nTensor Size [0] = 128\nTensor Size [1] = 7\nTensor Size [2] = 7\nTensor Size [3] = 64\nPixels-per-Column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = -1\nBounding Box Upper Corner W = -1\nTraversal Stride = 3\n\nTensor Coordinates in the instruction = (7, 2, -1, 0)\n\nShared memory buffer 1:\n   wHalo = 0\n   wOffset = 0\n\nShared memory buffer 2:\n   wHalo = 0\n   wOffset = 1\n\nShared memory buffer 3:\n   wHalo = 0\n   wOffset = 2\nFigure 22\ntensor copy operation to buffer 1 of Example 2\n\nFigure 23\ntensor copy operation to buffer 2 of Example 2\n\nFigure 24\ntensor copy operation to buffer 3 of Example 2\n\n5.5.6.\nInterleave layout\n\nTensor can be interleaved and the following interleave layouts are supported:\nNo interleave (NDHWC)\n8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel.\n16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel.\nThe\nC\ninformation is organized in slices where sequential C elements are grouped in 16 byte or 32\nbyte quantities.\nIf the total number of channels is not a multiple of the number of channels per slice, then the last\nslice must be padded with zeros to make it complete 16B or 32B slice.\nInterleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D.\nThe interleave layout is not supported for\n.im2col::w\nand\n.im2col::w::128\nmodes.\n5.5.7.\nSwizzling Modes\n\nThe layout of the data in the shared memory can be different to that of global memory, for access\nperformance reasons. The following describes various swizzling modes:\nNo swizzle mode:\nThere is no swizzling in this mode and the destination data layout is exactly similar to the\nsource data layout.\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n… Pattern repeats …\n32 byte swizzle mode:\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n256 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n… Pattern repeats …\nAn example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension,\nwith the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in\nFigure 25\n.\nFigure 25\n32-byte swizzle mode example\n\nFigure 26\nshows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1.\nFigure 26\n32-byte swizzle mode fragments\n\nFigure 27\nshows the destination data layout with 32 byte swizzling.\nFigure 27\n32-byte swizzle mode destination data layout\n\n64 byte swizzle mode:\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n512 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n2\n3\n0\n1\n6\n7\n4\n5\n3\n2\n1\n0\n7\n6\n5\n4\n… Pattern repeats …\nAn example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 32 channels, is shown in\nFigure 28\n.\nFigure 28\n64-byte swizzle mode example\n\nEach colored cell represents 8 channels.\nFigure 29\nshows the source data layout.\nFigure 29\n64-byte swizzle mode source data layout\n\nFigure 30\nshows the destination data layout with 64 byte swizzling.\nFigure 30\n64-byte swizzle mode destination data layout\n\n96 byte swizzle mode:\nThe following table where each element (numbered cell) is 16 byte shows the swizzling pattern at the destination\ndata layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n… Pattern repeats …\nAn example of the data layout in global memory and its swizzled data layout in shared memory where each element\n(colored cell) is 16 bytes and the starting address is 256 bytes aligned is shown in\nFigure 31\n.\nFigure 31\n96-byte swizzle mode example\n\n128 byte swizzle mode:\nThe 128-byte swizzling mode supports the following sub-modes:\n16-byte atomicity sub-mode:\nIn this sub-mode, the 16-byte of data is kept intact while swizzling.\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n1024 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n2\n3\n0\n1\n6\n7\n4\n5\n3\n2\n1\n0\n7\n6\n5\n4\n4\n5\n6\n7\n0\n1\n2\n3\n5\n4\n7\n6\n1\n0\n3\n2\n6\n7\n4\n5\n2\n3\n0\n1\n7\n6\n5\n4\n3\n2\n1\n0\n… Pattern repeats …\nAn example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 64 channels, is shown in\nFigure 32\n.\nFigure 32\n128-byte swizzle mode example\n\nEach colored cell represents 8 channels.\nFigure 33\nshows the source data layout.\nFigure 33\n128-byte swizzle mode source data layout\n\nFigure 34\nshows the destination data layout with 128 byte swizzling.\nFigure 34\n128-byte swizzle mode destination data layout\n\n32-byte atomicity sub-mode:\nIn this sub-mode, the 32-byte of data is kept intact while swizzling.\nThe following table where each element (numbered cell) is 16 byte shows the\nswizzling pattern at the destination data layout:\n0   1\n2   3\n4   5\n6   7\n2   3\n0   1\n6   7\n4   5\n4   5\n6   7\n0   1\n2   3\n6   7\n4   5\n2   3\n0   1\n… Pattern repeats …\nThis sub-mode requires 32 byte alignment at shared memory.\nAn example of the data layout in global memory and its swizzled data layout in shared memory\nwhere each element (colored cell) is 16 bytes is shown in\nFigure 35\nFigure 35\n128-byte swizzle mode example with 32-byte atomicity\n\n32-byte atomicity with 8-byte flip sub-mode:\nThe swizzling pattern for this sub-mode is similar to the 32-byte atomicity sub-mode except that\nthere is a flip of adjacent 8-bytes within the 16-byte data at every alternate shared memory line.\nAn example of the data layout in global memory and its swizzled data layout in shared memory where\neach element (colored cell) is 16 bytes (two 8-byte sub-elements for each 16-byte colored cell are\nshown to show the flip) is shown in\nFigure 36\nFigure 36\n128-byte swizzle mode example with 32-byte atomicity with 8-byte flip\n\n64-byte atomicity sub-mode:\nIn this sub-mode, the 64-byte of data is kept intact while swizzling.\nThe following table where each element (numbered cell) is 16 byte shows the swizzling\npattern at the destination data layout:\n0   1   2   3\n4   5   6   7\n4   5   6   7\n0   1   2   3\n… Pattern repeats …\nThis sub-mode requires 64-byte alignment at shared memory.\nAn example of the data layout in global memory and its swizzled data layout\nin shared memory where each element (colored cell) is 16 bytes is shown\nin\nFigure 37\nFigure 37\n128-byte swizzle mode example with 64-byte atomicity\n\nTable 14\nlists the valid combination of swizzle-atomicity with the swizzling-mode.\nTable 14\nValid combination of swizzle-atomicity with swizzling-mode\n\nSwizzling Mode\nSwizzle-Atomicity\nNo Swizzling\n–\n32B Swizzling Mode\n16B\n64B Swizzling Mode\n16B\n96B Swizzling Mode\n16B\n128B Swizzling Mode\n16B\n32B\n32B + 8B-flip\n64B\nThe value of swizzle base offset is 0 when the\ndstMem\nshared memory address is located\nat the following boundary:\nSwizzling Mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n96-Byte swizzle\n256-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the swizzle base offset is a non-zero value, computed using following formula:\nSwizzling Mode\nFormula\n128-Byte swizzle\nbase offset = (dstMem / 128) % 8\n96-Byte swizzle\nbase offset = (dstMem / 128) % 2\n64-Byte swizzle\nbase offset = (dstMem / 128) % 4\n32-Byte swizzle\nbase offset = (dstMem / 128) % 2\n5.5.8.\nTensor-map\n\nThe tensor-map is a 128-byte opaque object either in\n.const\nspace or\n.param\n(kernel function\nparameter) space or\n.global\nspace which describes the tensor properties and the access properties\nof the tensor data described in previous sections.\nTensor-Map can be created using CUDA APIs. Refer to\nCUDA programming guide\nfor more details.\n6.\nInstruction Operands\n\n6.1.\nOperand Type Information\n\nAll operands in instructions have a known type from their declarations. Each operand type must be\ncompatible with the type determined by the instruction template and instruction type. There is no\nautomatic conversion between types.\nThe bit-size type is compatible with every type having the same size. Integer types of a common size\nare compatible with each other. Operands having type different from but compatible with the\ninstruction type are silently cast to the instruction type.\n6.2.\nSource Operands\n\nThe source operands are denoted in the instruction descriptions by the names\na\n,\nb\n, and\nc\n. PTX describes a load-store machine, so operands for ALU instructions must all be in variables\ndeclared in the\n.reg\nregister state space. For most operations, the sizes of the operands must\nbe consistent.\nThe\ncvt\n(convert) instruction takes a variety of operand types and sizes, as its job is to\nconvert from nearly any data type to any other data type (and size).\nThe\nld\n,\nst\n,\nmov\n, and\ncvt\ninstructions copy data from one location to\nanother. Instructions\nld\nand\nst\nmove data from/to addressable state spaces to/from\nregisters. The\nmov\ninstruction copies data between registers.\nMost instructions have an optional predicate guard that controls conditional execution, and a few\ninstructions have additional predicate source operands. Predicate operands are denoted by the names\np\n,\nq\n,\nr\n,\ns\n.\n6.3.\nDestination Operands\n\nPTX instructions that produce a single result store the result in the field denoted by\nd\n(for\ndestination) in the instruction descriptions. The result operand is a scalar or vector variable in\nthe register state space.\n6.4.\nUsing Addresses, Arrays, and Vectors\n\nUsing scalar variables as operands is straightforward. The interesting capabilities begin with\naddresses, arrays, and vectors.\n6.4.1.\nAddresses as Operands\n\nAll the memory instructions take an address operand that specifies the memory location being\naccessed. This addressable operand is one of:\n[var]\nthe name of an addressable variable\nvar\n.\n[reg]\nan integer or bit-size type register\nreg\ncontaining a byte address.\n[reg+immOff]\na sum of register\nreg\ncontaining a byte address plus a constant integer byte offset (signed, 32-bit).\n[var+immOff]\na sum of address of addressable variable\nvar\ncontaining a byte address plus a constant integer\nbyte offset (signed, 32-bit).\n[immAddr]\nan immediate absolute byte address (unsigned, 32-bit).\nvar[immOff]\nan array element as described in\nArrays as Operands\n.\nThe register containing an address may be declared as a bit-size type or integer type.\nThe access size of a memory instruction is the total number of bytes accessed in memory. For\nexample, the access size of\nld.v4.b32\nis 16 bytes, while the access size of\natom.f16x2\nis 4\nbytes.\nThe address must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined. For example, among other things, the access\nmay proceed by silently masking off low-order address bits to achieve proper rounding, or the\ninstruction may fault.\nThe address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. Addresses are\nzero-extended to the specified width as needed, and truncated if the register width exceeds the\nstate space address width for the target architecture.\nAddress arithmetic is performed using integer arithmetic and logical instructions. Examples include\npointer arithmetic and pointer comparisons. All addresses and address computations are byte-based;\nthere is no support for C-style pointer arithmetic.\nThe\nmov\ninstruction can be used to move the address of a variable into a pointer. The address is\nan offset in the state space in which the variable is declared. Load and store operations move data\nbetween registers and locations in addressable state spaces. The syntax is similar to that used in\nmany assembly languages, where scalar variables are simply named and addresses are de-referenced by\nenclosing the address expression in square brackets. Address expressions include variable names,\naddress registers, address register plus byte offset, and immediate address expressions which\nevaluate at compile-time to a constant address.\nHere are a few examples:\n.shared .u16 x;\n.reg    .u16 r0;\n.global .v4 .f32 V;\n.reg    .v4 .f32 W;\n.const  .s32 tbl[256];\n.reg    .b32 p;\n.reg    .s32 q;\n\nld.shared.u16   r0,[x];\nld.global.v4.f32 W, [V];\nld.const.s32    q, [tbl+12];\nmov.u32         p, tbl;\n6.4.1.1.\nGeneric Addressing\n\nIf a memory instruction does not specify a state space, the operation is performed using generic\naddressing. The state spaces\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.local\nand\n.shared\nare modeled as\nwindows within the generic address space. Each window is defined by a window base and a window size\nthat is equal to the size of the corresponding state space. A generic address maps to\nglobal\nmemory unless it falls within the window for\nconst\n,\nlocal\n, or\nshared\nmemory. The\nKernel Function Parameters\n(\n.param\n) window is contained\nwithin the\n.global\nwindow. Within each window, a generic address maps to an address in the\nunderlying state space by subtracting the window base from the generic address.\n6.4.2.\nArrays as Operands\n\nArrays of all types can be declared, and the identifier becomes an address constant in the space\nwhere the array is declared. The size of the array is a constant in the program.\nArray elements can be accessed using an explicitly calculated byte address, or by indexing into the\narray using square-bracket notation. The expression within square brackets is either a constant\ninteger, a register variable, or a simple\nregister with constant offset\nexpression, where the\noffset is a constant expression that is either added or subtracted from a register variable. If more\ncomplicated indexing is desired, it must be written as an address calculation prior to use. Examples\nare:\nld.global.u32  s, a[0];\nld.global.u32  s, a[N-1];\nmov.u32        s, a[1];  // move address of a[1] into s\n6.4.3.\nVectors as Operands\n\nVector operands can be specified as source and destination operands for instructions. However, when\nspecified as destination operand, all elements in vector expression must be unique, otherwise behavior\nis undefined.\nVectors may also be passed as arguments to called functions.\nVector elements can be extracted from the vector with the suffixes\n.x\n,\n.y\n,\n.z\nand\n.w\n, as well as the typical color fields\n.r\n,\n.g\n,\n.b\nand\n.a\n.\nA brace-enclosed list is used for pattern matching to pull apart vectors.\n.reg .v4 .f32 V;\n.reg .f32     a, b, c, d;\n\nmov.v4.f32 {a,b,c,d}, V;\nVector loads and stores can be used to implement wide loads and stores, which may improve memory\nperformance. The registers in the load/store operations can be a vector, or a brace-enclosed list of\nsimilarly typed scalars. Here are examples:\nld.global.v4.f32  {a,b,c,d}, [addr+16];\nld.global.v2.u32  V2, [addr+8];\nElements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows:\nRa = V.x = V.r\nRb = V.y = V.g\nRc = V.z = V.b\nRd = V.w = V.a\n6.4.4.\nLabels and Function Names as Operands\n\nLabels and function names can be used only in\nbra\n/\nbrx.idx\nand\ncall\ninstructions\nrespectively. Function names can be used in\nmov\ninstruction to get the address of the function\ninto a register, for use in an indirect call.\nBeginning in PTX ISA version 3.1, the\nmov\ninstruction may be used to take the address of kernel\nfunctions, to be passed to a system call that initiates a kernel launch from the GPU. This feature\nis part of the support for CUDA Dynamic Parallelism. See the\nCUDA Dynamic Parallelism Programming\nGuide\nfor details.\n6.5.\nType Conversion\n\nAll operands to all arithmetic, logic, and data movement instruction must be of the same type and\nsize, except for operations where changing the size and/or type is part of the definition of the\ninstruction. Operands of different sizes or types must be converted prior to the operation.\n6.5.1.\nScalar Conversions\n\nTable 15\nand\nTable 16\nshow what\nprecision and format the cvt instruction uses given operands of differing types. For example, if a\ncvt.s32.u16\ninstruction is given a\nu16\nsource operand and\ns32\nas a destination operand,\nthe\nu16\nis zero-extended to\ns32\n.\nConversions to floating-point that are beyond the range of floating-point numbers are represented\nwith the maximum floating-point value (IEEE 754 Inf for\nf32\nand\nf64\n, and ~131,000 for\nf16\n).\nTable 15\nConvert Instruction Precision and Format Table 1\n\nDestination Format\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nbf16\ntf32\nSource\nFormat\ns8\n–\nsext\nsext\nsext\n–\nsext\nsext\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns16\nchop\n1\n–\nsext\nsext\nchop\n1\n–\nsext\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns32\nchop\n1\nchop\n1\n–\nsext\nchop\n1\nchop\n1\n–\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns64\nchop\n1\nchop\n1\nchop\n1\n–\nchop\n1\nchop\n1\nchop\n1\n–\ns2f\ns2f\ns2f\ns2f\n–\nu8\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu16\nchop\n1\n–\nzext\nzext\nchop\n1\n–\nzext\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu32\nchop\n1\nchop\n1\n–\nzext\nchop\n1\nchop\n1\n–\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu64\nchop\n1\nchop\n1\nchop\n1\n–\nchop\n1\nchop\n1\nchop\n1\n–\nu2f\nu2f\nu2f\nu2f\n–\nf16\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\n–\nf2f\nf2f\nf2f\n–\nf32\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\n–\nf2f\nf2f\nf2f\nf64\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\nf2f\n–\nf2f\n–\nbf16\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\nf2f\nf2f\nf2f\n–\ntf32\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\nTable 16\nConvert Instruction Precision and Format Table 2\n\nDestination Format\nf16\nf32\nbf16\ne4m3\ne5m2\ne2m3\ne3m2\ne2m1\nue8m0\ns2f6\nSource\nFormat\nf16\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\n–\n–\nf32\nf2f\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nbf16\nf2f\nf2f\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\ne4m3\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne5m2\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne2m3\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne3m2\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne2m1\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\nue8m0\n–\n–\nf2f\n–\n–\n–\n–\n–\n–\n–\ns2f6\n–\n–\nf2f\n–\n–\n–\n–\n–\n–\n–\nNotes\nsext = sign-extend; zext = zero-extend; chop = keep only low bits that fit;\ns2f = signed-to-float; f2s = float-to-signed; u2f = unsigned-to-float;\nf2u = float-to-unsigned; f2f = float-to-float.\n1\nIf the destination register is wider than the destination format, the result is extended to the\ndestination register width after chopping. The type of extension (sign or zero) is based on the\ndestination format. For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then\nsign-extends to 32-bit.\n6.5.2.\nRounding Modifiers\n\nConversion instructions may specify a rounding modifier. In PTX, there are four integer rounding\nmodifiers and six floating-point rounding\nmodifiers.\nTable 17\nand\nTable 18\nsummarize the rounding modifiers.\nTable 17\nFloating-Point Rounding Modifiers\n\nModifier\nDescription\n.rn\nrounds to nearest even\n.rna\nrounds to nearest, ties away from zero\n.rz\nrounds towards zero\n.rm\nrounds towards negative infinity\n.rp\nrounds towards positive infinity\n.rs\nrounds either towards zero or away from zero based\non the carry out of the integer addition of random\nbits and the discarded bits of mantissa\nTable 18\nInteger Rounding Modifiers\n\nModifier\nDescription\n.rni\nround to nearest integer, choosing even integer if source is equidistant between two integers.\n.rzi\nround to nearest integer in the direction of zero\n.rmi\nround to nearest integer in direction of negative infinity\n.rpi\nround to nearest integer in direction of positive infinity\n6.6.\nOperand Costs\n\nOperands from different state spaces affect the speed of an operation. Registers are fastest, while\nglobal memory is slowest. Much of the delay to memory can be hidden in a number of ways. The first\nis to have multiple threads of execution so that the hardware can issue a memory operation and then\nswitch to other execution. Another way to hide latency is to issue the load instructions as early as\npossible, as execution is not blocked until the desired result is used in a subsequent (in time)\ninstruction. The register in a store operation is available much more\nquickly.\nTable 19\ngives estimates of the\ncosts of using different kinds of memory.\nTable 19\nCost Estimates for Accessing State-Spaces\n\nSpace\nTime\nNotes\nRegister\n0\nShared\n0\nConstant\n0\nAmortized cost is low, first access is high\nLocal\n> 100 clocks\nParameter\n0\nImmediate\n0\nGlobal\n> 100 clocks\nTexture\n> 100 clocks\nSurface\n> 100 clocks\n7.\nAbstracting the ABI\n\nRather than expose details of a particular calling convention, stack layout, and Application Binary\nInterface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI\nimplementations. In this section, we describe the features of PTX needed to achieve this hiding of\nthe ABI. These include syntax for function definitions, function calls, parameter passing, and\nmemory allocated on the stack (\nalloca\n).\nRefer to\nPTX Writers Guide to Interoperability\nfor details on generating PTX compliant with\nApplication Binary Interface (ABI) for the CUDA\n®\narchitecture.\n7.1.\nFunction Declarations and Definitions\n\nIn PTX, functions are declared and defined using the\n.func\ndirective. A function\ndeclaration\nspecifies an optional list of return parameters, the function name, and an optional list of input\nparameters; together these specify the function’s interface, or prototype. A function\ndefinition\nspecifies both the interface and the body of the function. A function must be declared or defined\nprior to being called.\nThe simplest function has no parameters or return values, and is represented in PTX as follows:\n.func foo\n{\n    ...\n    ret;\n}\n\n    ...\n    call foo;\n    ...\nHere, execution of the\ncall\ninstruction transfers control to\nfoo\n, implicitly saving the\nreturn address. Execution of the\nret\ninstruction within\nfoo\ntransfers control to the\ninstruction following the call.\nScalar and vector base-type input and return parameters may be represented simply as register\nvariables. At the call, arguments may be register variables or constants, and return values may be\nplaced directly into register variables. The arguments and return variables at the call must have\ntype and size that match the callee’s corresponding formal parameters.\nExample\n.func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc )\n{\n    add.u32 %res, %ptr, %inc;\n    ret;\n}\n\n    ...\n    call (%r1), inc_ptr, (%r1,4);\n    ...\nWhen using the ABI,\n.reg\nstate space parameters must be at least 32-bits in size. Subword scalar\nobjects in the source language should be promoted to 32-bit registers in PTX, or use\n.param\nstate space byte arrays described next.\nObjects such as C structures and unions are flattened into registers or byte arrays in PTX and are\nrepresented using\n.param\nspace memory. For example, consider the following C structure, passed\nby value to a function:\nstruct {\n    double dbl;\n    char   c[4];\n};\nIn PTX, this structure will be flattened into a byte array. Since memory accesses are required to be\naligned to a multiple of the access size, the structure in this example will be a 12 byte array with\n8 byte alignment so that accesses to the\n.f64\nfield are aligned. The\n.param\nstate space is\nused to pass the structure by value:\nExample\n.func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12])\n{\n    .reg .f64 f1;\n    .reg .b32 c1, c2, c3, c4;\n    ...\n    ld.param.f64 f1, [y+0];\n    ld.param.b8  c1, [y+8];\n    ld.param.b8  c2, [y+9];\n    ld.param.b8  c3, [y+10];\n    ld.param.b8  c4, [y+11];\n    ...\n    ... // computation using x,f1,c1,c2,c3,c4;\n}\n\n{\n     .param .b8 .align 8 py[12];\n     ...\n     st.param.b64 [py+ 0], %rd;\n     st.param.b8  [py+ 8], %rc1;\n     st.param.b8  [py+ 9], %rc2;\n     st.param.b8  [py+10], %rc1;\n     st.param.b8  [py+11], %rc2;\n     // scalar args in .reg space, byte array in .param space\n     call (%out), bar, (%x, py);\n     ...\nIn this example, note that\n.param\nspace variables are used in two ways. First, a\n.param\nvariable\ny\nis used in function definition bar to represent a formal parameter. Second, a\n.param\nvariable\npy\nis declared in the body of the calling function and used to set up the\nstructure being passed to bar.\nThe following is a conceptual way to think about the\n.param\nstate space use in device functions.\nFor a caller,\nThe\n.param\nstate space is used to set values that will be passed to a called function and/or\nto receive return values from a called function. Typically, a\n.param\nbyte array is used to\ncollect together fields of a structure being passed by value.\nFor a callee,\nThe\n.param\nstate space is used to receive parameter values and/or pass return values back to\nthe caller.\nThe following restrictions apply to parameter passing.\nFor a caller,\nArguments may be\n.param\nvariables,\n.reg\nvariables, or constants.\nIn the case of\n.param\nspace formal parameters that are byte arrays, the argument must also be\na\n.param\nspace byte array with matching type, size, and alignment. A\n.param\nargument must\nbe declared within the local scope of the caller.\nIn the case of\n.param\nspace formal parameters that are base-type scalar or vector variables,\nthe corresponding argument may be either a\n.param\nor\n.reg\nspace variable with matching\ntype and size, or a constant that can be represented in the type of the formal parameter.\nIn the case of\n.reg\nspace formal parameters, the corresponding argument may be either a\n.param\nor\n.reg\nspace variable of matching type and size, or a constant that can be\nrepresented in the type of the formal parameter.\nIn the case of\n.reg\nspace formal parameters, the register must be at least 32-bits in size.\nAll\nst.param\ninstructions used for passing arguments to function call must immediately precede\nthe corresponding\ncall\ninstruction and\nld.param\ninstruction used for collecting return\nvalue must immediately follow the\ncall\ninstruction without any control flow\nalteration.\nst.param\nand\nld.param\ninstructions used for argument passing cannot be\npredicated. This enables compiler optimization and ensures that the\n.param\nvariable does not\nconsume extra space in the caller’s frame beyond that needed by the ABI. The\n.param\nvariable\nsimply allows a mapping to be made at the call site between data that may be in multiple\nlocations (e.g., structure being manipulated by caller is located in registers and memory) to\nsomething that can be passed as a parameter or return value to the callee.\nFor a callee,\nInput and return parameters may be\n.param\nvariables or\n.reg\nvariables.\nParameters in\n.param\nmemory must be aligned to a multiple of 1, 2, 4, 8, or 16 bytes.\nParameters in the\n.reg\nstate space must be at least 32-bits in size.\nThe\n.reg\nstate space can be used to receive and return base-type scalar and vector values,\nincluding sub-word size objects when compiling in non-ABI mode. Supporting the\n.reg\nstate\nspace provides legacy support.\nNote that the choice of\n.reg\nor\n.param\nstate space for parameter passing has no impact on\nwhether the parameter is ultimately passed in physical registers or on the stack. The mapping of\nparameters to physical registers and stack locations depends on the ABI definition and the order,\nsize, and alignment of parameters.\n7.1.1.\nChanges from PTX ISA Version 1.x\n\nIn PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no\nsupport for array parameters. Objects such as C structures were flattened and passed or returned\nusing multiple registers. PTX ISA version 1.x supports multiple return values for this purpose.\nBeginning with PTX ISA version 2.0, formal parameters may be in either\n.reg\nor\n.param\nstate\nspace, and\n.param\nspace parameters support arrays. For targets\nsm_20\nor higher, PTX\nrestricts functions to a single return value, and a\n.param\nbyte array should be used to return\nobjects that do not fit into a register. PTX continues to support multiple return registers for\nsm_1x\ntargets.\nNote\nPTX implements a stack-based ABI only for targets\nsm_20\nor higher.\nPTX ISA versions prior to 3.0 permitted variables in\n.reg\nand\n.local\nstate spaces to be\ndefined at module scope. When compiling to use the ABI, PTX ISA version 3.0 and later disallows\nmodule-scoped\n.reg\nand\n.local\nvariables and restricts their use to within function\nscope. When compiling without use of the ABI, module-scoped\n.reg\nand\n.local\nvariables are\nsupported as before. When compiling legacy PTX code (ISA versions prior to 3.0) containing\nmodule-scoped\n.reg\nor\n.local\nvariables, the compiler silently disables use of the ABI.\n7.2.\nVariadic Functions\n\nNote\nSupport for variadic functions which was unimplemented has been removed from the spec.\nPTX version 6.0 supports passing unsized array parameter to a function which can be used to\nimplement variadic functions.\nRefer to\nKernel and Function Directives: .func\nfor details\n7.3.\nAlloca\n\nPTX provides\nalloca\ninstruction for allocating storage at runtime on the per-thread local memory\nstack. The allocated stack memory can be accessed with\nld.local\nand\nst.local\ninstructions\nusing the pointer returned by\nalloca\n.\nIn order to facilitate deallocation of memory allocated with\nalloca\n, PTX provides two additional\ninstructions:\nstacksave\nwhich allows reading the value of stack pointer in a local variable, and\nstackrestore\nwhich can restore the stack pointer with the saved value.\nalloca\n,\nstacksave\n, and\nstackrestore\ninstructions are described in\nStack Manipulation Instructions\n.\n8.\nMemory Consistency Model\n\nIn multi-threaded executions, the side-effects of memory operations performed by each thread become\nvisible to other threads in a partial and non-identical order. This means that any two operations\nmay appear to happen in no order, or in different orders, to different threads. The axioms\nintroduced by the memory consistency model specify exactly which contradictions are forbidden\nbetween the orders observed by different threads.\nIn the absence of any constraint, each read operation returns the value committed by some write\noperation to the same memory location, including the initial write to that memory location. The\nmemory consistency model effectively constrains the set of such candidate writes from which a read\noperation can return a value.\n8.1.\nScope and applicability of the model\n\nThe constraints specified under this model apply to PTX programs with any PTX ISA version number,\nrunning on\nsm_70\nor later architectures.\nThe memory consistency model does not apply to texture (including\nld.global.nc\n) and surface\naccesses.\n8.1.1.\nLimitations on atomicity at system scope\n\nWhen communicating with the host CPU, certain strong operations with system scope may not be\nperformed atomically on some systems. For more details on atomicity guarantees to host memory, see\nthe\nCUDA Atomicity Requirements\n.\n8.2.\nMemory operations\n\nThe fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits. Each state\nspace available to a PTX program is a sequence of contiguous bytes in memory. Every byte in a PTX\nstate space has a unique address relative to all threads that have access to the same state space.\nEach PTX memory instruction specifies an address operand and a data type. The address operand\ncontains a virtual address that gets converted to a physical address during memory access. The\nphysical address and the size of the data type together define a physical memory location, which is\nthe range of bytes starting from the physical address and extending up to the size of the data type\nin bytes.\nThe memory consistency model specification uses the terms “address” or “memory address” to indicate\na virtual address, and the term “memory location” to indicate a physical memory location.\nEach PTX memory instruction also specifies the operation — either a read, a write or an atomic\nread-modify-write — to be performed on all the bytes in the corresponding memory location.\n8.2.1.\nOverlap\n\nTwo memory locations are said to overlap when the starting address of one location is within the\nrange of bytes constituting the other location. Two memory operations are said to overlap when they\nspecify the same virtual address and the corresponding memory locations overlap. The overlap is said\nto be complete when both memory locations are identical, and it is said to be partial otherwise.\n8.2.2.\nAliases\n\nTwo distinct virtual addresses are said to be aliases if they map to the same memory location.\n8.2.3.\nMultimem Addresses\n\nA multimem address is a virtual address which points to multiple distinct memory locations across\ndevices.\nOnly\nmultimem.\n* operations are valid on multimem addresses. That is, the behavior of accessing\na multimem address in any other memory operation is undefined.\n8.2.4.\nMemory Operations on Vector Data Types\n\nThe memory consistency model relates operations executed on memory locations with scalar data types,\nwhich have a maximum size and alignment of 64 bits. Memory operations with a vector data type are\nmodelled as a set of equivalent memory operations with a scalar data type, executed in an\nunspecified order on the elements in the vector.\n8.2.5.\nMemory Operations on Packed Data Types\n\nA packed data type consists of two values of the same scalar data type, as described in\nPacked Data Types\n. These values are accessed in adjacent memory locations. A\nmemory operation on a packed data type is modelled as a pair of equivalent memory operations on the\nscalar data type, executed in an unspecified order on each element of the packed data.\n8.2.6.\nInitialization\n\nEach byte in memory is initialized by a hypothetical write\nW0\nexecuted before starting any thread\nin the program. If the byte is included in a program variable, and that variable has an initial\nvalue, then\nW0\nwrites the corresponding initial value for that byte; else\nW0\nis assumed to have\nwritten an unknown but constant value to the byte.\n8.3.\nState spaces\n\nThe relations defined in the memory consistency model are independent of state spaces. In\nparticular, causality order closes over all memory operations across all the state spaces. But the\nside-effect of a memory operation in one state space can be observed directly only by operations\nthat also have access to the same state space. This further constrains the synchronizing effect of a\nmemory operation in addition to scope. For example, the synchronizing effect of the PTX instruction\nld.relaxed.shared.sys\nis identical to that of\nld.relaxed.shared.cluster\n, since no thread\noutside the same cluster can execute an operation that accesses the same memory location.\n8.4.\nOperation types\n\nFor simplicity, the rest of the document refers to the following operation types, instead of\nmentioning specific instructions that give rise to them.\nTable 20\nOperation Types\n\nOperation Type\nInstruction/Operation\natomic operation\natom\nor\nred\ninstruction.\nread operation\nAll variants of\nld\ninstruction and\natom\ninstruction (but not\nred\ninstruction).\nwrite operation\nAll variants of\nst\ninstruction, and\natomic\noperations if they result\nin a write.\nmemory operation\nA\nread\nor\nwrite\noperation.\nvolatile operation\nAn instruction with\n.volatile\nqualifier.\nacquire operation\nA\nmemory\noperation with\n.acquire\nor\n.acq_rel\nqualifier.\nrelease operation\nA\nmemory\noperation with\n.release\nor\n.acq_rel\nqualifier.\nmmio operation\nAn\nld\nor\nst\ninstruction with\n.mmio\nqualifier.\nmemory fence operation\nA\nmembar\n,\nfence.sc\nor\nfence.acq_rel\ninstruction.\nproxy fence operation\nA\nfence.proxy\nor a\nmembar.proxy\ninstruction.\nstrong operation\nA\nmemory fence\noperation, or a\nmemory\noperation with a\n.relaxed\n,\n.acquire\n,\n.release\n,\n.acq_rel\n,\n.volatile\n, or\n.mmio\nqualifier.\nweak operation\nAn\nld\nor\nst\ninstruction with a\n.weak\nqualifier.\nsynchronizing operation\nA\nbarrier\ninstruction,\nfence\noperation,\nrelease\noperation or\nacquire\noperation.\n8.4.1.\nmmio Operation\n\nAn\nmmio\noperation is a memory operation with\n.mmio\nqualifier specified. It is usually performed\non a memory location which is mapped to the control registers of peer I/O devices. It can also be\nused for communication between threads but has poor performance relative to non-\nmmio\noperations.\nThe semantic meaning of\nmmio\noperations cannot be defined precisely as it is defined by the\nunderlying I/O device. For formal specification of semantics of\nmmio\noperation from Memory\nConsistency Model perspective, it is equivalent to the semantics of a\nstrong\noperation. But it\nfollows a few implementation-specific properties, if it meets the\nCUDA atomicity requirements\nat\nthe specified scope:\nWrites are always performed and are never combined within the scope specified.\nReads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any\ncache within the scope specified.\nAs an exception, in some implementations, the surrounding locations may also be loaded. In such\ncases the amount of data loaded is implementation specific and varies between 32 and 128 bytes\nin size.\n8.4.2.\nvolatile Operation\n\nA\nvolatile\noperation is a memory operation with\n.volatile\nqualifier specified.\nThe semantics of volatile operations are equivalent to a relaxed memory operation with system-scope\nbut with the following extra implementation-specific constraints:\nThe number of volatile\ninstructions\n(not operations) executed by a program is preserved.\nHardware may combine and merge volatile\noperations\nissued by multiple different volatile\ninstructions\n, that is, the number of volatile\noperations\nin the program is not preserved.\nVolatile\ninstructions\nare not re-ordered around other volatile\ninstructions\n, but the memory\noperations\nperformed by those\ninstructions\nmay be re-ordered around each other.\nNote\nPTX volatile operations are intended for compilers to lower volatile read and write operations from\nCUDA C++, and other programming languages sharing CUDA C++ volatile semantics, to PTX.\nSince volatile operations are relaxed at system-scope with extra constraints, prefer using other\nstrong\nread or write operations (e.g.\nld.relaxed.sys\nor\nst.relaxed.sys\n) for\nInter-Thread Synchronization\ninstead, which may deliver better performance.\nPTX volatile operations are not suited for\nMemory Mapped IO (MMIO)\nbecause volatile operations\ndo not preserve the number of memory operations performed, and may perform more or less operations\nthan requested in a non-deterministic way.\nUse\n.mmio operations\ninstead, which strictly preserve the number of operations\nperformed.\n8.5.\nScope\n\nEach\nstrong\noperation must specify a\nscope\n, which is the set of threads that may interact\ndirectly with that operation and establish any of the relations described in the memory consistency\nmodel. There are four scopes:\nTable 21\nScopes\n\nScope\nDescription\n.cta\nThe set of all threads executing in the same CTA as the current thread.\n.cluster\nThe set of all threads executing in the same cluster as the current thread.\n.gpu\nThe set of all threads in the current program executing on the same compute\ndevice as the current thread. This also includes other kernel grids invoked by\nthe host program on the same compute device.\n.sys\nThe set of all threads in the current program, including all kernel grids\ninvoked by the host program on all compute devices, and all threads\nconstituting the host program itself.\nNote that the warp is not a\nscope\n; the CTA is the smallest collection of threads that qualifies as\na\nscope\nin the memory consistency model.\n8.6.\nProxies\n\nA\nmemory proxy\n, or a\nproxy\nis an abstract label applied to a method of memory access. When two\nmemory operations use distinct methods of memory access, they are said to be different\nproxies\n.\nMemory operations as defined in\nOperation types\nuse\ngeneric\nmethod of memory access, i.e. a\ngeneric proxy\n. Other operations such as textures and surfaces all\nuse distinct methods of memory access, also distinct from the\ngeneric\nmethod.\nA\nproxy fence\nis required to synchronize memory operations across different\nproxies\n. Although\nvirtual aliases use the\ngeneric\nmethod of memory access, since using distinct virtual addresses\nbehaves as if using different\nproxies\n, they require a\nproxy fence\nto establish memory ordering.\n8.7.\nMorally strong operations\n\nTwo operations are said to be\nmorally strong\nrelative to each other if they satisfy all of the\nfollowing conditions:\nThe operations are related in\nprogram order\n(i.e, they are both executed by the same thread),\nor each operation is\nstrong\nand specifies a\nscope\nthat includes the thread executing the\nother operation.\nBoth operations are performed via the same\nproxy\n.\nIf both are memory operations, then they overlap completely.\nMost (but not all) of the axioms in the memory consistency model depend on relations between\nmorally strong\noperations.\n8.7.1.\nConflict and Data-races\n\nTwo\noverlapping\nmemory operations are said to\nconflict\nwhen at least one of them is a\nwrite\n.\nTwo\nconflicting\nmemory operations are said to be in a\ndata-race\nif they are not related in\ncausality order\nand they are not\nmorally strong\n.\n8.7.2.\nLimitations on Mixed-size Data-races\n\nA\ndata-race\nbetween operations that\noverlap\ncompletely is called a\nuniform-size data-race\n,\nwhile a\ndata-race\nbetween operations that\noverlap\npartially is called a\nmixed-size data-race\n.\nThe axioms in the memory consistency model do not apply if a PTX program contains one or more\nmixed-size data-races\n. But these axioms are sufficient to describe the behavior of a PTX program\nwith only\nuniform-size data-races\n.\nAtomicity of mixed-size RMW operations\nIn any program with or without\nmixed-size data-races\n, the following property holds for every pair\nof\noverlapping atomic\noperations A1 and A2 such that each specifies a\nscope\nthat includes the\nother: Either the\nread-modify-write\noperation specified by A1 is performed completely before A2 is\ninitiated, or vice versa. This property holds irrespective of whether the two operations A1 and A2\noverlap partially or completely.\n8.8.\nRelease and Acquire Patterns\n\nSome sequences of instructions give rise to patterns that participate in memory synchronization as\ndescribed later. The\nrelease\npattern makes prior operations from the current thread\n1\nvisible to some operations from other threads. The\nacquire\npattern makes some operations from\nother threads visible to later operations from the current thread.\nA\nrelease\npattern on a location M consists of one of the following:\nA\nrelease\noperation on M\nE.g.:\nst.release\n[M];\nor\natom.release\n[M];\nor\nmbarrier.arrive.release\n[M];\nOr a\nrelease\nor\nacquire-release\noperation on M followed by a\nstrong\nwrite on M in\nprogram order\nE.g.:\nst.release\n[M]\n;\nst.relaxed\n[M];\nOr a\nrelease\nor\nacquire-release\nmemory fence\nfollowed by a\nstrong\nwrite on M in\nprogram order\nE.g.:\nfence.release;\nst.relaxed\n[M];\nor\nfence.release;\natom.relaxed\n[M];\nAny\nmemory synchronization\nestablished by a\nrelease\npattern only affects operations occurring in\nprogram order\nbefore the first instruction in that pattern.\nAn\nacquire\npattern on a location M consists of one of the following:\nAn\nacquire\noperation on M\nE.g.:\nld.acquire\n[M];\nor\natom.acquire\n[M];\nor\nmbarrier.test_wait.acquire\n[M];\nOr a\nstrong\nread on M followed by an\nacquire\noperation on M in\nprogram order\nE.g.:\nld.relaxed\n[M];\nld.acquire\n[M];\nOr a\nstrong\nread on M followed by an acquire\nmemory fence\nin\nprogram order\nE.g.:\nld.relaxed\n[M];\nfence.acquire;\nor\natom.relaxed\n[M];\nfence.acquire;\nAny\nmemory synchronization\nestablished by an\nacquire\npattern only affects operations occurring\nin\nprogram order\nafter the last instruction in that pattern.\nNote that while atomic reductions conceptually perform a strong read as part of its\nread-modify-write sequence, this strong read does not form an acquire pattern.\nE.g.:\nred.add\n[M],\n1;\nfence.acquire;\nis not an acquire pattern.\n1\nFor both\nrelease\nand\nacquire\npatterns, this effect is further extended to operations in\nother threads through the transitive nature of\ncausality order\n.\n8.9.\nOrdering of memory operations\n\nThe sequence of operations performed by each thread is captured as\nprogram order\nwhile\nmemory\nsynchronization\nacross threads is captured as\ncausality order\n. The visibility of the side-effects\nof memory operations to other memory operations is captured as\ncommunication order\n. The memory\nconsistency model defines contradictions that are disallowed between communication order on the one\nhand, and\ncausality order\nand\nprogram order\non the other.\n8.9.1.\nProgram Order\n\nThe\nprogram order\nrelates all operations performed by a thread to the order in which a sequential\nprocessor will execute instructions in the corresponding PTX source. It is a transitive relation\nthat forms a total order over the operations performed by the thread, but does not relate operations\nfrom different threads.\n8.9.1.1.\nAsynchronous Operations\n\nSome PTX instructions (all variants of\ncp.async\n,\ncp.async.bulk\n,\ncp.reduce.async.bulk\n,\nwgmma.mma_async\n) perform operations that are asynchronous to the thread that executed the\ninstruction. These asynchronous operations are ordered after prior instructions in the same thread\n(except in the case of\nwgmma.mma_async\n), but they are not part of the program order for that\nthread. Instead, they provide weaker ordering guarantees as documented in the instruction\ndescription.\nFor example, the loads and stores performed as part of a\ncp.async\nare ordered with respect to\neach other, but not to those of any other\ncp.async\ninstructions initiated by the same thread,\nnor any other instruction subsequently issued by the thread with the exception of\ncp.async.commit_group\nor\ncp.async.mbarrier.arrive\n. The asynchronous mbarrier\narrive-on\noperation\nperformed by a\ncp.async.mbarrier.arrive\ninstruction is ordered with respect to the memory\noperations performed by all prior\ncp.async\noperations initiated by the same thread, but not to\nthose of any other instruction issued by the thread. The implicit mbarrier\ncomplete-tx\noperation that is part of all variants of\ncp.async.bulk\nand\ncp.reduce.async.bulk\ninstructions is ordered only with respect to the memory operations performed by the same\nasynchronous instruction, and in particular it does not transitively establish ordering with respect\nto prior instructions from the issuing thread.\n8.9.2.\nObservation Order\n\nObservation order\nrelates a write W to a read R through an optional sequence of atomic\nread-modify-write operations.\nA write W precedes a read R in\nobservation order\nif:\nR and W are\nmorally strong\nand R reads the value written by W, or\nFor some atomic operation Z, W precedes Z and Z precedes R in\nobservation order\n.\n8.9.3.\nFence-SC Order\n\nThe\nFence-SC\norder is an acyclic partial order, determined at runtime, that relates every pair of\nmorally strong fence.sc\noperations.\n8.9.4.\nMemory synchronization\n\nSynchronizing operations performed by different threads synchronize with each other at runtime as\ndescribed here. The effect of such synchronization is to establish\ncausality order\nacross threads.\nA\nfence.sc\noperation X\nsynchronizes\nwith a\nfence.sc\noperation Y if X precedes Y in the\nFence-SC\norder.\nA\nbar{.cta}.sync\nor\nbar{.cta}.red\nor\nbar{.cta}.arrive\noperation\nsynchronizes\nwith a\nbar{.cta}.sync\nor\nbar{.cta}.red\noperation executed on the same barrier.\nA\nbarrier.cluster.arrive\noperation synchronizes with a\nbarrier.cluster.wait\noperation.\nA\nrelease\npattern X\nsynchronizes\nwith an\nacquire\npattern Y, if a\nwrite\noperation in X\nprecedes a\nread\noperation in Y in\nobservation order\n, and the first operation in X and the\nlast operation in Y are\nmorally strong\n.\nAPI synchronization\nA\nsynchronizes\nrelation can also be established by certain CUDA APIs.\nCompletion of a task enqueued in a CUDA stream\nsynchronizes\nwith the start of the following\ntask in the same stream, if any.\nFor purposes of the above, recording or waiting on a CUDA event in a stream, or causing a\ncross-stream barrier to be inserted due to\ncudaStreamLegacy\n, enqueues tasks in the associated\nstreams even if there are no direct side effects. An event record task\nsynchronizes\nwith\nmatching event wait tasks, and a barrier arrival task\nsynchronizes\nwith matching barrier wait\ntasks.\nStart of a CUDA kernel\nsynchronizes\nwith start of all threads in the kernel. End of all threads\nin a kernel\nsynchronize\nwith end of the kernel.\nStart of a CUDA graph\nsynchronizes\nwith start of all source nodes in the graph. Completion of\nall sink nodes in a CUDA graph\nsynchronizes\nwith completion of the graph. Completion of a graph\nnode\nsynchronizes\nwith start of all nodes with a direct dependency.\nStart of a CUDA API call to enqueue a task\nsynchronizes\nwith start of the task.\nCompletion of the last task queued to a stream, if any,\nsynchronizes\nwith return from\ncudaStreamSynchronize\n. Completion of the most recently queued matching event record task, if\nany,\nsynchronizes\nwith return from\ncudaEventSynchronize\n. Synchronizing a CUDA device or\ncontext behaves as if synchronizing all streams in the context, including ones that have been\ndestroyed.\nReturning\ncudaSuccess\nfrom an API to query a CUDA handle, such as a stream or event, behaves\nthe same as return from the matching synchronization API.\nIn addition to establishing a\nsynchronizes\nrelation, the CUDA API synchronization mechanisms above\nalso participate in\nproxy-preserved base causality order\n.\n8.9.5.\nCausality Order\n\nCausality order\ncaptures how memory operations become visible across threads through synchronizing\noperations. The axiom “Causality” uses this order to constrain the set of write operations from\nwhich a read operation may read a value.\nRelations in the\ncausality order\nprimarily consist of relations in\nBase causality order\n1\n, which is a transitive order, determined at runtime.\nBase causality order\nAn operation X precedes an operation Y in\nbase causality order\nif:\nX precedes Y in\nprogram order\n, or\nX\nsynchronizes\nwith Y, or\nFor some operation Z,\nX precedes Z in\nprogram order\nand Z precedes Y in\nbase causality order\n, or\nX precedes Z in\nbase causality order\nand Z precedes Y in\nprogram order\n, or\nX precedes Z in\nbase causality order\nand Z precedes Y in\nbase causality order\n.\nProxy-preserved base causality order\nA memory operation X precedes a memory operation Y in\nproxy-preserved base causality order\nif X\nprecedes Y in\nbase causality order\n, and:\nX and Y are performed to the same address, using the\ngeneric proxy\n, or\nX and Y are performed to the same address, using the same\nproxy\n, and by the same thread block,\nor\nX and Y are aliases and there is an alias\nproxy fence\nalong the base causality path from X\nto Y.\nCausality order\nCausality order\ncombines\nbase causality order\nwith some non-transitive relations as follows:\nAn operation X precedes an operation Y in\ncausality order\nif:\nX precedes Y in\nproxy-preserved base causality order\n, or\nFor some operation Z, X precedes Z in observation order, and Z precedes Y in\nproxy-preserved\nbase causality order\n.\n1\nThe transitivity of\nbase causality order\naccounts for the “cumulativity” of synchronizing\noperations.\n8.9.6.\nCoherence Order\n\nThere exists a partial transitive order that relates\noverlapping\nwrite operations, determined at\nruntime, called the\ncoherence order\n1\n. Two\noverlapping\nwrite operations are related in\ncoherence order\nif they are\nmorally strong\nor if they are related in\ncausality order\n. Two\noverlapping\nwrites are unrelated in\ncoherence order\nif they are in a\ndata-race\n, which gives\nrise to the partial nature of\ncoherence order\n.\n1\nCoherence order\ncannot be observed directly since it consists entirely of write\noperations. It may be observed indirectly by its use in constraining the set of candidate\nwrites that a read operation may read from.\n8.9.7.\nCommunication Order\n\nThe\ncommunication order\nis a non-transitive order, determined at runtime, that relates write\noperations to other\noverlapping\nmemory operations.\nA write W precedes an\noverlapping\nread R in\ncommunication order\nif R returns the value of any\nbyte that was written by W.\nA write W precedes a write W’ in\ncommunication order\nif W precedes W’ in\ncoherence order\n.\nA read R precedes an\noverlapping\nwrite W in\ncommunication order\nif, for any byte accessed by\nboth R and W, R returns the value written by a write W’ that precedes W in\ncoherence order\n.\nCommunication order\ncaptures the visibility of memory operations — when a memory operation X1\nprecedes a memory operation X2 in\ncommunication order\n, X1 is said to be visible to X2.\n8.10.\nAxioms\n\n8.10.1.\nCoherence\n\nIf a write W precedes an\noverlapping\nwrite W’ in\ncausality order\n, then W must precede W’ in\ncoherence order\n.\n8.10.2.\nFence-SC\n\nFence-SC\norder cannot contradict\ncausality order\n. For a pair of\nmorally strong\nfence.sc\noperations F1 and F2, if F1 precedes F2 in\ncausality order\n, then F1 must precede F2 in\nFence-SC\norder.\n8.10.3.\nAtomicity\n\nSingle-Copy Atomicity\nConflicting\nmorally strong\noperations are performed with\nsingle-copy atomicity\n. When a read R\nand a write W are\nmorally strong\n, then the following two communications cannot both exist in the\nsame execution, for the set of bytes accessed by both R and W:\nR reads any byte from W.\nR reads any byte from any write W’ which precedes W in\ncoherence order\n.\nAtomicity of read-modify-write (RMW) operations\nWhen an\natomic\noperation A and a write W\noverlap\nand are\nmorally strong\n, then the following\ntwo communications cannot both exist in the same execution, for the set of bytes accessed by both A\nand W:\nA reads any byte from a write W’ that precedes W in\ncoherence order\n.\nA follows W in\ncoherence order\n.\nLitmus Test 1\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2\nA1\n:\natom\n.\nsys\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nA2\n:\natom\n.\nsys\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nFINAL\nSTATE\n:\nx\n==\n2\nAtomicity is guaranteed when the operations are\nmorally strong\n.\nLitmus Test 2\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2 (In a different CTA)\nA1\n:\natom\n.\ncta\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nA2\n:\natom\n.\ngpu\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nFINAL\nSTATE\n:\nx\n==\n1\nOR\nx\n==\n2\nAtomicity is not guaranteed if the operations are not\nmorally strong\n.\n8.10.4.\nNo Thin Air\n\nValues may not appear “out of thin air”: an execution cannot speculatively produce a value in such a\nway that the speculation becomes self-satisfying through chains of instruction dependencies and\ninter-thread communication. This matches both programmer intuition and hardware reality, but is\nnecessary to state explicitly when performing formal analysis.\nLitmus Test: Load Buffering\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\ny\n=\n0\n;\nT1\nT2\nA1\n:\nld\n.\nglobal\n.\nu32\n%\nr0\n,\n[\nx\n];\nB1\n:\nst\n.\nglobal\n.\nu32\n[\ny\n],\n%\nr0\n;\nA2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ny\n];\nB2\n:\nst\n.\nglobal\n.\nu32\n[\nx\n],\n%\nr1\n;\nFINAL\nSTATE\n:\nx\n==\n0\nAND\ny\n==\n0\nThe litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of\nthin air. Two threads T1 and T2 each read from a first variable and copy the observed result into a\nsecond variable, with the first and second variable exchanged between the threads. If each variable\nis initially zero, the final result shall also be zero. If A1 reads from B2 and A2 reads from B1,\nthen values passing through the memory operations in this example form a cycle:\nA1->B1->A2->B2->A1. Only the values x == 0 and y == 0 are allowed to satisfy this cycle. If any of\nthe memory operations in this example were to speculatively associate a different value with the\ncorresponding memory location, then such a speculation would become self-fulfilling, and hence\nforbidden.\n8.10.5.\nSequential Consistency Per Location\n\nWithin any set of\noverlapping\nmemory operations that are pairwise\nmorally strong\n,\ncommunication\norder\ncannot contradict\nprogram order\n, i.e., a concatenation of\nprogram order\nbetween\noverlapping\noperations and\nmorally strong\nrelations in\ncommunication order\ncannot result in a\ncycle. This ensures that each program slice of\noverlapping\npairwise morally\nstrong operations\nis\nstrictly\nsequentially-consistent\n.\nLitmus Test: CoRR\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n[\nx\n],\n1\n;\nR1\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr0\n,\n[\nx\n];\nR2\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr1\n,\n[\nx\n];\nIF\n%\nr0\n==\n1\nTHEN\n%\nr1\n==\n1\nThe litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee. A\nthread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence\nof) reads R1 and R2 on the same location x. No other writes are executed on x, except the one\nmodelling the initial value. The operations W1, R1 and R2 are pairwise\nmorally strong\n. If R1 reads\nfrom W1, then the subsequent read R2 must also observe the same value. If R2 observed the initial\nvalue of x instead, then this would form a sequence of\nmorally-strong\nrelations R2->W1->R1 in\ncommunication order\nthat contradicts the\nprogram order\nR1->R2 in thread T2. Hence R2 cannot read\nthe initial value of x in such an execution.\n8.10.6.\nCausality\n\nRelations in\ncommunication order\ncannot contradict\ncausality order\n. This constrains the set of\ncandidate write operations that a read operation may read from:\nIf a read R precedes an\noverlapping\nwrite W in\ncausality order\n, then R cannot read from W.\nIf a write W precedes an\noverlapping\nread R in\ncausality order\n, then for any byte accessed by\nboth R and W, R cannot read from any write W’ that precedes W in\ncoherence order\n.\nLitmus Test: Message Passing\n.\nglobal\n.\nu32\ndata\n=\n0\n;\n.\nglobal\n.\nu32\nflag\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nu32\n[\ndata\n],\n1\n;\nF1\n:\nfence\n.\nsys\n;\nW2\n:\nst\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n[\nflag\n],\n1\n;\nR1\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr0\n,\n[\nflag\n];\nF2\n:\nfence\n.\nsys\n;\nR2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ndata\n];\nIF\n%\nr0\n==\n1\nTHEN\n%\nr1\n==\n1\nThe litmus test known as “MP” (Message Passing) represents the essence of typical synchronization\nalgorithms. A vast majority of useful programs can be reduced to sequenced applications of this\npattern.\nThread T1 first writes to a data variable and then to a flag variable while a second thread T2 first\nreads from the flag variable and then from the data variable. The operations on the flag are\nmorally strong\nand the memory operations in each thread are separated by a\nfence\n, and these\nfences\nare\nmorally strong\n.\nIf R1 observes W2, then the release pattern “F1; W2”\nsynchronizes\nwith the\nacquire pattern\n“R1;\nF2”. This establishes the\ncausality order\nW1 -> F1 -> W2 -> R1 -> F2 -> R2. Then axiom\ncausality\nguarantees that R2 cannot read from any write that precedes W1 in\ncoherence order\n. In the absence\nof any other writes in this example, R2 must read from W1.\nLitmus Test: CoWR\n// These addresses are aliases\n.\nglobal\n.\nu32\ndata_alias_1\n;\n.\nglobal\n.\nu32\ndata_alias_2\n;\nT1\nW1\n:\nst\n.\nglobal\n.\nu32\n[\ndata_alias_1\n],\n1\n;\nF1\n:\nfence\n.\nproxy\n.\nalias\n;\nR1\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ndata_alias_2\n];\n%\nr1\n==\n1\nVirtual aliases require an alias\nproxy fence\nalong the synchronization path.\nLitmus Test: Store Buffering\nThe litmus test known as “SB” (Store Buffering) demonstrates the\nsequential consistency\nenforced\nby the\nfence.sc\n. A thread T1 writes to a first variable, and then reads the value of a second\nvariable, while a second thread T2 writes to the second variable and then reads the value of the\nfirst variable. The memory operations in each thread are separated by\nfence.\nsc instructions,\nand these\nfences\nare\nmorally strong\n.\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\ny\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nu32\n[\nx\n],\n1\n;\nF1\n:\nfence\n.\nsc\n.\nsys\n;\nR1\n:\nld\n.\nglobal\n.\nu32\n%\nr0\n,\n[\ny\n];\nW2\n:\nst\n.\nglobal\n.\nu32\n[\ny\n],\n1\n;\nF2\n:\nfence\n.\nsc\n.\nsys\n;\nR2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\nx\n];\n%\nr0\n==\n1\nOR\n%\nr1\n==\n1\nIn any execution, either F1 precedes F2 in\nFence-SC\norder, or vice versa. If F1 precedes F2 in\nFence-SC\norder, then F1\nsynchronizes\nwith F2. This establishes the\ncausality order\nin W1 -> F1\n-> F2 -> R2. Axiom\ncausality\nensures that R2 cannot read from any write that precedes W1 in\ncoherence order\n. In the absence of any other write to that variable, R2 must read from\nW1. Similarly, in the case where F2 precedes F1 in\nFence-SC\norder, R1 must read from W2. If each\nfence.sc\nin this example were replaced by a\nfence.acq_rel\ninstruction, then this outcome is\nnot guaranteed. There may be an execution where the write from each thread remains unobserved from\nthe other thread, i.e., an execution is possible, where both R1 and R2 return the initial value “0”\nfor variables y and x respectively.\n8.11.\nSpecial Cases\n\n8.11.1.\nReductions do not form Acquire Patterns\n\nAtomic reduction operations like\nred\ndo not form acquire patterns with acquire fences.\nLitmus Test: Message Passing with a Red Instruction\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\nflag\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nu32\n[\nx\n],\n42\n;\nW2\n:\nst\n.\nrelease\n.\ngpu\n.\nu32\n[\nflag\n],\n1\n;\nRMW1\n:\nred\n.\nsys\n.\nglobal\n.\nadd\n.\nu32\n[\nflag\n],\n1\n;\nF2\n:\nfence\n.\nacquire\n.\ngpu\n;\nR2\n:\nld\n.\nweak\n.\nu32\n%\nr1\n,\n[\nx\n];\n%\nr1\n==\n0\nAND\nflag\n==\n2\nThe litmus test known as “MP” (Message Passing) demonstrates the consequence\nof reductions being excluded from acquire patterns.\nIt is possible to observe the outcome where\nR2\nreads the value\n0\nfrom\nx\nand\nflag\nhas the final value of\n2\n.\nThis outcome is possible since the release pattern in\nT1\ndoes not synchronize\nwith any acquire pattern in\nT2\n.\nUsing the\natom\ninstruction instead of\nred\nforbids this outcome.\n9.\nInstruction Set\n\n9.1.\nFormat and Semantics of Instruction Descriptions\n\nThis section describes each PTX instruction. In addition to the name and the format of the\ninstruction, the semantics are described, followed by some examples that attempt to show several\npossible instantiations of the instruction.\n9.2.\nPTX Instructions\n\nPTX instructions generally have from zero to four operands, plus an optional guard predicate\nappearing after an\n@\nsymbol to the left of the\nopcode\n:\n@p\nopcode;\n@p\nopcode\na;\n@p\nopcode\nd,\na;\n@p\nopcode\nd,\na,\nb;\n@p\nopcode\nd,\na,\nb,\nc;\nFor instructions that create a result value, the\nd\noperand is the destination operand, while\na\n,\nb\n, and\nc\nare source operands.\nThe\nsetp\ninstruction writes two destination registers. We use a\n|\nsymbol to separate\nmultiple destination registers.\nsetp.lt.s32  p|q, a, b;  // p = (a < b); q = !(a < b);\nFor some instructions the destination operand is optional. A\nbit bucket\noperand denoted with an\nunderscore (\n_\n) may be used in place of a destination register.\n9.3.\nPredicated Execution\n\nIn PTX, predicate registers are virtual and have\n.pred\nas the type specifier. So, predicate\nregisters can be declared as\n.reg .pred p, q, r;\nAll instructions have an optional\nguard predicate\nwhich controls conditional execution of the\ninstruction. The syntax to specify conditional execution is to prefix an instruction with\n@{!}p\n,\nwhere\np\nis a predicate variable, optionally negated. Instructions without a guard predicate are\nexecuted unconditionally.\nPredicates are most commonly set as the result of a comparison performed by the\nsetp\ninstruction.\nAs an example, consider the high-level code\nif (i < n)\n    j = j + 1;\nThis can be written in PTX as\nsetp.lt.s32  p, i, n;    // p = (i < n)\n@p    add.s32      j, j, 1;    // if i < n, add 1 to j\nTo get a conditional branch or conditional function call, use a predicate to control the execution\nof the branch or call instructions. To implement the above example as a true conditional branch, the\nfollowing PTX instruction sequence might be used:\nsetp.lt.s32  p, i, n;    // compare i to n\n@!p   bra  L1;                 // if False, branch over\n      add.s32      j, j, 1;\nL1:     ...\n9.3.1.\nComparisons\n\n9.3.1.1.\nInteger and Bit-Size Comparisons\n\nThe signed integer comparisons are the traditional\neq\n(equal),\nne\n(not-equal),\nlt\n(less-than),\nle\n(less-than-or-equal),\ngt\n(greater-than), and\nge\n(greater-than-or-equal). The unsigned comparisons are\neq\n,\nne\n,\nlo\n(lower),\nls\n(lower-or-same),\nhi\n(higher), and\nhs\n(higher-or-same). The bit-size comparisons are\neq\nand\nne\n; ordering comparisons are not defined for bit-size types.\nTable 22\nshows the operators for signed integer, unsigned integer, and bit-size types.\nTable 22\nOperators for Signed Integer, Unsigned Integer, and Bit-Size Types\n\nMeaning\nSigned Operator\nUnsigned Operator\nBit-Size Operator\na\n==\nb\neq\neq\neq\na\n!=\nb\nne\nne\nne\na\n<\nb\nlt\nlo\nn/a\na\n<=\nb\nle\nls\nn/a\na\n>\nb\ngt\nhi\nn/a\na\n>=\nb\nge\nhs\nn/a\n9.3.1.2.\nFloating Point Comparisons\n\nThe ordered floating-point comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n, and\nge\n. If\neither operand is\nNaN\n, the result is\nFalse\n.\nTable 23\nlists the floating-point\ncomparison operators.\nTable 23\nFloating-Point Comparison Operators\n\nMeaning\nFloating-Point Operator\na\n==\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\neq\na\n!=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nne\na\n<\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nlt\na\n<=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nle\na\n>\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\ngt\na\n>=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nge\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered floating-point comparisons\nare provided:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n, and\ngeu\n. If both operands are numeric\nvalues (not\nNaN\n), then the comparison has the same result as its ordered counterpart. If either\noperand is\nNaN\n, then the result of the comparison is\nTrue\n.\nTable 24\nlists the floating-point\ncomparison operators accepting\nNaN\nvalues.\nTable 24\nFloating-Point Comparison Operators Accepting NaN\n\nMeaning\nFloating-Point Operator\na\n==\nb\n||\nisNaN(a)\n||\nisNaN(b)\nequ\na\n!=\nb\n||\nisNaN(a)\n||\nisNaN(b)\nneu\na\n<\nb\n||\nisNaN(a)\n||\nisNaN(b)\nltu\na\n<=\nb\n||\nisNaN(a)\n||\nisNaN(b)\nleu\na\n>\nb\n||\nisNaN(a)\n||\nisNaN(b)\ngtu\na\n>=\nb\n||\nisNaN(a)\n||\nisNaN(b)\ngeu\nTo test for\nNaN\nvalues, two operators\nnum\n(\nnumeric\n) and\nnan\n(\nisNaN\n) are\nprovided.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nTable 25\nlists the\nfloating-point comparison operators testing for\nNaN\nvalues.\nTable 25\nFloating-Point Comparison Operators Testing for NaN\n\nMeaning\nFloating-Point Operator\n!isNaN(a)\n&&\n!isNaN(b)\nnum\nisNaN(a)\n||\nisNaN(b)\nnan\n9.3.2.\nManipulating Predicates\n\nPredicate values may be computed and manipulated using the following instructions:\nand\n,\nor\n,\nxor\n,\nnot\n, and\nmov\n.\nThere is no direct conversion between predicates and integer values, and no direct way to load or\nstore predicate register values. However,\nsetp\ncan be used to generate a predicate from an\ninteger, and the predicate-based select (\nselp\n) instruction can be used to generate an integer\nvalue based on the value of a predicate; for example:\nselp.u32 %r1,1,0,%p;    // convert predicate to 32-bit value\n9.4.\nType Information for Instructions and Operands\n\nTyped instructions must have a type-size modifier. For example, the\nadd\ninstruction requires\ntype and size information to properly perform the addition operation (signed, unsigned, float,\ndifferent sizes), and this information must be specified as a suffix to the opcode.\nExample\n.reg .u16 d, a, b;\n\nadd.u16 d, a, b;    // perform a 16-bit unsigned add\nSome instructions require multiple type-size modifiers, most notably the data conversion instruction\ncvt\n. It requires separate type-size modifiers for the result and source, and these are placed in\nthe same order as the operands. For example:\n.reg .u16 a;\n.reg .f32 d;\n\ncvt.f32.u16 d, a;   // convert 16-bit unsigned to 32-bit float\nIn general, an operand’s type must agree with the corresponding instruction-type modifier. The rules\nfor operand and instruction type conformance are as follows:\nBit-size types agree with any type of the same size.\nSigned and unsigned integer types agree provided they have the same size, and integer operands are\nsilently cast to the instruction type if needed. For example, an unsigned integer operand used in\na signed integer instruction will be treated as a signed integer by the instruction.\nFloating-point types agree only if they have the same size; i.e., they must match exactly.\nTable 26\nsummarizes these type\nchecking rules.\nTable 26\nType Checking Rules\n\nOperand Type\n.bX\n.sX\n.uX\n.fX\nInstruction Type\n.bX\nokay\nokay\nokay\nokay\n.sX\nokay\nokay\nokay\ninvalid\n.uX\nokay\nokay\nokay\ninvalid\n.fX\nokay\ninvalid\ninvalid\nokay\nNote\nSome operands have their type and size defined independently from the instruction type-size. For\nexample, the shift amount operand for left and right shift instructions always has type\n.u32\n,\nwhile the remaining operands have their type and size determined by the instruction type.\nExample\n// 64-bit arithmetic right shift; shift amount 'b' is .u32\n    shr.s64 d,a,b;\n9.4.1.\nOperand Size Exceeding Instruction-Type Size\n\nFor convenience,\nld\n,\nst\n, and\ncvt\ninstructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes. The operand type checking rules are relaxed for bit-size and integer (signed and unsigned)\ninstruction types; floating-point instruction types still require that the operand type-size matches\nexactly, unless the operand is of bit-size type.\nWhen a source operand has a size that exceeds the instruction-type size, the source data is\ntruncated (chopped) to the appropriate number of bits specified by the instruction type-size.\nTable 27\nsummarizes the relaxed type-checking rules for source operands. Note that some combinations may\nstill be invalid for a particular instruction; for example, the\ncvt\ninstruction does not support\n.bX\ninstruction types, so those rows are invalid for\ncvt\n.\nTable 27\nRelaxed Type-checking Rules for Source Operands\n\nSource Operand Type\nb8\nb16\nb32\nb64\nb128\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nInstruction Type\nb8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\nchop\nchop\nchop\nb16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\n–\nchop\nchop\nb32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\n–\nchop\nb64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\n–\nb128\ninv\ninv\ninv\ninv\n–\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ns8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\ninv\ninv\ninv\ns16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\ninv\ninv\ninv\ns32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\ninv\ninv\ns64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nu8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\ninv\ninv\ninv\nu16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\ninv\ninv\ninv\nu32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\ninv\ninv\nu64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nf16\ninv\n–\nchop\nchop\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\ninv\nf32\ninv\ninv\n–\nchop\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\nf64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\nNotes\nchop = keep only low bits that fit; “–” = allowed, but no conversion needed;\ninv = invalid, parse error.\nSource register size must be of equal or greater size than the instruction-type size.\nBit-size source registers may be used with any appropriately-sized instruction type. The data are\ntruncated (“chopped”) to the instruction-type size and interpreted according to the instruction\ntype.\nInteger source registers may be used with any appropriately-sized bit-size or integer instruction\ntype. The data are truncated to the instruction-type size and interpreted according to the\ninstruction type.\nFloating-point source registers can only be used with bit-size or floating-point instruction types.\nWhen used with a narrower bit-size instruction type, the data are truncated. When used with a\nfloating-point instruction type, the size must match exactly.\nWhen a destination operand has a size that exceeds the instruction-type size, the destination data\nis zero- or sign-extended to the size of the destination register. If the corresponding instruction\ntype is signed integer, the data is sign-extended; otherwise, the data is zero-extended.\nTable 28\nsummarizes the relaxed type-checking rules for destination operands.\nTable 28\nRelaxed Type-checking Rules for Destination Operands\n\nDestination Operand Type\nb8\nb16\nb32\nb64\nb128\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nInstruction Type\nb8\n–\nzext\nzext\nzext\nzext\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\nzext\nzext\nzext\nb16\ninv\n–\nzext\nzext\nzext\ninv\n–\nzext\nzext\ninv\n–\nzext\nzext\n–\nzext\nzext\nb32\ninv\ninv\n–\nzext\nzext\ninv\ninv\n–\nzext\ninv\ninv\n–\nzext\ninv\n–\nzext\nb64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\n–\nb128\ninv\ninv\ninv\ninv\n–\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ns8\n–\nsext\nsext\nsext\nsext\n–\nsext\nsext\nsext\n–\nsext\nsext\nsext\ninv\ninv\ninv\ns16\ninv\n–\nsext\nsext\nsext\ninv\n–\nsext\nsext\ninv\n–\nsext\nsext\ninv\ninv\ninv\ns32\ninv\ninv\n–\nsext\nsext\ninv\ninv\n–\nsext\ninv\ninv\n–\nsext\ninv\ninv\ninv\ns64\ninv\ninv\ninv\n–\nsext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nu8\n–\nzext\nzext\nzext\nzext\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\ninv\ninv\ninv\nu16\ninv\n–\nzext\nzext\nzext\ninv\n–\nzext\nzext\ninv\n–\nzext\nzext\ninv\ninv\ninv\nu32\ninv\ninv\n–\nzext\nzext\ninv\ninv\n–\nzext\ninv\ninv\n–\nzext\ninv\ninv\ninv\nu64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nf16\ninv\n–\nzext\nzext\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\ninv\nf32\ninv\ninv\n–\nzext\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\nf64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\nNotes\nsext = sign-extend; zext = zero-extend; “–” = allowed, but no conversion needed;\ninv = invalid, parse error.\nDestination register size must be of equal or greater size than the instruction-type size.\nBit-size destination registers may be used with any appropriately-sized instruction type. The data\nare sign-extended to the destination register width for signed integer instruction types, and are\nzero-extended to the destination register width otherwise.\nInteger destination registers may be used with any appropriately-sized bit-size or integer\ninstruction type. The data are sign-extended to the destination register width for signed integer\ninstruction types, and are zero-extended to the destination register width for bit-size an d\nunsigned integer instruction types.\nFloating-point destination registers can only be used with bit-size or floating-point instruction\ntypes. When used with a narrower bit-size instruction type, the data are zero-extended. When used\nwith a floating-point instruction type, the size must match exactly.\n9.5.\nDivergence of Threads in Control Constructs\n\nThreads in a CTA execute together, at least in appearance, until they come to a conditional control\nconstruct such as a conditional branch, conditional function call, or conditional return. If threads\nexecute down different control flow paths, the threads are called\ndivergent\n. If all of the threads\nact in unison and follow a single control flow path, the threads are called\nuniform\n. Both\nsituations occur often in programs.\nA CTA with divergent threads may have lower performance than a CTA with uniformly executing threads,\nso it is important to have divergent threads re-converge as soon as possible. All control constructs\nare assumed to be divergent points unless the control-flow instruction is marked as uniform, using\nthe\n.uni\nsuffix. For divergent control flow, the optimizing code generator automatically\ndetermines points of re-convergence. Therefore, a compiler or code author targeting PTX can ignore\nthe issue of divergent threads, but has the opportunity to improve performance by marking branch\npoints as uniform when the compiler or author can guarantee that the branch point is non-divergent.\n9.6.\nSemantics\n\nThe goal of the semantic description of an instruction is to describe the results in all cases in as\nsimple language as possible. The semantics are described using C, until C is not expressive enough.\n9.6.1.\nMachine-Specific Semantics of 16-bit Code\n\nA PTX program may execute on a GPU with either a 16-bit or a 32-bit data path. When executing on a\n32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit\ncomputations are\npromoted\nto 32-bit computations. This can lead to computational differences\nbetween code run on a 16-bit machine versus the same code run on a 32-bit machine, since the\npromoted computation may have bits in the high-order half-word of registers that are not present in\n16-bit physical registers. These extra precision bits can become visible at the application level,\nfor example, by a right-shift instruction.\nAt the PTX language level, one solution would be to define semantics for 16-bit code that is\nconsistent with execution on a 16-bit data path. This approach introduces a performance penalty for\n16-bit code executing on a 32-bit data path, since the translated code would require many additional\nmasking instructions to suppress extra precision bits in the high-order half-word of 32-bit\nregisters.\nRather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of\n16-bit instructions in PTX is machine-specific. A compiler or programmer may chose to enforce\nportable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at\nappropriate points in the program to guarantee portability of the code. However, for many\nperformance-critical applications, this is not desirable, and for many applications the difference\nin execution is preferable to limiting performance.\n9.7.\nInstructions\n\nAll PTX instructions may be predicated. In the following descriptions, the optional guard predicate\nis omitted from the syntax.\n9.7.1.\nInteger Arithmetic Instructions\n\nInteger arithmetic instructions operate on the integer types in register and constant immediate\nforms. The integer arithmetic instructions are:\nadd\nsub\nmul\nmad\nmul24\nmad24\nsad\ndiv\nrem\nabs\nneg\nmin\nmax\npopc\nclz\nbfind\nfns\nbrev\nbfe\nbfi\nbmsk\nszext\ndp4a\ndp2a\n9.7.1.1.\nInteger Arithmetic Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd.type       d, a, b;\nadd{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64,\n          .u16x2, .s16x2 };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce\n.u16x2\n,\n.s16x2\nresult in\ndestination.\nOperands\nd\n,\na\nand\nb\nhave type\n.type\n. For instruction types\n.u16x2\n,\n.s16x2\n,\noperands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = iA[i] + iB[i];\n    }\n} else {\n    d = a + b;\n}\nNotes\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation. Applies only to\n.s32\ntype.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nadd.u16x2\nand\nadd.s16x2\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nadd.u16x2\nand\nadd.s16x2\nrequire\nsm_90\nor higher.\nExamples\n@p  add.u32     x,y,z;\n    add.sat.s32 c,c,1;\n    add.u16x2   u,v,w;\n9.7.1.2.\nInteger Arithmetic Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub.type       d, a, b;\nsub{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nSemantics\nd = a - b;\nNotes\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation. Applies only to\n.s32\ntype.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsub.s32 c,a,b;\n9.7.1.3.\nInteger Arithmetic Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul.mode.type  d, a, b;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nCompute the product of two values.\nSemantics\nt = a * b;\nn = bitwidth of type;\nd = t;            // for .wide\nd = t<2n-1..n>;   // for .hi variant\nd = t<n-1..0>;    // for .lo variant\nNotes\nThe type of the operation represents the types of the\na\nand\nb\noperands. If\n.hi\nor\n.lo\nis specified, then\nd\nis the same size as\na\nand\nb\n, and either the upper or lower\nhalf of the result is written to the destination register. If\n.wide\nis specified, then\nd\nis\ntwice as wide as\na\nand\nb\nto receive the full result of the multiplication.\nThe\n.wide\nsuffix is supported only for 16- and 32-bit integer types.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmul.wide.s16 fa,fxs,fys;   // 16*16 bits yields 32 bits\nmul.lo.s16 fa,fxs,fys;     // 16*16 bits, save only the low 16 bits\nmul.wide.s32 z,x,y;        // 32*32 bits, creates 64 bit result\n9.7.1.4.\nInteger Arithmetic Instructions:\nmad\n\nmad\nMultiply two values, optionally extract the high or low half of the intermediate result, and add a third value.\nSyntax\nmad.mode.type  d, a, b, c;\nmad.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nMultiplies two values, optionally extracts the high or low half of the intermediate result, and adds\na third value. Writes the result into a destination register.\nSemantics\nt = a * b;\nn = bitwidth of type;\nd = t + c;           // for .wide\nd = t<2n-1..n> + c;  // for .hi variant\nd = t<n-1..0> + c;   // for .lo variant\nNotes\nThe type of the operation represents the types of the\na\nand\nb\noperands. If .hi or .lo is\nspecified, then\nd\nand\nc\nare the same size as\na\nand\nb\n, and either the upper or lower\nhalf of the result is written to the destination register. If\n.wide\nis specified, then\nd\nand\nc\nare twice as wide as\na\nand\nb\nto receive the result of the multiplication.\nThe\n.wide\nsuffix is supported only for 16-bit and 32-bit integer types.\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation.\nApplies only to\n.s32\ntype in\n.hi\nmode.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n@p  mad.lo.s32 d,a,b,c;\n    mad.lo.s32 r,p,q,r;\n9.7.1.5.\nInteger Arithmetic Instructions:\nmul24\n\nmul24\nMultiply two 24-bit integer values.\nSyntax\nmul24.mode.type  d, a, b;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 };\nDescription\nCompute the product of two 24-bit integer values held in 32-bit source registers, and return either\nthe high or low 32-bits of the 48-bit result.\nSemantics\nt = a * b;\nd = t<47..16>;    // for .hi variant\nd = t<31..0>;     // for .lo variant\nNotes\nInteger multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.\nmul24.hi\nperforms a 24x24-bit multiply and returns the high 32 bits of the 48-bit result.\nmul24.lo\nperforms a 24x24-bit multiply and returns the low 32 bits of the 48-bit result.\nAll operands are of the same type and size.\nmul24.hi\nmay be less efficient on machines without hardware support for 24-bit multiply.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmul24.lo.s32 d,a,b;   // low 32-bits of 24x24-bit signed multiply.\n9.7.1.6.\nInteger Arithmetic Instructions:\nmad24\n\nmad24\nMultiply two 24-bit integer values and add a third value.\nSyntax\nmad24.mode.type  d, a, b, c;\nmad24.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 };\nDescription\nCompute the product of two 24-bit integer values held in 32-bit source registers, and add a third,\n32-bit value to either the high or low 32-bits of the 48-bit result. Return either the high or low\n32-bits of the 48-bit result.\nSemantics\nt = a * b;\nd = t<47..16> + c;   // for .hi variant\nd = t<31..0> + c;    // for .lo variant\nNotes\nInteger multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.\nmad24.hi\nperforms a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third\nvalue.\nmad24.lo\nperforms a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third\nvalue.\nAll operands are of the same type and size.\nSaturation modifier:\n.sat\nlimits result of 32-bit signed addition to\nMININT..MAXINT\n(no overflow). Applies only to\n.s32\ntype in .hi mode.\nmad24.hi\nmay be less efficient on machines without hardware support for 24-bit multiply.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmad24.lo.s32 d,a,b,c;   // low 32-bits of 24x24-bit signed multiply.\n9.7.1.7.\nInteger Arithmetic Instructions:\nsad\n\nsad\nSum of absolute differences.\nSyntax\nsad.type  d, a, b, c;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nAdds the absolute value of\na-b\nto\nc\nand writes the resulting value into\nd\n.\nSemantics\nd = c + ((a<b) ? b-a : a-b);\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsad.s32  d,a,b,c;\nsad.u32  d,a,b,d;  // running sum\n9.7.1.8.\nInteger Arithmetic Instructions:\ndiv\n\ndiv\nDivide one value by another.\nSyntax\ndiv.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nDivides\na\nby\nb\n, stores result in\nd\n.\nSemantics\nd = a / b;\nNotes\nDivision by zero yields an unspecified, machine-specific value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ndiv.s32  b,n,i;\n9.7.1.9.\nInteger Arithmetic Instructions:\nrem\n\nrem\nThe remainder of integer division.\nSyntax\nrem.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nDivides\na\nby\nb\n, store the remainder in\nd\n.\nSemantics\nd = a % b;\nNotes\nThe behavior for negative numbers is machine-dependent and depends on whether divide rounds towards\nzero or negative infinity.\nDivision by zero yields an unspecified, machine-specific value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nrem.s32  x,x,8;    // x = x%8;\n9.7.1.10.\nInteger Arithmetic Instructions:\nabs\n\nabs\nAbsolute value.\nSyntax\nabs.type  d, a;\n\n.type = { .s16, .s32, .s64 };\nDescription\nTake the absolute value of\na\nand store it in\nd\n.\nSemantics\nd = |a|;\nNotes\nOnly for signed integers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nabs.s32  r0,a;\n9.7.1.11.\nInteger Arithmetic Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg.type  d, a;\n\n.type = { .s16, .s32, .s64 };\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nSemantics\nd = -a;\nNotes\nOnly for signed integers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nneg.s32  r0,a;\n9.7.1.12.\nInteger Arithmetic Instructions:\nmin\n\nmin\nFind the minimum of two values.\nSyntax\nmin.atype         d, a, b;\nmin{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 };\nDescription\nStore the minimum of\na\nand\nb\nin\nd\n.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce\n.u16x2\n,\n.s16x2\nresult\nin destination.\nOperands\nd\n,\na\nand\nb\nhave the same type as the instruction type. For instruction types\n.u16x2\n,\n.s16x2\n, operands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] < iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a < b) ? a : b; // Integer (signed and unsigned)\n}\nNotes\nSigned and unsigned differ.\nSaturation modifier:\nmin.relu.{s16x2,\ns32}\nclamps the result to 0 if negative.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmin.u16x2\n,\nmin{.relu}.s16x2\nand\nmin.relu.s32\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nmin.u16x2\n,\nmin{.relu}.s16x2\nand\nmin.relu.s32\nrequire\nsm_90\nor higher.\nExamples\nmin.s32  r0,a,b;\n@p  min.u16  h,i,j;\n    min.s16x2.relu u,v,w;\n9.7.1.13.\nInteger Arithmetic Instructions:\nmax\n\nmax\nFind the maximum of two values.\nSyntax\nmax.atype         d, a, b;\nmax{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 };\nDescription\nStore the maximum of\na\nand\nb\nin\nd\n.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce\n.u16x2\n,\n.s16x2\nresult\nin destination.\nOperands\nd\n,\na\nand\nb\nhave the same type as the instruction type. For instruction types\n.u16x2\n,\n.s16x2\n, operands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] > iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a > b) ? a : b; // Integer (signed and unsigned)\n}\nNotes\nSigned and unsigned differ.\nSaturation modifier:\nmax.relu.{s16x2,\ns32}\nclamps the result to 0 if negative.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmax.u16x2\n,\nmax{.relu}.s16x2\nand\nmax.relu.s32\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nmax.u16x2\n,\nmax{.relu}.s16x2\nand\nmax.relu.s32\nrequire\nsm_90\nor higher.\nExamples\nmax.u32  d,a,b;\nmax.s32  q,q,0;\nmax.relu.s16x2 t,t,u;\n9.7.1.14.\nInteger Arithmetic Instructions:\npopc\n\npopc\nPopulation count.\nSyntax\npopc.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nCount the number of one bits in\na\nand place the resulting\npopulation count\nin 32-bit\ndestination register\nd\n. Operand\na\nhas the instruction type and destination\nd\nhas type\n.u32\n.\nSemantics\n.u32  d = 0;\nwhile (a != 0) {\n   if (a & 0x1)  d++;\n   a = a >> 1;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\npopc\nrequires\nsm_20\nor higher.\nExamples\npopc.b32  d, a;\npopc.b64  cnt, X;  // cnt is .u32\n9.7.1.15.\nInteger Arithmetic Instructions:\nclz\n\nclz\nCount leading zeros.\nSyntax\nclz.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nCount the number of leading zeros in\na\nstarting with the most-significant bit and place the\nresult in 32-bit destination register\nd\n. Operand\na\nhas the instruction type, and destination\nd\nhas type\n.u32\n. For\n.b32\ntype, the number of leading zeros is between 0 and 32,\ninclusively. For\n.b64\ntype, the number of leading zeros is between 0 and 64, inclusively.\nSemantics\n.u32  d = 0;\nif (.type == .b32)   { max = 32; mask = 0x80000000; }\nelse                 { max = 64; mask = 0x8000000000000000; }\n\nwhile (d < max && (a&mask == 0) ) {\n    d++;\n    a = a << 1;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nclz\nrequires\nsm_20\nor higher.\nExamples\nclz.b32  d, a;\nclz.b64  cnt, X;  // cnt is .u32\n9.7.1.16.\nInteger Arithmetic Instructions:\nbfind\n\nbfind\nFind most significant non-sign bit.\nSyntax\nbfind.type           d, a;\nbfind.shiftamt.type  d, a;\n\n.type = { .u32, .u64,\n          .s32, .s64 };\nDescription\nFind the bit position of the most significant non-sign bit in\na\nand place the result in\nd\n. Operand\na\nhas the instruction type, and destination\nd\nhas type\n.u32\n. For unsigned\nintegers,\nbfind\nreturns the bit position of the most significant\n1\n. For signed integers,\nbfind\nreturns the bit position of the most significant\n0\nfor negative inputs and the most\nsignificant\n1\nfor non-negative inputs.\nIf\n.shiftamt\nis specified,\nbfind\nreturns the shift amount needed to left-shift the found bit\ninto the most-significant bit position.\nbfind\nreturns\n0xffffffff\nif no non-sign bit is found.\nSemantics\nmsb = (.type==.u32 || .type==.s32) ? 31 : 63;\n// negate negative signed inputs\nif ( (.type==.s32 || .type==.s64) && (a & (1<<msb)) ) {\n    a = ~a;\n}\n.u32  d = 0xffffffff;\nfor (.s32 i=msb; i>=0; i--) {\n    if (a & (1<<i))  { d = i; break; }\n}\nif (.shiftamt && d != 0xffffffff)  { d = msb - d; }\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfind\nrequires\nsm_20\nor higher.\nExamples\nbfind.u32  d, a;\nbfind.shiftamt.s64  cnt, X;  // cnt is .u32\n9.7.1.17.\nInteger Arithmetic Instructions:\nfns\n\nfns\nFind the n-th set bit\nSyntax\nfns.b32 d, mask, base, offset;\nDescription\nGiven a 32-bit value\nmask\nand an integer value\nbase\n(between 0 and 31), find the n-th (given\nby offset) set bit in\nmask\nfrom the\nbase\nbit, and store the bit position in\nd\n. If not\nfound, store 0xffffffff in\nd\n.\nOperand\nmask\nhas a 32-bit type. Operand\nbase\nhas\n.b32\n,\n.u32\nor\n.s32\ntype. Operand offset has\n.s32\ntype. Destination\nd\nhas type\n.b32.\nOperand\nbase\nmust be <= 31, otherwise behavior is undefined.\nSemantics\nd = 0xffffffff;\nif (offset == 0) {\n    if (mask[base] == 1) {\n        d = base;\n    }\n} else {\n    pos = base;\n    count = |offset| - 1;\n    inc = (offset > 0) ? 1 : -1;\n\n    while ((pos >= 0) && (pos < 32)) {\n        if (mask[pos] == 1) {\n            if (count == 0) {\n              d = pos;\n              break;\n           } else {\n               count = count - 1;\n           }\n        }\n        pos = pos + inc;\n    }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nfns\nrequires\nsm_30\nor higher.\nExamples\nfns.b32 d, 0xaaaaaaaa, 3, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 3, -1;  // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, -1;  // d = 1\n9.7.1.18.\nInteger Arithmetic Instructions:\nbrev\n\nbrev\nBit reverse.\nSyntax\nbrev.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nPerform bitwise reversal of input.\nSemantics\nmsb = (.type==.b32) ? 31 : 63;\n\nfor (i=0; i<=msb; i++) {\n    d[i] = a[msb-i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbrev\nrequires\nsm_20\nor higher.\nExamples\nbrev.b32  d, a;\n9.7.1.19.\nInteger Arithmetic Instructions:\nbfe\n\nbfe\nBit Field Extract.\nSyntax\nbfe.type  d, a, b, c;\n\n.type = { .u32, .u64,\n          .s32, .s64 };\nDescription\nExtract bit field from\na\nand place the zero or sign-extended result in\nd\n. Source\nb\ngives\nthe bit field starting bit position, and source\nc\ngives the bit field length in bits.\nOperands\na\nand\nd\nhave the same type as the instruction type. Operands\nb\nand\nc\nare\ntype\n.u32\n, but are restricted to the 8-bit value range\n0..255\n.\nThe sign bit of the extracted field is defined as:\n.u32\n,\n.u64\n:\nzero\n.s32\n,\n.s64\n:\nmsb\nof input a if the extracted field extends beyond the\nmsb\nof a\nmsb\nof extracted\nfield, otherwise\nIf the bit field length is zero, the result is zero.\nThe destination\nd\nis padded with the sign bit of the extracted field. If the start position is\nbeyond the\nmsb\nof the input, the destination\nd\nis filled with the replicated sign bit of the\nextracted field.\nSemantics\nmsb = (.type==.u32 || .type==.s32) ? 31 : 63;\npos = b & 0xff;  // pos restricted to 0..255 range\nlen = c & 0xff;  // len restricted to 0..255 range\n\nif (.type==.u32 || .type==.u64 || len==0)\n    sbit = 0;\nelse\n    sbit = a[min(pos+len-1,msb)];\n\nd = 0;\nfor (i=0; i<=msb; i++) {\n    d[i] = (i<len && pos+i<=msb) ? a[pos+i] : sbit;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfe\nrequires\nsm_20\nor higher.\nExamples\nbfe.b32  d,a,start,len;\n9.7.1.20.\nInteger Arithmetic Instructions:\nbfi\n\nbfi\nBit Field Insert.\nSyntax\nbfi.type  f, a, b, c, d;\n\n.type = { .b32, .b64 };\nDescription\nAlign and insert a bit field from\na\ninto\nb\n, and place the result in\nf\n. Source\nc\ngives the starting bit position for the insertion, and source\nd\ngives the bit field length in\nbits.\nOperands\na\n,\nb\n, and\nf\nhave the same type as the instruction type. Operands\nc\nand\nd\nare type\n.u32\n, but are restricted to the 8-bit value range\n0..255\n.\nIf the bit field length is zero, the result is\nb\n.\nIf the start position is beyond the msb of the input, the result is\nb\n.\nSemantics\nmsb = (.type==.b32) ? 31 : 63;\npos = c & 0xff;  // pos restricted to 0..255 range\nlen = d & 0xff;  // len restricted to 0..255 range\n\nf = b;\nfor (i=0; i<len && pos+i<=msb; i++) {\n    f[pos+i] = a[i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfi\nrequires\nsm_20\nor higher.\nExamples\nbfi.b32  d,a,b,start,len;\n9.7.1.21.\nInteger Arithmetic Instructions:\nszext\n\nszext\nSign-extend or Zero-extend.\nSyntax\nszext.mode.type  d, a, b;\n\n.mode = { .clamp, .wrap };\n.type = { .u32, .s32 };\nDescription\nSign-extends or zero-extends an N-bit value from operand\na\nwhere N is specified in operand\nb\n. The resulting value is stored in the destination operand\nd\n.\nFor the\n.s32\ninstruction type, the value in\na\nis treated as an N-bit signed value and the\nmost significant bit of this N-bit value is replicated up to bit 31. For the\n.u32\ninstruction\ntype, the value in\na\nis treated as an N-bit unsigned number and is zero-extended to 32\nbits. Operand\nb\nis an unsigned 32-bit value.\nIf the value of N is 0, then the result of\nszext\nis 0. If the value of N is 32 or higher, then\nthe result of\nszext\ndepends upon the value of the\n.mode\nqualifier as follows:\nIf\n.mode\nis\n.clamp\n, then the result is the same as the source operand\na\n.\nIf\n.mode\nis\n.wrap\n, then the result is computed using the wrapped value of N.\nSemantics\nb1        = b & 0x1f;\ntoo_large = (b >= 32 && .mode == .clamp) ? true : false;\nmask      = too_large ? 0 : (~0) << b1;\nsign_pos  = (b1 - 1) & 0x1f;\n\nif (b1 == 0 || too_large || .type != .s32) {\n    sign_bit = false;\n} else {\n    sign_bit = (a >> sign_pos) & 1;\n}\nd = (a & ~mask) | (sign_bit ? mask | 0);\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nszext\nrequires\nsm_70\nor higher.\nExamples\nszext.clamp.s32 rd, ra, rb;\nszext.wrap.u32  rd, 0xffffffff, 0; // Result is 0.\n9.7.1.22.\nInteger Arithmetic Instructions:\nbmsk\n\nbmsk\nBit Field Mask.\nSyntax\nbmsk.mode.b32  d, a, b;\n\n.mode = { .clamp, .wrap };\nDescription\nGenerates a 32-bit mask starting from the bit position specified in operand\na\n, and of the width\nspecified in operand\nb\n. The generated bitmask is stored in the destination operand\nd\n.\nThe resulting bitmask is 0 in the following cases:\nWhen the value of\na\nis 32 or higher and\n.mode\nis\n.clamp\n.\nWhen either the specified value of\nb\nor the wrapped value of\nb\n(when\n.mode\nis\nspecified as\n.wrap\n) is 0.\nSemantics\na1    = a & 0x1f;\nmask0 = (~0) << a1;\nb1    = b & 0x1f;\nsum   = a1 + b1;\nmask1 = (~0) << sum;\n\nsum-overflow          = sum >= 32 ? true : false;\nbit-position-overflow = false;\nbit-width-overflow    = false;\n\nif (.mode == .clamp) {\n    if (a >= 32) {\n        bit-position-overflow = true;\n        mask0 = 0;\n    }\n    if (b >= 32) {\n        bit-width-overflow = true;\n    }\n}\n\nif (sum-overflow || bit-position-overflow || bit-width-overflow) {\n    mask1 = 0;\n} else if (b1 == 0) {\n    mask1 = ~0;\n}\nd = mask0 & ~mask1;\nNotes\nThe bitmask width specified by operand\nb\nis limited to range\n0..32\nin\n.clamp\nmode and to\nrange\n0..31\nin\n.wrap\nmode.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nbmsk\nrequires\nsm_70\nor higher.\nExamples\nbmsk.clamp.b32  rd, ra, rb;\nbmsk.wrap.b32   rd, 1, 2; // Creates a bitmask of 0x00000006.\n9.7.1.23.\nInteger Arithmetic Instructions:\ndp4a\n\ndp4a\nFour-way byte dot product-accumulate.\nSyntax\ndp4a.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\nDescription\nFour-way byte dot product which is accumulated in 32-bit result.\nOperand\na\nand\nb\nare 32-bit inputs which hold 4 byte inputs in packed form for dot product.\nOperand\nc\nhas type\n.u32\nif both\n.atype\nand\n.btype\nare\n.u32\nelse operand\nc\nhas type\n.s32\n.\nSemantics\nd = c;\n\n// Extract 4 bytes from a 32bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_4(a, .atype);\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nfor (i = 0; i < 4; ++i) {\n    d += Va[i] * Vb[i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\nRequires\nsm_61\nor higher.\nExamples\ndp4a.u32.u32           d0, a0, b0, c0;\ndp4a.u32.s32           d1, a1, b1, c1;\n9.7.1.24.\nInteger Arithmetic Instructions:\ndp2a\n\ndp2a\nTwo-way dot product-accumulate.\nSyntax\ndp2a.mode.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\n.mode = { .lo, .hi };\nDescription\nTwo-way 16-bit to 8-bit dot product which is accumulated in 32-bit result.\nOperand\na\nand\nb\nare 32-bit inputs. Operand\na\nholds two 16-bits inputs in packed form and\noperand\nb\nholds 4 byte inputs in packed form for dot product.\nDepending on the\n.mode\nspecified, either lower half or upper half of operand\nb\nwill be used\nfor dot product.\nOperand\nc\nhas type\n.u32\nif both\n.atype\nand\n.btype\nare\n.u32\nelse operand\nc\nhas type\n.s32\n.\nSemantics\nd = c;\n// Extract two 16-bit values from a 32-bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_2(a, .atype);\n\n// Extract four 8-bit values from a 32-bit input and sign or zer extend\n// based on input type.\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nb_select = (.mode == .lo) ? 0 : 2;\n\nfor (i = 0; i < 2; ++i) {\n    d += Va[i] * Vb[b_select + i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\nRequires\nsm_61\nor higher.\nExamples\ndp2a.lo.u32.u32           d0, a0, b0, c0;\ndp2a.hi.u32.s32           d1, a1, b1, c1;\n9.7.2.\nExtended-Precision Integer Arithmetic Instructions\n\nInstructions\nadd.cc\n,\naddc\n,\nsub.cc\n,\nsubc\n,\nmad.cc\nand\nmadc\nreference an\nimplicitly specified condition code register (\nCC\n) having a single carry flag bit (\nCC.CF\n)\nholding carry-in/carry-out or borrow-in/borrow-out. These instructions support extended-precision\ninteger addition, subtraction, and multiplication. No other instructions access the condition code,\nand there is no support for setting, clearing, or testing the condition code. The condition code\nregister is not preserved across calls and is mainly intended for use in straight-line code\nsequences for computing extended-precision integer addition, subtraction, and multiplication.\nThe extended-precision arithmetic instructions are:\nadd.cc\n,\naddc\nsub.cc\n,\nsubc\nmad.cc\n,\nmadc\n9.7.2.1.\nExtended-Precision Arithmetic Instructions:\nadd.cc\n\nadd.cc\nAdd two values with carry-out.\nSyntax\nadd.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer addition and writes the carry-out value into the condition code register.\nSemantics\nd = a + b;\ncarry-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nadd.cc\nintroduced in PTX ISA version 1.2.\n64-bit\nadd.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nadd.cc\nis supported on all target architectures.\n64-bit\nadd.cc\nrequires\nsm_20\nor higher.\nExamples\n@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4;\n9.7.2.2.\nExtended-Precision Arithmetic Instructions:\naddc\n\naddc\nAdd two values with carry-in and optional carry-out.\nSyntax\naddc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer addition with carry-in and optionally writes the carry-out value into the condition\ncode register.\nSemantics\nd = a + b + CC.CF;\nif\n.cc\nspecified, carry-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\naddc\nintroduced in PTX ISA version 1.2.\n64-bit\naddc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\naddc\nis supported on all target architectures.\n64-bit\naddc\nrequires\nsm_20\nor higher.\nExamples\n@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4;\n9.7.2.3.\nExtended-Precision Arithmetic Instructions:\nsub.cc\n\nsub.cc\nSubtract one value from another, with borrow-out.\nSyntax\nsub.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer subtraction and writes the borrow-out value into the condition code register.\nSemantics\nd = a - b;\nborrow-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nsub.cc\nintroduced in PTX ISA version 1.2.\n64-bit\nsub.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nsub.cc\nis supported on all target architectures.\n64-bit\nsub.cc\nrequires\nsm_20\nor higher.\nExamples\n@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4;\n9.7.2.4.\nExtended-Precision Arithmetic Instructions:\nsubc\n\nsubc\nSubtract one value from another, with borrow-in and optional borrow-out.\nSyntax\nsubc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer subtraction with borrow-in and optionally writes the borrow-out value into the\ncondition code register.\nSemantics\nd = a  - (b + CC.CF);\nif\n.cc\nspecified, borrow-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nsubc\nintroduced in PTX ISA version 1.2.\n64-bit\nsubc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nsubc\nis supported on all target architectures.\n64-bit\nsubc\nrequires\nsm_20\nor higher.\nExamples\n@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4;\n9.7.2.5.\nExtended-Precision Arithmetic Instructions:\nmad.cc\n\nmad.cc\nMultiply two values, extract high or low half of result, and add a third value with carry-out.\nSyntax\nmad{.hi,.lo}.cc.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nMultiplies two values, extracts either the high or low part of the result, and adds a third\nvalue. Writes the result to the destination register and the carry-out from the addition into the\ncondition code register.\nSemantics\nt = a * b;\nd = t<63..32> + c;    // for .hi variant\nd = t<31..0> + c;     // for .lo variant\ncarry-out from addition is written to\nCC.CF\nNotes\nGenerally used in combination with\nmadc\nand\naddc\nto implement extended-precision multi-word\nmultiplication. See\nmadc\nfor an example.\nPTX ISA Notes\n32-bit\nmad.cc\nintroduced in PTX ISA version 3.0.\n64-bit\nmad.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nRequires target\nsm_20\nor higher.\nExamples\n@p  mad.lo.cc.u32 d,a,b,c;\n    mad.lo.cc.u32 r,p,q,r;\n9.7.2.6.\nExtended-Precision Arithmetic Instructions:\nmadc\n\nmadc\nMultiply two values, extract high or low half of result, and add a third value with carry-in and\noptional carry-out.\nSyntax\nmadc{.hi,.lo}{.cc}.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nMultiplies two values, extracts either the high or low part of the result, and adds a third value\nalong with carry-in. Writes the result to the destination register and optionally writes the\ncarry-out from the addition into the condition code register.\nSemantics\nt = a * b;\nd = t<63..32> + c + CC.CF;     // for .hi variant\nd = t<31..0> + c + CC.CF;      // for .lo variant\nif\n.cc\nspecified, carry-out from addition is written to\nCC.CF\nNotes\nGenerally used in combination with\nmad.cc\nand\naddc\nto implement extended-precision\nmulti-word multiplication. See example below.\nPTX ISA Notes\n32-bit\nmadc\nintroduced in PTX ISA version 3.0.\n64-bit\nmadc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nRequires target\nsm_20\nor higher.\nExamples\n// extended-precision multiply:  [r3,r2,r1,r0] = [r5,r4] * [r7,r6]\nmul.lo.u32     r0,r4,r6;      // r0=(r4*r6).[31:0], no carry-out\nmul.hi.u32     r1,r4,r6;      // r1=(r4*r6).[63:32], no carry-out\nmad.lo.cc.u32  r1,r5,r6,r1;   // r1+=(r5*r6).[31:0], may carry-out\nmadc.hi.u32    r2,r5,r6,0;    // r2 =(r5*r6).[63:32]+carry-in,\n                              // no carry-out\nmad.lo.cc.u32   r1,r4,r7,r1;  // r1+=(r4*r7).[31:0], may carry-out\nmadc.hi.cc.u32  r2,r4,r7,r2;  // r2+=(r4*r7).[63:32]+carry-in,\n                              // may carry-out\naddc.u32        r3,0,0;       // r3 = carry-in, no carry-out\nmad.lo.cc.u32   r2,r5,r7,r2;  // r2+=(r5*r7).[31:0], may carry-out\nmadc.hi.u32     r3,r5,r7,r3;  // r3+=(r5*r7).[63:32]+carry-in\n9.7.3.\nFloating-Point Instructions\n\nFloating-point instructions operate on\n.f32\nand\n.f64\nregister operands and constant\nimmediate values. The floating-point instructions are:\ntestp\ncopysign\nadd\nsub\nmul\nfma\nmad\ndiv\nabs\nneg\nmin\nmax\nrcp\nsqrt\nrsqrt\nsin\ncos\nlg2\nex2\ntanh\nInstructions that support rounding modifiers are IEEE-754 compliant. Double-precision instructions\nsupport subnormal inputs and results. Single-precision instructions support subnormal inputs and\nresults by default for\nsm_20\nand subsequent targets, and flush subnormal inputs and results to\nsign-preserving zero for\nsm_1x\ntargets. The optional\n.ftz\nmodifier on single-precision\ninstructions provides backward compatibility with\nsm_1x\ntargets by flushing subnormal inputs and\nresults to sign-preserving zero regardless of the target architecture.\nSingle-precision\nadd\n,\nsub\n,\nmul\n, and\nmad\nsupport saturation of results to the range\n[0.0, 1.0], with\nNaN\ns being flushed to positive zero.\nNaN\npayloads are supported for\ndouble-precision instructions (except for\nrcp.approx.ftz.f64\nand\nrsqrt.approx.ftz.f64\n, which\nmaps input\nNaN\ns to a canonical\nNaN\n). Single-precision instructions return an unspecified\nNaN\n. Note that future implementations may support\nNaN\npayloads for single-precision\ninstructions, so PTX programs should not rely on the specific single-precision\nNaN\ns being\ngenerated.\nTable 29\nsummarizes\nfloating-point instructions in PTX.\nTable 29\nSummary of Floating-Point Instructions\n\nInstruction\n.rn\n.rz\n.rm\n.rp\n.ftz\n.sat\nNotes\n{add,sub,mul}.rnd.f32\nx\nx\nx\nx\nx\nx\nIf no rounding modifier is specified,\ndefault is\n.rn\nand instructions may\nbe folded into a multiply-add.\n{add,sub,mul}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\nIf no rounding modifier is specified,\ndefault is\n.rn\nand instructions may\nbe folded into a multiply-add.\nmad.f32\nn/a\nn/a\nn/a\nn/a\nx\nx\n.target\nsm_1x\nNo rounding modifier.\n{mad,fma}.rnd.f32\nx\nx\nx\nx\nx\nx\n.target\nsm_20\nor higher\nmad.f32\nand\nfma.f32\nare the same.\n{mad,fma}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\nmad.f64\nand\nfma.f64\nare the same.\ndiv.full.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nNo rounding modifier.\n{div,rcp,sqrt}.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nn/a\nrcp.approx.ftz.f64\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n.target\nsm_20\nor higher\n{div,rcp,sqrt}.rnd.f32\nx\nx\nx\nx\nx\nn/a\n.target\nsm_20\nor higher\n{div,rcp,sqrt}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\n.target\nsm_20\nor higher\n{abs,neg,min,max}.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n{abs,neg,min,max}.f64\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nrsqrt.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nrsqrt.approx.f64\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nrsqrt.approx.ftz.f64\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n.target\nsm_20\nor higher\n{sin,cos,lg2,ex2}.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\ntanh.approx.f32\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\n.target\nsm_75\nor higher\n9.7.3.1.\nFloating Point Instructions:\ntestp\n\ntestp\nTest floating-point property.\nSyntax\ntestp.op.type  p, a;  // result is .pred\n\n.op   = { .finite, .infinite,\n          .number, .notanumber,\n          .normal, .subnormal };\n.type = { .f32, .f64 };\nDescription\ntestp\ntests common properties of floating-point numbers and returns a predicate value of\n1\nif\nTrue\nand\n0\nif\nFalse\n.\ntestp.finite\nTrue\nif the input is not infinite or\nNaN\ntestp.infinite\nTrue\nif the input is positive or negative infinity\ntestp.number\nTrue\nif the input is not\nNaN\ntestp.notanumber\nTrue\nif the input is\nNaN\ntestp.normal\nTrue\nif the input is a normal number (not\nNaN\n, not infinity)\ntestp.subnormal\nTrue\nif the input is a subnormal number (not\nNaN\n, not infinity)\nAs a special case, positive and negative zero are considered normal numbers.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ntestp.notanumber.f32  isnan, f0;\ntestp.infinite.f64    p, X;\n9.7.3.2.\nFloating Point Instructions:\ncopysign\n\ncopysign\nCopy sign of one input to another.\nSyntax\ncopysign.type  d, a, b;\n\n.type = { .f32, .f64 };\nDescription\nCopy sign bit of\na\ninto value of\nb\n, and return the result as\nd\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ncopysign.f32  x, y, z;\ncopysign.f64  A, B, C;\n9.7.3.3.\nFloating Point Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd{.rnd}{.ftz}{.sat}.f32  d, a, b;\nadd{.rnd}{.ftz}.f32x2      d, a, b;\nadd{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values from\nsource operands. Single precision (\n.f32\n) operands are then added in parallel to produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a + b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] + fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nadd.ftz.f32\n,\nadd.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nadd.f64\nsupports subnormal numbers.\nadd.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nadd.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nadd.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nadd.f32\nsupported on all target architectures.\nadd.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nadd.f64\n, requires\nsm_13\nor higher.\nfor\nadd.f32\n, requires\nsm_20\nor higher.\nadd.f32x2\nrequires\nsm_100\nor higher.\nExamples\n@p  add.rz.ftz.f32  f1,f2,f3;\nadd.rp.ftz.f32x2    d, a, b;\n9.7.3.4.\nFloating Point Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub{.rnd}{.ftz}{.sat}.f32  d, a, b;\nsub{.rnd}{.ftz}.f32x2      d, a, b;\nsub{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values\nfrom source operands. Single precision (\n.f32\n) operands are then subtracted in parallel\nto produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a - b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] - fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that a\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsub.ftz.f32\n,\nsub.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nsub.f64\nsupports subnormal numbers.\nsub.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nsub.sat.f32\nclamps the result to [0.0, 1.0]. NaN results are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nsub.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nsub.f32\nsupported on all target architectures.\nsub.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nsub.f64\n, requires\nsm_13\nor higher.\nfor\nsub.f32\n, requires\nsm_20\nor higher.\nsub.f32x2\nrequires\nsm_100\nor higher.\nExamples\nsub.f32 c,a,b;\nsub.rn.ftz.f32  f1,f2,f3;\n9.7.3.5.\nFloating Point Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul{.rnd}{.ftz}{.sat}.f32  d, a, b;\nmul{.rnd}{.ftz}.f32x2      d, a, b;\nmul{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute the product of two values.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values\nfrom source operands. Single precision (\n.f32\n) operands are then multiplied in parallel\nto produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a * b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] * fB[i];\n    }\n}\nNotes\nFor floating-point multiplication, all operands must be the same size.\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that a\nmul\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nmul\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul/add\nand\nmul/sub\nsequences with no rounding modifiers may be\noptimized to use fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmul.ftz.f32\n,\nmul.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmul.f64\nsupports subnormal numbers.\nmul.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmul.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmul.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nmul.f32\nsupported on all target architectures.\nmul.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nmul.f64\n, requires\nsm_13\nor higher.\nfor\nmul.f32\n, requires\nsm_20\nor higher.\nmul.f32x2\nrequires\nsm_100\nor higher.\nExamples\nmul.ftz.f32 circumf,radius,pi  // a single-precision multiply\n9.7.3.6.\nFloating Point Instructions:\nfma\n\nfma\nFused multiply-add.\nSyntax\nfma.rnd{.ftz}{.sat}.f32  d, a, b, c;\nfma.rnd{.ftz}.f32x2      d, a, b, c;\nfma.rnd.f64              d, a, b, c;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms a fused multiply-add with no loss of precision in the intermediate product and addition.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values from\nsource operands. Single precision (\n.f32\n) operands are then operated in parallel to produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a * b + c;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    fC[0] = c[0:31];\n    fC[1] = c[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] * fB[i] + fC[i];\n    }\n}\nNotes\nfma.f32\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by\n.rnd\n.\nfma.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n.\nfma.f64\nis the same as\nmad.f64\n.\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nfma.ftz.f32\n,\nfma.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nfma.f64\nsupports subnormal numbers.\nfma.f32\nis unimplemented for\nsm_1x\ntargets.\nSaturation:\nfma.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nfma.f64\nintroduced in PTX ISA version 1.4.\nfma.f32\nintroduced in PTX ISA version 2.0.\nfma.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nfma.f32\nrequires\nsm_20\nor higher.\nfma.f64\nrequires\nsm_13\nor higher.\nfma.f32x2\nrequires\nsm_100\nor higher.\nExamples\nfma.rn.ftz.f32  w,x,y,z;\n@p  fma.rn.f64      d,a,b,c;\n    fma.rp.ftz.f32x2 p,q,r,s;\n9.7.3.7.\nFloating Point Instructions:\nmad\n\nmad\nMultiply two values and add a third value.\nSyntax\nmad{.ftz}{.sat}.f32      d, a, b, c;    // .target sm_1x\nmad.rnd{.ftz}{.sat}.f32  d, a, b, c;    // .target sm_20\nmad.rnd.f64              d, a, b, c;    // .target sm_13 and higher\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nMultiplies two values and adds a third, and then writes the resulting value into a destination\nregister.\nSemantics\nd = a*b + c;\nNotes\nFor\n.target\nsm_20\nand higher:\nmad.f32\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by\n.rnd\n.\nmad.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n.\nmad.{f32,f64}\nis the same as\nfma.{f32,f64}\n.\nFor\n.target\nsm_1x\n:\nmad.f32\ncomputes the product of\na\nand\nb\nat double precision, and then the mantissa is\ntruncated to 23 bits, but the exponent is preserved. Note that this is different from computing\nthe product with\nmul\n, where the mantissa can be rounded and the exponent will be clamped. The\nexception for\nmad.f32\nis when\nc\n=\n+/-0.0\n,\nmad.f32\nis identical to the result computed\nusing separate mul and add instructions. When JIT-compiled for SM 2.0 devices,\nmad.f32\nis\nimplemented as a fused multiply-add (i.e.,\nfma.rn.ftz.f32\n). In this case,\nmad.f32\ncan\nproduce slightly different numeric results and backward compatibility is not guaranteed in this\ncase.\nmad.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n. Unlike\nmad.f32\n, the treatment of subnormal\ninputs and output follows IEEE 754 standard.\nmad.f64\nis the same as\nfma.f64\n.\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmad.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmad.f64\nsupports subnormal numbers.\nmad.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmad.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nIn PTX ISA versions 1.4 and later, a rounding modifier is required for\nmad.f64\n.\nLegacy\nmad.f64\ninstructions having no rounding modifier will map to\nmad.rn.f64\n.\nIn PTX ISA versions 2.0 and later, a rounding modifier is required for\nmad.f32\nfor\nsm_20\nand higher targets.\nErrata\nmad.f32\nrequires a rounding modifier for\nsm_20\nand higher targets. However for PTX ISA\nversion 3.0 and earlier, ptxas does not enforce this requirement and\nmad.f32\nsilently defaults\nto\nmad.rn.f32\n. For PTX ISA version 3.1, ptxas generates a warning and defaults to\nmad.rn.f32\n, and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later.\nTarget ISA Notes\nmad.f32\nsupported on all target architectures.\nmad.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\n,\n.rm\n,\n.rp\nfor\nmad.f64\n, requires\nsm_13\nor higher.\n.rn\n,\n.rz\n,\n.rm\n,\n.rp\nfor\nmad.f32\n, requires\nsm_20\nor higher.\nExamples\n@p  mad.f32  d,a,b,c;\n9.7.3.8.\nFloating Point Instructions:\ndiv\n\ndiv\nDivide one value by another.\nSyntax\ndiv.approx{.ftz}.f32  d, a, b;  // fast, approximate divide\ndiv.full{.ftz}.f32    d, a, b;  // full-range approximate divide\ndiv.rnd{.ftz}.f32     d, a, b;  // IEEE 754 compliant rounding\ndiv.rnd.f64           d, a, b;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nDivides\na\nby\nb\n, stores result in\nd\n.\nSemantics\nd = a / b;\nNotes\nFast, approximate single-precision divides:\ndiv.approx.f32\nimplements a fast approximation to divide, computed as\nd\n=\na\n*\n(1/b)\n. For\n|b|\nin [2\n-126\n, 2\n126\n], the maximum\nulp\nerror is 2. For 2\n126\n<\n|b|\n< 2\n128\n, if\na\nis infinity,\ndiv.approx.f32\nreturns\nNaN\n, otherwise it\nreturns a sign-preserving zero.\ndiv.full.f32\nimplements a relatively fast, full-range approximation that scales operands to\nachieve better accuracy, but is not fully IEEE 754 compliant and does not support rounding\nmodifiers. The maximum\nulp\nerror is 2 across the full range of inputs.\nDivide with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\ndiv.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\ndiv.f64\nsupports subnormal numbers.\ndiv.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\ndiv.f32\nand\ndiv.f64\nintroduced in PTX ISA version 1.0.\nExplicit modifiers\n.approx\n,\n.full\n,\n.ftz\n, and rounding introduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, one of\n.approx\n,\n.full\n, or\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\ndiv.f32\ndefaults to\ndiv.approx.ftz.f32\n, and\ndiv.f64\ndefaults to\ndiv.rn.f64\n.\nTarget ISA Notes\ndiv.approx.f32\nand\ndiv.full.f32\nsupported on all target architectures.\ndiv.rnd.f32\nrequires\nsm_20\nor higher.\ndiv.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32\n.\ndiv.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\ndiv.approx.ftz.f32  diam,circum,3.14159;\ndiv.full.ftz.f32    x, y, z;\ndiv.rn.f64          xd, yd, zd;\n9.7.3.9.\nFloating Point Instructions:\nabs\n\nabs\nAbsolute value.\nSyntax\nabs{.ftz}.f32  d, a;\nabs.f64        d, a;\nDescription\nTake the absolute value of\na\nand store the result in\nd\n.\nSemantics\nd = |a|;\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nabs.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nabs.f64\nsupports subnormal numbers.\nabs.f32\nflushes subnormal inputs and results to sign-preserving zero.\nFor\nabs.f32\n,\nNaN\ninput yields unspecified\nNaN\n. For\nabs.f64\n,\nNaN\ninput is passed\nthrough unchanged. Future implementations may comply with the IEEE 754 standard by preserving\npayload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nabs.f32\nsupported on all target architectures.\nabs.f64\nrequires\nsm_13\nor higher.\nExamples\nabs.ftz.f32  x,f0;\n9.7.3.10.\nFloating Point Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg{.ftz}.f32  d, a;\nneg.f64        d, a;\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nSemantics\nd = -a;\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nneg.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nneg.f64\nsupports subnormal numbers.\nneg.f32\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nneg.f32\nsupported on all target architectures.\nneg.f64\nrequires\nsm_13\nor higher.\nExamples\nneg.ftz.f32  x,f0;\n9.7.3.11.\nFloating Point Instructions:\nmin\n\nmin\nFind the minimum of given values.\nSyntax\nmin{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmin{.ftz}{.NaN}{.abs}.f32          d, a, b, c;\nmin.f64                            d, a, b;\nDescription\nStore the minimum of\na\n,\nb\nand optionally\nc\nin\nd\n.\nIf\n.NaN\nmodifier is specified, then the result is canonical\nNaN\nif any of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the minimum of\nabsolute values of both input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both inputs\na\nand\nb\n. The\n.xorsign\nqualifier cannot be specified for three\ninputs operation.\nQualifier\n.xorsign\nrequires qualifier\n.abs\nto be specified. In such cases,\n.xorsign\nconsiders the sign bit of both inputs before applying\n.abs\noperation.\nIf the result of\nmin\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\ndef min_num (z, x, y) {\n    if (isNaN(x) && isNaN(y))\n        z = NaN;\n    else if (isNaN(x))\n        z = y;\n    else if (isNaN(y))\n        z = x;\n    else\n        // note: -0.0 < +0.0 here\n        z = (x < y) ? x : y;\n    return z;\n}\n\ndef min_nan (z, x, y) {\n    if (isNaN(x) || isNaN(y))\n        z = NaN;\n    else\n        // note: -0.0 < +0.0 here\n        z = (x < y) ? x : y;\n    return z;\n}\n\ndef two_inputs_min (z, x, y) {\n    if (.NaN)\n        z = min_nan(z, x, y);\n    else\n        z = min_num(z, x, y);\n    return z;\n}\n\nif (.xorsign && !isPresent(c)) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n}\nif (.abs) {\n    a = |a|;\n    b = |b|;\n    if (isPresent(c)) {\n        c = |c|;\n    }\n}\n\nd = two_inputs_min(d, a, b)\nif (isPresent(c)) {\n    d = two_inputs_min(d, d, c)\n}\nif (.xorsign && !isPresent(c) && !isNaN(d)) {\n    setSignBit(d, xorsign);\n}\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmin.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmin.f64\nsupports subnormal numbers.\nmin.f32\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmin.NaN\nintroduced in PTX ISA version 7.0.\nmin.xorsign.abs\nintroduced in PTX ISA version 7.2.\nmin\nwith three input arguments introduced in PTX ISA version 8.8.\nTarget ISA Notes\nmin.f32\nsupported on all target architectures.\nmin.f64\nrequires\nsm_13\nor higher.\nmin.NaN\nrequires\nsm_80\nor higher.\nmin.xorsign.abs\nrequires\nsm_86\nor higher.\nmin\nwith three input arguments requires\nsm_100\nor higher.\nExamples\n@p  min.ftz.f32  z,z,x;\n    min.f64      a,b,c;\n    // fp32 min with .NaN\n    min.NaN.f32  f0,f1,f2;\n    // fp32 min with .xorsign.abs\n    min.xorsign.abs.f32 Rd, Ra, Rb;\n9.7.3.12.\nFloating Point Instructions:\nmax\n\nmax\nFind the maximum of given values.\nSyntax\nmax{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmax{.ftz}{.NaN}{.abs}.f32          d, a, b, c;\nmax.f64                            d, a, b;\nDescription\nStore the maximum of\na\n,\nb\nand optionally\nc\nin\nd\n.\nIf\n.NaN\nmodifier is specified, the result is canonical\nNaN\nif any of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the maximum of\nabsolute values of the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of the inputs:\na\nand\nb\n. The\n.xorsign\nqualifier cannot be specified for three\ninputs operation.\nQualifier\n.xorsign\nrequires qualifier\n.abs\nto be specified. In such cases,\n.xorsign\nconsiders the sign bit of both inputs before applying\n.abs\noperation.\nIf the result of\nmax\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\ndef max_num (z, x, y) {\n    if (isNaN(x) && isNaN(y))\n        z = NaN;\n    else if (isNaN(x))\n        z = y;\n    else if (isNaN(y))\n        z = x;\n    else\n        // note: +0.0 > -0.0 here\n        z = (x > y) ? x : y;\n    return z;\n}\n\ndef max_nan (z, x, y) {\n    if (isNaN(x) || isNaN(y))\n        z = NaN;\n    else\n        // note: +0.0 > -0.0 here\n        z = (x > y) ? x : y;\n    return z;\n}\n\ndef two_inputs_max (z, x, y) {\n    if (.NaN)\n        z = max_nan(z, x, y);\n    else\n        z = max_num(z, x, y);\n    return z;\n}\n\nif (.xorsign && !isPresent(c)) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n}\nif (.abs) {\n    a = |a|;\n    b = |b|;\n    if (isPresent(c)) {\n        c = |c|;\n    }\n}\n\nd = two_inputs_max (d, a, b)\nif (isPresent(c)) {\n    d = two_inputs_max (d, d, c)\n}\n\nif (.xorsign && !isPresent(c) !isNaN(d)) {\n    setSignBit(d, xorsign);\n}\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmax.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmax.f64\nsupports subnormal numbers.\nmax.f32\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmax.NaN\nintroduced in PTX ISA version 7.0.\nmax.xorsign.abs\nintroduced in PTX ISA version 7.2.\nmax\nwith three input arguments introduced in PTX ISA version 8.8.\nTarget ISA Notes\nmax.f32\nsupported on all target architectures.\nmax.f64\nrequires\nsm_13\nor higher.\nmax.NaN\nrequires\nsm_80\nor higher.\nmax.xorsign.abs\nrequires\nsm_86\nor higher.\nmax\nwith three input arguments requires\nsm_100\nor higher.\nExamples\nmax.ftz.f32  f0,f1,f2;\nmax.f64      a,b,c;\n// fp32 max with .NaN\nmax.NaN.f32  f0,f1,f2;\n// fp32 max with .xorsign.abs\nmax.xorsign.abs.f32 Rd, Ra, Rb;\n9.7.3.13.\nFloating Point Instructions:\nrcp\n\nrcp\nTake the reciprocal of a value.\nSyntax\nrcp.approx{.ftz}.f32  d, a;  // fast, approximate reciprocal\nrcp.rnd{.ftz}.f32     d, a;  // IEEE 754 compliant rounding\nrcp.rnd.f64           d, a;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute\n1/a\n, store result in\nd\n.\nSemantics\nd = 1 / a;\nNotes\nFast, approximate single-precision reciprocal:\nrcp.approx.f32\nimplements a fast approximation to reciprocal.\nThe maximum ulp error is 1 across the full range of inputs.\nInput\nResult\n-Inf\n-0.0\n-0.0\n-Inf\n+0.0\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nReciprocal with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nrcp.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nrcp.f64\nsupports subnormal numbers.\nrcp.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nrcp.f32\nand\nrcp.f64\nintroduced in PTX ISA version 1.0.\nrcp.rn.f64\nand explicit modifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4. General rounding modifiers were\nadded in PTX ISA version 2.0.\nFor PTX ISA version 1.4 and later, one of\n.approx\nor\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\nrcp.f32\ndefaults to\nrcp.approx.ftz.f32\n, and\nrcp.f64\ndefaults to\nrcp.rn.f64\n.\nTarget ISA Notes\nrcp.approx.f32\nsupported on all target architectures.\nrcp.rnd.f32\nrequires\nsm_20\nor higher.\nrcp.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32.\nrcp.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\nrcp.approx.ftz.f32  ri,r;\nrcp.rn.ftz.f32      xi,x;\nrcp.rn.f64          xi,x;\n9.7.3.14.\nFloating Point Instructions:\nrcp.approx.ftz.f64\n\nrcp.approx.ftz.f64\nCompute a fast, gross approximation to the reciprocal of a value.\nSyntax\nrcp.approx.ftz.f64  d, a;\nDescription\nCompute a fast, gross approximation to the reciprocal as follows:\nextract the most-significant 32 bits of\n.f64\noperand\na\nin 1.11.20 IEEE floating-point\nformat (i.e., ignore the least-significant 32 bits of\na\n),\ncompute an approximate\n.f64\nreciprocal of this value using the most-significant 20 bits of\nthe mantissa of operand\na\n,\nplace the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits\nof destination\nd\n,and\nzero the least significant 32 mantissa bits of\n.f64\ndestination\nd\n.\nSemantics\ntmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / tmp;\nd[31:0] = 0x00000000;\nNotes\nrcp.approx.ftz.f64\nimplements a fast, gross approximation to reciprocal.\nInput a[63:32]\nResult d[63:32]\n-Inf\n-0.0\n-subnormal\n-Inf\n-0.0\n-Inf\n+0.0\n+Inf\n+subnormal\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nInput\nNaN\ns map to a canonical\nNaN\nwith encoding\n0x7fffffff00000000\n.\nSubnormal inputs and results are flushed to sign-preserving zero.\nPTX ISA Notes\nrcp.approx.ftz.f64\nintroduced in PTX ISA version 2.1.\nTarget ISA Notes\nrcp.approx.ftz.f64\nrequires\nsm_20\nor higher.\nExamples\nrcp.approx.ftz.f64  xi,x;\n9.7.3.15.\nFloating Point Instructions:\nsqrt\n\nsqrt\nTake the square root of a value.\nSyntax\nsqrt.approx{.ftz}.f32  d, a; // fast, approximate square root\nsqrt.rnd{.ftz}.f32     d, a; // IEEE 754 compliant rounding\nsqrt.rnd.f64           d, a; // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute sqrt(\na\n) and store the result in\nd\n.\nSemantics\nd = sqrt(a);\nNotes\nsqrt.approx.f32\nimplements a fast approximation to square root.\nThe maximum relative error over the entire positive finite floating-point\nrange is 2\n-23\n.\nFor various corner-case inputs, results of\nsqrt\ninstruction are shown\nin below table:\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n+Inf\nNaN\nNaN\nSquare root with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsqrt.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nsqrt.f64\nsupports subnormal numbers.\nsqrt.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nsqrt.f32\nand\nsqrt.f64\nintroduced in PTX ISA version 1.0.\nsqrt.rn.f64\nand explicit\nmodifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4. General rounding\nmodifiers were added in PTX ISA version 2.0.\nFor PTX ISA version 1.4 and later, one of\n.approx\nor\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\nsqrt.f32\ndefaults to\nsqrt.approx.ftz.f32\n, and\nsqrt.f64\ndefaults to\nsqrt.rn.f64\n.\nTarget ISA Notes\nsqrt.approx.f32\nsupported on all target architectures.\nsqrt.rnd.f32\nrequires\nsm_20\nor higher.\nsqrt.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32\n.\nsqrt.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\nsqrt.approx.ftz.f32  r,x;\nsqrt.rn.ftz.f32      r,x;\nsqrt.rn.f64          r,x;\n9.7.3.16.\nFloating Point Instructions:\nrsqrt\n\nrsqrt\nTake the reciprocal of the square root of a value.\nSyntax\nrsqrt.approx{.ftz}.f32  d, a;\nrsqrt.approx.f64        d, a;\nDescription\nCompute\n1/sqrt(a)\nand store the result in\nd\n.\nSemantics\nd = 1/sqrt(a);\nNotes\nrsqrt.approx\nimplements an approximation to the reciprocal square root.\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-Inf\n+0.0\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nThe maximum relative error for\nrsqrt.f32\nover the entire positive\nfinite floating-point range is 2\n-22.9\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nrsqrt.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nrsqrt.f64\nsupports subnormal numbers.\nrsqrt.f32\nflushes subnormal inputs and results to sign-preserving zero.\nNote that\nrsqrt.approx.f64\nis emulated in software and are relatively slow.\nPTX ISA Notes\nrsqrt.f32\nand\nrsqrt.f64\nwere introduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nrsqrt.f32\ndefaults to\nrsqrt.approx.ftz.f32\n, and\nrsqrt.f64\ndefaults to\nrsqrt.approx.f64\n.\nTarget ISA Notes\nrsqrt.f32\nsupported on all target architectures.\nrsqrt.f64\nrequires\nsm_13\nor higher.\nExamples\nrsqrt.approx.ftz.f32  isr, x;\nrsqrt.approx.f64      ISR, X;\n9.7.3.17.\nFloating Point Instructions:\nrsqrt.approx.ftz.f64\n\nrsqrt.approx.ftz.f64\nCompute an approximation of the square root reciprocal of a value.\nSyntax\nrsqrt.approx.ftz.f64 d, a;\nDescription\nCompute a double-precision (\n.f64\n) approximation of the square root reciprocal of a value. The\nleast significant 32 bits of the double-precision (\n.f64\n) destination\nd\nare all zeros.\nSemantics\ntmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / sqrt(tmp);\nd[31:0] = 0x00000000;\nNotes\nrsqrt.approx.ftz.f64\nimplements a fast approximation of the square root reciprocal of a value.\nInput\nResult\n-Inf\nNaN\n-subnormal\n-Inf\n-0.0\n-Inf\n+0.0\n+Inf\n+subnormal\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nInput\nNaN\ns map to a canonical\nNaN\nwith encoding\n0x7fffffff00000000\n.\nSubnormal inputs and results are flushed to sign-preserving zero.\nPTX ISA Notes\nrsqrt.approx.ftz.f64\nintroduced in PTX ISA version 4.0.\nTarget ISA Notes\nrsqrt.approx.ftz.f64\nrequires\nsm_20\nor higher.\nExamples\nrsqrt.approx.ftz.f64 xi,x;\n9.7.3.18.\nFloating Point Instructions:\nsin\n\nsin\nFind the sine of a value.\nSyntax\nsin.approx{.ftz}.f32  d, a;\nDescription\nFind the sine of the angle\na\n(in radians).\nSemantics\nd = sin(a);\nNotes\nsin.approx.f32\nimplements a fast approximation to sine.\nInput\nResult\n-Inf\nNaN\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\nNaN\nNaN\nNaN\nThe maximum absolute error over input range is as follows:\nRange\n[-2pi .. 2pi]\n[-100pi .. +100pi]\nError\n2\n-20.5\n2\n-14.7\nOutside of the range [-100pi .. +100pi], only best effort\nis provided. There are no defined error guarantees.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsin.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nsin.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the .approx modifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nsin.f32\ndefaults to\nsin.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsin.approx.ftz.f32  sa, a;\n9.7.3.19.\nFloating Point Instructions:\ncos\n\ncos\nFind the cosine of a value.\nSyntax\ncos.approx{.ftz}.f32  d, a;\nDescription\nFind the cosine of the angle\na\n(in radians).\nSemantics\nd = cos(a);\nNotes\ncos.approx.f32\nimplements a fast approximation to cosine.\nInput\nResult\n-Inf\nNaN\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\nNaN\nNaN\nNaN\nThe maximum absolute error over input range is as follows:\nRange\n[-2pi .. 2pi]\n[-100pi .. +100pi]\nError\n2\n-20.5\n2\n-14.7\nOutside of the range [-100pi .. +100pi], only best effort\nis provided. There are no defined error guarantees.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\ncos.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\ncos.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\ncos.f32\ndefaults to\ncos.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ncos.approx.ftz.f32  ca, a;\n9.7.3.20.\nFloating Point Instructions:\nlg2\n\nlg2\nFind the base-2 logarithm of a value.\nSyntax\nlg2.approx{.ftz}.f32  d, a;\nDescription\nDetermine the log\n2\nof\na\n.\nSemantics\nd = log(a) / log(2);\nNotes\nlg2.approx.f32\nimplements a fast approximation to log\n2\n(a).\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-Inf\n+0.0\n-Inf\n+Inf\n+Inf\nNaN\nNaN\nThe maximum absolute error is 2\n-22\nwhen the input operand is in the\nrange (0.5, 2). For positive finite inputs outside of this interval, maximum\nrelative error is 2\n-22\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nlg2.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nlg2.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nlg2.f32\ndefaults to\nlg2.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nlg2.approx.ftz.f32  la, a;\n9.7.3.21.\nFloating Point Instructions:\nex2\n\nex2\nFind the base-2 exponential of a value.\nSyntax\nex2.approx{.ftz}.f32  d, a;\nDescription\nRaise 2 to the power\na\n.\nSemantics\nd = 2 ^ a;\nNotes\nex2.approx.f32\nimplements a fast approximation to 2\na\n.\nInput\nResult\n-Inf\n+0.0\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nThe maximum ulp error is 2 ulp from correctly rounded result across the\nfull range of inputs.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nex2.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nex2.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nex2.f32\ndefaults to\nex2.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nex2.approx.ftz.f32  xa, a;\n9.7.3.22.\nFloating Point Instructions:\ntanh\n\ntanh\nFind the hyperbolic tangent of a value (in radians)\nSyntax\ntanh.approx.f32 d, a;\nDescription\nTake hyperbolic tangent value of\na\n.\nThe operands\nd\nand\na\nare of type\n.f32\n.\nSemantics\nd = tanh(a);\nNotes\ntanh.approx.f32\nimplements a fast approximation to FP32 hyperbolic-tangent.\nResults of\ntanh\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n-1.0\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n1.0\nNaN\nNaN\nThe maximum relative error over the entire floating point\nrange is 2\n-11\n.\nThe subnormal numbers are supported.\nNote\nThe subnormal inputs gets passed through to the output since the value of\ntanh(x)\nfor small\nvalues of\nx\nis approximately the same as\nx\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\ntanh.approx.f32 ta, a;\n9.7.4.\nHalf Precision Floating-Point Instructions\n\nHalf precision floating-point instructions operate on\n.f16\nand\n.f16x2\nregister operands. The\nhalf precision floating-point instructions are:\nadd\nsub\nmul\nfma\nneg\nabs\nmin\nmax\ntanh\nex2\nHalf-precision\nadd\n,\nsub\n,\nmul\n, and\nfma\nsupport saturation of results to the range\n[0.0, 1.0], with\nNaN\ns being flushed to positive zero. Half-precision instructions return an\nunspecified\nNaN\n.\n9.7.4.1.\nHalf Precision Floating Point Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd{.rnd}{.ftz}{.sat}.f16   d, a, b;\nadd{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nadd{.rnd}.bf16   d, a, b;\nadd{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce\n.f16x2\nor\n.bf16x2\nresult\nin destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a + b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] + fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nadd.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nadd.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nadd{.rnd}.bf16\nand\nadd{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nadd{.rnd}.bf16\nand\nadd{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 additions\nadd.f16        d0, a0, b0;\nadd.rn.f16     d1, a1, b1;\nadd.bf16       bd0, ba0, bb0;\nadd.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 addition\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nadd.f16x2  p3, p1, p2;   // SIMD f16x2 addition\n\n// SIMD bf16 addition\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nadd.bf16x2  p6, p4, p5;       // SIMD bf16x2 addition\n\n// SIMD fp16 addition\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nadd.f16x2       f2, f0, f1;     // SIMD f16x2 addition\n\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nadd.bf16x2      f5, f3, f4;      // SIMD bf16x2 addition\n9.7.4.2.\nHalf Precision Floating Point Instructions:\nsub\n\nsub\nSubtract two values.\nSyntax\nsub{.rnd}{.ftz}{.sat}.f16   d, a, b;\nsub{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nsub{.rnd}.bf16   d, a, b;\nsub{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then subtracted in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a - b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] - fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that a\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nsub.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nsub.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nsub{.rnd}.bf16\nand\nsub{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nsub{.rnd}.bf16\nand\nsub{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 subtractions\nsub.f16        d0, a0, b0;\nsub.rn.f16     d1, a1, b1;\nsub.bf16       bd0, ba0, bb0;\nsub.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 subtraction\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nsub.f16x2  p3, p1, p2;   // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nsub.bf16x2  p6, p4, p5;       // SIMD bf16x2 subtraction\n\n// SIMD fp16 subtraction\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nsub.f16x2       f2, f0, f1;     // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nsub.bf16x2      f5, f3, f4;      // SIMD bf16x2 subtraction\n9.7.4.3.\nHalf Precision Floating Point Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul{.rnd}{.ftz}{.sat}.f16   d, a, b;\nmul{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nmul{.rnd}.bf16   d, a, b;\nmul{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms multiplication and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then multiplied in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a * b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that a\nmul\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nmul\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nand\nmul/sub\nsequences with no rounding modifiers may\nbe optimized to use fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmul.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmul.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nmul{.rnd}.bf16\nand\nmul{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nmul{.rnd}.bf16\nand\nmul{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 multiplications\nmul.f16        d0, a0, b0;\nmul.rn.f16     d1, a1, b1;\nmul.bf16       bd0, ba0, bb0;\nmul.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 multiplication\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nmul.f16x2  p3, p1, p2;   // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nmul.bf16x2  p6, p4, p5;       // SIMD bf16x2 multiplication\n\n// SIMD fp16 multiplication\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nmul.f16x2       f2, f0, f1;     // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nmul.bf16x2      f5, f3, f4;      // SIMD bf16x2 multiplication\n9.7.4.4.\nHalf Precision Floating Point Instructions:\nfma\n\nfma\nFused multiply-add\nSyntax\nfma.rnd{.ftz}{.sat}.f16     d, a, b, c;\nfma.rnd{.ftz}{.sat}.f16x2   d, a, b, c;\nfma.rnd{.ftz}.relu.f16      d, a, b, c;\nfma.rnd{.ftz}.relu.f16x2    d, a, b, c;\nfma.rnd{.relu}.bf16         d, a, b, c;\nfma.rnd{.relu}.bf16x2       d, a, b, c;\nfma.rnd.oob.{relu}.type     d, a, b, c;\n\n.rnd = { .rn };\nDescription\nPerforms a fused multiply-add with no loss of precision in the intermediate product and addition.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then operated in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a * b + c;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    fC[0] = c[0:15];\n    fC[1] = c[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i] + fC[i];\n    }\n}\nNotes\nRounding modifiers (default is\n.rn\n):\n.rn\nmantissa LSB rounds to nearest even\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nfma.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nfma.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nfma.relu.{f16,\nf16x2,\nbf16,\nbf16x2}\nclamps the result to 0 if negative.\nNaN\nresult is\nconverted to canonical\nNaN\n.\nOut Of Bounds modifier:\nfma.oob.{f16,\nf16x2,\nbf16,\nbf16x2}\nclamps the result to 0 if either of the operands\nis\nOOB\nNaN\n(defined under\nTensors\n) value. The test for the special\nNaN\nvalue\nand resultant forcing of the result to +0.0 is performed independently for each of the\ntwo SIMD operations.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nfma.relu.{f16,\nf16x2}\nand\nfma{.relu}.{bf16,\nbf16x2}\nintroduced in PTX ISA version 7.0.\nSupport for modifier\n.oob\nintroduced in PTX ISA version 8.1.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nfma.relu.{f16,\nf16x2}\nand\nfma{.relu}.{bf16,\nbf16x2}\nrequire\nsm_80\nor higher.\nfma{.oob}.{f16,\nf16x2,\nbf16,\nbf16x2}\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 fused multiply-add\nfma.rn.f16         d0, a0, b0, c0;\nfma.rn.f16         d1, a1, b1, c1;\nfma.rn.relu.f16    d1, a1, b1, c1;\nfma.rn.oob.f16      d1, a1, b1, c1;\nfma.rn.oob.relu.f16 d1, a1, b1, c1;\n\n// scalar bf16 fused multiply-add\nfma.rn.bf16        d1, a1, b1, c1;\nfma.rn.relu.bf16   d1, a1, b1, c1;\nfma.rn.oob.bf16       d1, a1, b1, c1;\nfma.rn.oob.relu.bf16  d1, a1, b1, c1;\n\n// SIMD f16 fused multiply-add\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1}; // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3}; // pack two f16 to 32bit f16x2\nfma.rn.f16x2  p3, p1, p2, p2;   // SIMD f16x2 fused multiply-add\nfma.rn.relu.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.f16x2 p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier and relu saturation mode\n\n// SIMD fp16 fused multiply-add\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nfma.rn.f16x2    f2, f0, f1, f1; // SIMD f16x2 fused multiply-add\n\n// SIMD bf16 fused multiply-add\nfma.rn.bf16x2       f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add\nfma.rn.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier and relu saturation mode\n9.7.4.5.\nHalf Precision Floating Point Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg{.ftz}.f16    d, a;\nneg{.ftz}.f16x2  d, a;\nneg.bf16         d, a;\nneg.bf16x2       d, a;\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vector by extracting half word values\nfrom the source operand. Half-word operands are then negated in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.b32\ntype. For\n.bf16\ninstruction\ntype, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type, operands\nd\nand\na\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = -a;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = -fA[i];\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nneg.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nneg.bf16\nand\nneg.bf16x2\nintroduced in PTX ISA 7.0.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nneg.bf16\nand\nneg.bf16x2\nrequires architecture\nsm_80\nor higher.\nExamples\nneg.ftz.f16  x,f0;\nneg.bf16     x,b0;\nneg.bf16x2   x1,b1;\n9.7.4.6.\nHalf Precision Floating Point Instructions:\nabs\n\nabs\nAbsolute value\nSyntax\nabs{.ftz}.f16    d, a;\nabs{.ftz}.f16x2  d, a;\nabs.bf16         d, a;\nabs.bf16x2       d, a;\nDescription\nTake absolute value of\na\nand store the result in\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vector by extracting half word values\nfrom the source operand. Absolute values of half-word operands are then computed in parallel to\nproduce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = |a|;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = |fA[i]|;\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nabs.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nabs.bf16\nand\nabs.bf16x2\nintroduced in PTX ISA 7.0.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nabs.bf16\nand\nabs.bf16x2\nrequires architecture\nsm_80\nor higher.\nExamples\nabs.ftz.f16  x,f0;\nabs.bf16     x,b0;\nabs.bf16x2   x1,b1;\n9.7.4.7.\nHalf Precision Floating Point Instructions:\nmin\n\nmin\nFind the minimum of two values.\nSyntax\nmin{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmin{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16x2         d, a, b;\nDescription\nStore the minimum of\na\nand\nb\nin\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nIf\n.NaN\nmodifier is specified, then the result is canonical\nNaN\nif either of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the minimum of\nabsolute values of both the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both the inputs.\nModifiers\n.abs\nand\n.xorsign\nmust be specified together and\n.xorsign\nconsiders the sign\nbit of both inputs before applying\n.abs\noperation.\nIf the result of\nmin\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\nif (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a < b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n               fA[i] = |fA[i]|;\n               fB[i] = |fB[i]|;\n           }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] < fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(d[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmin.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nmin.xorsign\nintroduced in PTX ISA version 7.2.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nmin.xorsign.abs\nsupport requires\nsm_86\nor higher.\nExamples\nmin.ftz.f16       h0,h1,h2;\nmin.f16x2         b0,b1,b2;\n// SIMD fp16 min with .NaN\nmin.NaN.f16x2     b0,b1,b2;\nmin.bf16          h0, h1, h2;\n// SIMD bf16 min with NaN\nmin.NaN.bf16x2    b0, b1, b2;\n// scalar bf16 min with xorsign.abs\nmin.xorsign.abs.bf16 Rd, Ra, Rb\n9.7.4.8.\nHalf Precision Floating Point Instructions:\nmax\n\nmax\nFind the maximum of two values.\nSyntax\nmax{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmax{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16x2         d, a, b;\nDescription\nStore the maximum of\na\nand\nb\nin\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nIf\n.NaN\nmodifier is specified, the result is canonical\nNaN\nif either of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the maximum of\nabsolute values of both the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both the inputs.\nModifiers\n.abs\nand\n.xorsign\nmust be specified together and\n.xorsign\nconsiders the sign\nbit of both inputs before applying\n.abs\noperation.\nIf the result of\nmax\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\nif (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a > b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n                fA[i] = |fA[i]|;\n                fB[i] = |fB[i]|;\n            }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] > fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(fA[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmax.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nmax.xorsign.abs\nintroduced in PTX ISA version 7.2.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nmax.xorsign.abs\nsupport requires\nsm_86\nor higher.\nExamples\nmax.ftz.f16       h0,h1,h2;\nmax.f16x2         b0,b1,b2;\n// SIMD fp16 max with NaN\nmax.NaN.f16x2     b0,b1,b2;\n// scalar f16 max with xorsign.abs\nmax.xorsign.abs.f16 Rd, Ra, Rb;\nmax.bf16          h0, h1, h2;\n// scalar bf16 max and NaN\nmax.NaN.bf16x2    b0, b1, b2;\n// SIMD bf16 max with xorsign.abs\nmax.xorsign.abs.bf16x2 Rd, Ra, Rb;\n9.7.4.9.\nHalf Precision Floating Point Instructions:\ntanh\n\ntanh\nFind the hyperbolic tangent of a value (in radians)\nSyntax\ntanh.approx.type d, a;\n\n.type = {.f16, .f16x2, .bf16, .bf16x2}\nDescription\nTake hyperbolic tangent value of\na\n.\nThe type of operands\nd\nand\na\nare as specified by\n.type\n.\nFor\n.f16x2\nor\n.bf16x2\ninstruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a\n.f16x2\nor\n.bf16x2\n.\nSemantics\nif (.type == .f16 || .type == .bf16) {\n  d = tanh(a)\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = tanh(fA[0])\n  d[1] = tanh(fA[1])\n}\nNotes\ntanh.approx.{f16,\nf16x2,\nbf16,\nbf16x2}\nimplements an approximate hyperbolic tangent in the\ntarget format.\nResults of\ntanh\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n-1.0\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n1.0\nNaN\nNaN\nThe maximum absolute error for\n.f16\ntype is 2-10.987. The maximum absolute error for\n.bf16\ntype is 2-8.\nThe subnormal numbers are supported.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\ntanh.approx.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\ntanh.approx.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\ntanh.approx.f16    h1, h0;\ntanh.approx.f16x2  hd1, hd0;\ntanh.approx.bf16   b1, b0;\ntanh.approx.bf16x2 hb1, hb0;\n9.7.4.10.\nHalf Precision Floating Point Instructions:\nex2\n\nex2\nFind the base-2 exponent of input.\nSyntax\nex2.approx.atype     d, a;\nex2.approx.ftz.btype d, a;\n\n.atype = { .f16,  .f16x2}\n.btype = { .bf16, .bf16x2}\nDescription\nRaise 2 to the power\na\n.\nThe type of operands\nd\nand\na\nare as specified by\n.type\n.\nFor\n.f16x2\nor\n.bf16x2\ninstruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a\n.f16x2\nor\n.bf16x2\n.\nSemantics\nif (.type == .f16 || .type == .bf16) {\n  d = 2 ^ a\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = 2 ^ fA[0]\n  d[1] = 2 ^ fA[1]\n}\nNotes\nex2.approx.{f16,\nf16x2,\nbf16,\nbf16x2}\nimplement a fast approximation to 2\na\n.\nFor the\n.f16\ntype, subnormal inputs are supported.\nex2.approx.ftz.bf16\nflushes subnormal\ninputs and results to sign-preserving zero.\nResults of\nex2.approx.ftz.bf16\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n+0.0\n-subnormal\n+1.0\n-0.0\n+1.0\n+0.0\n+1.0\n+subnormal\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nResults of\nex2.approx.f16\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n+0.0\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nThe maximum relative error for\n.f16\ntype is 2-9.9. The maximum relative error for\n.bf16\ntype\nis 2-7.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nex2.approx.ftz.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nex2.approx.ftz.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\nex2.approx.f16         h1, h0;\nex2.approx.f16x2       hd1, hd0;\nex2.approx.ftz.bf16    b1, b2;\nex2.approx.ftz.bf16x2  hb1, hb2;\n9.7.5.\nMixed Precision Floating-Point Instructions\n\nMixed precision floating-point instructions operate on data with varied floating point precision.\nBefore executing the specified operation, operands with different precision needs to be converted\nsuch that all the instruction operands can be represented with a consistent floating-point precision.\nThe register variable to be used for holding a particular operand depends upon the combination of\nthe instruction types. Refer\nFundamental Types\nand\nAlternate Floating-Point Data Formats\nfor more details\naround exact register operand to be used for a given data type.\nThe mixed precision floating point instructions are:\nadd\nsub\nfma\nMixed precision\nadd\n,\nsub\n,\nfma\nsupport saturation of results to the range [0.0, 1.0],\nwith\nNaN\nbeing flushed to positive zero.\n9.7.5.1.\nMixed Precision Floating Point Instructions:\nadd\n\nadd\nAdd 2 values.\nSyntax\nadd{.rnd}{.sat}.f32.atype  d, a, c;\n\n.atype = { .f16, .bf16};\n.rnd   = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operand\na\nfrom\n.atype\ninto\n.f32\ntype. The converted value is then\nused for the addition. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) + c;\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nadd.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nadd.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nadd.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .b16 ba;\nadd.rz.f32.bf16.sat   fd, fa, fc;\n9.7.5.2.\nMixed Precision Floating Point Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub{.rnd}{.sat}.f32.atype  d, a, c;\n\n.atype = { .f16, .bf16};\n.rnd   = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operand\na\nfrom\n.atype\ninto\n.f32\ntype. The converted value is then\nused for the subtraction. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) - c;\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nsub.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nsub.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nsub.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .f16 ha;\nsub.rz.f32.f16.sat   fd, ha, fc;\n9.7.5.3.\nMixed Precision Floating Point Instructions:\nfma\n\nfma\nFused multiply-add.\nSyntax\nfma.rnd{.sat}.f32.abtype  d, a, b, c;\n\n.abtype = { .f16, .bf16};\n.rnd    = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operands\na\nand\nb\nfrom\n.atype\ninto\n.f32\ntype. The converted values\nare then used to perform fused multiply-add operation with no loss of precision in the intermediate\nproduct and addition. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) * convert(b) + c;\nNotes\nfma.f32.{f16/bf16}\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto this product, again in infinite precision. The resulting value is then rounded to single\nprecision using the rounding mode specified by\n.rnd\n.\nRounding modifiers(no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nfma.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nfma.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nfma.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .f16 ha, hb;\nfma.rz.sat.f32.f16.sat   fd, ha, hb, fc;\n9.7.6.\nComparison and Selection Instructions\n\nThe comparison select instructions are:\nset\nsetp\nselp\nslct\nAs with single-precision floating-point instructions, the\nset\n,\nsetp\n, and\nslct\ninstructions support subnormal numbers for\nsm_20\nand higher targets and flush single-precision\nsubnormal inputs to sign-preserving zero for\nsm_1x\ntargets. The optional\n.ftz\nmodifier\nprovides backward compatibility with\nsm_1x\ntargets by flushing subnormal inputs and results to\nsign-preserving zero regardless of the target architecture.\n9.7.6.1.\nComparison and Selection Instructions:\nset\n\nset\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nset.CmpOp{.ftz}.dtype.stype         d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.stype  d, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.dtype  = { .u32, .s32, .f32 };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 };\nDescription\nCompares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator. If this result is\nTrue\n,\n1.0f\nis written for floating-point\ndestination types, and\n0xffffffff\nis written for integer destination types. Otherwise,\n0x00000000\nis written.\nOperand\nd\nhas type\n.dtype\n; operands\na\nand\nb\nhave type\n.stype\n; operand\nc\nhas\ntype\n.pred\n.\nSemantics\nt = (a CmpOp b) ? 1 : 0;\nif (isFloat(dtype))\n    d = BoolOp(t, c) ? 1.0f : 0x00000000;\nelse\n    d = BoolOp(t, c) ? 0xffffffff : 0x00000000;\nInteger Notes\nThe signed and unsigned comparison operators are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n.\nFor unsigned values, the comparison operators\nlo\n,\nls\n,\nhi\n, and\nhs\nfor lower,\nlower-or-same, higher, and higher-or-same may be used instead of\nlt\n,\nle\n,\ngt\n,\nge\n,\nrespectively.\nThe untyped, bit-size comparisons are\neq\nand\nne\n.\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nset.ftz.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nsm_1x\nset.dtype.f64\nsupports subnormal numbers.\nset.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nset\nwith\n.f64\nsource type requires\nsm_13\nor higher.\nExamples\n@p  set.lt.and.f32.s32  d,a,b,r;\n    set.eq.u32.u32      d,i,n;\n9.7.6.2.\nComparison and Selection Instructions:\nsetp\n\nsetp\nCompare two numeric values with a relational operator, and (optionally) combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nsetp.CmpOp{.ftz}.type         p[|q], a, b;\nsetp.CmpOp.BoolOp{.ftz}.type  p[|q], a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.type   = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 };\nDescription\nCompares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the first destination operand. A related value computed using\nthe complement of the compare result is written to the second destination operand.\nApplies to all numeric types. Operands\na\nand\nb\nhave type\n.type\n; operands\np\n,\nq\n,\nand\nc\nhave type\n.pred\n. The sink symbol ‘_’ may be used in place of any one of the\ndestination operands.\nSemantics\nt = (a CmpOp b) ? 1 : 0;\np = BoolOp(t, c);\nq = BoolOp(!t, c);\nInteger Notes\nThe signed and unsigned comparison operators are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n.\nFor unsigned values, the comparison operators\nlo\n,\nls\n,\nhi\n, and\nhs\nfor lower,\nlower-or-same, higher, and higher-or-same may be used instead of\nlt\n,\nle\n,\ngt\n,\nge\n,\nrespectively.\nThe untyped, bit-size comparisons are\neq\nand\nne\n.\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsetp.ftz.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nsm_1x\nsetp.dtype.f64\nsupports subnormal numbers.\nsetp.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nsetp\nwith\n.f64\nsource type requires\nsm_13\nor higher.\nExamples\nsetp.lt.and.s32  p|q,a,b,r;\n@q  setp.eq.u32      p,i,n;\n9.7.6.3.\nComparison and Selection Instructions:\nselp\n\nselp\nSelect between source operands, based on the value of the predicate source operand.\nSyntax\nselp.type d, a, b, c;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 };\nDescription\nConditional selection. If\nc\nis\nTrue\n,\na\nis stored in\nd\n,\nb\notherwise. Operands\nd\n,\na\n, and\nb\nmust be of the same type. Operand\nc\nis a predicate.\nSemantics\nd = (c == 1) ? a : b;\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nselp.f64\nrequires\nsm_13\nor higher.\nExamples\nselp.s32  r0,r,g,p;\n@q  selp.f32  f0,t,x,xp;\n9.7.6.4.\nComparison and Selection Instructions:\nslct\n\nslct\nSelect one source operand, based on the sign of the third operand.\nSyntax\nslct.dtype.s32        d, a, b, c;\nslct{.ftz}.dtype.f32  d, a, b, c;\n\n.dtype = { .b16, .b32, .b64,\n           .u16, .u32, .u64,\n           .s16, .s32, .s64,\n                 .f32, .f64 };\nDescription\nConditional selection. If\nc\n>= 0,\na\nis stored in\nd\n, otherwise\nb\nis stored in\nd\n. Operands\nd\n,\na\n, and\nb\nare treated as a bitsize type of the same width as the first\ninstruction type; operand\nc\nmust match the second instruction type (\n.s32\nor\n.f32\n). The\nselected input is copied to the output without modification.\nSemantics\nd = (c >= 0) ? a : b;\nFloating Point Notes\nFor\n.f32\ncomparisons, negative zero equals zero.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nslct.ftz.dtype.f32\nflushes subnormal values of operand\nc\nto sign-preserving zero, and\noperand\na\nis selected.\nsm_1x\nslct.dtype.f32\nflushes subnormal values of operand\nc\nto sign-preserving zero, and operand\na\nis selected.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nIf operand\nc\nis\nNaN\n, the comparison is ordered and operand\nb\nis selected.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nslct.f64\nrequires\nsm_13\nor higher.\nExamples\nslct.u32.s32  x, y, z, val;\nslct.ftz.u64.f32  A, B, C, fval;\n9.7.7.\nHalf Precision Comparison Instructions\n\nThe comparison instructions are:\nset\nsetp\n9.7.7.1.\nHalf Precision Comparison Instructions:\nset\n\nset\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nset.CmpOp{.ftz}.f16.stype            d, a, b;\nset.CmpOp.BoolOp{.ftz}.f16.stype     d, a, b, {!}c;\n\nset.CmpOp.bf16.stype                 d, a, b;\nset.CmpOp.BoolOp.bf16.stype          d, a, b, {!}c;\n\nset.CmpOp{.ftz}.dtype.f16            d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16     d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp.dtype.bf16                 d, a, b;\nset.CmpOp.BoolOp.dtype.bf16          d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp{.ftz}.dtype.f16x2          d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16x2   d, a, b, {!}c;\n.dtype  = { .f16x2, .u32, .s32}\n\nset.CmpOp.dtype.bf16x2               d, a, b;\nset.CmpOp.BoolOp.dtype.bf16x2        d, a, b, {!}c;\n.dtype  = { .bf16x2, .u32, .s32}\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n            .f16, .f32, .f64};\nDescription\nCompares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator.\nResult of this computation is written in destination register in the following way:\nIf result is\nTrue\n,\n0xffffffff\nis written for destination types\n.u32\n/\n.s32\n.\n0xffff\nis written for destination types\n.u16\n/\n.s16\n.\n1.0\nin target precision floating point format is written for destination type\n.f16\n,\n.bf16\n.\nIf result is\nFalse\n,\n0x0\nis written for all integer destination types.\n0.0\nin target precision floating point format is written for destination type\n.f16\n,\n.bf16\n.\nIf the source type is\n.f16x2\nor\n.bf16x2\nthen result of individual operations are packed in\nthe 32-bit destination operand.\nOperand\nc\nhas type\n.pred\n.\nSemantics\nif (stype == .f16x2 || stype == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0]   = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1]   = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    if (dtype == .f16x2 || stype == .bf16x2) {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 1.0 : 0.0;\n        }\n    } else {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 0xffff : 0;\n        }\n    }\n} else if (dtype == .f16 || stype == .bf16) {\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? 1.0 : 0.0;\n} else  { // Integer destination type\n    trueVal = (isU16(dtype) || isS16(dtype)) ?  0xffff : 0xffffffff;\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? trueVal : 0;\n}\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nWhen\n.ftz\nmodifier is specified then subnormal inputs and results are flushed to sign\npreserving zero.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nset.{u16,\nu32,\ns16,\ns32}.f16\nand\nset.{u32,\ns32}.f16x2\nare introduced in PTX ISA version 6.5.\nset.{u16,\nu32,\ns16,\ns32}.bf16\n,\nset.{u32,\ns32,\nbf16x2}.bf16x2\n,\nset.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64}\nare introduced in PTX ISA version\n7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nset.{u16,\nu32,\ns16,\ns32}.bf16\n,\nset.{u32,\ns32,\nbf16x2}.bf16x2\n,\nset.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64}\nrequire\nsm_90\nor higher.\nExamples\nset.lt.and.f16.f16  d,a,b,r;\nset.eq.f16x2.f16x2  d,i,n;\nset.eq.u32.f16x2    d,i,n;\nset.lt.and.u16.f16  d,a,b,r;\nset.ltu.or.bf16.f16    d,u,v,s;\nset.equ.bf16x2.bf16x2  d,j,m;\nset.geu.s32.bf16x2     d,j,m;\nset.num.xor.s32.bf16   d,u,v,s;\n9.7.7.2.\nHalf Precision Comparison Instructions:\nsetp\n\nsetp\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nsetp.CmpOp{.ftz}.f16           p, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16    p, a, b, {!}c;\n\nsetp.CmpOp{.ftz}.f16x2         p|q, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16x2  p|q, a, b, {!}c;\n\nsetp.CmpOp.bf16                p, a, b;\nsetp.CmpOp.BoolOp.bf16         p, a, b, {!}c;\n\nsetp.CmpOp.bf16x2              p|q, a, b;\nsetp.CmpOp.BoolOp.bf16x2       p|q, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\nDescription\nCompares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the destination operand.\nOperand\nc\n,\np\nand\nq\nhas type\n.pred\n.\nFor instruction type\n.f16\n, operands\na\nand\nb\nhave type\n.b16\nor\n.f16\n.\nFor instruction type\n.f16x2\n, operands\na\nand\nb\nhave type\n.b32\n.\nFor instruction type\n.bf16\n, operands\na\nand\nb\nhave type\n.b16\n.\nFor instruction type\n.bf16x2\n, operands\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == .f16 || type == .bf16) {\n     t = (a CmpOp b) ? 1 : 0;\n     p = BoolOp(t, c);\n} else if (type == .f16x2 || type == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    p = BoolOp(t[0], c);\n    q = BoolOp(t[1], c);\n}\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nsetp.ftz.{f16,f16x2}\nflushes subnormal inputs to sign-preserving zero.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nsetp.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nsetp.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\nsetp.lt.and.f16x2  p|q,a,b,r;\n@q  setp.eq.f16    p,i,n;\n\nsetp.gt.or.bf16x2  u|v,c,d,s;\n@q  setp.eq.bf16   u,j,m;\n9.7.8.\nLogic and Shift Instructions\n\nThe logic and shift instructions are fundamentally untyped, performing bit-wise operations on\noperands of any type, provided the operands are of the same size. This permits bit-wise operations\non floating point values without having to define a union to access the bits. Instructions\nand\n,\nor\n,\nxor\n, and\nnot\nalso operate on predicates.\nThe logical shift instructions are:\nand\nor\nxor\nnot\ncnot\nlop3\nshf\nshl\nshr\n9.7.8.1.\nLogic and Shift Instructions:\nand\n\nand\nBitwise AND.\nSyntax\nand.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise and operation for the bits in\na\nand\nb\n.\nSemantics\nd = a & b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nand.b32  x,q,r;\nand.b32  sign,fpvalue,0x80000000;\n9.7.8.2.\nLogic and Shift Instructions:\nor\n\nor\nBiwise OR.\nSyntax\nor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise or operation for the bits in\na\nand\nb\n.\nSemantics\nd = a | b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nor.b32  mask mask,0x00010001\nor.pred  p,q,r;\n9.7.8.3.\nLogic and Shift Instructions:\nxor\n\nxor\nBitwise exclusive-OR (inequality).\nSyntax\nxor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise exclusive-or operation for the bits in\na\nand\nb\n.\nSemantics\nd = a ^ b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nxor.b32  d,q,r;\nxor.b16  d,x,0x0001;\n9.7.8.4.\nLogic and Shift Instructions:\nnot\n\nnot\nBitwise negation; one’s complement.\nSyntax\nnot.type d, a;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nInvert the bits in\na\n.\nSemantics\nd = ~a;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicates.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nnot.b32  mask,mask;\nnot.pred  p,q;\n9.7.8.5.\nLogic and Shift Instructions:\ncnot\n\ncnot\nC/C++ style logical negation.\nSyntax\ncnot.type d, a;\n\n.type = { .b16, .b32, .b64 };\nDescription\nCompute the logical negation using C/C++ semantics.\nSemantics\nd = (a==0) ? 1 : 0;\nNotes\nThe size of the operands must match, but not necessarily the type.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ncnot.b32 d,a;\n9.7.8.6.\nLogic and Shift Instructions:\nlop3\n\nlop3\nArbitrary logical operation on 3 inputs.\nSyntax\nlop3.b32 d, a, b, c, immLut;\nlop3.BoolOp.b32 d|p, a, b, c, immLut, q;\n\n.BoolOp   = { .or , .and };\nDescription\nCompute bitwise logical operation on inputs\na\n,\nb\n,\nc\nand store the result in destination\nd\n.\nOptionally,\n.BoolOp\ncan be specified to compute the predicate result\np\nby performing a\nBoolean operation on the destination operand\nd\nwith the predicate\nq\nin the following manner:\np = (d != 0) BoolOp q;\nThe sink symbol ‘_’ may be used in place of the destination operand\nd\nwhen\n.BoolOp\nqualifier\nis specified.\nThe logical operation is defined by a look-up table which, for 3 inputs, can be represented as an\n8-bit value specified by operand\nimmLut\nas described below.\nimmLut\nis an integer constant\nthat can take values from 0 to 255, thereby allowing up to 256 distinct logical operations on inputs\na\n,\nb\n,\nc\n.\nFor a logical operation\nF(a,\nb,\nc)\nthe value of\nimmLut\ncan be computed by applying the same\noperation to three predefined constant values as follows:\nta = 0xF0;\ntb = 0xCC;\ntc = 0xAA;\n\nimmLut = F(ta, tb, tc);\nExamples:\nIf F = (a & b & c);\nimmLut = 0xF0 & 0xCC & 0xAA = 0x80\n\nIf F = (a | b | c);\nimmLut = 0xF0 | 0xCC | 0xAA = 0xFE\n\nIf F = (a & b & ~c);\nimmLut = 0xF0 & 0xCC & (~0xAA) = 0x40\n\nIf F = ((a & b | c) ^ a);\nimmLut = (0xF0 & 0xCC | 0xAA) ^ 0xF0 = 0x1A\nThe following table illustrates computation of\nimmLut\nfor various logical operations:\nta\ntb\ntc\nOper 0 (False)\nOper 1 (ta & tb & tc)\nOper 2 (ta & tb & ~tc)\n…\nOper 254 (ta | tb | tc)\nOper 255 (True)\n0\n0\n0\n0\n0\n0\n…\n0\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n0\n1\n1\nimmLut\n0x0\n0x80\n0x40\n…\n0xFE\n0xFF\nSemantics\nF = GetFunctionFromTable(immLut); // returns the function corresponding to immLut value\nd = F(a, b, c);\nif (BoolOp specified) {\n    p = (d != 0) BoolOp q;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 4.3.\nSupport for\n.BoolOp\nqualifier introduced in PTX ISA version 8.2.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nQualifier\n.BoolOp\nrequires\nsm_70\nor higher.\nExamples\nlop3.b32       d, a, b, c, 0x40;\nlop3.or.b32  d|p, a, b, c, 0x3f, q;\nlop3.and.b32 _|p, a, b, c, 0x3f, q;\n9.7.8.7.\nLogic and Shift Instructions:\nshf\n\nshf\nFunnel shift.\nSyntax\nshf.l.mode.b32  d, a, b, c;  // left shift\nshf.r.mode.b32  d, a, b, c;  // right shift\n\n.mode = { .clamp, .wrap };\nDescription\nShift the 64-bit value formed by concatenating operands\na\nand\nb\nleft or right by the amount\nspecified by the unsigned 32-bit value in\nc\n. Operand\nb\nholds bits\n63:32\nand operand a\nholds bits\n31:0\nof the 64-bit source value. The source is shifted left or right by the clamped\nor wrapped value in\nc\n. For\nshf.l\n, the most-significant 32-bits of the result are written\ninto\nd\n; for\nshf.r\n, the least-significant 32-bits of the result are written into\nd\n.\nSemantics\nu32  n = (.mode == .clamp) ? min(c, 32) : c & 0x1f;\nswitch (shf.dir) {  // shift concatenation of [b, a]\n    case shf.l:     // extract 32 msbs\n           u32  d = (b << n)      | (a >> (32-n));\n    case shf.r:     // extract 32 lsbs\n           u32  d = (b << (32-n)) | (a >> n);\n}\nNotes\nUse funnel shift for multi-word shift operations and for rotate operations. The shift amount is\nlimited to the range\n0..32\nin clamp mode and\n0..31\nin wrap mode, so shifting multi-word\nvalues by distances greater than 32 requires first moving 32-bit words, then using\nshf\nto shift\nthe remaining\n0..31\ndistance.\nTo shift data sizes greater than 64 bits to the right, use repeated\nshf.r\ninstructions applied\nto adjacent words, operating from least-significant word towards most-significant word. At each\nstep, a single word of the shifted result is computed. The most-significant word of the result is\ncomputed using a\nshr.{u32,s32}\ninstruction, which zero or sign fills based on the instruction\ntype.\nTo shift data sizes greater than 64 bits to the left, use repeated\nshf.l\ninstructions applied to\nadjacent words, operating from most-significant word towards least-significant word. At each step, a\nsingle word of the shifted result is computed. The least-significant word of the result is computed\nusing a\nshl\ninstruction.\nUse funnel shift to perform 32-bit left or right rotate by supplying the same value for source\narguments\na\nand\nb\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nRequires\nsm_32\nor higher.\nExample\nshf.l.clamp.b32  r3,r1,r0,16;\n\n// 128-bit left shift; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] << n\nshf.l.clamp.b32  r7,r2,r3,n;\nshf.l.clamp.b32  r6,r1,r2,n;\nshf.l.clamp.b32  r5,r0,r1,n;\nshl.b32          r4,r0,n;\n\n// 128-bit right shift, arithmetic; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] >> n\nshf.r.clamp.b32  r4,r0,r1,n;\nshf.r.clamp.b32  r5,r1,r2,n;\nshf.r.clamp.b32  r6,r2,r3,n;\nshr.s32          r7,r3,n;     // result is sign-extended\n\nshf.r.clamp.b32  r1,r0,r0,n;  // rotate right by n; n < 32\nshf.l.clamp.b32  r1,r0,r0,n;  // rotate left by n; n < 32\n\n// extract 32-bits from [r1,r0] starting at position n < 32\nshf.r.clamp.b32  r0,r0,r1,n;\n9.7.8.8.\nLogic and Shift Instructions:\nshl\n\nshl\nShift bits left, zero-fill on right.\nSyntax\nshl.type d, a, b;\n\n.type = { .b16, .b32, .b64 };\nDescription\nShift\na\nleft by the amount specified by unsigned 32-bit value in\nb\n.\nSemantics\nd = a << b;\nNotes\nShift amounts greater than the register width\nN\nare clamped to\nN\n.\nThe sizes of the destination and first source operand must match, but not necessarily the type. The\nb\noperand must be a 32-bit value, regardless of the instruction type.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExample\nshl.b32  q,a,2;\n9.7.8.9.\nLogic and Shift Instructions:\nshr\n\nshr\nShift bits right, sign or zero-fill on left.\nSyntax\nshr.type d, a, b;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nShift\na\nright by the amount specified by unsigned 32-bit value in\nb\n. Signed shifts fill with\nthe sign bit, unsigned and untyped shifts fill with\n0\n.\nSemantics\nd = a >> b;\nNotes\nShift amounts greater than the register width\nN\nare clamped to\nN\n.\nThe sizes of the destination and first source operand must match, but not necessarily the type. The\nb\noperand must be a 32-bit value, regardless of the instruction type.\nBit-size types are included for symmetry with\nshl\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExample\nshr.u16  c,a,2;\nshr.s32  i,i,1;\nshr.b16  k,i,j;\n9.7.9.\nData Movement and Conversion Instructions\n\nThese instructions copy data from place to place, and from state space to state space, possibly\nconverting it from one format to another.\nmov\n,\nld\n,\nldu\n, and\nst\noperate on both\nscalar and vector types. The\nisspacep\ninstruction is provided to query whether a generic address\nfalls within a particular state space window. The\ncvta\ninstruction converts addresses between\ngeneric\nand\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate spaces.\nInstructions\nld\n,\nst\n,\nsuld\n, and\nsust\nsupport optional cache operations.\nThe Data Movement and Conversion Instructions are:\nmov\nshfl.sync\nprmt\nld\nldu\nst\nst.async\nst.bulk\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nprefetch\n,\nprefetchu\nisspacep\ncvta\ncvt\ncvt.pack\ncp.async\ncp.async.commit_group\ncp.async.wait_group\n,\ncp.async.wait_all\ncp.async.bulk\ncp.reduce.async.bulk\ncp.async.bulk.prefetch\nmultimem.cp.async.bulk\nmultimem.cp.reduce.async.bulk\ncp.async.bulk.tensor\ncp.reduce.async.bulk.tensor\ncp.async.bulk.prefetch.tensor\ncp.async.bulk.commit_group\ncp.async.bulk.wait_group\ntensormap.replace\n9.7.9.1.\nCache Operators\n\nPTX ISA version 2.0 introduced optional cache operators on load and store instructions. The cache\noperators require a target architecture of\nsm_20\nor higher.\nCache operators on load or store instructions are treated as performance hints only. The use of a\ncache operator on an\nld\nor\nst\ninstruction does not change the memory consistency behavior of\nthe program.\nFor\nsm_20\nand higher, the cache operators have the following definitions and behavior.\nTable 30\nCache Operators for Memory Load Instructions\n\nOperator\nMeaning\n.ca\nCache at all levels, likely to be accessed again.\nThe default load instruction cache operation is ld.ca, which allocates cache lines in all\nlevels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but\nmultiple L1 caches are not coherent for global data. If one thread stores to global memory\nvia one L1 cache, and a second thread loads that address via a second L1 cache with\nld.ca\n,\nthe second thread may get stale L1 cache data, rather than the data stored by the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of parallel threads.\nStores by the first grid program are then correctly fetched by the second grid program issuing\ndefault\nld.ca\nloads cached in L1.\n.cg\nCache at global level (cache in L2 and below, not L1).\nUse\nld.cg\nto cache loads only globally, bypassing the L1 cache, and cache only in the L2\ncache.\n.cs\nCache streaming, likely to be accessed once.\nThe\nld.cs\nload cached streaming operation allocates global lines with evict-first policy in\nL1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or\ntwice. When\nld.cs\nis applied to a Local window address, it performs the\nld.lu\noperation.\n.lu\nLast use.\nThe compiler/programmer may use\nld.lu\nwhen restoring spilled registers and popping function\nstack frames to avoid needless write-backs of lines that will not be used again. The\nld.lu\ninstruction performs a load cached streaming operation (\nld.cs\n) on global addresses.\n.cv\nDon’t cache and fetch again (consider cached system memory lines stale, fetch again).\nThe ld.cv load operation applied to a global System Memory address invalidates (discards) a\nmatching L2 line and re-fetches the line on each new load.\nTable 31\nCache Operators for Memory Store Instructions\n\nOperator\nMeaning\n.wb\nCache write-back all coherent levels.\nThe default store instruction cache operation is\nst.wb\n, which writes back cache lines of\ncoherent cache levels with normal eviction policy.\nIf one thread stores to global memory, bypassing its L1 cache, and a second thread in a\ndifferent SM later loads from that address via a different L1 cache with\nld.ca\n, the second\nthread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored\nby the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of thread arrays.\nStores by the first grid program are then correctly missed in L1 and fetched by the second grid\nprogram issuing default\nld.ca\nloads.\n.cg\nCache at global level (cache in L2 and below, not L1).\nUse\nst.cg\nto cache global store data only globally, bypassing the L1 cache, and cache only\nin the L2 cache.\n.cs\nCache streaming, likely to be accessed once.\nThe\nst.cs\nstore cached-streaming operation allocates cache lines with evict-first policy to\nlimit cache pollution by streaming output data.\n.wt\nCache write-through (to system memory).\nThe\nst.wt\nstore write-through operation applied to a global System Memory address writes\nthrough the L2 cache.\n9.7.9.2.\nCache Eviction Priority Hints\n\nPTX ISA version 7.4 adds optional cache eviction priority hints on load and store\ninstructions. Cache eviction priority requires target architecture\nsm_70\nor higher.\nCache eviction priority on load or store instructions is treated as a performance hint. It is\nsupported for\n.global\nstate space and generic addresses where the address points to\n.global\nstate space.\nTable 32\nCache Eviction Priority Hints for Memory Load and Store Instructions\n\nCache Eviction Priority\nMeaning\nevict_normal\nCache data with normal eviction priority. This is the default eviction priority.\nevict_first\nData cached with this priority will be first in the eviction priority order and\nwill likely be evicted when cache eviction is required. This priority is suitable\nfor streaming data.\nevict_last\nData cached with this priority will be last in the eviction priority order and will\nlikely be evicted only after other data with\nevict_normal\nor\nevict_first\neviction priotity is already evicted. This priority is suitable for data that\nshould remain persistent in cache.\nevict_unchanged\nDo not change eviction priority order as part of this operation.\nno_allocate\nDo not allocate data to cache. This priority is suitable for streaming data.\n9.7.9.3.\nData Movement and Conversion Instructions:\nmov\n\nmov\nSet a register variable with the value of a register variable or an immediate value. Take the\nnon-generic address of a variable in global, local, or shared state space.\nSyntax\nmov.type  d, a;\nmov.type  d, sreg;\nmov.type  d, avar;       // get address of variable\nmov.type  d, avar+imm;   // get address of variable with offset\nmov.u32   d, fname;      // get address of device function\nmov.u64   d, fname;      // get address of device function\nmov.u32   d, kernel;     // get address of entry function\nmov.u64   d, kernel;     // get address of entry function\n\n.type = { .pred,\n          .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 };\nDescription\nWrite register\nd\nwith the value of\na\n.\nOperand\na\nmay be a register, special register, variable with optional offset in an addressable\nmemory space, or function name.\nFor variables declared in\n.const\n,\n.global\n,\n.local\n, and\n.shared\nstate spaces,\nmov\nplaces the non-generic address of the variable (i.e., the address of the variable in its state\nspace) into the destination register. The generic address of a variable in\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate space may be generated by first taking the address within the state\nspace with\nmov\nand then converting it to a generic address using the\ncvta\ninstruction;\nalternately, the generic address of a variable declared in\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate space may be taken directly using the\ncvta\ninstruction.\nNote that if the address of a device function parameter is moved to a register, the parameter will\nbe copied onto the stack and the address will be in the local state space.\nSemantics\nd = a;\nd = sreg;\nd = &avar;        // address is non-generic; i.e., within the variable's declared state space\nd = &avar+imm;\nNotes\nAlthough only predicate and bit-size types are required, we include the arithmetic types for the\nprogrammer’s convenience: their use enhances program readability and allows additional type\nchecking.\nWhen moving address of a kernel or a device function, only\n.u32\nor\n.u64\ninstruction types\nare allowed. However, if a signed type is used, it is not treated as a compilation error. The\ncompiler issues a warning in this case.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTaking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function\naddresses should only be used in the context of CUDA Dynamic Parallelism system calls. See the\nCUDA\nDynamic Parallelism Programming Guide\nfor details.\nTarget ISA Notes\nmov.f64\nrequires\nsm_13\nor higher.\nTaking the address of kernel entry functions requires\nsm_35\nor higher.\nExamples\nmov.f32  d,a;\nmov.u16  u,v;\nmov.f32  k,0.1;\nmov.u32  ptr, A;        // move address of A into ptr\nmov.u32  ptr, A[5];     // move address of A[5] into ptr\nmov.u32  ptr, A+20;     // move address with offset into ptr\nmov.u32  addr, myFunc;  // get address of device function 'myFunc'\nmov.u64  kptr, main;    // get address of entry function 'main'\n9.7.9.4.\nData Movement and Conversion Instructions:\nmov\n\nmov\nMove vector-to-scalar (pack) or scalar-to-vector (unpack).\nSyntax\nmov.type  d, a;\n\n.type = { .b16, .b32, .b64, .b128 };\nDescription\nWrite scalar register\nd\nwith the packed value of vector register\na\n, or write vector register\nd\nwith the unpacked values from scalar register\na\n.\nWhen destination operand\nd\nis a vector register, the sink symbol\n'_'\nmay be used for one or\nmore elements provided that at least one element is a scalar register.\nFor bit-size types,\nmov\nmay be used to pack vector elements into a scalar register or unpack\nsub-fields of a scalar register into a vector. Both the overall size of the vector and the size of\nthe scalar must match the size of the instruction type.\nSemantics\n// pack two 8-bit elements into .b16\nd = a.x | (a.y << 8)\n// pack four 8-bit elements into .b32\nd = a.x | (a.y << 8)  | (a.z << 16) | (a.w << 24)\n// pack two 16-bit elements into .b32\nd = a.x | (a.y << 16)\n// pack four 16-bit elements into .b64\nd = a.x | (a.y << 16)  | (a.z << 32) | (a.w << 48)\n// pack two 32-bit elements into .b64\nd = a.x | (a.y << 32)\n// pack four 32-bit elements into .b128\nd = a.x | (a.y << 32)  | (a.z << 64) | (a.w << 96)\n// pack two 64-bit elements into .b128\nd = a.x | (a.y << 64)\n\n// unpack 8-bit elements from .b16\n{ d.x, d.y } = { a[0..7], a[8..15] }\n// unpack 8-bit elements from .b32\n{ d.x, d.y, d.z, d.w }\n        { a[0..7], a[8..15], a[16..23], a[24..31] }\n\n// unpack 16-bit elements from .b32\n{ d.x, d.y }  = { a[0..15], a[16..31] }\n// unpack 16-bit elements from .b64\n{ d.x, d.y, d.z, d.w } =\n        { a[0..15], a[16..31], a[32..47], a[48..63] }\n\n// unpack 32-bit elements from .b64\n{ d.x, d.y } = { a[0..31], a[32..63] }\n\n// unpack 32-bit elements from .b128\n{ d.x, d.y, d.z, d.w } =\n        { a[0..31], a[32..63], a[64..95], a[96..127] }\n// unpack 64-bit elements from .b128\n{ d.x, d.y } = { a[0..63], a[64..127] }\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nTarget ISA Notes\nSupported on all target architectures.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nExamples\nmov.b32 %r1,{a,b};      // a,b have type .u16\nmov.b64 {lo,hi}, %x;    // %x is a double; lo,hi are .u32\nmov.b32 %r1,{x,y,z,w};  // x,y,z,w have type .b8\nmov.b32 {r,g,b,a},%r1;  // r,g,b,a have type .u8\nmov.b64 {%r1, _}, %x;   // %x is.b64, %r1 is .b32\nmov.b128 {%b1, %b2}, %y;   // %y is.b128, %b1 and % b2 are .b64\nmov.b128 %y, {%b1, %b2};   // %y is.b128, %b1 and % b2 are .b64\n9.7.9.5.\nData Movement and Conversion Instructions:\nshfl\n(deprecated)\n\nshfl\n(deprecated)\nRegister data shuffle within threads of a warp.\nSyntax\nshfl.mode.b32  d[|p], a, b, c;\n\n.mode = { .up, .down, .bfly, .idx };\nDeprecation Note\nThe\nshfl\ninstruction without a\n.sync\nqualifier is deprecated in PTX ISA version 6.0.\nSupport for this instruction with\n.target\nlower than\nsm_70\nmay be removed in a future PTX ISA version.\nRemoval Note\nSupport for\nshfl\ninstruction without a\n.sync\nqualifier is removed in PTX ISA version 6.4 for\n.target\nsm_70\nor higher.\nDescription\nExchange register data between threads of a warp.\nEach thread in the currently executing warp will compute a source lane index\nj\nbased on input\noperands\nb\nand\nc\nand the\nmode\n. If the computed source lane index\nj\nis in range, the\nthread will copy the input operand\na\nfrom lane\nj\ninto its own destination register\nd\n;\notherwise, the thread will simply copy its own input\na\nto destination\nd\n. The optional\ndestination predicate\np\nis set to\nTrue\nif the computed source lane is in range, and\notherwise set to\nFalse\n.\nNote that an out of range value of\nb\nmay still result in a valid computed source lane index\nj\n. In this case, a data transfer occurs and the destination predicate\np\nis True.\nNote that results are undefined in divergent control flow within a warp, if an active thread sources\na register from an inactive thread.\nOperand\nb\nspecifies a source lane or source lane offset, depending on the mode.\nOperand\nc\ncontains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index.\nSemantics\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & mask[4:0]) | (cval[4:0] & ~mask[4:0]);\nminLane = (lane[4:0] & mask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~mask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval;\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nDeprecated in PTX ISA version 6.0 in favor of\nshfl.sync\n.\nNot supported in PTX ISA version 6.4 for .target\nsm_70\nor higher.\nTarget ISA Notes\nshfl\nrequires\nsm_30\nor higher.\nshfl\nis not supported on\nsm_70\nor higher starting PTX ISA version 6.4.\nExamples\n// Warp-level INCLUSIVE PLUS SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.up.b32  Ry|p, Rx, 0x1,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x2,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x4,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x8,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x10, 0x0;\n@p  add.f32      Rx, Ry, Rx;\n\n\n    // Warp-level INCLUSIVE PLUS REVERSE-SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.down.b32  Ry|p, Rx, 0x1,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x2,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x4,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x8,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x10, 0x1f;\n@p  add.f32        Rx, Ry, Rx;\n\n\n    // BUTTERFLY REDUCTION:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.bfly.b32  Ry, Rx, 0x10, 0x1f;   // no predicate dest\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x8,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x4,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x2,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x1,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    //\n    // All threads now hold sum in Rx\n9.7.9.6.\nData Movement and Conversion Instructions:\nshfl.sync\n\nshfl.sync\nRegister data shuffle within threads of a warp.\nSyntax\nshfl.sync.mode.b32  d[|p], a, b, c, membermask;\n\n.mode = { .up, .down, .bfly, .idx };\nDescription\nExchange register data between threads of a warp.\nshfl.sync\nwill cause executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nshfl.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s\nlaneid\n.\nshfl.sync\nexchanges register data between threads in\nmembermask\n.\nEach thread in the currently executing warp will compute a source lane index\nj\nbased on input\noperands\nb\nand\nc\nand the\nmode\n. If the computed source lane index\nj\nis in range, the\nthread will copy the input operand\na\nfrom lane\nj\ninto its own destination register\nd\n;\notherwise, the thread will simply copy its own input\na\nto destination\nd\n. The optional\ndestination predicate\np\nis set to\nTrue\nif the computed source lane is in range, and\notherwise set to\nFalse\n.\nNote that an out of range value of\nb\nmay still result in a valid computed source lane index\nj\n. In this case, a data transfer occurs and the destination predicate\np\nis True.\nNote that results are undefined if a thread sources a register from an inactive thread or a thread\nthat is not in\nmembermask\n.\nOperand\nb\nspecifies a source lane or source lane offset, depending on the mode.\nOperand\nc\ncontains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index.\nThe behavior of\nshfl.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nshfl.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when\nthe\nshfl.sync\ninstruction is executed. Otherwise, the behavior is undefined.\nSemantics\n// wait for all threads in membermask to arrive\nwait_for_specified_threads(membermask);\n\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nsegmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & segmask[4:0]) | (cval[4:0] & ~segmask[4:0]);\nminLane = (lane[4:0] & segmask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~segmask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval;\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nshfl.sync.up.b32  Ry|p, Rx, 0x1,  0x0, 0xffffffff;\n9.7.9.7.\nData Movement and Conversion Instructions:\nprmt\n\nprmt\nPermute bytes from register pair.\nSyntax\nprmt.b32{.mode}  d, a, b, c;\n\n.mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 };\nDescription\nPick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination\nregister.\nIn the generic form (no mode specified), the permute control consists of four 4-bit selection\nvalues. The bytes in the two source registers are numbered from 0 to 7:\n{b,\na}\n=\n{{b7,\nb6,\nb5,\nb4},\n{b3,\nb2,\nb1,\nb0}}\n. For each byte in the target register, a 4-bit selection value is defined.\nThe 3 lsbs of the selection value specify which of the 8 source bytes should be moved into the\ntarget position. The msb defines if the byte value should be copied, or if the sign (msb of the\nbyte) should be replicated over all 8 bits of the target position (sign extend of the byte value);\nmsb=0\nmeans copy the literal value;\nmsb=1\nmeans replicate the sign. Note that the sign\nextension is only performed as part of generic form.\nThus, the four 4-bit values fully specify an arbitrary byte permute, as a\n16b\npermute code.\ndefault mode\nd.b3\nsource select\nd.b2\nsource select\nd.b1\nsource select\nd.b0\nsource select\nindex\nc[15:12]\nc[11:8]\nc[7:4]\nc[3:0]\nThe more specialized form of the permute control uses the two lsb’s of operand\nc\n(which is\ntypically an address pointer) to control the byte extraction.\nmode\nselector\nc[1:0]\nd.b3\nsource\nd.b2\nsource\nd.b1\nsource\nd.b0\nsource\nf4e\n(forward 4 extract)\n0\n3\n2\n1\n0\n1\n4\n3\n2\n1\n2\n5\n4\n3\n2\n3\n6\n5\n4\n3\nb4e\n(backward 4 extract)\n0\n5\n6\n7\n0\n1\n6\n7\n0\n1\n2\n7\n0\n1\n2\n3\n0\n1\n2\n3\nrc8\n(replicate 8)\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\necl\n(edge clamp left)\n0\n3\n2\n1\n0\n1\n3\n2\n1\n1\n2\n3\n2\n2\n2\n3\n3\n3\n3\n3\necr\n(edge clamp right)\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n2\n2\n2\n1\n0\n3\n3\n2\n1\n0\nrc16\n(replicate 16)\n0\n1\n0\n1\n0\n1\n3\n2\n3\n2\n2\n1\n0\n1\n0\n3\n3\n2\n3\n2\nSemantics\ntmp64 = (b<<32) | a;  // create 8 byte source\n\nif ( ! mode ) {\n   ctl[0] = (c >>  0) & 0xf;\n   ctl[1] = (c >>  4) & 0xf;\n   ctl[2] = (c >>  8) & 0xf;\n   ctl[3] = (c >> 12) & 0xf;\n} else {\n   ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >>  0) & 0x3;\n}\n\ntmp[07:00] = ReadByte( mode, ctl[0], tmp64 );\ntmp[15:08] = ReadByte( mode, ctl[1], tmp64 );\ntmp[23:16] = ReadByte( mode, ctl[2], tmp64 );\ntmp[31:24] = ReadByte( mode, ctl[3], tmp64 );\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nprmt\nrequires\nsm_20\nor higher.\nExamples\nprmt.b32      r1, r2, r3, r4;\nprmt.b32.f4e  r1, r2, r3, r4;\n9.7.9.8.\nData Movement and Conversion Instructions:\nld\n\nld\nLoad a register variable from an addressable state space variable.\nSyntax\nld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld.volatile{.ss}{.level::prefetch_size}{.vec}.type  d, [a];\n\nld.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.acquire.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.mmio.relaxed.sys{.global}.type  d, [a];\n\n.ss =                       { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} };\n.cop =                      { .ca, .cg, .cs, .lu, .cv };\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nLoad register variable\nd\nfrom the location specified by the source address operand\na\nin\nspecified state space. If no state space is given, perform the load using\nGeneric Addressing\n.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nIf no sub-qualifier is specified with\n.param\nstate space, then:\n::func\nis assumed when access is inside a device function.\n::entry\nis assumed when accessing kernel function parameters from entry function. Otherwise, when\naccessing device function parameters or any other\n.param\nvariables from entry function\n::func\nis assumed by default.\nFor\nld.param::entry\ninstruction, operand a must be a kernel parameter address, otherwise behavior\nis undefined. For\nld.param::func\ninstruction, operand a must be a device function parameter address,\notherwise behavior is undefined.\nInstruction\nld.param{::func}\nused for reading value returned from device function call cannot be\npredicated. See\nParameter State Space\nand\nFunction Declarations and Definitions\nfor descriptions\nof the proper use of\nld.param\n.\nThe\n.relaxed\nand\n.acquire\nqualifiers indicate memory synchronization as described in the\nMemory Consistency Model\n. The\n.scope\nqualifier\nindicates the set of threads with which an\nld.relaxed\nor\nld.acquire\ninstruction can directly\nsynchronize\n1\n. The\n.weak\nqualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means.\nThe semantic details of\n.mmio\nqualifier are described in the\nMemory Consistency Model\n.\nOnly\n.sys\nthread scope is valid for\nld.mmio\noperation. The\nqualifiers\n.mmio\nand\n.relaxed\nmust be specified together.\nThe semantic details of\n.volatile\nqualifier are described in the\nMemory Consistency Model\n.\nThe\n.weak\n,\n.volatile\n,\n.relaxed\nand\n.acquire\nqualifiers are mutually exclusive. When\nnone of these is specified, the\n.weak\nqualifier is assumed by default.\n.relaxed\nand\n.acquire\n:\nMay be used with\n.global\n,\n.shared\nspaces, or with generic addressing where the address\npoints to\n.global\nor\n.shared\nspace.\nCache operations are not allowed.\n.volatile\n:\nMay be used with\n.global\n,\n.shared\n,\n.local\nspaces, or with generic addressing where\nthe address points to\n.global\n,\n.shared\n, or\n.local\nspace.\nCache operations are not allowed.\n.mmio\n:\nMay be used only with\n.global\nspace or with generic addressing where the address points to\n.global\nspace.\nThe optional qualifier\n.unified\nmust be specified on operand\na\nif\na\nis the address of a\nvariable declared with\n.unified\nattribute as described in\nVariable and Function Attribute Directive: .attribute\n.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nd\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that data from corresponding memory location is not read.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe qualifier\n.level::prefetch_size\nmay only be used with\n.global\nstate space and with\ngeneric addressing where the address points to\n.global\nstate space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifiers\n.unified\nand\n.level::cache_hint\nare only supported for\n.global\nstate\nspace and for generic addressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\n1\nThis synchronization is further extended to other threads through the transitive nature of\ncausality order\n, as described in the memory consistency model.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See\nTable 28\nfor a description of these relaxed type-checking rules.\n.f16\ndata may be loaded using\nld.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\nor can be used in half precision floating point instructions.\n.f16x2\ndata may be loaded using\nld.b32\nand then used in half precision floating point\ninstructions.\nPTX ISA Notes\nld introduced in PTX ISA version 1.0.\nld.volatile\nintroduced in PTX ISA version 1.1.\nGeneric addressing and cache operations introduced in PTX ISA version 2.0.\nSupport for scope qualifier,\n.relaxed\n,\n.acquire\n,\n.weak\nqualifiers introduced in PTX ISA\nversion 6.0.\nSupport for generic addressing of .const space added in PTX ISA version 3.1.\nSupport for\n.level1::eviction_priority\n,\n.level::prefetch_size\nand\n.level::cache_hint\nqualifiers introduced in PTX ISA version 7.4.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for\n.unified\nqualifier introduced in PTX ISA version 8.0.\nSupport for\n.mmio\nqualifier introduced in PTX ISA version 8.2.\nSupport for\n::entry\nand\n::func\nsub-qualifiers on\n.param\nspace introduced in PTX ISA\nversion 8.3.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nSupport for\n.volatile\nqualifier with\n.local\nstate space introduced in PTX ISA version 9.1.\nTarget ISA Notes\nld.f64\nrequires\nsm_13\nor higher.\nSupport for scope qualifier,\n.relaxed\n,\n.acquire\n,\n.weak\nqualifiers require\nsm_70\nor\nhigher.\nGeneric addressing requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nSupport for\n.level::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::prefetch_size\nqualifier requires\nsm_75\nor higher.\nSupport for\n.L2::256B\nand\n.L2::cache_hint\nqualifiers requires\nsm_80\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.unified\nqualifier requires\nsm_90\nor higher.\nSupport for\n.mmio\nqualifier requires\nsm_70\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nld.global.f32    d,[a];\nld.shared.v4.b32 Q,[p];\nld.const.s32     d,[p+4];\nld.local.b32     x,[p+-8]; // negative offset\nld.local.b64     x,[240];  // immediate address\n\nld.global.b16    %r,[fs];  // load .f16 data into 32-bit reg\ncvt.f32.f16      %r,%r;    // up-convert f16 data to f32\n\nld.global.b32    %r0, [fs];     // load .f16x2 data in 32-bit reg\nld.global.b32    %r1, [fs + 4]; // load .f16x2 data in 32-bit reg\nadd.rn.f16x2     %d0, %r0, %r1; // addition of f16x2 data\nld.global.relaxed.gpu.u32 %r0, [gbl];\nld.shared.acquire.gpu.u32 %r1, [sh];\nld.global.relaxed.cluster.u32 %r2, [gbl];\nld.shared::cta.acquire.gpu.u32 %r2, [sh + 4];\nld.shared::cluster.u32 %r3, [sh + 8];\nld.global.mmio.relaxed.sys.u32 %r3, [gbl];\nld.local.volatile.u32 %r4, [lcl];\n\nld.global.f32    d,[ugbl].unified;\nld.b32           %r0, [%r1].unified;\n\nld.global.L1::evict_last.u32  d, [p];\n\nld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2\nld.L2::128B.f64         %r1, [gbl]; // Prefetch 128B to L2\nld.global.L2::256B.f64  %r2, [gbl]; // Prefetch 256B to L2\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1;\nld.global.L2::cache_hint.b64  x, [p], cache-policy;\nld.param::entry.b32 %rp1, [kparam1];\n\nld.global.b128   %r0, [gbl];   // 128-bit load\n\n// 256-bit load\nld.global.L2::evict_last.v8.f32 { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7}, [addr];\nld.global.L2::evict_last.L1::evict_last.v4.u64 { %reg0, %reg1, %reg2, %reg3}, [addr];\n9.7.9.9.\nData Movement and Conversion Instructions:\nld.global.nc\n\nld.global.nc\nLoad a register variable from global state space via non-coherent cache.\nSyntax\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type                 d, [a]{, cache-policy};\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type             d, [a]{, cache-policy};\n\nld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type      d, [a]{, cache-policy};\nld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type  d, [a]{, cache-policy};\n\n.cop  =                     { .ca, .cg, .cs };     // cache operation\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate};\n.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.vec  =                     { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nLoad register variable\nd\nfrom the location specified by the source address operand\na\nin the\nglobal state space, and optionally cache in non-coherent read-only cache.\nNote\nOn some architectures, the texture cache is larger, has higher bandwidth, and longer latency than\nthe global memory cache. For applications with sufficient parallelism to cover the longer\nlatency,\nld.global.nc\nshould offer better performance than\nld.global\non such\narchitectures.\nThe address operand\na\nshall contain a global address.\nSupported addressing modes for operand\na\nand alignment requirements are\ndescribed in\nAddresses as Operands\n.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\n,\n.s32\n,\n.u32\n, or\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nd\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that data from corresponding memory location is not read.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types.\n.f16\ndata may be loaded using\nld.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nSupport for\n.level::eviction_priority\n,\n.level::prefetch_size\nand\n.level::cache_hint\nqualifiers introduced in PTX ISA version 7.4.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nRequires\nsm_32\nor higher.\nSupport for\n.level1::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::prefetch_size\nqualifier requires\nsm_75\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nld.global.nc.f32           d, [a];\nld.gloal.nc.L1::evict_last.u32 d, [a];\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.5;\nld.global.nc.L2::cache_hint.f32  d, [a], cache-policy;\n\nld.global.nc.L2::64B.b32      d,  [a];     // Prefetch 64B to L2\nld.global.nc.L2::256B.f64     d,  [a];     // Prefetch 256B to L2\n\nld.global.nc.b128             d,  [a];\n\nld.global.nc.L2::evict_first.v4.f64 {%reg0, %reg1. %reg2, %reg3}. [a]; // 256-bit load\n9.7.9.10.\nData Movement and Conversion Instructions:\nldu\n\nldu\nLoad read-only data from an address that is common across threads in the warp.\nSyntax\nldu{.ss}.type      d, [a];       // load from address\nldu{.ss}.vec.type  d, [a];       // vec load from address\n\n.ss   = { .global };             // state space\n.vec  = { .v2, .v4 };\n.type = { .b8, .b16, .b32, .b64, .b128,\n          .u8, .u16, .u32, .u64,\n          .s8, .s16, .s32, .s64,\n                     .f32, .f64 };\nDescription\nLoad\nread-only\ndata into register variable\nd\nfrom the location specified by the source address\noperand\na\nin the global state space, where the address is guaranteed to be the same across all\nthreads in the warp. If no state space is given, perform the load using\nGeneric Addressing\n.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\n.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See\nTable 28\nfor a description of these relaxed type-checking rules.\n.f16\ndata may be loaded using\nldu.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\nor can be used in half precision floating point instructions.\n.f16x2\ndata may be loaded using\nldu.b32\nand then used in half precision floating point\ninstructions.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nTarget ISA Notes\nldu.f64\nrequires\nsm_13\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nExamples\nldu.global.f32    d,[a];\nldu.global.b32    d,[p+4];\nldu.global.v4.f32 Q,[p];\nldu.global.b128   d,[a];\n9.7.9.11.\nData Movement and Conversion Instructions:\nst\n\nst\nStore data to an addressable state space variable.\nSyntax\nst{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type   [a], b{, cache-policy};\nst{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.volatile{.ss}{.vec}.type                           [a], b;\nst.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.release.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.mmio.relaxed.sys{.global}.type         [a], b;\n\n.ss =                       { .global, .local, .param{::func}, .shared{::cta, ::cluster} };\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level2::eviction_priority = { .L2::evict_normal, .L2::evict_first, .L2::evict_last };\n.level::cache_hint =        { .L2::cache_hint };\n.cop =                      { .wb, .cg, .cs, .wt };\n.sem =                      { .relaxed, .release };\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nStore the value of operand\nb\nin the location specified by the destination address\noperand\na\nin specified state space. If no state space is given, perform the store using\nGeneric Addressing\n. Stores to const memory are illegal.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\n.\nIf\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::func\n.\nInstruction\nst.param{::func}\nused for passing arguments to device function cannot be predicated.\nSee\nParameter State Space\nand\nFunction Declarations and Definitions\nfor descriptions of the proper use\nof\nst.param\n.\nThe qualifiers\n.relaxed\nand\n.release\nindicate memory synchronization as described in the\nMemory Consistency Model\n. The\n.scope\nqualifier\nindicates the set of threads with which an\nst.relaxed\nor\nst.release\ninstruction can directly\nsynchronize\n1\n. The\n.weak\nqualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means.\nThe semantic details of\n.mmio\nqualifier are described in the\nMemory Consistency Model\n.\nOnly\n.sys\nthread scope is valid for\nst.mmio\noperation. The\nqualifiers\n.mmio\nand\n.relaxed\nmust be specified together.\nThe semantic details of\n.volatile\nqualifier are described in the\nMemory Consistency Model\n.\nThe\n.weak\n,\n.volatile\n,\n.relaxed\nand\n.release\nqualifiers are mutually exclusive. When\nnone of these is specified, the\n.weak\nqualifier is assumed by default.\n.relaxed\nand\n.release\n:\nMay be used with\n.global\n,\n.shared\nspaces or with generic addressing where the address points\nto\n.global\nor\n.shared\nspace.\nCache operations are not allowed.\n.volatile\n:\nMay be used with\n.global\n,\n.shared\n,\n.local\nspaces or with generic addressing where the\naddress points to\n.global\n,\n.shared\n, or\n.local\nspace.\nCache operations are not allowed.\n.mmio\n:\nMay be used only with\n.global\nspace or with generic addressing where the address points to\n.global\nspace.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\n,\n.s32\n,\n.u32\n, or\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nb\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that no data is being written at the corresponding destination address.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\n1\nThis synchronization is further extended to other threads through the transitive nature of\ncausality order\n, as described in the memory consistency model.\nSemantics\nd = a;                // named variable d\n*(&a+immOffset) = b;            // variable-plus-offset\n*a = b;               // register\n*(a+immOffset) = b;   // register-plus-offset\n*(immAddr) = b;       // immediate address\nNotes\nOperand\nb\nmust be in the\n.reg\nstate space.\nA source register wider than the specified type may be used. The lower\nn\nbits corresponding to\nthe instruction-type width are stored to memory. See\nTable 27\nfor a description of these relaxed type-checking rules.\n.f16\ndata resulting from a\ncvt\ninstruction may be stored using\nst.b16\n.\n.f16x2\ndata may be stored using\nst.b32\n.\nPTX ISA Notes\nst introduced in PTX ISA version 1.0.\nst.volatile\nintroduced in PTX ISA version 1.1.\nGeneric addressing and cache operations introduced in PTX ISA version 2.0.\nSupport for scope qualifier,\n.relaxed\n,\n.release\n,\n.weak\nqualifiers introduced in PTX ISA\nversion 6.0.\nSupport for\n.level1::eviction_priority\nand\n.level::cache_hint\nqualifiers introduced in PTX\nISA version 7.4.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for\n.mmio\nqualifier introduced in PTX ISA version 8.2.\nSupport for\n::func\nsub-qualifier on\n.param\nspace introduced in PTX ISA version 8.3.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nSupport for\n.volatile\nqualifier with\n.local\nstate space introduced in PTX ISA version 9.1.\nTarget ISA Notes\nst.f64\nrequires\nsm_13\nor higher.\nSupport for scope qualifier,\n.relaxed\n,\n.release\n,\n.weak\nqualifiers require\nsm_70\nor\nhigher.\nGeneric addressing requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nSupport for\n.level1::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.mmio\nqualifier requires\nsm_70\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nst.global.f32    [a],b;\nst.local.b32     [q+4],a;\nst.global.v4.s32 [p],Q;\nst.local.b32     [q+-8],a; // negative offset\nst.local.s32     [100],r7; // immediate address\n\ncvt.f16.f32      %r,%r;    // %r is 32-bit register\nst.b16           [fs],%r;  // store lower\nst.global.relaxed.sys.u32 [gbl], %r0;\nst.shared.release.cta.u32 [sh], %r1;\nst.global.relaxed.cluster.u32 [gbl], %r2;\nst.shared::cta.release.cta.u32 [sh + 4], %r1;\nst.shared::cluster.u32 [sh + 8], %r1;\nst.global.mmio.relaxed.sys.u32 [gbl], %r1;\nst.local.volatile.u32 [lcl], %r2;\n\nst.global.L1::no_allocate.f32 [p], a;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nst.global.L2::cache_hint.b32  [a], b, cache-policy;\n\nst.param::func.b64 [param1], %rp1;\n\nst.global.b128  [a], b;  // 128-bit store\n\n// 256-bit store\nst.global.L2::evict_last.v8.f32 [addr], { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7};\n9.7.9.12.\nData Movement and Conversion Instructions:\nst.async\n\nst.async\nAsynchronous store operation.\nSyntax\nst.async{.weak}{.ss}.completion_mechanism{.vec}.type [a], b, [mbar];\nst.async{.scope}{.ss}.completion_mechanism{.vec}.type [a], b, [mbar];\n\n.scope =                { .cluster };\n.ss   =                 { .shared::cluster };\n.type =                 { .b32, .b64,\n                          .u32, .u64,\n                          .s32, .s64,\n                          .f32, .f64 };\n.vec  =                 { .v2, .v4 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\nst.async{.mmio}.sem.scope{.ss}.type [a], b;\n\n.sem =                  { .release };\n.scope =                { .gpu, .sys };\n.ss =                   { .global };\n.type =                 { .b8, .b16, .b32, .b64,\n                          .u8, .u16, .u32, .u64,\n                          .s8, .s16, .s32, .s64,\n                                     .f32, .f64 };\nDescription\nst.async\nis a non-blocking instruction which initiates an asynchronous store operation that\nstores the value specified by source operand\nb\nto the destination memory location\nspecified by operand\na\n.\nOperands\na\nis a destination address, and must be either a register, or of the form\nregister\n+\nimmOff\n,\nas described in\nAddresses as Operands\n.\nb\nis a source value, of the type indicated by qualifier\n.type\n.\nmbar\nis an mbarrier object address.\nQualifiers\n.mmio\nindicates whether this is an\nmmio Operation\n.\n.sem\nspecifies the memory ordering semantics as described in the\nMemory Consistency Model\n.\nIf\n.sem\nis not specified, it defaults to\n.weak\n.\n.scope\nspecifies the set of threads with which this instruction can directly synchronize.\n.ss\nspecifies the state space of the destination operand\na\nand the mbarrier\noperand\nmbar\n.\nIf\n.ss\nis not specified,\nGeneric Addressing\nis used.\n.completion_mechanism\nspecifies the mechanism for observing the completion of the\nasynchronous operation.\nWhen\n.completion_mechanism\nis\n.mbarrier::complete_tx::bytes\n: upon completion of the\nasynchronous operation, a\ncomplete-tx\noperation will be performed on the mbarrier object specified by the operand\nmbar\n, with\ncompleteCount\nargument equal to the amount of data stored in bytes.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\n.type\nspecifies the type of the source operand\nb\n.\nConditions\nWhen\n.sem\nis\n.weak\n:\nThis is a weak store to shared memory, which signals its completion through an mbarrier object.\nThe store operation is treated as a weak memory operation.\nThe complete-tx operation on the mbarrier has\n.release\nsemantics at\n.cluster\nscope.\nRequires:\nThe shared memory addresses of destination operand\na\nand the\nmbarrier object\nmbar\nbelong\nto the same CTA within the same cluster as the executing thread.\nThe number of CTAs within the cluster is strictly greater than one;\n%cluster_nctarank\n>\n1\nis true.\nOtherwise, the behavior is undefined.\n.mmio\nmust not be specified.\nIf\n.ss\nis specified, it must be\n.shared::cluster\n.\nIf\n.ss\nis not specified, generic addressing is used for operands\na\nand\nmbar\n.\nIf the generic addresses specified do not fall within the address window of\n.shared::cluster\nstate space, the behavior is undefined.\n.completion_mechanism\nmust be specified and must be\n.mbarrier::complete_tx::bytes\n.\nWhen\n.sem\nis\n.release\n:\nThis is a release store to global memory.\nThe store operation is a strong memory operation with\n.release\nsemantics at the\nscope specified by\n.scope\n.\nIf\n.mmio\nis specified,\n.scope\nmust be\n.sys\n.\nIf\n.ss\nis specified, it must be\n.global\n.\nIf\n.ss\nis not specified, generic addressing is used for operand\na\n.\nIf the generic address specified does not fall within the address window of\n.global\nstate space, the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space, and\n.scope\nqualifier introduced in PTX ISA version 8.7.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space, and\n.scope\nqualifier require\nsm_100\nor higher.\nExamples\nst.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr];\n\nst.async.sys.release.global.u32 [addr], b;\n9.7.9.13.\nData Movement and Conversion Instructions:\nst.bulk\n\nst.bulk\nInitializes a region of memory as specified by state space.\nSyntax\nst.bulk{.weak}{.shared::cta}  [a], size, initval; // initval must be zero\nDescription\nst.bulk\ninstruction initializes a region of shared memory starting from the location specified\nby destination address operand\na\n.\nThe 32-bit or 64-bit integer operand\nsize\nspecifies the amount of memory to be initialized in terms of\nnumber of bytes.\nsize\nmust be a multiple of 8. If the value is not a multiple of 8, then the\nbehavior is undefined. The maximum value of\nsize\noperand can be 16777216.\nThe integer immediate operand\ninitval\nspecifies the initialization value for the memory\nlocations. The only numeric value allowed for operand\ninitval\nis 0.\nIf no state space is specified then\nGeneric Addressing\nis used. If the\naddress specified by\na\ndoes not fall within the address window of\n.shared\nstate space then\nthe behavior is undefined.\nThe optional qualifier\n.weak\nspecify the memory synchronizing effect of the\nst.bulk\ninstruction as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nSupport for\nsize\noperand with 32-bit length is introduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nExamples\nst.bulk.weak.shared::cta  [dst], n, 0;\n\nst.bulk                   [gdst], 4096, 0;\n9.7.9.14.\nData Movement and Conversion Instructions:\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\n\nThe multimem.* operations operate on multimem addresses and accesses all of the multiple memory\nlocations which the multimem address points to.\nMultimem addresses can only be accessed only by multimem.* operations. Accessing a multimem address\nwith\nld\n,\nst\nor any other memory operations results in undefined behavior.\nRefer to\nCUDA programming guide\nfor creation and management of the multimem addresses.\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nPerform memory operations on the multimem address.\nSyntax\n// Integer type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type      d, [a];\nmultimem.ld_reduce.weak{.ss}.op.type                 d, [a];\n\nmultimem.st{.stsem}{.scope}{.ss}.type                [a], b;\nmultimem.st.weak{.ss}.type                           [a], b;\n\nmultimem.red{.redsem}{.scope}{.ss}.op.type           [a], b;\n\n.ss =       { .global }\n.ldsem =    { .relaxed, .acquire }\n.stsem =    { .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add, .and, .or, .xor }\n.type =     { .b32, .b64,  .u32, .u64, .s32, .s64 }\n\n// Floating point type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type    d, [a];\nmultimem.ld_reduce.weak{.ss}.op{.acc_prec}{.vec}.type               d, [a];\n\nmultimem.st{.stsem}{.scope}{.ss}{.vec}.type                         [a], b;\nmultimem.st.weak{.ss}{.vec}.type                                    [a], b;\n\nmultimem.red{.redsem}{.scope}{.ss}.redop{.vec}.redtype              [a], b;\n\n.ss =       { .global }\n.ldsem =    { .relaxed, .acquire }\n.stsem =    { .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add }\n.redop  =   { .add }\n.acc_prec = { .acc::f32, .acc::f16 }\n.vec =      { .v2, .v4, .v8 }\n.type=      { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64, .e5m2, .e5m2x2, .e5m2x4, .e4m3, .e4m3x2, .e4m3x4 }\n.redtype =  { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 }\nDescription\nInstruction\nmultimem.ld_reduce\nperforms the following operations:\nload operation on the multimem address\na\n, which involves loading of data from all of the\nmultiple memory locations pointed to by the multimem address\na\n,\nreduction operation specified by\n.op\non the multiple data loaded from the multimem address\na\n.\nThe result of the reduction operation in returned in register\nd\n.\nInstruction\nmultimem.st\nperforms a store operation of the input operand\nb\nto all the memory\nlocations pointed to by the multimem address\na\n.\nInstruction\nmultimem.red\nperforms a reduction operation on all the memory locations pointed to\nby the multimem address\na\n, with operand\nb\n.\nInstruction\nmultimem.ld_reduce\nperforms reduction on the values loaded from all the memory\nlocations that the multimem address points to. In contrast, the\nmultimem.red\nperform reduction\non all the memory locations that the multimem address points to.\nAddress operand\na\nmust be a multimem address. Otherwise, the behavior is undefined.  Supported\naddressing modes for operand a and alignment requirements are described in\nAddresses as Operands\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\na\ndoes not fall within the address window of\n.global\nstate\nspace then the behavior is undefined.\nFor floating-point type multi- operations, the size of the specified type along with\n.vec\nmust\nequal either 32-bits or 64-bits or 128-bits. No other combinations of\n.vec\nand type are\nallowed. Type\n.f64\ncannot be used with\n.vec\nqualifier.\nThe following table describes the valid usage of\n.vec\nand base floating-point type:\n.vec\nBase float-type supported\nNo\n.vec\nspecified\n.f16x2\n,\n.bf16x2\n,\n.f32\n,\n.f64\n,\n.e5m2x4\n,\n.e4m3x4\n.v2\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3x2\n,\n.e4m3x4\n.v4\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\n.v8\n.f16\n,\n.bf16\n,\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\nThe following table describes the valid combinations of\n.op\nand base type:\nop\nBase type\n.add\n.u32\n,\n.u64\n,\n.s32\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.f64\n,\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nFor\nmultimem.ld_reduce\n, the default precision of the intermediate accumulation is same as the\nspecified type.\nOptionally,\n.acc_prec\nqualifier can be specified to change the precision of intermediate\naccumulation as follows:\n.type\n.acc::prec\nChanges precision to\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.acc::f32\n.f32\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\n,\n.e4m3x4\n,\n.e5m2x4\n.acc::f16\n.f16\nOptional qualifiers\n.ldsem\n,\n.stsem\nand\n.redsem\nspecify the memory synchronizing effect\nof the\nmultimem.ld_reduce\n,\nmultimem.st\nand\nmultimem.red\nrespectively, as described in\nMemory Consistency Model\n. If explicit semantics qualifiers\nare not specified, then\nmultimem.ld_reduce\nand\nmultimem.st\ndefault to\n.weak\nand\nmultimem.red\ndefaults to\n.relaxed\n.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in\nMemory Consistency Model\n. If the\n.scope\nqualifier is not specified for\nmultimem.red\nthen\n.sys\nscope is assumed by default.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.acc::f32\nqualifier introduced in PTX ISA version 8.2.\nSupport for types\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nintroduced in PTX ISA version 8.6.\nSupport for\n.acc::f16\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nTypes\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nsm_121a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.acc::f16\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nsm_121a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nmultimem.ld_reduce.and.b32                    val1_b32, [addr1];\nmultimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2];\n\nmultimem.st.relaxed.gpu.b32                [addr3], val3_b32;\nmultimem.st.release.cta.global.u32         [addr4], val4_u32;\n\nmultimem.red.relaxed.gpu.max.f64           [addr5], val5_f64;\nmultimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9};\nmultimem.ld_reduce.add.acc::f32.v2.f16x2   {val_10, val_11}, [addr7];\n\nmultimem.ld_reduce.relaxed.cta.min.v2.e4m3x2 {val_12, val_13}, [addr8];\nmultimem.ld_reduce.relaxed.cta.add.v4.e4m3   {val_14, val_15, val_16, val_17}, [addr9];\nmultimem.ld_reduce.add.acc::f16.v4.e5m2      {val_18, val_19, val_20, val_21}, [addr10];\n9.7.9.15.\nData Movement and Conversion Instructions:\nprefetch\n,\nprefetchu\n\nprefetch\n,\nprefetchu\nPrefetch line containing a generic address at a specified level of memory hierarchy, in specified\nstate space.\nSyntax\nprefetch{.space}.level                    [a];   // prefetch to data cache\nprefetch.global.level::eviction_priority  [a];   // prefetch to data cache\n\nprefetchu.L1  [a];             // prefetch to uniform cache\n\nprefetch{.tensormap_space}.tensormap [a];  // prefetch the tensormap\n\n.space =                    { .global, .local };\n.level =                    { .L1, .L2 };\n.level::eviction_priority = { .L2::evict_last, .L2::evict_normal };\n.tensormap_space =          { .const, .param };\nDescription\nThe\nprefetch\ninstruction brings the cache line containing the specified address in global or\nlocal memory state space into the specified cache level.\nIf the\n.tensormap\nqualifier is specified then the\nprefetch\ninstruction brings the cache line\ncontaining the specified address in the\n.const\nor\n.param\nmemory state space for subsequent\nuse by the\ncp.async.bulk.tensor\ninstruction.\nIf no state space is given, the\nprefetch\nuses\nGeneric Addressing\n.\nOptionally, the eviction priority to be applied on the prefetched cache line can be specified by the\nmodifier\n.level::eviction_priority\n.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe\nprefetchu\ninstruction brings the cache line containing the specified generic address into\nthe specified uniform cache level.\nA\nprefetch\nto a shared memory location performs no operation.\nA\nprefetch\ninto the uniform cache requires a generic address, and no operation occurs if the\naddress maps to a\nconst\n,\nlocal\n, or\nshared\nmemory location.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nSupport for\n.level::eviction_priority\nqualifier introduced in PTX ISA version 7.4.\nSupport for the\n.tensormap\nqualifier is introduced in PTX ISA version 8.0.\nTarget ISA Notes\nprefetch\nand\nprefetchu\nrequire\nsm_20\nor higher.\nSupport for\n.level::eviction_priority\nqualifier requires\nsm_80\nor higher.\nSupport for the\n.tensormap\nqualifier requires\nsm_90\nor higher.\nExamples\nprefetch.global.L1             [ptr];\nprefetch.global.L2::evict_last [ptr];\nprefetchu.L1  [addr];\nprefetch.const.tensormap       [ptr];\n9.7.9.16.\nData Movement and Conversion Instructions:\napplypriority\n\napplypriority\nApply the cache eviction priority to the specified address in the specified cache level.\nSyntax\napplypriority{.global}.level::eviction_priority  [a], size;\n\n.level::eviction_priority = { .L2::evict_normal };\nDescription\nThe\napplypriority\ninstruction applies the cache eviction priority specified by the\n.level::eviction_priority\nqualifier to the address range\n[a..a+size)\nin the specified cache\nlevel.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nThe operand\nsize\nis an integer constant that specifies the amount of data, in bytes, in the\nspecified cache level on which the priority is to be applied. The only supported value for the\nsize\noperand is 128.\nSupported addressing modes for operand\na\nare described in\nAddresses as Operands\n.\na\nmust be aligned to 128 bytes.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\napplypriority.global.L2::evict_normal [ptr], 128;\n9.7.9.17.\nData Movement and Conversion Instructions:\ndiscard\n\ndiscard\nDiscard the data at the specified address range and cache level.\nSyntax\ndiscard{.global}.level  [a], size;\n\n.level = { .L2 };\nDescription\nSemantically, this behaves like a weak write of an\nunstable indeterminate value\n:\nreads of memory locations with\nunstable indeterminate values\nmay return different\nbit patterns each time until the memory is overwritten.\nThis operation\nhints\nto the implementation that data in the specified cache\n.level\ncan be destructively discarded without writing it back to memory.\nThe operand\nsize\nis an integer constant that specifies the length in bytes of the\naddress range\n[a,\na\n+\nsize)\nto write\nunstable indeterminate values\ninto.\nThe only supported value for the\nsize\noperand is\n128\n.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nSupported addressing modes for address operand\na\nare described in\nAddresses as Operands\n.\na\nmust be aligned to 128 bytes.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\ndiscard.global.L2 [ptr], 128;\nld.weak.u32 r0, [ptr];\nld.weak.u32 r1, [ptr];\n// The values in r0 and r1 may differ!\n9.7.9.18.\nData Movement and Conversion Instructions:\ncreatepolicy\n\ncreatepolicy\nCreate a cache eviction policy for the specified cache level.\nSyntax\n// Range-based policy\ncreatepolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy, [a], primary-size, total-size;\n\n// Fraction-based policy\ncreatepolicy.fractional.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy{, fraction};\n\n// Converting the access property from CUDA APIs\ncreatepolicy.cvt.L2.b64            cache-policy, access-property;\n\n.level::primary_priority =   { .L2::evict_last, .L2::evict_normal,\n                               .L2::evict_first, .L2::evict_unchanged };\n.level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged };\nDescription\nThe\ncreatepolicy\ninstruction creates a cache eviction policy for the specified cache level in an\nopaque 64-bit register specified by the destination operand\ncache-policy\n. The cache eviction\npolicy specifies how cache eviction priorities are applied to global memory addresses used in memory\noperations with\n.level::cache_hint\nqualifier.\nThere are two types of cache eviction policies:\nRange-based policy\nThe cache eviction policy created using\ncreatepolicy.range\nspecifies the cache eviction\nbehaviors for the following three address ranges:\n[a\n..\na\n+\n(primary-size\n-\n1)]\nreferred to as primary range.\n[a\n+\nprimary-size\n..\na\n+\n(total-size\n-\n1)]\nreferred to as trailing secondary range.\n[a\n-\n(total-size\n-\nprimary-size)\n..\n(a\n-\n1)]\nreferred to as preceding secondary range.\nWhen a range-based cache eviction policy is used in a memory operation with\n.level::cache_hint\nqualifier, the eviction priorities are applied as follows:\nIf the memory address falls in the primary range, the eviction priority specified by\n.L2::primary_priority\nis applied.\nIf the memory address falls in any of the secondary ranges, the eviction priority specified by\n.L2::secondary_priority\nis applied.\nIf the memory address does not fall in either of the above ranges, then the applied eviction\npriority is unspecified.\nThe 32-bit operand\nprimary-size\nspecifies the size, in bytes, of the primary range. The\n32-bit operand\ntotal-size\nspecifies the combined size, in bytes, of the address range\nincluding primary and secondary ranges. The value of\nprimary-size\nmust be less than or equal\nto the value of\ntotal-size\n. Maximum allowed value of\ntotal-size\nis 4GB.\nIf\n.L2::secondary_priority\nis not specified, then it defaults to\n.L2::evict_unchanged\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nFraction-based policy\nA memory operation with\n.level::cache_hint\nqualifier can use the fraction-based cache\neviction policy to request the cache eviction priority specified by\n.L2:primary_priority\nto\nbe applied to a fraction of cache accesses specified by the 32-bit floating point operand\nfraction\n. The remainder of the cache accesses get the eviction priority specified by\n.L2::secondary_priority\n. This implies that in a memory operation that uses a fraction-based\ncache policy, the memory access has a probability specified by the operand\nfraction\nof\ngetting the cache eviction priority specified by\n.L2::primary_priority\n.\nThe valid range of values for the operand\nfraction\nis\n(0.0,..,\n1.0]\n. If the operand\nfraction\nis not specified, it defaults to 1.0.\nIf\n.L2::secondary_priority\nis not specified, then it defaults to\n.L2::evict_unchanged\n.\nThe access property created using the CUDA APIs can be converted into cache eviction policy by the\ninstruction\ncreatepolicy.cvt\n. The source operand\naccess-property\nis a 64-bit opaque\nregister. Refer to\nCUDA programming guide\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\ncreatepolicy.fractional.L2::evict_last.b64                      policy, 1.0;\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64  policy, 0.5;\n\ncreatepolicy.range.L2::evict_last.L2::evict_first.b64\n                                            policy, [ptr], 0x100000, 0x200000;\n\n// access-prop is created by CUDA APIs.\ncreatepolicy.cvt.L2.b64 policy, access-prop;\n9.7.9.19.\nData Movement and Conversion Instructions:\nisspacep\n\nisspacep\nQuery whether a generic address falls within a specified state space window.\nSyntax\nisspacep.space  p, a;    // result is .pred\n\n.space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\nDescription\nWrite predicate register\np\nwith\n1\nif generic address a falls within the specified state\nspace window and with\n0\notherwise. Destination\np\nhas type\n.pred\n; the source address\noperand must be of type\n.u32\nor\n.u64\n.\nisspacep.param{::entry}\nreturns\n1\nif the generic address falls within the window of\nKernel Function Parameters\n, otherwise returns\n0\n. If\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::entry\n.\nisspacep.global\nreturns\n1\nfor\nKernel Function Parameters\nas\n.param\nwindow is contained within the\n.global\nwindow.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nNote\nispacep.shared::cluster\nwill return 1 for every shared memory address that is accessible to\nthe threads in the cluster, whereas\nispacep.shared::cta\nwill return 1 only if the address is\nof a variable declared in the executing CTA.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nisspacep.const\nintroduced in PTX ISA version 3.1.\nisspacep.param\nintroduced in PTX ISA version 7.7.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::entry\non\n.param\nspace introduced in PTX ISA version 8.3.\nTarget ISA Notes\nisspacep\nrequires\nsm_20\nor higher.\nisspacep.param{::entry}\nrequires\nsm_70\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nExamples\nisspacep.const           iscnst, cptr;\nisspacep.global          isglbl, gptr;\nisspacep.local           islcl,  lptr;\nisspacep.shared          isshrd, sptr;\nisspacep.param::entry    isparam, pptr;\nisspacep.shared::cta     isshrdcta, sptr;\nisspacep.shared::cluster ishrdany sptr;\n9.7.9.20.\nData Movement and Conversion Instructions:\ncvta\n\ncvta\nConvert address from\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space to generic, or vice-versa. Take the generic address of a variable declared in\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space.\nSyntax\n// convert const, global, local, or shared address to generic address\ncvta.space.size  p, a;        // source address in register a\ncvta.space.size  p, var;      // get generic address of var\ncvta.space.size  p, var+imm;  // generic address of var+offset\n\n// convert generic address to const, global, local, or shared address\ncvta.to.space.size  p, a;\n\n.space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\n.size  = { .u32, .u64 };\nDescription\nConvert a\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\naddress to a generic address, or vice-versa. The\nsource and destination addresses must be the same size. Use\ncvt.u32.u64\nor\ncvt.u64.u32\nto\ntruncate or zero-extend addresses.\nFor variables declared in\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space, the generic address of the variable may be taken using\ncvta\n. The source is either a\nregister or a variable defined in\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\nmemory\nwith an optional offset.\nWhen converting a generic address into a\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\naddress, the resulting address is undefined in cases where the generic address does not fall within\nthe address window of the specified state space. A program may use\nisspacep\nto guard against\nsuch incorrect behavior.\nFor\ncvta\nwith\n.shared\nstate space, the address must belong to the space specified by\n::cta\nor\n::cluster\nsub-qualifier, otherwise the behavior is undefined. If no sub-qualifier\nis specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nIf\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::entry\n. For\n.param{::entry}\nstate space, operand\na\nmust be a kernel parameter address, otherwise\nbehavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\ncvta.const\nand\ncvta.to.const\nintroduced in PTX ISA version 3.1.\ncvta.param\nand\ncvta.to.param\nintroduced in PTX ISA version 7.7.\nNote:\nThe current implementation does not allow generic pointers to\nconst\nspace variables in\nprograms that contain pointers to constant buffers passed as kernel parameters.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::entry\non\n.param\nspace introduced in PTX ISA version 8.3.\nTarget ISA Notes\ncvta\nrequires\nsm_20\nor higher.\ncvta.param{::entry}\nand\ncvta.to.param{::entry}\nrequires\nsm_70\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nExamples\ncvta.const.u32   ptr,cvar;\ncvta.local.u32   ptr,lptr;\ncvta.shared::cta.u32  p,As+4;\ncvta.shared::cluster.u32 ptr, As;\ncvta.to.global.u32  p,gptr;\ncvta.param.u64   ptr,pvar;\ncvta.to.param::entry.u64  epptr, ptr;\n9.7.9.21.\nData Movement and Conversion Instructions:\ncvt\n\ncvt\nConvert a value from one type to another.\nSyntax\ncvt{.irnd}{.ftz}{.sat}.dtype.atype         d, a;  // integer rounding\ncvt{.frnd}{.ftz}{.sat}.dtype.atype         d, a;  // fp rounding\n\ncvt.frnd2{.relu}{.satfinite}.f16.f32       d, a;\ncvt.frnd2{.relu}{.satfinite}.f16x2.f32     d, a, b;\ncvt.rs{.relu}{.satfinite}.f16x2.f32        d, a, b, rbits;\n\ncvt.frnd2{.relu}{.satfinite}.bf16.f32      d, a;\ncvt.frnd2{.relu}{.satfinite}.bf16x2.f32    d, a, b;\ncvt.rs{.relu}{.satfinite}.bf16x2.f32       d, a, b, rbits;\n\ncvt.rna{.satfinite}.tf32.f32               d, a;\ncvt.frnd2{.satfinite}{.relu}.tf32.f32      d, a;\n\ncvt.rn.satfinite{.relu}.f8x2type.f32       d, a, b;\ncvt.rn.satfinite{.relu}.f8x2type.fp16x2    d, a;\ncvt.rn.{.relu}.f16x2.f8x2type              d, a;\ncvt.rs{.relu}.satfinite.f8x4type.f32       d, {a, b, e, f}, rbits;\n\ncvt.rn.satfinite{.relu}.f4x2type.f32        d, a, b;\ncvt.rn.satfinite{.relu}.f4x2type.fp16x2type d, a;\ncvt.rn{.relu}.f16x2.f4x2type                d, a;\ncvt.rs{.relu}.satfinite.f4x4type.f32        d, {a, b, e, f}, rbits;\n\ncvt.rn.satfinite{.relu}.f6x2type.f32        d, a, b;\ncvt.rn.satfinite{.relu}.f6x2type.fp16x2type d, a;\ncvt.rn{.relu}.f16x2.f6x2type                d, a;\ncvt.rs{.relu}.satfinite.f6x4type.f32        d, {a, b, e, f}, rbits;\n\ncvt.frnd3{.satfinite}.ue8m0x2.f32          d, a, b;\ncvt.frnd3{.satfinite}.ue8m0x2.bf16x2       d, a;\ncvt.rn.bf16x2.ue8m0x2                      d, a;\n\ncvt.rn.satfinite{.relu}{.scaled::n2::ue8m0}.s2f6x2.f32      d, a, b{, scale-factor};\ncvt.rn.satfinite{.relu}{.scaled::n2::ue8m0}.s2f6x2.bf16x2   d, a{, scale-factor};\ncvt.rn{.satfinite}{.relu}{.scaled::n2::ue8m0}.bf16x2.s2f6x2 d, a{, scale-factor};\n\n.irnd   = { .rni, .rzi, .rmi, .rpi };\n.frnd   = { .rn,  .rz,  .rm,  .rp  };\n.frnd2  = { .rn,  .rz };\n.frnd3  = { .rz,  .rp };\n.dtype = .atype = { .u8,   .u16, .u32, .u64,\n                    .s8,   .s16, .s32, .s64,\n                    .bf16, .f16, .f32, .f64 };\n.f8x2type = { .e4m3x2, .e5m2x2 };\n.f4x2type = { .e2m1x2 };\n.f6x2type = { .e2m3x2, .e3m2x2 };\n.f4x4type = { .e2m1x4 };\n.f8x4type = { .e4m3x4, .e5m2x4 };\n.f6x4type = { .e2m3x4, .e3m2x4 };\n.fp16x2type = { .f16x2, .bf16x2 };\nDescription\nConvert between different types and sizes.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, two inputs\na\nand\nb\nof\n.f32\ntype are\nconverted into\n.f16\nor\n.bf16\ntype and the converted values are packed in the destination\nregister\nd\n, such that the value converted from input\na\nis stored in the upper half of\nd\nand the value converted from input\nb\nis stored in the lower half of\nd\nFor\n.f16x2\ninstruction type, destination operand\nd\nhas\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operand\nd\nhas\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperand\nd\nhas\n.b32\ntype. For\n.tf32\ninstruction type, operand\nd\nhas\n.b32\ntype.\nWhen converting to\n.e4m3x2\n/\n.e5m2x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When converting two\n.f32\ninputs to\n.e4m3x2\n/\n.e5m2x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nand the value converted from\ninput\nb\nis stored in the lower 8 bits of\nd\n. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e4m3x2\n/\n.e5m2x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified\nformat. The converted values are packed in the destination operand\nd\nsuch that the value\nconverted from the upper 16 bits of input\na\nis stored in the upper 8 bits of\nd\nand the value\nconverted from the lower 16 bits of input\na\nis stored in the lower 8 bits of\nd\n.\nWhen converting from\n.e4m3x2\n/\n.e5m2x2\nto\n.f16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit input value in operand\na\nis converted to\n.f16\ntype. The converted values\nare packed in the destination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in the upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the lower 16 bits of\nd\n.\nWhen converting to\n.e2m1x2\ndata formats, the destination operand\nd\nhas\n.b8\ntype.\nWhen converting two\n.f32\ninputs to\n.e2m1x2\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom input\na\nis stored in the upper 4 bits of\nd\nand the value converted from input\nb\nis\nstored in the lower 4 bits of\nd\n. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e2m1x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified format.\nThe converted values are packed in\nd\nso that the value from the upper 16 bits of\na\nis stored\nin the upper 4 bits of\nd\n, and the value from the lower 16 bits of\na\nis stored in the lower 4\nbits of\nd\n.\nWhen converting from\n.e2m1x2\nto\n.f16x2\n, source operand\na\nhas\n.b8\ntype. Each 4-bit\ninput value in operand\na\nis converted to\n.f16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 4 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 4 bits of\na\nis stored in the\nlower 16 bits of\nd\n.\nWhen converting to\n.e2m1x4\ndata format, the destination operand\nd\nhas\n.b16\ntype. When\nconverting four\n.f32\ninputs to\n.e2m1x4\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom inputs\na\n,\nb\n,\ne\n,\nf\nare stored in each 4 bits starting from upper bits of\nd\n.\nWhen converting to\n.e2m3x2\n/\n.e3m2x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When converting two\n.f32\ninputs to\n.e2m3x2\n/\n.e3m2x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nwith 2 MSB bits padded with\nzeros and the value converted from input\nb\nis stored in the lower 8 bits of\nd\nwith 2 MSB bits\npadded with zeros. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e2m3x2\n/\n.e3m2x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified format. The converted\nvalues are packed in the destination operand\nd\nso that the value from the upper 16 bits of input\na\nis stored in the upper 8 bits of\nd\nwith 2 MSB bits padded with zeros, and the value from\nthe lower 16 bits of input\na\nis stored in the lower 8 bits of\nd\nwith 2 MSB bits padded with\nzeros.\nWhen converting from\n.e2m3x2\n/\n.e3m2x2\nto\n.f16x2\n, source operand\na\nhas\n.b16\ntype.\nEach 8-bit input value with 2 MSB bits 0 in operand\na\nis converted to\n.f16\ntype. The converted\nvalues are packed in the destination operand\nd\nsuch that the value converted from the upper 8 bits\nof\na\nis stored in the upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the lower 16 bits of\nd\n.\nWhen converting to\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\ndata format, the destination\noperand\nd\nhas\n.b32\ntype. When converting four\n.f32\ninputs to\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom inputs\na\n,\nb\n,\ne\n,\nf\nare stored in each 8 bits starting from upper bits of\nd\n.\nFor\n.e3m2x4\n/\n.e2m3x4\n, each 8-bit output will have 2 MSB bits padded with zeros.\nWhen converting to\n.ue8m0x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When\nconverting two\n.f32\nor two packed\n.bf16\ninputs to\n.ue8m0x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nand the value converted from\ninput\nb\nis stored in the lower 8 bits of\nd\n.\nWhen converting from\n.ue8m0x2\nto\n.bf16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit\ninput value in operand\na\nis converted to\n.bf16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the\nlower 16 bits of\nd\n.\nWhen converting to\n.s2f6x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When\nconverting two\n.f32\ninputs to\n.s2f6x2\n, each input is converted to the\n.s2f6\nvalue, and\nthe converted values are packed in the destination operand\nd\nsuch that the value converted from\ninput\na\nis stored in the upper 8 bits of\nd\nand the value converted from input\nb\nis\nstored in the lower 8 bits of\nd\n. When converting from\n.bf16x2\n, each input packed in operand\na\nis converted to\n.s2f6\nvalue and converted values are packed in the destination operand\nd\nsuch that value converted from upper 8 bits of input\na\nis stored in upper 8 bits of\noperand\nd\nand the value converted from lower 8 bits of input\na\nis stored in lower 8 bits of\noperand\nd\n. Optional operand\nscale-factor\nhas type\n.b16\nand stores two packed scaling\nfactors of type\n.ue8m0\n. For down conversion, inputs are divided by\nscale-factor\nand then\nconversion is performed. The scaling factor of input\na\n/\n.bf16\ninput from upper 16 bits of\na\nis stored in the upper 8 bits of operand\nscale-factor\nand the scaling factor of input\nb\n/\n.bf16\ninput from lower 16 bits of\na\nis stored in the lower 8 bits of operand\nscale-factor\n.\nIf not specified explicitly, value of\nscale-factor\nwill be assumed to be\n0x7f7f\n,\nthat is, assumed as value\n1\nfor both input scale factors.\nWhen converting from\n.s2f6x2\nto\n.bf16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit\ninput value in operand\na\nis converted to\n.bf16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the\nlower 16 bits of\nd\n. Optional operand\nscale-factor\nhas type\n.b16\nand stores two packed\nscaling factors of type\n.ue8m0\n. For up-conversion, inputs are converted to destination type and\nthen multiplied by\nscale-factor\n. The scaling factor of\n.s2f6\ninput from upper 8 bits of\na\nis stored in the upper 8 bits of operand\nscale-factor\nand the scaling factor of\n.s2f6\ninput\nfrom lower 8 bits of\na\nis stored in the lower 8 bits of operand\nscale-factor\n.\nIf not specified explicitly, value of\nscale-factor\nwill be assumed to\nbe\n0x7f7f\n, that is, assumed as value\n1\nfor both input scale factors.\nOptional qualifier\n.scaled::n2::ue8m0\nspecifies that the instruction uses packed\nscale-factor with 2 scale values of\nue8m0\ntype. Operand\nscale-factor\nand qualifier\n.scaled::n2::ue8m0\nmust be used together.\nrbits\nis a\n.b32\ntype register operand used for providing random bits for\n.rs\nrounding mode.\nWhen converting to\n.f16x2\n, two 16-bit values are provided from\nrbits\nwhere 13 LSBs from\nupper 16-bits are used as random bits for operand\na\nwith 3 MSBs are 0 and 13 LSBs from lower\n16-bits are used as random bits for operand\nb\nwith 3 MSBs are 0.\nWhen converting to\n.bf16x2\n, two 16-bit values are provided from\nrbits\nwhere upper 16-bits\nare used as random bits for operand\na\nand lower 16-bits are used as random bits for operand\nb\n.\nWhen converting to\n.e4m3x4\n/\n.e5m2x4\n/\n.e2m3x4\n/\n.e3m2x4\n, two 16-bit values are provided\nfrom\nrbits\nwhere lower 16-bits are used for operands\ne\n,\nf\nand upper 16 bits are used\nfor operands\na\n,\nb\n.\nWhen converting to\n.e2m1x4\n, two 16-bit values are provided from\nrbits\nwhere lower 8-bits\nfrom both 16-bits half of\nrbits\nare used for operands\ne\n,\nf\nand upper 8-bits from both\n16-bits half of\nrbits\nare used for operands\na\n,\nb\n.\nRounding modifier is mandatory in all of the following cases:\nfloat-to-float conversions, when destination type is smaller than source type\nAll float-to-int conversions\nAll int-to-float conversions\nAll conversions involving\n.f16x2\n,\n.e4m3x2,\n.e5m2x2,\n,\n.bf16x2\n,\n.tf32\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\nand\n.ue8m0x2\ninstruction types.\n.satfinite\nmodifier is only supported for conversions involving the following types:\n.e4m3x2\n,\n.e5m2x2\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\ndestination types.\n.satfinite\nmodifier is mandatory for such conversions.\n.f16\n,\n.bf16\n,\n.f16x2\n,\n.bf16x2\n,\n.tf32\n,\n.ue8m0x2\nas destination types.\nSemantics\nif (/* inst type is .f16x2 or .bf16x2 */) {\n    d[31:16] = convert(a);\n    d[15:0]  = convert(b);\n} else if (/* inst destination type is .e5m2x2 or .e4m3x2 or .ue8m0x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:8] = convert(a);\n        d[7:0]  = convert(b);\n    } else {\n        d[15:8] = convert(a[31:16]);\n        d[7:0]  = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .s2f6x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:8] = convert(a / scale-factor[15:8]);\n        d[7:0]  = convert(b / scale-factor[7:0]);\n    } else {\n        d[15:8] = convert(a[15:8] / scale-factor[15:8]);\n        d[7:0]  = convert(a[7:0] / scale-factor[7:0]);\n    }\n} else if (/* inst source type is .s2f6x2 */) {\n        d[31:16] = convert(a[15:8]) * scale-factor[15:8];\n        d[15:0]  = convert(a[7:0]) * scale-factor[7:0];\n} else if (/* inst destination type is .e2m1x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[7:4] = convert(a);\n        d[3:0] = convert(b);\n    } else {\n        d[7:4] = convert(a[31:16]);\n        d[3:0]  = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .e2m3x2 or .e3m2x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:14] = 0;\n        d[13:8] = convert(a);\n        d[7:6] = 0;\n        d[5:0] = convert(b);\n    } else {\n        d[15:14] = 0;\n        d[13:8] = convert(a[31:16]);\n        d[7:6] = 0;\n        d[5:0] = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .e2m1x4 */) {\n    d[15:12] = convert(a);\n    d[11:8] = convert(b);\n    d[7:4] = convert(e);\n    d[3:0] = convert(f);\n} else if (/* inst destination type is .e4m3x4 or .e5m2x4 */) {\n    d[31:24] = convert(a);\n    d[23:16] = convert(b);\n    d[15:8] = convert(e);\n    d[7:0] = convert(f);\n} else if (/* inst destination type is .e2m3x4 or .e3m2x4 */) {\n    d[31:30] = 0;\n    d[29:24] = convert(a);\n    d[23:22] = 0;\n    d[21:16] = convert(b);\n    d[15:14] = 0;\n    d[13:8] = convert(e);\n    d[7:6] = 0;\n    d[5:0] = convert(f);\n} else {\n    d = convert(a);\n}\n// Random bits\nrbits\nsemantics for\n.rs\nrounding:\nDestination type\n.f16\n:\nRefer\nFigure 38\nfor random bits layout details.\nFigure 38\nRandom bits layout for\n.rs\nrounding with\n.f16\ndestination type\n\nDestination type\n.bf16\n:\nRefer\nFigure 39\nfor random bits layout details.\nFigure 39\nRandom bits layout for\n.rs\nrounding with\n.bf16\ndestination type\n\nDestination type\n.e2m1x4\n:\nRefer\nFigure 40\nfor random bits layout details.\nFigure 40\nRandom bits layout for\n.rs\nrounding with\n.e2m1x4\ndestination type\n\nDestination type\n.e5m2x4\n,\n.e4m3x4\n,\n.e3m2x4\n,\n.e2m3x4\n:\nRefer\nFigure 41\nfor random bits layout details.\nFigure 41\nRandom bits layout for\n.rs\nrounding with\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\ndestination type\n\nInteger Notes\nInteger rounding is required for float-to-integer conversions, and for same-size float-to-float\nconversions where the value is rounded to an integer. Integer rounding is illegal in all other\ninstances.\nInteger rounding modifiers:\n.rni\nround to nearest integer, choosing even integer if source is equidistant between two integers\n.rzi\nround to nearest integer in the direction of zero\n.rmi\nround to nearest integer in direction of negative infinity\n.rpi\nround to nearest integer in direction of positive infinity\nIn float-to-integer conversions, depending upon conversion types,\nNaN\ninput results in following\nvalue:\nZero if source is not\n.f64\nand destination is not\n.s64\n,\n.u64\n.\nOtherwise 1 << (BitWidth(dst) - 1) corresponding to the value of (\nMAXINT\n>> 1) + 1 for unsigned type\nor\nMININT\nfor signed type.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nFor\ncvt.ftz.dtype.f32\nfloat-to-integer conversions and\ncvt.ftz.f32.f32\nfloat-to-float\nconversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier\n.ftz\ncan only be specified when either\n.dtype\nor\n.atype\nis\n.f32\nand applies only\nto single precision (\n.f32\n) inputs and results.\nsm_1x\nFor\ncvt.ftz.dtype.f32\nfloat-to-integer conversions and\ncvt.ftz.f32.f32\nfloat-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving\nzero. The optional\n.ftz\nmodifier may be specified in these cases for clarity.\nNote:\nIn PTX ISA versions 1.4 and earlier, the\ncvt\ninstruction did not flush single-precision\nsubnormal inputs or results to zero if the destination type size was 64-bits. The compiler will\npreserve this behavior for legacy PTX code.\nSaturation modifier:\n.sat\nFor integer destination types,\n.sat\nlimits the result to\nMININT..MAXINT\nfor the size of\nthe operation. Note that saturation applies to both signed and unsigned integer types.\nThe saturation modifier is allowed only in cases where the destination type’s value range is not\na superset of the source type’s value range; i.e., the\n.sat\nmodifier is illegal in cases\nwhere saturation is not possible based on the source and destination types.\nFor float-to-integer conversions, the result is clamped to the destination range by default; i.e,\n.sat\nis redundant.\nFloating Point Notes\nFloating-point rounding is required for float-to-float conversions that result in loss of precision,\nand for integer-to-float conversions. Floating-point rounding is illegal in all other instances.\nFloating-point rounding modifiers:\n.rn\nrounding to nearest, with ties to even\n.rna\nrounding to nearest, with ties away from zero\n.rz\nrounding toward zero\n.rm\nrounding toward negative infinity\n.rp\nrounding toward positive infinity\n.rs\nStochastic rounding is achieved through the use of the supplied random bits. Operation’s result\nis rounded in the direction toward zero or away from zero based on the carry out of the integer\naddition of the supplied random bits (\nrbits\n) to the truncated off (discarded) bits of\nmantissa from the input.\nA floating-point value may be rounded to an integral value using the integer rounding modifiers (see\nInteger Notes). The operands must be of the same size. The result is an integral value, stored in\nfloating-point format.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported. Modifier\n.ftz\nmay be specified to flush\nsingle-precision subnormal inputs and results to sign-preserving zero. Modifier\n.ftz\ncan only\nbe specified when either\n.dtype\nor\n.atype\nis\n.f32\nand applies only to single\nprecision (\n.f32\n) inputs and results.\nsm_1x\nSingle-precision subnormal inputs and results are flushed to sign-preserving zero. The optional\n.ftz\nmodifier may be specified in these cases for clarity.\nNote:\nIn PTX ISA versions 1.4 and earlier, the\ncvt\ninstruction did not flush\nsingle-precision subnormal inputs or results to zero if either source or destination type was\n.f64\n. The compiler will preserve this behavior for legacy PTX code. Specifically, if the PTX\nISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to\nsign-preserving zero only for\ncvt.f32.f16\n,\ncvt.f16.f32\n, and\ncvt.f32.f32\ninstructions.\nSaturation modifier:\n.sat\n:\nFor floating-point destination types,\n.sat\nlimits the result to the range [0.0, 1.0].\nNaN\nresults are flushed to positive zero. Applies to\n.f16\n,\n.f32\n, and\n.f64\ntypes.\n.relu\n:\nFor\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.e4m3x2\n,\n.e5m2x2\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\nand\n.tf32\ndestination types,\n.relu\nclamps the result to 0 if negative.\nNaN\nresults are converted\nto canonical\nNaN\n.\n.satfinite\n:\nFor\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.e4m3x2\n,\n.e5m2x2\n,\n.ue8m0x2\n,\n.e4m3x4\n,\n.e5m2x4\nand\n.tf32\ndestination formats, if the input value is\nNaN\n, then the result is\nNaN\nin the specified destination format. For\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\ndestination formats\nNaN\nresults are converted to positive\nMAX_NORM\n.\nIf the absolute value of input (ignoring sign) is greater than\nMAX_NORM\nof the specified destination\nformat, then the result is sign-preserved\nMAX_NORM\nof the destination format and a positive\nMAX_NORM\nin\n.ue8m0x2\nfor which the destination sign is not supported.\nNotes\nA source register wider than the specified type may be used, except when the source operand has\n.bf16\nor\n.bf16x2\nformat. The lower\nn\nbits corresponding to the instruction-type width\nare used in the conversion. See\nOperand Size Exceeding Instruction-Type Size\nfor a description of these relaxed\ntype-checking rules.\nA destination register wider than the specified type may be used, except when the destination\noperand has\n.bf16\n,\n.bf16x2\nor\n.tf32\nformat. The result of conversion is sign-extended to\nthe destination register width for signed integers, and is zero-extended to the destination register\nwidth for unsigned, bit-size, and floating-point types. See\nOperand Size Exceeding Instruction-Type Size\nfor a description of these relaxed\ntype-checking rules.\nFor\ncvt.f32.bf16\n,\nNaN\ninput yields unspecified\nNaN\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\n.relu\nmodifier and {\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.tf32\n} destination formats\nintroduced in PTX ISA version 7.0.\ncvt.f32.bf16\nintroduced in PTX ISA version 7.1.\ncvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}\n,\ncvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16\n, and\ncvt.tf32.f32.{relu}.{rn/rz}\nintroduced\nin PTX ISA version 7.8.\n.ftz\nqualifier for\ncvt.f32.bf16\nintroduced in PTX ISA version 7.8.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nfor\nsm_90\nor higher introduced in PTX ISA version 7.8.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nfor\nsm_90\nor higher introduced in PTX ISA version 7.8.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nfor\nsm_89\nintroduced in PTX ISA version 8.1.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nfor\nsm_89\nintroduced in PTX ISA version 8.1.\ncvt.satfinite.{f16,\nbf16,\nf16x2,\nbf16x2,\ntf32}.f32\nintroduced in PTX ISA version 8.1.\ncvt.{rn/rz}.satfinite.tf32.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}\nintroduced in PTX ISA version 8.6.\ncvt.{rp/rz}{.satfinite}{.relu}.ue8m0x2.bf16x2\nintroduced in PTX ISA version 8.6.\ncvt.{rz/rp}.satfinite.ue8m0x2.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn.bf16x2.ue8m0x2\nintroduced in PTX ISA version 8.6.\n.rs\nrounding mode introduced in PTX ISA version 8.7.\ncvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32\nintroduced in PTX ISA version 8.7.\ncvt.rn.satfinite{.relu}{.e5m2x2/.e4m3x2}{.bf16x2}\nintroduced in PTX ISA version 9.1.\ncvt.rn.satfinite{.relu}{.e2m3x2/.e3m2x2/.e2m1x2}{.f16x2/.bf16x2}\nintroduced in PTX ISA\nversion 9.1.\ncvt\nwith\n.s2f6x2\ninstruction type introduced in PTX ISA version 9.1.\nTarget ISA Notes\ncvt\nto or from\n.f64\nrequires\nsm_13\nor higher.\n.relu\nmodifier and {\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.tf32\n} destination formats require\nsm_80\nor higher.\ncvt.f32.bf16\nrequires\nsm_80\nor higher.\ncvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}\n,\ncvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16\n, and\ncvt.tf32.f32.{relu}.{rn/rz}\nrequire\nsm_90\nor higher.\n.ftz\nqualifier for\ncvt.f32.bf16\nrequires\nsm_90\nor higher.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nrequires\nsm89\nor higher.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nrequires\nsm_89\nor higher.\ncvt.{rn/rz}.satfinite.tf32.f32\nrequires\nsm_100\nor higher.\ncvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.{rz/rp}{.satfinite}{.relu}.ue8m0x2.bf16x2\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.{rz/rp}.satfinite.ue8m0x2.f32\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.rn.bf16x2.ue8m0x2\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.rs\nrounding mode is supported on following architectures:\nsm_100a\nsm_103a\ncvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32\nis supported on following architectures:\nsm_100a\nsm_103a\ncvt.rn.satfinite{.relu}{.e5m2x2/.e4m3x2}{.bf16x2}\nis supported on following family-specific\narchitectures:\nsm_100f\nor higher in the same family\nsm_110f\nor higher in the same family\nsm_120f\nor higher in the same family\ncvt.rn.satfinite{.relu}{.e2m3x2/.e3m2x2/.e2m1x2}{.f16x2/.bf16x2}\nis supported on following\nfamily-specific architectures:\nsm_100f\nor higher in the same family\nsm_110f\nor higher in the same family\nsm_120f\nor higher in the same family\ncvt\nwith\n.s2f6x2\ninstruction type is supported on following architectures:\nsm_100a\nsm_103a\nsm_110a\nsm_120a\nsm_121a\nExamples\ncvt.f32.s32 f,i;\ncvt.s32.f64 j,r;     // float-to-int saturates by default\ncvt.rni.f32.f32 x,y; // round to nearest int, result is fp\ncvt.f32.f32 x,y;     // note .ftz behavior for sm_1x targets\ncvt.rn.relu.f16.f32      b, f;        // result is saturated with .relu saturation mode\ncvt.rz.f16x2.f32         b1, f, f1;   // convert two fp32 values to packed fp16 outputs\ncvt.rn.relu.satfinite.f16x2.f32    b1, f, f1;   // convert two fp32 values to packed fp16 outputs with .relu saturation on each output\ncvt.rn.bf16.f32          b, f;        // convert fp32 to bf16\ncvt.rz.relu.satfinite.bf16.f3 2    b, f;        // convert fp32 to bf16 with .relu and .satfinite saturation\ncvt.rz.satfinite.bf16x2.f32        b1, f, f1;   // convert two fp32 values to packed bf16 outputs\ncvt.rn.relu.bf16x2.f32   b1, f, f1;   // convert two fp32 values to packed bf16 outputs with .relu saturation on each output\ncvt.rna.satfinite.tf32.f32         b1, f;       // convert fp32 to tf32 format\ncvt.rn.relu.tf32.f32     d, a;        // convert fp32 to tf32 format\ncvt.f64.bf16.rp          f, b;        // convert bf16 to f64 format\ncvt.bf16.f16.rz          b, f         // convert f16 to bf16 format\ncvt.bf16.u64.rz          b, u         // convert u64 to bf16 format\ncvt.s8.bf16.rpi          s, b         // convert bf16 to s8 format\ncvt.bf16.bf16.rpi        b1, b2       // convert bf16 to corresponding int represented in bf16 format\ncvt.rn.satfinite.e4m3x2.f32 d, a, b;  // convert a, b to .e4m3 and pack as .e4m3x2 output\ncvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu\n                                         // saturation on each output and pack as .e5m2x2\ncvt.rn.f16x2.e4m3x2 d, a;             // unpack a, convert two .e4m3 values to packed f16x2 output\ncvt.rn.satfinite.tf32.f32 d, a;       // convert fp32 to tf32 format\ncvt.rn.relu.f16x2.e2m1x2 d, a;        // unpack a, convert two .e2m1 values to packed f16x2 output\ncvt.rn.satfinite.e2m3x2.f32 d, a, b;  // convert a, b to .e2m3 and pack as .e2m3x2 output\ncvt.rn.relu.f16x2.e3m2x2 d, a;        // unpack a, convert two .e3m2 values to packed f16x2 output\n\ncvt.rs.f16x2.f32    d, a, b, rbits;  // convert 2 fp32 values to packed fp16 with applying .rs rounding\ncvt.rs.satfinite.e2m1x4.f32  d, {a, b, e, f}, rbits; // convert 4 fp32 values to packed 4 e2m1 values with applying .rs rounding\n\ncvt.rn.satfinite.relu.e2m1x2type.f16x2  d, a; // unpack a and covert to two .e2m1 values\ncvt.rn.satfinite.e2m3x2type.bf16x2  d, a; // unpack a and covert to two .e2m3 values\n\n// Convert 2 f32 values to s2f6 after applying scale factor for dividing the value\ncvt.rn.satfinite.scaled::n2::ue8m0.s2f6x2.f32 d, a, b, scale-factor;\n9.7.9.22.\nData Movement and Conversion Instructions:\ncvt.pack\n\ncvt.pack\nConvert two integer values from one integer type to another and pack the results.\nSyntax\ncvt.pack.sat.convertType.abType  d, a, b;\n    .convertType  = { .u16, .s16 }\n    .abType       = { .s32 }\n\ncvt.pack.sat.convertType.abType.cType  d, a, b, c;\n    .convertType  = { .u2, .s2, .u4, .s4, .u8, .s8 }\n    .abType       = { .s32 }\n    .cType        = { .b32 }\nDescription\nConvert two 32-bit integers\na\nand\nb\ninto specified type and pack the results into\nd\n.\nDestination\nd\nis an unsigned 32-bit integer. Source operands\na\nand\nb\nare integers of\ntype\n.abType\nand the source operand\nc\nis an integer of type\n.cType\n.\nThe inputs\na\nand\nb\nare converted to values of type specified by\n.convertType\nwith\nsaturation and the results after conversion are packed into lower bits of\nd\n.\nIf operand\nc\nis specified then remaining bits of\nd\nare copied from lower bits of\nc\n.\nSemantics\nta = a < MIN(convertType) ? MIN(convertType) : a;\nta = a > MAX(convertType) ? MAX(convertType) : a;\ntb = b < MIN(convertType) ? MIN(convertType) : b;\ntb = b > MAX(convertType) ? MAX(convertType) : b;\n\nsize = sizeInBits(convertType);\ntd = tb ;\nfor (i = size; i <= 2 * size - 1; i++) {\n    td[i] = ta[i - size];\n}\n\nif (isU16(convertType) || isS16(convertType)) {\n    d = td;\n} else {\n    for (i = 0; i < 2 * size; i++) {\n        d[i] = td[i];\n    }\n    for (i = 2 * size; i <= 31; i++) {\n        d[i] = c[i - 2 * size];\n    }\n}\n.sat\nmodifier limits the converted values to\nMIN(convertType)\n..\nMAX(convertedType)\n(no\noverflow) if the corresponding inputs are not in the range of datatype specified as\n.convertType\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nTarget ISA Notes\nRequires\nsm_72\nor higher.\nSub byte types (\n.u4\n/\n.s4\nand\n.u2\n/\n.s2\n) requires\nsm_75\nor higher.\nExamples\ncvt.pack.sat.s16.s32      %r1, %r2, %r3;           // 32-bit to 16-bit conversion\ncvt.pack.sat.u8.s32.b32   %r4, %r5, %r6, 0;        // 32-bit to 8-bit conversion\ncvt.pack.sat.u8.s32.b32   %r7, %r8, %r9, %r4;      // %r7 = { %r5, %r6, %r8, %r9 }\ncvt.pack.sat.u4.s32.b32   %r10, %r12, %r13, %r14;  // 32-bit to 4-bit conversion\ncvt.pack.sat.s2.s32.b32   %r15, %r16, %r17, %r18;  // 32-bits to 2-bit conversion\n9.7.9.23.\nData Movement and Conversion Instructions:\nmapa\n\nmapa\nMap the address of the shared variable in the target CTA.\nSyntax\nmapa{.space}.type          d, a, b;\n\n// Maps shared memory address in register a into CTA b.\nmapa.shared::cluster.type  d, a, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh + imm, b;\n\n// Maps generic address in register a into CTA b.\nmapa.type                  d, a, b;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 }\nDescription\nGet address in the CTA specified by operand\nb\nwhich corresponds to the address specified by\noperand\na\n.\nInstruction type\n.type\nindicates the type of the destination operand\nd\nand the source\noperand\na\n.\nWhen space is\n.shared::cluster\n, source\na\nis either a shared memory variable or a register\ncontaining a valid shared memory address and register\nd\ncontains a shared memory address. When\nthe optional qualifier\n.space\nis not specified, both\na\nand\nd\nare registers containing\ngeneric addresses pointing to shared memory.\nb\nis a 32-bit integer operand representing the rank of the target CTA.\nDestination register\nd\nwill hold an address in CTA\nb\ncorresponding to operand\na\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmapa.shared::cluster.u64 d1, %reg1, cta;\nmapa.shared::cluster.u32 d2, sh, 3;\nmapa.u64                 d3, %reg2, cta;\n9.7.9.24.\nData Movement and Conversion Instructions:\ngetctarank\n\ngetctarank\nGenerate the CTA rank of the address.\nSyntax\ngetctarank{.space}.type d, a;\n\n// Get cta rank from source shared memory address in register a.\ngetctarank.shared::cluster.type d, a;\n\n// Get cta rank from shared memory variable.\ngetctarank.shared::cluster.type d, var;\n\n// Get cta rank from shared memory variable+offset.\ngetctarank.shared::cluster.type d, var + imm;\n\n// Get cta rank from generic address of shared memory variable in register a.\ngetctarank.type d, a;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 }\nDescription\nWrite the destination register\nd\nwith the rank of the CTA which contains the address specified\nin operand\na\n.\nInstruction type\n.type\nindicates the type of source operand\na\n.\nWhen space is\n.shared::cluster\n, source\na\nis either a shared memory variable or a register\ncontaining a valid shared memory address. When the optional qualifier\n.space\nis not specified,\na\nis a register containing a generic addresses pointing to shared memory. Destination\nd\nis\nalways a 32-bit register which holds the rank of the CTA.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ngetctarank.shared::cluster.u32 d1, addr;\ngetctarank.shared::cluster.u64 d2, sh + 4;\ngetctarank.u64                 d3, src;\n9.7.9.25.\nData Movement and Conversion Instructions: Asynchronous copy\n\nAn asynchronous copy operation performs the underlying operation asynchronously in the background,\nthus allowing the issuing threads to perform subsequent tasks.\nAn asynchronous copy operation can be a\nbulk\noperation that operates on a large amount of data, or\na\nnon-bulk\noperation that operates on smaller sized data. The amount of data handled by a bulk\nasynchronous operation must be a multiple of 16 bytes.\nAn asynchronous copy operation typically includes the following sequence:\nOptionally, reading from the tensormap.\nReading data from the source location(s).\nWriting data to the destination location(s).\nWrites being made visible to the executing thread or other threads.\n9.7.9.25.1.\nCompletion Mechanisms for Asynchronous Copy Operations\n\nA thread must explicitly wait for the completion of an asynchronous copy operation in order to\naccess the result of the operation. Once an asynchronous copy operation is initiated, modifying the\nsource memory location or tensor descriptor or reading from the destination memory location before\nthe asynchronous operation completes, exhibits undefined behavior.\nThis section describes two asynchronous copy operation completion mechanisms supported in PTX:\nAsync-group mechanism and mbarrier-based mechanism.\nAsynchronous operations may be tracked by either of the completion mechanisms or both mechanisms.\nThe tracking mechanism is instruction/instruction-variant specific.\n9.7.9.25.1.1.\nAsync-group mechanism\n\nWhen using the async-group completion mechanism, the issuing thread specifies a group of\nasynchronous operations, called\nasync-group\n, using a\ncommit\noperation and tracks the completion\nof this group using a\nwait\noperation. The thread issuing the asynchronous operation must create\nseparate\nasync-groups\nfor bulk and non-bulk asynchronous operations.\nA\ncommit\noperation creates a per-thread\nasync-group\ncontaining all prior asynchronous operations\ntracked by\nasync-group\ncompletion and initiated by the executing thread but none of the asynchronous\noperations following the commit operation. A committed asynchronous operation belongs to a single\nasync-group\n.\nWhen an\nasync-group\ncompletes, all the asynchronous operations belonging to that group are\ncomplete and the executing thread that initiated the asynchronous operations can read the result of\nthe asynchronous operations. All\nasync-groups\ncommitted by an executing thread always complete in\nthe order in which they were committed. There is no ordering between asynchronous operations within\nan\nasync-group\n.\nA typical pattern of using\nasync-group\nas the completion mechanism is as follows:\nInitiate the asynchronous operations.\nGroup the asynchronous operations into an\nasync-group\nusing a\ncommit\noperation.\nWait for the completion of the async-group using the wait operation.\nOnce the\nasync-group\ncompletes, access the results of all asynchronous operations in that\nasync-group\n.\n9.7.9.25.1.2.\nMbarrier-based mechanism\n\nA thread can track the completion of one or more asynchronous operations using the current phase of\nan\nmbarrier object\n. When the current phase of the\nmbarrier object\nis complete, it implies that\nall asynchronous operations tracked by this phase are complete, and all threads participating in\nthat\nmbarrier object\ncan access the result of the asynchronous operations.\nThe\nmbarrier object\nto be used for tracking the completion of an asynchronous operation can be\neither specified along with the asynchronous operation as part of its syntax, or as a separate\noperation. For a bulk asynchronous operation, the\nmbarrier object\nmust be specified in the\nasynchronous operation, whereas for non-bulk operations, it can be specified after the asynchronous\noperation.\nA typical pattern of using mbarrier-based completion mechanism is as follows:\nInitiate the asynchronous operations.\nSet up an\nmbarrier object\nto track the asynchronous operations in its current phase, either as\npart of the asynchronous operation or as a separate operation.\nWait for the\nmbarrier object\nto complete its current phase using\nmbarrier.test_wait\nor\nmbarrier.try_wait\n.\nOnce the\nmbarrier.test_wait\nor\nmbarrier.try_wait\noperation returns\nTrue\n, access the\nresults of the asynchronous operations tracked by the\nmbarrier object\n.\n9.7.9.25.2.\nAsync Proxy\n\nThe\ncp{.reduce}.async.bulk\noperations are performed in the\nasynchronous proxy\n(or\nasync\nproxy\n).\nAccessing the same memory location across multiple proxies needs a cross-proxy fence. For the\nasync proxy\n,\nfence.proxy.async\nshould be used to synchronize memory between\ngeneric\nproxy\nand the\nasync proxy\n.\nThe completion of a\ncp{.reduce}.async.bulk\noperation is followed by an implicit\ngeneric-async\nproxy fence. So the result of the asynchronous operation is made visible to the generic proxy as\nsoon as its completion is observed.\nAsync-group\nOR\nmbarrier-based\ncompletion mechanism must\nbe used to wait for the completion of the\ncp{.reduce}.async.bulk\ninstructions.\n9.7.9.25.3.\nData Movement and Conversion Instructions: Non-bulk copy\n\n9.7.9.25.3.1.\nData Movement and Conversion Instructions:\ncp.async\n\ncp.async\nInitiates an asynchronous copy operation from one state space to another.\nSyntax\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, src-size}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, src-size}{, cache-policy} ;\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, ignore-src}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, ignore-src}{, cache-policy} ;\n\n.level::cache_hint =     { .L2::cache_hint }\n.level::prefetch_size =  { .L2::64B, .L2::128B, .L2::256B }\ncp-size =                { 4, 8, 16 }\nDescription\ncp.async\nis a non-blocking instruction which initiates an asynchronous copy operation of data\nfrom the location specified by source address operand\nsrc\nto the location specified by\ndestination address operand\ndst\n. Operand\nsrc\nspecifies a location in the global state space\nand\ndst\nspecifies a location in the shared state space.\nOperand\ncp-size\nis an integer constant which specifies the size of data in bytes to be copied to\nthe destination\ndst\n.\ncp-size\ncan only be 4, 8 and 16.\nInstruction\ncp.async\nallows optionally specifying a 32-bit integer operand\nsrc-size\n. Operand\nsrc-size\nrepresents the size of the data in bytes to be copied from\nsrc\nto\ndst\nand must\nbe less than\ncp-size\n. In such case, remaining bytes in destination\ndst\nare filled with\nzeros. Specifying\nsrc-size\nlarger than\ncp-size\nresults in undefined behavior.\nThe optional and non-immediate predicate argument\nignore-src\nspecifies whether the data from the\nsource location\nsrc\nshould be ignored completely. If the source data is ignored then zeros will\nbe copied to destination\ndst\n. If the argument\nignore-src\nis not specified then it defaults\nto\nFalse\n.\nSupported alignment requirements and addressing modes for operand\nsrc\nand\ndst\nare described\nin\nAddresses as Operands\n.\nThe mandatory\n.async\nqualifier indicates that the\ncp\ninstruction will initiate the memory\ncopy operation asynchronously and control will return to the executing thread before the copy\noperation is complete. The executing thread can then use\nasync-group based completion mechanism\nor the\nmbarrier based completion mechanism\nto wait for completion of the asynchronous copy operation.\nNo other synchronization mechanism guarantees the completion of the asynchronous\ncopy operations.\nThere is no ordering guarantee between two\ncp.async\noperations if they are not explicitly\nsynchronized using\ncp.async.wait_all\nor\ncp.async.wait_group\nor\nmbarrier instructions\n.\nAs described in\nCache Operators\n, the\n.cg\nqualifier indicates\ncaching of data only at global level cache L2 and not at L1 whereas\n.ca\nqualifier indicates\ncaching of data at all levels including L1 cache. Cache operator are treated as performance hints\nonly.\ncp.async\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe qualifier\n.level::prefetch_size\nmay only be used with\n.global\nstate space and with\ngeneric addressing where the address points to\n.global\nstate space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for\n.level::cache_hint\nand\n.level::prefetch_size\nqualifiers introduced in PTX ISA\nversion 7.4.\nSupport for\nignore-src\noperand introduced in PTX ISA version 7.5.\nSupport for sub-qualifier\n::cta\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nExamples\ncp.async.ca.shared.global  [shrd],    [gbl + 4], 4;\ncp.async.ca.shared::cta.global  [%r0 + 8], [%r1],     8;\ncp.async.cg.shared.global  [%r2],     [%r3],     16;\n\ncp.async.cg.shared.global.L2::64B   [%r2],      [%r3],     16;\ncp.async.cg.shared.global.L2::128B  [%r0 + 16], [%r1],     16;\ncp.async.cg.shared.global.L2::256B  [%r2 + 32], [%r3],     16;\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 0.25;\ncp.async.ca.shared.global.L2::cache_hint [%r2], [%r1], 4, cache-policy;\n\ncp.async.ca.shared.global                   [shrd], [gbl], 4, p;\ncp.async.cg.shared.global.L2::cache_hint   [%r0], [%r2], 16, q, cache-policy;\n9.7.9.25.3.2.\nData Movement and Conversion Instructions:\ncp.async.commit_group\n\ncp.async.commit_group\nCommits all prior initiated but uncommitted\ncp.async\ninstructions into a\ncp.async-group\n.\nSyntax\ncp.async.commit_group ;\nDescription\ncp.async.commit_group\ninstruction creates a new\ncp.async-group\nper thread and batches all\nprior\ncp.async\ninstructions initiated by the executing thread but not committed to any\ncp.async-group\ninto the new\ncp.async-group\n. If there are no uncommitted\ncp.async\ninstructions then\ncp.async.commit_group\nresults in an empty\ncp.async-group.\nAn executing thread can wait for the completion of all\ncp.async\noperations in a\ncp.async-group\nusing\ncp.async.wait_group\n.\nThere is no memory ordering guarantee provided between any two\ncp.async\noperations within the\nsame\ncp.async-group\n. So two or more\ncp.async\noperations within a\ncp.async-group\ncopying data\nto the same location results in undefined behavior.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example 1:\ncp.async.ca.shared.global [shrd], [gbl], 4;\ncp.async.commit_group ; // Marks the end of a cp.async group\n\n// Example 2:\ncp.async.ca.shared.global [shrd1],   [gbl1],   8;\ncp.async.ca.shared.global [shrd1+8], [gbl1+8], 8;\ncp.async.commit_group ; // Marks the end of cp.async group 1\n\ncp.async.ca.shared.global [shrd2],    [gbl2],    16;\ncp.async.cg.shared.global [shrd2+16], [gbl2+16], 16;\ncp.async.commit_group ; // Marks the end of cp.async group 2\n9.7.9.25.3.3.\nData Movement and Conversion Instructions:\ncp.async.wait_group\n/\ncp.async.wait_all\n\ncp.async.wait_group\n,\ncp.async.wait_all\nWait for completion of prior asynchronous copy operations.\nSyntax\ncp.async.wait_group N;\ncp.async.wait_all ;\nDescription\ncp.async.wait_group\ninstruction will cause executing thread to wait till only\nN\nor fewer of\nthe most recent\ncp.async-group\ns are pending and all the prior\ncp.async-group\ns committed by\nthe executing threads are complete. For example, when\nN\nis 0, the executing thread waits on all\nthe prior\ncp.async-group\ns to complete. Operand\nN\nis an integer constant.\ncp.async.wait_all\nis equivalent to :\ncp.async.commit_group;\ncp.async.wait_group 0;\nAn empty\ncp.async-group\nis considered to be trivially complete.\nWrites performed by\ncp.async\noperations are made visible to the executing thread only after:\nThe completion of\ncp.async.wait_all\nor\nThe completion of\ncp.async.wait_group\non the\ncp.async-group\nin which the\ncp.async\nbelongs to or\nmbarrier.test_wait\nreturns\nTrue\non an\nmbarrier object\nwhich is tracking the completion of the\ncp.async\noperation.\nThere is no ordering between two\ncp.async\noperations that are not synchronized with\ncp.async.wait_all\nor\ncp.async.wait_group\nor\nmbarrier objects\n.\ncp.async.wait_group\nand\ncp.async.wait_all\ndoes not provide any ordering and visibility\nguarantees for any other memory operation apart from\ncp.async\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example of .wait_all:\ncp.async.ca.shared.global [shrd1], [gbl1], 4;\ncp.async.cg.shared.global [shrd2], [gbl2], 16;\ncp.async.wait_all;  // waits for all prior cp.async to complete\n\n// Example of .wait_group :\ncp.async.ca.shared.global [shrd3], [gbl3], 8;\ncp.async.commit_group;  // End of group 1\n\ncp.async.cg.shared.global [shrd4], [gbl4], 16;\ncp.async.commit_group;  // End of group 2\n\ncp.async.cg.shared.global [shrd5], [gbl5], 16;\ncp.async.commit_group;  // End of group 3\n\ncp.async.wait_group 1;  // waits for group 1 and group 2 to complete\n9.7.9.25.4.\nData Movement and Conversion Instructions: Bulk copy\n\n9.7.9.25.4.1.\nData Movement and Conversion Instructions:\ncp.async.bulk\n\ncp.async.bulk\nInitiates an asynchronous copy operation from one state space to another.\nSyntax\n// global -> shared::cta\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, cache-policy}\n\n.dst =                  { .shared::cta }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n\n\n// global -> shared::cluster\ncp.async.bulk.dst.src.completion_mechanism{.multicast}{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> shared::cluster\ncp.async.bulk.dst.src.completion_mechanism [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n\n\n// shared::cta -> global\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}{.cp_mask}\n                      [dstMem], [srcMem], size {, cache-policy} {, byteMask}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint =    { .L2::cache_hint }\nDescription\ncp.async.bulk\nis a non-blocking instruction which initiates an asynchronous bulk-copy operation\nfrom the location specified by source address operand\nsrcMem\nto the location specified by\ndestination address operand\ndstMem\n.\nThe direction of bulk-copy is from the state space specified by the\n.src\nmodifier to the state\nspace specified by the\n.dst\nmodifiers.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied, in terms of number of\nbytes.\nsize\nmust be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined. The memory range\n[dstMem,\ndstMem\n+\nsize\n-\n1]\nmust not overflow the destination memory\nspace and the memory range\n[srcMem,\nsrcMem\n+\nsize\n-\n1]\nmust not overflow the source memory\nspace. Otherwise, the behavior is undefined. The addresses\ndstMem\nand\nsrcMem\nmust be aligned\nto 16 bytes.\nWhen the destination of the copy is\n.shared::cta\nthe destination address has to be in the shared\nmemory of the executing CTA within the cluster, otherwise the behavior is undefined.\nWhen the source of the copy is\n.shared::cta\nand the destination is\n.shared::cluster\n, the\ndestination has to be in the shared memory of a different CTA within the cluster.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\n.mbarrier::...\n.shared::cta\n.global\nmbarrier based\n.shared::cluster\n.global\n.shared::cluster\n.shared::cta\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.async.bulk\nvariant uses\nmbarrier based completion mechanism. The\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.bulk_group\nspecifies that the\ncp.async.bulk\nvariant uses\nbulk async-group\nbased completion mechanism.\nThe optional modifier\n.multicast::cluster\nallows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand\nctaMask\nspecifies the destination CTAs in the\ncluster such that each bit position in the 16-bit\nctaMask\noperand corresponds to the\n%ctaid\nof the destination CTA. The source data is multicast to the same CTA-relative offset as\ndstMem\nin the shared memory of each destination CTA. The mbarrier signal is also multicast to the same\nCTA-relative offset as\nmbar\nin the shared memory of the destination CTA.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nWhen the optional qualifier\n.cp_mask\nis specified, the argument\nbyteMask\nis required.\nThe i-th bit in the 16-bit wide\nbyteMask\noperand specifies whether the i-th byte of each 16-byte\nwide chunk of source data is copied to the destination. If the bit is set, the byte is copied.\nThe copy operation in\ncp.async.bulk\nis treated as a weak memory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nNotes\n.multicast::cluster\nqualifier is optimized for target architecture\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\nand may have substantially reduced performance on other\ntargets and hence\n.multicast::cluster\nis advised to be used with\n.target\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.shared::cta\nas destination state space is introduced in PTX ISA version 8.6.\nSupport for\n.cp_mask\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.multicast::cluster\nqualifier advised to be used with\n.target\nsm_90a\nor\nsm_100f\nor\nsm_100a\nor\nsm_103f\nor\nsm_103a\nor\nsm_110f\nor\nsm_110a\n.\nSupport for\n.cp_mask\nqualifier requires\nsm_100\nor higher.\nExamples\n// .global -> .shared::cta (strictly non-remote):\ncp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n// .global -> .shared::cluster:\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                                             [dstMem], [srcMem], size, [mbar], ctaMask;\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n\n// .shared::cta -> .shared::cluster (strictly remote):\ncp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\n// .shared::cta -> .global:\ncp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy;\n\n// .shared::cta -> .global with .cp_mask:\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.cp_mask [dstMem], [srcMem], size, cache-policy, byteMask;\n9.7.9.25.4.2.\nData Movement and Conversion Instructions:\ncp.reduce.async.bulk\n\ncp.reduce.async.bulk\nInitiates an asynchronous reduction operation.\nSyntax\ncp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type\n              [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .b32, .u32, .s32, .b64, .u64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type\n               [dstMem], [srcMem], size{, cache-policy}\n\n.dst =                  { .global      }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint    = { .L2::cache_hint }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type\n               [dstMem], [srcMem], size{, cache-policy}\n.dst  =                 { .global }\n.src  =                 { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.type =                 { .f16, .bf16 }\nDescription\ncp.reduce.async.bulk\nis a non-blocking instruction which initiates an asynchronous reduction\noperation on an array of memory locations specified by the destination address operand\ndstMem\nwith the source array whose location is specified by the source address operand\nsrcMem\n. The size\nof the source and the destination array must be the same and is specified by the operand\nsize\n.\nEach data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier\n.redOp\n. The type of each\ndata element in the source and the destination array is specified by the modifier\n.type\n.\nThe source address operand\nsrcMem\nis located in the state space specified by\n.src\nand the\ndestination address operand\ndstMem\nis located in the state specified by the\n.dst\n.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied from the source location and\nused in the reduction operation, in terms of number of bytes.\nsize\nmust be a multiple of 16. If\nthe value is not a multiple of 16, then the behavior is undefined. The memory range\n[dstMem,\ndstMem\n+\nsize\n-\n1]\nmust not overflow the destination memory space and the memory range\n[srcMem,\nsrcMem\n+\nsize\n-\n1]\nmust not overflow the source memory space. Otherwise, the behavior is\nundefined. The addresses\ndstMem\nand\nsrcMem\nmust be aligned to 16 bytes.\nThe operations supported by\n.redOp\nare classified as follows:\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n, and\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..x]\nwhere\nx\nis the value at the source\nstate space.\nThe floating point operation\n.add\nrounds to the nearest even. The current implementation of\ncp.reduce.async.bulk.add.f32\nflushes subnormal inputs and results to sign-preserving zero. The\ncp.reduce.async.bulk.add.f16\nand\ncp.reduce.async.bulk.add.bf16\noperations require\n.noftz\nqualifier. It preserves input and result subnormals, and does not flush them to zero.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.dst\n.redOp\nElement type\n.shared::cluster\n.add\n.u32\n,\n.s32\n,\n.u64\n.min\n,\n.max\n.u32\n,\n.s32\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n.global\n.add\n.u32\n,\n.s32\n,\n.u64\n,\n.f32\n,\n.f64\n,\n.f16\n,\n.bf16\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n,\n.f16\n,\n.bf16\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\n.mbarrier::...\n.shared::cluster\n.global\nmbarrier based\n.shared::cluster\n.shared::cta\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.reduce.async.bulk\nvariant\nuses mbarrier based completion mechanism. The\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.bulk_group\nspecifies that the\ncp.reduce.async.bulk\nvariant uses\nbulk\nasync-group\nbased completion mechanism.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nEach reduction operation performed by the\ncp.reduce.async.bulk\nhas individually\n.relaxed.gpu\nmemory ordering semantics. The load operations in\ncp.reduce.async.bulk\nare treated as weak\nmemory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;\n9.7.9.25.4.3.\nData Movement and Conversion Instructions:\ncp.async.bulk.prefetch\n\ncp.async.bulk.prefetch\nProvides a hint to the system to initiate the asynchronous prefetch of data to the cache.\nSyntax\ncp.async.bulk.prefetch.L2.src{.level::cache_hint}   [srcMem], size {, cache-policy}\n\n.src =                { .global }\n.level::cache_hint =  { .L2::cache_hint }\nDescription\ncp.async.bulk.prefetch\nis a non-blocking instruction which may initiate an asynchronous prefetch\nof data from the location specified by source address operand\nsrcMem\n, in\n.src\nstatespace, to\nthe L2 cache.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be prefetched in terms of number of\nbytes.\nsize\nmust be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined.  The address\nsrcMem\nmust be aligned to 16 bytes.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\ncp.async.bulk.prefetch\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.prefetch.L2.global                 [srcMem], size;\n\ncp.async.bulk.prefetch.L2.global.L2::cache_hint  [srcMem], size, policy;\n9.7.9.26.\nData Movement and Conversion Instructions:\nmultimem.cp.async.bulk\n\nmultimem.cp.async.bulk\nInitiates an asynchronous copy operation to a multimem address range.\nSyntax\nmultimem.cp.async.bulk.dst.src.completion_mechanism{.cp_mask}\n    [dstMem], [srcMem], size{, byteMask};\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\nDescription\nInstruction\nmultimem.cp.async.bulk\ninitiates an asynchronous bulk-copy operation from source\naddress range\n[srcMem,\nsrcMem\n+\nsize)\nto memory locations residing on each GPU’s memory referred\nto by the destination multimem address range\n[dstMem,\ndstMem\n+\nsize)\n. The direction of\nbulk-copy is from the state space specified by the\n.src\nmodifier to the state space specified\nby the\n.dst\nmodifiers.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied, in terms of number of\nbytes. Operand\nsize\nmust be a multiple of\n16\n. The memory range\n[dstMem,\ndstMem\n+\nsize)\nmust not overflow the destination multimem memory space. The memory range\n[srcMem,\nsrcMem\n+\nsize)\nmust not overflow the source memory space. The addresses\ndstMem\nand\nsrcMem\nmust be aligned\nto\n16\nbytes. If any of these pre-conditions is not met, the behavior is undefined.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported by the\ninstruction. The modifier\n.bulk_group\nspecifies that the\nmultimem.cp.async.bulk\ninstruction\nuses bulk async-group based completion mechanism.\nWhen the optional modifier\n.cp_mask\nis specified, the argument\nbyteMask\nis required. The\ni-th bit in the 16-bit wide\nbyteMask\noperand specifies whether the i-th byte of each 16-byte\nwide chunk of source data is copied to the destination. If the bit is set, the byte is copied.\nThe reads and writes of the copy operation in\nmultimem.cp.async.bulk\nare weak memory operations\nas described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nSupport for\n.cp_mask\nqualifier requires\nsm_100\nor higher.\nExamples\nmultimem.cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\nmultimem.cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], 512;\n\nmultimem.cp.async.bulk.global.shared::cta.bulk_group.cp_mask [dstMem], [srcMem], size, byteMask;\n9.7.9.27.\nData Movement and Conversion Instructions:\nmultimem.cp.reduce.async.bulk\n\nmultimem.cp.reduce.async.bulk\nInitiates an asynchronous reduction operation to a multimem address range.\nSyntax\nmultimem.cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type  [dstMem], [srcMem], size;\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\n    .redOp                = { .and, .or, .xor,\n                              .add, .inc, .dec,\n                              .min, .max }\n    .type                 = { .f16, .bf16,\n                              .b32, .u32, .s32,\n                              .b64, .u64, .s64,\n                              .f32, .f64 }\n\nmultimem.cp.reduce.async.bulk.dst.src.completion_mechanism.add.noftz.type  [dstMem], [srcMem], size;\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\n    .type                 = { .f16, .bf16 }\nDescription\nInstruction\nmultimem.cp.reduce.async.bulk\ninitiates an element-wise asynchronous reduction\noperation with elements from source memory address range\n[srcMem,\nsrcMem\n+\nsize)\nto memory\nlocations residing on each GPU’s memory referred to by the multimem destination address range\n[dstMem,\ndstMem\n+\nsize)\n.\nEach data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier\n.redOp\n. The type of each\ndata element in the source and the destination array is specified by the modifier\n.type\n.\nThe source address operand\nsrcMem\nis in the state space specified by\n.src\nand the\ndestination address operand\ndstMem\nis in the state specified by the\n.dst\n.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied from the source location\nand used in the reduction operation, in terms of number of bytes. Operand\nsize\nmust be a\nmultiple of 16. The memory range\n[dstMem,\ndstMem\n+\nsize)\nmust not overflow the destination\nmultimem memory space. The memory range\n[srcMem,\nsrcMem\n+\nsize)\nmust not overflow the source\nmemory space. The addresses\ndstMem\nand\nsrcMem\nmust be aligned to 16 bytes. If any of these\npreconditions is not met, the behavior is undefined.\nThe operations supported by\n.redOp\nare classified as follows:\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n, and\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..x]\nwhere\nx\nis the value at the source\nstate space.\nThe floating point operation\n.add\nrounds to the nearest even, preserve input and result\nsubnormals, and does not flush them to zero, except for the current implementation of\nmultimem.cp.reduce.async.bulk.add.f32\nwhich flushes subnormal inputs and results to\nsign-preserving zero. The\nmultimem.cp.reduce.async.bulk.add.f16\nand\nmultimem.cp.reduce.async.bulk.add.bf16\noperations require\n.noftz\nqualifier. It preserves\ninput and result subnormals, and does not flush them to zero.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.redOp\nelement type\n.add\n.u32, .s32, .u64, .f32, .f64,\n.f16, .bf16\n.min,\n.max\n.u32, .s32, .u64, .s64, .f16,\n.bf16\n.inc,\n.dec\n.u32\n.and,\n.or,\n.xor\n.b32, .b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported by the\ninstruction. The modifier\n.bulk_group\nspecifies that the\nmultimem.cp.reduce.async.bulk\nuses\nbulk async-group based completion mechanism.\nEach reduction operation performed by the\nmultimem.cp.reduce.async.bulk\nhas individually\n.relaxed.sys\nmemory ordering semantics. The load operations in\nmultimem.cp.reduce.async.bulk\nare treated as weak memory operations as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.xor.b64 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.inc.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.dec.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.max.s32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.min.bf16 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.bf16 [dstMem], [srcMem], size;\n9.7.9.27.1.\nData Movement and Conversion Instructions: Tensor copy\n\n9.7.9.27.1.1.\nRestriction on Tensor Copy instructions\n\nFollowing are the restrictions on the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n:\ncp.reduce.async.bulk\ndoesn’t support the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\ncp.async.bulk.tensor\nwith the direction\n.global.shared::cta\ndoesn’t support the\ntype\n.b4x16_p64\n.\ncp.async.bulk.tensor\nwith the direction\n.shared::cluster.global\ndoesn’t support\nthe sub-byte types on\nsm_120a\n.\nOOB-NaN fill mode doesn’t support the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\nBox-Size[0] must be exactly:\n96B for\nb6x16_p32\nand\n.b6p2x16\n.\n64B for\nb4x16_p64\n.\nTensor-Size[0] must be a multiple of:\n96B for\nb6x16_p32\nand\n.b6p2x16\n.\n64B for\nb4x16_p64\n.\nFor\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n, the first coordinate in the tensorCoords\nargument vector must be a multiple of 128.\nFor\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n, the global memory address must be 32B aligned.\nAdditionally, tensor stride in every dimension must be 32B aligned.\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\nsupports the following swizzling modes:\nNone.\n128B (With all potential swizzle atomicity values except: 32B with 8B flip)\nFollowing are the restrictions on the 96B swizzle mode:\nThe\n.swizzle_atomicity\nmust be 16B.\nThe\n.interleave_layout\nmust not be set.\nBox-Size[0] must be less than or equal to 96B.\nThe type must not be among following:\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\nThe\n.load_mode\nmust not be set to\n.im2col::w::128\n.\nFollowing are the restrictions on the\n.global.shared::cta\ndirection:\nStarting co-ordinates for Bounding Box (\ntensorCoords\n) must be non-negative.\nThe bounding box along the D, W and H dimensions must stay within the tensor boundaries.\nThis implies:\nBounding-Box Lower-Corner must be non-negative.\nBounding-Box Upper-Corner must be non-positive.\nFollowing are the restrictions for\nsm_120a\n:\ncp.async.bulk.tensor\nwith the direction\n.shared::cluster.global\ndoesn’t support:\nthe sub-byte types\nthe qualifier\n.swizzle_atomicity\nFollowing are the restrictions for\nsm_103a\nwhile using type\n.b6p2x16\non\ncp.async.bulk.tensor\nwith the direction\n.global.shared::cta\n:\nBox-Size[0] must be exactly either of 48B or 96B.\nThe global memory address must be 16B aligned.\nTensor Stride in every dimension must be 16B aligned.\nThe first coordinate in the tensorCoords argument vector must be a multiple of 64.\nTensor-Size[0] must be a multiple of 48B.\nThe following swizzle modes are supported:\nNone.\n128B (With all potential swizzle atomicity values except: 32B with 8B flip)\n64B swizzle with 16B swizzle atomicity\n9.7.9.27.1.2.\nData Movement and Conversion Instructions:\ncp.async.bulk.tensor\n\ncp.async.bulk.tensor\nInitiates an asynchronous copy operation on the tensor data from one state space to another.\nSyntax\n// global -> shared::cta\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.cta_group}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo} {, cache-policy}\n\n.dst =                  { .shared::cta }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.cta_group =            { .cta_group::1, .cta_group::2 }\n.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =    { .L2::cache_hint }\n\n\n// global -> shared::cluster\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.cta_group}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo}\n                                   {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.cta_group =            { .cta_group::1, .cta_group::2 }\n.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> global\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint}\n                                   [tensorMap, tensorCoords], [srcMem] {, cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .tile::scatter4, .im2col_no_offs }\n.level::cache_hint =    { .L2::cache_hint }\nDescription\ncp.async.bulk.tensor\nis a non-blocking instruction which initiates an asynchronous copy\noperation of tensor data from the location in\n.src\nstate space to the location in the\n.dst\nstate space.\nThe operand\ndstMem\nspecifies the location in the\n.dst\nstate space into which the tensor data\nhas to be copied and\nsrcMem\nspecifies the location in the\n.src\nstate space from which the\ntensor data has to be copied.\nWhen\n.dst\nis specified as\n.shared::cta\n, the address\ndstMem\nmust be in the shared memory\nof the executing CTA within the cluster, otherwise the behavior is undefined.\nWhen\n.dst\nis specified as\n.shared::cluster\n, the address\ndstMem\ncan be in the shared memory\nof any of the CTAs within the current cluster.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nThe dimension of the tensor data is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates in the tensor data in the\nglobal memory from or to which the copy operation has to be performed. The individual tensor\ncoordinates in\ntensorCoords\nare of type\n.s32\n. The format of vector argument\ntensorCoords\nis dependent on\n.load_mode\nspecified and is as follows:\n.load_mode\ntensorCoords\nSemantics\n.tile::scatter4\n{col_idx, row_idx0, row_idx1, row_idx2, row_idx3}\nFixed length vector of size 5.\nThe five elements together specify the start\nco-ordinates of the four rows.\n.tile::gather4\nRest all\n{d0, .., dn}\nfor n = .dim\nVector of n elements where n = .dim.\nThe elements indicate the offset in each of the\ndimension.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\nNeeded for completion of\nentire Async operation\noptionally can be used for the completion of\nreading of the tensormap object\n.mbarrier::...\n.shared::cta\n.global\nmbarrier based\nBulk async-group\nbased\n.shared::cluster\n.global\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.async.bulk.tensor\nvariant\nuses mbarrier based completion mechanism. Upon the completion of the asynchronous copy operation, the\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.cta_group\ncan only be specified with the mbarrier based completion mechanism. The\nmodifier\n.cta_group\nis used to signal either the odd numbered CTA or the even numbered CTA among\nthe\nCTA-Pair\n. When\n.cta_group::1\nis specified, the mbarrier object\nmbar\nthat is specified must be in the shared memory of the same CTA as the shared memory destination\ndstMem\n.\nWhen\n.cta_group::2\nis specified, the mbarrier object\nmbar\ncan be in shared memory of either the\nsame CTA as the shared memory destination\ndstMem\nor in its\npeer-CTA\n. If\n.cta_group\nis not specified, then it defaults to\n.cta_group::1\n.\nThe modifier\n.bulk_group\nspecifies that the\ncp.async.bulk.tensor\nvariant uses\nbulk\nasync-group\nbased completion mechanism.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n.\nIn\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination.\nIn\n.tile::gather4\nmode, four rows in 2-dimnesional source tensor are combined to form a single 2-dimensional\ndestination tensor. In\n.tile::scatter4\nmode, single 2-dimensional source tensor is divided into four rows\nin 2-dimensional destination tensor. Details of\n.tile::scatter4\n/\n.tile::gather4\nmodes are described\nin\n.tile::scatter4 and .tile::gather4 modes\n.\nIn\n.im2col\nand\n.im2col::*\nmodes, some dimensions of the source tensors are unrolled in a single\ndimensional column at the destination. Details of the\nim2col\nand\n.im2col::*\nmodes are described\nin\nim2col mode\nand\nim2col::w and im2col::w::128 modes\nrespectively. In\n.im2col\nand\n.im2col::*\nmodes, the tensor has to be at least 3-dimensional. The vector\noperand\nim2colInfo\ncan be specified only when\n.load_mode\nis\n.im2col\nor\n.im2col::w\nor\n.im2col::w::128\n. The format of the vector argument\nim2colInfo\nis dependent on the exact im2col mode\nand is as follows:\nExact im2col mode\nim2colInfo argument\nSemantics\n.im2col\n{ i2cOffW , i2cOffH , i2cOffD }\nfor\n.dim\n=\n.5d\nA vector of im2col offsets whose vector size is two\nless than number of dimensions .dim.\n.im2col::w\n{ wHalo, wOffset }\nA vector of 2 arguments containing\nwHalo\nand\nwOffset\narguments.\n.im2col::w::128\n.im2col_no_offs\nim2colInfo\nis not applicable.\nim2colInfo\nis not applicable.\nArgument\nwHalo\nis a 16bit unsigned integer whose valid set of values differs on the load-mode and is as follows:\n- Im2col::w mode : valid range is [0, 512).\n- Im2col::w::128 mode : valid range is [0, 32).\nArgument\nwOffset\nis a 16bit unsigned integer whose valid range of values is [0, 32).\nThe optional modifier\n.multicast::cluster\nallows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand\nctaMask\nspecifies the destination CTAs in the\ncluster such that each bit position in the 16-bit\nctaMask\noperand corresponds to the\n%ctaid\nof the destination CTA. The source data is multicast to the same offset as\ndstMem\nin the shared\nmemory of each destination CTA. When\n.cta_group\nis specified as:\n.cta_group::1\n: The mbarrier signal is also multicasted to the same offset as\nmbar\nin\nthe shared memory of the destination CTA.\n.cta_group::2\n: The mbarrier signal is multicasted either to all the odd numbered CTAs or the\neven numbered CTAs within the corresponding\nCTA-Pair\n. For each destination\nCTA specified in the\nctaMask\n, the mbarrier signal is sent either to the destination CTA or its\npeer-CTA\nbased on CTAs\n%cluster_ctarank\nparity of shared memory where\nthe mbarrier object\nmbar\nresides.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nThe copy operation in\ncp.async.bulk.tensor\nis treated as a weak memory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nNotes\n.multicast::cluster\nqualifier is optimized for target architecture\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\nand may have substantially reduced performance on other\ntargets and hence\n.multicast::cluster\nis advised to be used with\n.target\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.shared::cta\nas destination state space is introduced in PTX ISA version 8.6.\nSupport for qualifiers\n.tile::gather4\nand\n.tile::scatter4\nintroduced in PTX ISA version 8.6.\nSupport for qualifiers\n.im2col::w\nand\n.im2col::w::128\nintroduced in PTX ISA version 8.6.\nSupport for qualifier\n.cta_group\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.multicast::cluster\nqualifier advised to be used with\n.target\nsm_90a\nor\nsm_100f\nor\nsm_100a\nor\nsm_103f\nor\nsm_103a\nor\nsm_110f\nor\nsm_110a\n.\nQualifiers\n.tile::gather4\nand\n.im2col::w\nrequire:\nsm_100a\nwhen destination state space is\n.shared::cluster\nand is supported on\nsm_100f\nfrom PTX ISA version 8.8.\nsm_100\nor higher when destination state space is\n.shared::cta\n.\nQualifier\n.tile::scatter4\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.im2col::w::128\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.cta_group\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n.reg .b16 ctaMask;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.tensor.1d.shared::cta.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.5d.shared::cta.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\ncp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                     [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask;\n\n@p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                     [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy;\n\n@p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group  [tensorMap3, {tc0}], [sMem3];\n\ncp.async.bulk.tensor.2d.tile::gather4.shared::cluster.global.mbarrier::complete_tx::bytes\n                     [sMem5], [tensorMap6, {x0, y0, y1, y2, y3}], [mbar5];\n\ncp.async.bulk.tensor.3d.im2col::w.shared::cluster.global.mbarrier::complete_tx::bytes\n                     [sMem4], [tensorMap5, {t0, t1, t2}], [mbar4], {im2colwHalo, im2colOff};\n\ncp.async.bulk.tensor.1d.shared::cluster.global.tile.cta_group::2\n                     [sMem6], [tensorMap7, {tc0}], [peerMbar];\n9.7.9.27.1.3.\nData Movement and Conversion Instructions:\ncp.reduce.async.bulk.tensor\n\ncp.reduce.async.bulk.tensor\nInitiates an asynchronous reduction operation on the tensor data.\nSyntax\n// shared::cta -> global:\ncp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint}\n                                          [tensorMap, tensorCoords], [srcMem] {,cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .im2col_no_offs }\n.redOp =                { .add, .min, .max, .inc, .dec, .and, .or, .xor}\nDescription\ncp.reduce.async.bulk.tensor\nis a non-blocking instruction which initiates an asynchronous\nreduction operation of tensor data in the\n.dst\nstate space with tensor data in the\n.src\nstate space.\nThe operand\nsrcMem\nspecifies the location of the tensor data in the\n.src\nstate space using\nwhich the reduction operation has to be performed.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nEach element of the tensor data in the\n.dst\nstate space is reduced inline with the corresponding\nelement from the tensor data in the\n.src\nstate space. The modifier\n.redOp\nspecifies the\nreduction operation used for the inline reduction. The type of each tensor data element in the\nsource and the destination tensor is specified in\nTensor-map\n.\nThe dimension of the tensor is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates of the tensor data in the\nglobal memory on which the reduce operation is to be performed. The number of tensor coordinates in\nthe vector argument\ntensorCoords\nshould be equal to the dimension specified by the modifier\n.dim\n. The individual tensor coordinates are of the type\n.s32\n.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.redOp\nElement type\n.add\n.u32\n,\n.s32\n,\n.u64\n,\n.f32\n,\n.f16\n,\n.bf16\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n,\n.f16\n,\n.bf16\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. Value\n.bulk_group\nof the modifier\n.completion_mechanism\nspecifies that\ncp.reduce.async.bulk.tensor\ninstruction uses\nbulk async-group\nbased completion mechanism.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n. In\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination. In\n.im2col_no_offs\nmode, some dimensions of the source tensors are unrolled in a single dimensional\ncolumn at the destination. Details of the\nim2col\nmode are described in\nim2col mode\n. In\n.im2col\nmode, the tensor has to be at least\n3-dimensional.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nEach reduction operation performed by\ncp.reduce.async.bulk.tensor\nhas individually\n.relaxed.gpu\nmemory ordering semantics. The load operations in\ncp.reduce.async.bulk.tensor\nare treated as weak memory operations and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group\n                                             [tensorMap0, {tc0}], [sMem0];\n\ncp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint\n                                             [tensorMap1, {tc0, tc1}], [sMem1] , policy;\n\ncp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group\n                                             [tensorMap2, {tc0, tc1, tc2}], [sMem2]\n9.7.9.27.1.4.\nData Movement and Conversion Instructions:\ncp.async.bulk.prefetch.tensor\n\ncp.async.bulk.prefetch.tensor\nProvides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache.\nSyntax\n// global -> shared::cluster:\ncp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords]\n                                                             {, im2colInfo } {, cache-policy}\n\n.src =                { .global }\n.dim =                { .1d, .2d, .3d, .4d, .5d }\n.load_mode =          { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =  { .L2::cache_hint }\nDescription\ncp.async.bulk.prefetch.tensor\nis a non-blocking instruction which may initiate an asynchronous\nprefetch of tensor data from the location in\n.src\nstatespace to the L2 cache.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nThe dimension of the tensor data is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates in the tensor data in the\nglobal memory from which the copy operation has to be performed. The individual tensor\ncoordinates in\ntensorCoords\nare of type\n.s32\n. The format of vector argument\ntensorCoords\nis dependent on\n.load_mode\nspecified and is as follows:\n.load_mode\ntensorCoords\nSemantics\n.tile::gather4\n{col_idx, row_idx0, row_idx1, row_idx2, row_idx3}\nFixed length vector of size 5.\nThe five elements together specify the start\nco-ordinates of the four rows.\nRest all\n{d0, .., dn}\nfor n = .dim\nVector of n elements where n = .dim.\nThe elements indicate the offset in each of the\ndimension.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n.\nIn\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination.\nIn\n.tile::gather4\nmode, four rows in the 2-dimnesional source tensor are fetched to L2 cache.\nDetails of\n.tile::gather4\nmodes are described\nin\n.tile::scatter4 and .tile::gather4 modes\n.\nIn\n.im2col\nand\n.im2col::*\nmodes, some dimensions of the source tensors are unrolled in a single\ndimensional column at the destination. Details of the\nim2col\nand\n.im2col::*\nmodes are described in\nim2col mode\nand\nim2col::w and im2col::w::128 modes\nrespectively. In\n.im2col\nand\n.im2col::*\nmodes, the tensor has to be at least 3-dimensional. The vector\noperand\nim2colInfo\ncan be specified only when\n.load_mode\nis\n.im2col\nor\n.im2col::w\nor\n.im2col::w::128\n. The format of the vector argument\nim2colInfo\nis dependent on the exact im2col mode\nand is as follows:\nExact im2col mode\nim2colInfo argument\nSemantics\n.im2col\n{ i2cOffW , i2cOffH , i2cOffD }\nfor\n.dim\n=\n.5d\nA vector of im2col offsets whose vector size is two\nless than number of dimensions .dim.\n.im2col::w\n{ wHalo, wOffset }\nA vector of 2 arguments containing\nwHalo\nand\nwOffset\narguments.\n.im2col::w::128\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\ncp.async.bulk.prefetch.tensor\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for qualifier\n.tile::gather4\nintroduced in PTX ISA version 8.6.\nSupport for qualifiers\n.im2col::w\nand\n.im2col::w::128\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nQualifier\n.tile::gather4\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifiers\n.im2col::w\nand\n.im2col::w::128\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n.reg .b16 ctaMask, im2colwHalo, im2colOff;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.prefetch.tensor.1d.L2.global.tile  [tensorMap0, {tc0}];\n\n@p cp.async.bulk.prefetch.tensor.2d.L2.global    [tensorMap1, {tc0, tc1}];\n\n@p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col\n                      [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint\n                      [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy;\n\ncp.async.bulk.prefetch.tensor.2d.L2.global.tile::gather4 [tensorMap5, {col_idx, row_idx0, row_idx1, row_idx2, row_idx3}];\n\ncp.async.bulk.prefetch.tensor.4d.L2.global.im2col::w::128\n                      [tensorMap4, {t0, t1, t2, t3}], {im2colwHalo, im2colOff};\n9.7.9.27.2.\nData Movement and Conversion Instructions: Bulk and Tensor copy completion instructions\n\n9.7.9.27.2.1.\nData Movement and Conversion Instructions:\ncp.async.bulk.commit_group\n\ncp.async.bulk.commit_group\nCommits all prior initiated but uncommitted\ncp.async.bulk\ninstructions into a\ncp.async.bulk-group\n.\nSyntax\ncp.async.bulk.commit_group;\nDescription\ncp.async.bulk.commit_group\ninstruction creates a new per-thread\nbulk async-group\nand batches\nall prior\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions satisfying the following\nconditions into the new\nbulk async-group\n:\nThe prior\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions use\nbulk_group\nbased\ncompletion mechanism, and\nThey are initiated by the executing thread but not committed to any\nbulk async-group\n.\nIf there are no uncommitted\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions then\ncp.async.bulk.commit_group\nresults in an empty\nbulk async-group\n.\nAn executing thread can wait for the completion of all\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\noperations in a\nbulk async-group\nusing\ncp.async.bulk.wait_group\n.\nThere is no memory ordering guarantee provided between any two\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\noperations within the same\nbulk async-group\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.commit_group;\n9.7.9.27.2.2.\nData Movement and Conversion Instructions:\ncp.async.bulk.wait_group\n\ncp.async.bulk.wait_group\nWait for completion of\nbulk async-groups\n.\nSyntax\ncp.async.bulk.wait_group{.read} N;\nDescription\ncp.async.bulk.wait_group\ninstruction will cause the executing thread to wait until only N or\nfewer of the most recent\nbulk async-groups\nare pending and all the prior\nbulk async-groups\ncommitted by the executing threads are complete. For example, when N is 0, the executing thread\nwaits on all the prior\nbulk async-groups\nto complete. Operand N is an integer constant.\nBy default,\ncp.async.bulk.wait_group\ninstruction will cause the executing thread to wait until\ncompletion of all the bulk async operations in the specified\nbulk async-group\n. A bulk async\noperation includes the following:\nOptionally, reading from the tensormap.\nReading from the source locations.\nWriting to their respective destination locations.\nWrites being made visible to the executing thread.\nThe optional\n.read\nmodifier indicates that the waiting has to be done until all the bulk\nasync operations in the specified\nbulk async-group\nhave completed:\nreading from the tensormap\nthe reading from their source locations.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.wait_group.read   0;\ncp.async.bulk.wait_group        2;\n9.7.9.28.\nData Movement and Conversion Instructions:\ntensormap.replace\n\ntensormap.replace\nModifies the field of a tensor-map object.\nSyntax\ntensormap.replace.mode.field1{.ss}.b1024.type  [addr], new_val;\ntensormap.replace.mode.field2{.ss}.b1024.type  [addr], ord, new_val;\ntensormap.replace.mode.field3{.ss}.b1024.type  [addr], new_val;\n\n.mode    = { .tile }\n.field1  = { .global_address, .rank }\n.field2  = { .box_dim, .global_dim, .global_stride, .element_stride  }\n.field3  = { .elemtype,  .interleave_layout, .swizzle_mode, .swizzle_atomicity, .fill_mode }\n.ss      = { .global, .shared::cta }\n.type    = { .b32, .b64 }\nDescription\nThe\ntensormap.replace\ninstruction replaces the field, specified by\n.field\nqualifier,\nof the tensor-map object at the location specified by the address operand\naddr\nwith a\nnew value. The new value is specified by the argument\nnew_val\n.\nQualifier\n.mode\nspecifies the mode of the\ntensor-map\nobject\nlocated at the address operand\naddr\n.\nInstruction type\n.b1024\nindicates the size of the\ntensor-map\nobject, which is 1024 bits.\nOperand\nnew_val\nhas the type\n.type\n. When\n.field\nis specified as\n.global_address\nor\n.global_stride\n,\n.type\nmust be\n.b64\n. Otherwise,\n.type\nmust be\n.b32\n.\nThe immediate integer operand\nord\nspecifies the ordinal of the field across the rank of the\ntensor which needs to be replaced in the\ntensor-map\nobject.\nFor field\n.rank\n, the operand\nnew_val\nmust be ones less than the desired tensor rank as\nthis field uses zero-based numbering.\nWhen\n.field3\nis specified, the operand\nnew_val\nmust be an immediate and the\nTable 33\nshows the mapping of the operand\nnew_val\nacross various fields.\nTable 33\nTensormap new_val validity\n\nnew_val\n.field3\n.elemtype\n.interleave_layout\n.swizzle_mode\n.swizzle_atomicity\n.fill_mode\n0\n.u8\nNo interleave\nNo swizzling\n16B\nZero fill\n1\n.u16\n16B interleave\n32B swizzling\n32B\nOOB-NaN fill\n2\n.u32\n32B interleave\n64B swizzling\n32B + 8B flip\nx\n3\n.s32\nx\n128B swizzling\n64B\nx\n4\n.u64\nx\n96B swizzling\nx\nx\n5\n.s64\nx\nx\nx\nx\n6\n.f16\nx\nx\nx\nx\n7\n.f32\nx\nx\nx\nx\n8\n.f32.ftz\nx\nx\nx\nx\n9\n.f64\nx\nx\nx\nx\n10\n.bf16\nx\nx\nx\nx\n11\n.tf32\nx\nx\nx\nx\n12\n.tf32.ftz\nx\nx\nx\nx\n13\n.b4x16\nx\nx\nx\nx\n14\n.b4x16_p64\nx\nx\nx\nx\n15\n.b6x16_p32\nor\n.b6p2x16\nx\nx\nx\nx\nNote\nThe values of\n.elemtype\ndo not correspond to the values of the\nCUtensorMapDataType\nenum used in the driver API.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the address specified by\naddr\ndoes not fall within the address window of\n.global\nor\n.shared::cta\nstate space then the behavior is undefined.\ntensormap.replace\nis treated as a weak memory operation, on the entire 1024-bit opaque\ntensor-map\nobject, in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nQualifier\n.swizzle_atomicity\nintroduced in PTX ISA version 8.6.\nQualifier\n.elemtype\nwith values from\n13\nto\n15\n, both inclusive, is\nsupported in PTX ISA version 8.7 onwards.\nQualifier\n.swizzle_mode\nwith value\n4\nis supported from PTX ISA version 8.8 onwards.\nTarget ISA Notes\nSupported on following architectures:\nsm_90a\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nQualifier\n.swizzle_atomicity\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\n(refer to\nsection\nfor restrictions on sm_120a)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.field3\nvariant\n.elemtype\ncorresponding to\nnew_val\nvalues\n13\n,\n14\nand\n15\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\n(refer to\nsection\nfor restrictions on sm_120a)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.field3\nvariant\n.swizzle_mode\ncorresponding to\nnew_val\nvalue\n4\nis supported on\nfollowing architectures:\nsm_103a\n(refer to\nsection\nfor restrictions on sm_103a)\nExamples\ntensormap.replace.tile.global_address.shared::cta.b1024.b64   [sMem], new_val;\n9.7.10.\nTexture Instructions\n\nThis section describes PTX instructions for accessing textures and samplers. PTX supports the\nfollowing operations on texture and sampler descriptors:\nStatic initialization of texture and sampler descriptors.\nModule-scope and per-entry scope definitions of texture and sampler descriptors.\nAbility to query fields within texture and sampler descriptors.\n9.7.10.1.\nTexturing Modes\n\nFor working with textures and samplers, PTX has two modes of operation. In the\nunified mode,\ntexture and sampler information is accessed through a single\n.texref\nhandle. In the\nindependent\nmode\n, texture and sampler information each have their own handle, allowing them to be defined\nseparately and combined at the site of usage in the program.\nThe advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior\nto\nsm_3x\n), with the restriction that they correspond 1-to-1 with the 256 possible textures per\nkernel (128 for architectures prior to\nsm_3x\n). The advantage of independent mode is that\ntextures and samplers can be mixed and matched, but the number of samplers is greatly restricted to\n32 per kernel (16 for architectures prior to\nsm_3x\n).\nTable 34\nsummarizes the number of textures, samplers and\nsurfaces available in different texturing modes.\nTable 34\nTexture, sampler and surface limits\n\nTexturing mode\nResource\nsm_1x\n,\nsm_2x\nsm_3x+\nUnified mode\nTextures\n128\n256\nSamplers\n128\n256\nSurfaces\n8\n16\nIndependent mode\nTextures\n128\n256\nSamplers\n16\n32\nSurfaces\n8\n16\nThe texturing mode is selected using\n.target\noptions\ntexmode_unified\nand\ntexmode_independent\n. A PTX module may declare only one texturing mode. If no texturing mode is\ndeclared, the module is assumed to use unified mode.\nExample\n: calculate an element’s power contribution as element’s power/total number of elements.\n.target texmode_independent\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n...\n.entry compute_power\n  ( .param .texref tex1 )\n{\n  txq.width.b32  r6, [tex1]; // get tex1's width\n  txq.height.b32 r5, [tex1]; // get tex1's height\n  tex.2d.v4.f32.f32  {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}];\n  mul.u32 r5, r5, r6;\n  add.f32 r1, r1, r2;\n  add.f32 r3, r3, r4;\n  add.f32 r1, r1, r3;\n  cvt.f32.u32 r5, r5;\n  div.f32 r1, r1, r5;\n}\n9.7.10.2.\nMipmaps\n\nA\nmipmap\nis a sequence of textures, each of which is a progressively lower resolution\nrepresentation of the same image. The height and width of each image, or level of detail (LOD), in\nthe mipmap is a power of two smaller than the previous level. Mipmaps are used in graphics\napplications to improve rendering speed and reduce aliasing artifacts. For example, a\nhigh-resolution mipmap image is used for objects that are close to the user; lower-resolution images\nare used as the object appears farther away. Mipmap filtering modes are provided when switching\nbetween two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity.\nExample:\nIf the texture has a basic size of 256 by 256 pixels, then the associated mipmap set\nmay contain a series of eight images, each one-fourth the total area of the previous one: 128x128\npixels, 64x64, 32x32, 16x16, 8x8, 4x4, 2x2, 1x1 (a single pixel). If, for example, a scene is\nrendering this texture in a space of 40x40 pixels, then either a scaled up version of the 32x32\n(without trilinear interpolation) or an interpolation of the 64x64 and the 32x32 mipmaps (with\ntrilinear interpolation) would be used.\nThe total number of LODs in a complete mipmap pyramid is calculated through the following equation:\nnumLODs = 1 + floor(log2(max(w, h, d)))\nThe finest LOD is called the base level and is the 0th level. The next (coarser) level is the 1st\nlevel, and so on. The coarsest level is the level of size (1 x 1 x 1). Each successively smaller\nmipmap level has half the {width, height, depth} of the previous level, but if this half value is a\nfractional value, it’s rounded down to the next largest integer. Essentially, the size of a mipmap\nlevel can be specified as:\nmax(1, floor(w_b / 2^i)) x\nmax(1, floor(h_b / 2^i)) x\nmax(1, floor(d_b / 2^i))\nwhere\ni\nis the ith level beyond the 0th level (the base level). And\nw_b\n,\nh_b\nand\nd_b\nare the\nwidth, height and depth of the base level respectively.\nPTX support for mipmaps\nThe PTX\ntex\ninstruction supports three modes for specifying the LOD:\nbase\n,\nlevel\n, and\ngrad\nient. In base mode, the instruction always picks level 0. In level mode, an additional\nargument is provided to specify the LOD to fetch from. In gradmode, two floating-point vector\narguments provide\npartials\n(e.g.,\n{ds/dx,\ndt/dx}\nand\n{ds/dy,\ndt/dy}\nfor a 2d texture),\nwhich the\ntex\ninstruction uses to compute the LOD.\nThese instructions provide access to texture memory.\ntex\ntld4\ntxq\n9.7.10.3.\nTexture Instructions:\ntex\n\ntex\nPerform a texture memory lookup.\nSyntax\ntex.geom.v4.dtype.ctype  d, [a, c] {, e} {, f};\ntex.geom.v4.dtype.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\ntex.geom.v2.f16x2.ctype  d[|p], [a, c] {, e} {, f};\ntex.geom.v2.f16x2.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n// mipmaps\ntex.base.geom.v4.dtype.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v4.dtype.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v4.dtype.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\ntex.base.geom.v2.f16x2.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v2.f16x2.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v2.f16x2.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms };\n.dtype = { .u32, .s32, .f16,  .f32 };\n.ctype = {       .s32, .f32 };          // .cube, .acube require .f32\n                                        // .2dms, .a2dms require .s32\nDescription\ntex.{1d,2d,3d}\nTexture lookup using a texture coordinate vector. The instruction loads data from the texture named\nby operand\na\nat coordinates given by operand\nc\ninto destination\nd\n. Operand\nc\nis a\nscalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a\nfour-element vector for 3d textures, where the fourth element is ignored. An optional texture\nsampler\nb\nmay be specified. If no sampler is specified, the sampler behavior is a property of\nthe named texture. The optional destination predicate\np\nis set to\nTrue\nif data from texture\nat specified coordinates is resident in memory,\nFalse\notherwise. When optional destination\npredicate\np\nis set to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data\nat specified coordinates is dependent on execution environment setup using Driver API calls, prior\nto kernel launch. Refer to Driver API documentation for more details including any\nsystem/implementation specific behavior.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nvalues that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand\ne\nis a singleton tuple for 1d textures; is a two\nelement vector 2d textures; and is four-element vector for 3d textures, where the fourth element is\nignored.\nAn optional operand\nf\nmay be specified for\ndepth\ntextures\n. Depth textures are special type\nof textures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in\nf\noperand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering. When using depth compare operand, the elements in texture coordinate vector\nc\nhave\n.f32\ntype.\nDepth compare operand is not supported for\n3d\ntextures.\nThe instruction returns a two-element vector for destination type\n.f16x2\n. For all other\ndestination types, the instruction returns a four-element vector. Coordinates may be given in either\nsigned 32-bit integer or 32-bit floating point form.\nA texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\ntex.{a1d,a2d}\nTexture array selection, followed by texture lookup. The instruction first selects a texture from\nthe texture array named by operand\na\nusing the index given by the first element of the array\ncoordinate vector\nc\n. The instruction then loads data from the selected texture at coordinates\ngiven by the remaining elements of operand\nc\ninto destination\nd\n. Operand\nc\nis a bit-size\ntype vector or tuple containing an index into the array of textures followed by coordinates within\nthe selected texture, as follows:\nFor 1d texture arrays, operand\nc\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the second element is interpreted as\na 1d texture coordinate of type\n.ctype\n.\nFor 2d texture arrays, operand\nc\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the next two elements are\ninterpreted as 2d texture coordinates of type\n.ctype\n. The fourth element is ignored.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nvalues that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand\ne\nis a singleton tuple for 1d texture arrays; and is\na two element vector 2d texture arrays.\nAn optional operand\nf\nmay be specified for depth textures arrays. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth textures. When using depth compare\noperand, the coordinates in texture coordinate vector\nc\nhave\n.f32\ntype.\nThe instruction returns a two-element vector for destination type\n.f16x2\n. For all other\ndestination types, the instruction returns a four-element vector. The texture array index is a\n32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point\nvalues.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.cube\nCubemap\ntexture lookup. The instruction loads data from the cubemap texture named by operand\na\nat coordinates given by operand\nc\ninto destination\nd\n. Cubemap textures are special\ntwo-dimensional layered textures consisting of six layers that represent the faces of a cube. All\nlayers in a cubemap are of the same size and are square (i.e., width equals height).\nWhen accessing a cubemap, the texture coordinate vector\nc\nhas type\n.v4.f32\n, and comprises\nthree floating-point coordinates (\ns\n,\nt\n,\nr\n) and a fourth padding argument which is\nignored. Coordinates (\ns\n,\nt\n,\nr\n) are projected onto one of the six cube faces. The (\ns\n,\nt\n,\nr\n) coordinates can be thought of as a direction vector emanating from the center of the\ncube. Of the three coordinates (\ns\n,\nt\n,\nr\n), the coordinate of the largest magnitude (the\nmajor axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by\nthe absolute value of the major axis to produce a new (\ns\n,\nt\n) coordinate pair to lookup into\nthe selected cube face.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nOffset vector operand\ne\nis not supported for cubemap textures.\nan optional operand\nf\nmay be specified for cubemap depth textures. operand\nf\nis\n.f32\nscalar value that specifies depth compare value for cubemap depth textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.acube\nCubemap array selection, followed by cubemap lookup. The instruction first selects a cubemap texture\nfrom the cubemap array named by operand\na\nusing the index given by the first element of the\narray coordinate vector\nc\n. The instruction then loads data from the selected cubemap texture at\ncoordinates given by the remaining elements of operand\nc\ninto destination\nd\n.\nCubemap array\ntextures consist of an array of cubemaps, i.e., the total number of layers is a\nmultiple of six. When accessing a cubemap array texture, the coordinate vector\nc\nhas type\n.v4.b32\n. The first element is interpreted as an unsigned integer index (\n.u32\n) into the\ncubemap array, and the remaining three elements are interpreted as floating-point cubemap\ncoordinates (\ns\n,\nt\n,\nr\n), used to lookup in the selected cubemap as described above.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nOffset vector operand\ne\nis not supported for cubemap texture arrays.\nAn optional operand\nf\nmay be specified for cubemap depth texture arrays. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for cubemap depth textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.2dms\nMulti-sample texture lookup using a texture coordinate vector. Multi-sample textures consist of\nmultiple samples per data element. The instruction loads data from the texture named by operand\na\nfrom sample number given by first element of the operand\nc\n, at coordinates given by\nremaining elements of operand\nc\ninto destination\nd\n. When accessing a multi-sample texture,\ntexture coordinate vector\nc\nhas type\n.v4.b32\n. The first element in operand\nc\nis\ninterpreted as unsigned integer sample number (\n.u32\n), and the next two elements are interpreted\nas signed integer (\n.s32\n) 2d texture coordinates. The fourth element is ignored. An optional\ntexture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior is a\nproperty of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7.\nDepth compare operand\nf\nis not supported for multi-sample textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.a2dms\nMulti-sample texture array selection, followed by multi-sample texture lookup. The instruction first\nselects a multi-sample texture from the multi-sample texture array named by operand a using the\nindex given by the first element of the array coordinate vector\nc\n. The instruction then loads\ndata from the selected multi-sample texture from sample number given by second element of the\noperand\nc\n, at coordinates given by remaining elements of operand\nc\ninto destination\nd\n. When accessing a multi-sample texture array, texture coordinate vector\nc\nhas type\n.v4.b32\n. The first element in operand c is interpreted as unsigned integer sampler number, the\nsecond element is interpreted as unsigned integer index (\n.u32\n) into the multi-sample texture\narray and the next two elements are interpreted as signed integer (\n.s32\n) 2d texture\ncoordinates. An optional texture sampler\nb\nmay be specified. If no sampler is specified, the\nsampler behavior is a property of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nvalues\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7.\nDepth compare operand\nf\nis not supported for multi-sample texture arrays.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nMipmaps\n.base\n(lod zero)\nPick level 0 (base level). This is the default if no mipmap mode is specified. No additional arguments.\n.level\n(lod explicit)\nRequires an additional 32-bit scalar argument,\nlod\n, which contains the LOD to fetch from. The\ntype of\nlod\nfollows\n.ctype\n(either\n.s32\nor\n.f32\n). Geometries\n.2dms\nand\n.a2dms\nare not supported in this mode.\n.grad\n(lod gradient)\nRequires two\n.f32\nvectors,\ndPdx\nand\ndPdy\n, that specify the partials. The vectors are\nsingletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are\nfour-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d\nand cube geometries. Geometries\n.2dms\nand\n.a2dms\nare not supported in this mode.\nFor mipmap texture lookup, an optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7. Offset vector operand is not supported for cube\nand cubemap geometries.\nAn optional operand\nf\nmay be specified for mipmap textures. Operand\nf\nis\n.f32\nscalar\nvalue that specifies depth compare value for depth textures. When using depth compare operand, the\ncoordinates in texture coordinate vector\nc\nhave\n.f32\ntype.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nDepth compare operand is not supported for\n3d\ntextures.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nNotes\nFor compatibility with prior versions of PTX, the square brackets are not required and\n.v4\ncoordinate vectors are allowed for any geometry, with the extra elements being ignored.\nPTX ISA Notes\nUnified mode texturing introduced in PTX ISA version 1.0. Extension using opaque\n.texref\nand\n.samplerref\ntypes and independent mode texturing introduced in PTX ISA version 1.5.\nTexture arrays\ntex.{a1d,a2d}\nintroduced in PTX ISA version 2.3.\nCubemaps and cubemap arrays introduced in PTX ISA version 3.0.\nSupport for mipmaps introduced in PTX ISA version 3.1.\nIndirect texture access introduced in PTX ISA version 3.1.\nMulti-sample textures and multi-sample texture arrays introduced in PTX ISA version 3.2.\nSupport for textures returning\n.f16\nand\n.f16x2\ndata introduced in PTX ISA version 4.2.\nSupport for\ntex.grad.{cube,\nacube}\nintroduced in PTX ISA version 4.3.\nOffset vector operand introduced in PTX ISA version 4.3.\nDepth compare operand introduced in PTX ISA version 4.3.\nSupport for optional destination predicate introduced in PTX ISA version 7.1.\nTarget ISA Notes\nSupported on all target architectures.\nThe cubemap array geometry (\n.acube\n) requires\nsm_20\nor higher.\nMipmaps require\nsm_20\nor higher.\nIndirect texture access requires\nsm_20\nor higher.\nMulti-sample textures and multi-sample texture arrays require\nsm_30\nor higher.\nTexture fetch returning\n.f16\nand\n.f16x2\ndata require\nsm_53\nor higher.\ntex.grad.{cube,\nacube}\nrequires\nsm_20\nor higher.\nOffset vector operand requires\nsm_30\nor higher.\nDepth compare operand requires\nsm_30\nor higher.\nSupport for optional destination predicate requires\nsm_60\nor higher.\nExamples\n// Example of unified mode texturing\n // - f4 is required to pad four-element tuple and is ignored\n tex.3d.v4.s32.s32  {r1,r2,r3,r4}, [tex_a,{f1,f2,f3,f4}];\n\n // Example of independent mode texturing\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,smpl_x,{f1}];\n\n // Example of 1D texture array, independent texturing mode\n tex.a1d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,smpl_x,{idx,s1}];\n\n // Example of 2D texture array, unified texturing mode\n // - f3 is required to pad four-element tuple and is ignored\n tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2,f3}];\n\n // Example of cubemap array, unified textureing mode\n tex.acube.v4.f32.f32 {r0,r1,r2,r3}, [tex_cuarray,{idx,f1,f2,f3}];\n\n // Example of multi-sample texture, unified texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture, independent texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms, smpl_x,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture array, unified texturing mode\n tex.a2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ams,{idx,sample,r6,r7}];\n\n // Example of texture returning .f16 data\n tex.1d.v4.f16.f32  {h1,h2,h3,h4}, [tex_a,smpl_x,{f1}];\n\n // Example of texture returning .f16x2 data\n tex.1d.v2.f16x2.f32  {h1,h2}, [tex_a,smpl_x,{f1}];\n\n // Example of 3d texture array access with tex.grad,unified texturing mode\n tex.grad.3d.v4.f32.f32 {%f4,%f5,%f6,%f7},[tex_3d,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n// Example of cube texture array access with tex.grad,unified texturing mode\n tex.grad.cube.v4.f32.f32{%f4,%f5,%f6,%f7},[tex_cube,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n // Example of 1d texture lookup with offset, unified texturing mode\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a, {f1}], {r5};\n\n // Example of 2d texture array lookup with offset, unified texturing mode\n tex.a2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{idx,f1,f2}], {f5,f6};\n\n // Example of 2d mipmap texture lookup with offset, unified texturing mode\n tex.level.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}],\n                          flvl, {r7, r8};\n\n // Example of 2d depth texture lookup with compare, unified texturing mode\n tex.1d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a, {f1}], f0;\n\n // Example of depth 2d texture array lookup with offset, compare\n tex.a2d.v4.s32.f32  {f0,f1,f2,f3}, [tex_a,{idx,f4,f5}], {r5,r6}, f6;\n\n // Example of destination predicate use\n tex.3d.v4.s32.s32 {r1,r2,r3,r4}|p, [tex_a,{f1,f2,f3,f4}];\n9.7.10.4.\nTexture Instructions:\ntld4\n\ntld4\nPerform a texture fetch of the 4-texel bilerp footprint.\nSyntax\ntld4.comp.2d.v4.dtype.f32    d[|p], [a, c] {, e} {, f};\ntld4.comp.geom.v4.dtype.f32  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n.comp  = { .r, .g, .b, .a };\n.geom  = { .2d, .a2d, .cube, .acube };\n.dtype = { .u32, .s32, .f32 };\nDescription\nTexture fetch of the 4-texel bilerp footprint using a texture coordinate vector. The instruction\nloads the bilerp footprint from the texture named by operand\na\nat coordinates given by operand\nc\ninto vector destination\nd\n. The texture component fetched for each texel sample is\nspecified by\n.comp\n. The four texel samples are placed into destination vector\nd\nin\ncounter-clockwise order starting at lower left.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nAn optional operand\nf\nmay be specified for\ndepth textures\n. Depth textures are special type of\ntextures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in\nf\noperand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering.\nA texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\ntld4.2d\nFor 2D textures, operand\nc\nspecifies coordinates as a two-element, 32-bit floating-point vector.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7.\ntld4.a2d\nTexture array selection, followed by\ntld4\ntexture fetch of 2d texture. For 2d texture arrays\noperand\nc\nis a four element, 32-bit vector. The first element in operand c is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the next two elements are interpreted\nas 32-bit floating point coordinates of 2d texture. The fourth element is ignored.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7.\ntld4.cube\nFor cubemap textures, operand\nc\nspecifies four-element vector which comprises three\nfloating-point coordinates (s, t, r) and a fourth padding argument which is ignored.\nCubemap textures are special two-dimensional layered textures consisting of six layers that\nrepresent the faces of a cube. All layers in a cubemap are of the same size and are square (i.e.,\nwidth equals height).\nCoordinates (s, t, r) are projected onto one of the six cube faces. The (s, t, r) coordinates can be\nthought of as a direction vector emanating from the center of the cube. Of the three coordinates (s,\nt, r), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the\nother two coordinates (the minor axes) are divided by the absolute value of the major axis to\nproduce a new (s, t) coordinate pair to lookup into the selected cube face.\nOffset vector operand\ne\nis not supported for cubemap textures.\ntld4.acube\nCubemap array selection, followed by\ntld4\ntexture fetch of cubemap texture. The first element in\noperand\nc\nis interpreted as an unsigned integer index (\n.u32\n) into the cubemap texture array,\nand the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r),\nused to lookup in the selected cubemap.\nOffset vector operand\ne\nis not supported for cubemap texture arrays.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.2.\nIndirect texture access introduced in PTX ISA version 3.1.\ntld4.{a2d,cube,acube}\nintroduced in PTX ISA version 4.3.\nOffset vector operand introduced in PTX ISA version 4.3.\nDepth compare operand introduced in PTX ISA version 4.3.\nSupport for optional destination predicate introduced in PTX ISA version 7.1.\nTarget ISA Notes\ntld4\nrequires\nsm_20\nor higher.\nIndirect texture access requires\nsm_20\nor higher.\ntld4.{a2d,cube,acube}\nrequires\nsm_30\nor higher.\nOffset vector operand requires\nsm_30\nor higher.\nDepth compare operand requires\nsm_30\nor higher.\nSupport for optional destination predicate requires\nsm_60\nor higher.\nExamples\n//Example of unified mode texturing\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}];\n\n// Example of independent mode texturing\ntld4.r.2d.v4.u32.f32  {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}];\n\n// Example of unified mode texturing using offset\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};\n\n// Example of unified mode texturing using compare\ntld4.r.2d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7;\n\n// Example of optional destination predicate\ntld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7;\n9.7.10.5.\nTexture Instructions:\ntxq\n\ntxq\nQuery texture and sampler attributes.\nSyntax\ntxq.tquery.b32         d, [a];       // texture attributes\ntxq.level.tlquery.b32  d, [a], lod;  // texture attributes\ntxq.squery.b32         d, [a];       // sampler attributes\n\n.tquery  = { .width, .height, .depth,\n             .channel_data_type, .channel_order,\n             .normalized_coords, .array_size,\n             .num_mipmap_levels, .num_samples};\n\n.tlquery = { .width, .height, .depth };\n\n.squery  = { .force_unnormalized_coords, .filter_mode,\n             .addr_mode_0, addr_mode_1, addr_mode_2 };\nDescription\nQuery an attribute of a texture or sampler. Operand\na\nis either a\n.texref\nor\n.samplerref\nvariable, or a\n.u64\nregister.\nQuery\nReturns\n.width\n.height\n.depth\nvalue in elements\n.channel_data_type\nUnsigned integer corresponding to source language’s channel data type\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both\nchannel_data_type\nand channel_order queries.\n.channel_order\nUnsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.normalized_coords\n1\n(\nTrue\n) or\n0\n(\nFalse\n).\n.force_unnormalized_coords\n1\n(\nTrue)\nor\n0\n(\nFalse).\nDefined only for\n.samplerref\nvariables in independent texture mode. Overrides the\nnormalized_coords\nfield of a\n.texref\nvariable used with a\n.samplerref\nin a\ntex\ninstruction.\n.filter_mode\nInteger from\nenum\n{\nnearest,\nlinear\n}\n.addr_mode_0\n.addr_mode_1\n.addr_mode_2\nInteger from\nenum\n{\nwrap,\nmirror,\nclamp_ogl,\nclamp_to_edge,\nclamp_to_border\n}\n.array_size\nFor a texture array, number of textures in array, 0 otherwise.\n.num_mipmap_levels\nFor a mipmapped texture, number of levels of details (LOD), 0 otherwise.\n.num_samples\nFor a multi-sample texture, number of samples, 0 otherwise.\nTexture attributes are queried by supplying a\n.texref\nargument to\ntxq\n. In unified mode,\nsampler attributes are also accessed via a\n.texref\nargument, and in independent mode sampler\nattributes are accessed via a separate\n.samplerref\nargument.\ntxq.level\ntxq.level\nrequires an additional 32bit integer argument,\nlod\n, which specifies LOD and\nqueries requested attribute for the specified LOD.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.5.\nChannel data type and channel order queries were added in PTX ISA version 2.1.\nThe\n.force_unnormalized_coords\nquery was added in PTX ISA version 2.2.\nIndirect texture access introduced in PTX ISA version 3.1.\n.array_size\n,\n.num_mipmap_levels\n,\n.num_samples\nsamples queries were added in PTX ISA\nversion 4.1.\ntxq.level\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nSupported on all target architectures.\nIndirect texture access requires\nsm_20\nor higher.\nQuerying the number of mipmap levels requires\nsm_20\nor higher.\nQuerying the number of samples requires\nsm_30\nor higher.\ntxq.level\nrequires\nsm_30\nor higher.\nExamples\ntxq.width.b32       %r1, [tex_A];\ntxq.filter_mode.b32 %r1, [tex_A];   // unified mode\ntxq.addr_mode_0.b32 %r1, [smpl_B];  // independent mode\ntxq.level.width.b32 %r1, [tex_A], %r_lod;\n9.7.10.6.\nTexture Instructions:\nistypep\n\nistypep\nQuery whether a register points to an opaque variable of a specified type.\nSyntax\nistypep.type   p, a;  // result is .pred\n\n.type = { .texref, .samplerref, .surfref };\nDescription\nWrite predicate register\np\nwith 1 if register\na\npoints to an opaque variable of the\nspecified type, and with 0 otherwise. Destination\np\nhas type\n.pred\n; the source address\noperand must be of type\n.u64\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.0.\nTarget ISA Notes\nistypep requires\nsm_30\nor higher.\nExamples\nistypep.texref istex, tptr;\nistypep.samplerref issampler, sptr;\nistypep.surfref issurface, surfptr;\n9.7.11.\nSurface Instructions\n\nThis section describes PTX instructions for accessing surfaces. PTX supports the following\noperations on surface descriptors:\nStatic initialization of surface descriptors.\nModule-scope and per-entry scope definitions of surface descriptors.\nAbility to query fields within surface descriptors.\nThese instructions provide access to surface memory.\nsuld\nsust\nsured\nsuq\n9.7.11.1.\nSurface Instructions:\nsuld\n\nsuld\nLoad from surface memory.\nSyntax\nsuld.b.geom{.cop}.vec.dtype.clamp  d, [a, b];  // unformatted\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d };\n.cop   = { .ca, .cg, .cs, .cv };               // cache operation\n.vec   = { none, .v2, .v4 };\n.dtype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero };\nDescription\nsuld.b.{1d,2d,3d}\nLoad from surface memory using a surface coordinate vector. The instruction loads data from the\nsurface named by operand\na\nat coordinates given by operand\nb\ninto destination\nd\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a scalar or singleton tuple\nfor 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d\nsurfaces, where the fourth element is ignored. Coordinate elements are of type\n.s32\n.\nsuld.b\nperforms an unformatted load of binary data. The lowest dimension coordinate represents a\nbyte offset into the surface and is not scaled, and the size of the data transfer matches the size\nof destination operand\nd\n.\nsuld.b.{a1d,a2d}\nSurface layer selection, followed by a load from the selected surface. The instruction first selects\na surface layer from the surface array named by operand\na\nusing the index given by the first\nelement of the array coordinate vector\nb\n. The instruction then loads data from the selected\nsurface at coordinates given by the remaining elements of operand\nb\ninto destination\nd\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a bit-size\ntype vector or tuple containing an index into the array of surfaces followed by coordinates within\nthe selected surface, as follows:\nFor 1d surface arrays, operand\nb\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the second element is interpreted as a\n1d surface coordinate of type\n.s32\n.\nFor 2d surface arrays, operand\nb\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the next two elements are interpreted\nas 2d surface coordinates of type\n.s32\n. The fourth element is ignored.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nloads data at the nearest surface location (sized appropriately)\n.zero\nloads zero for out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nsuld.b.trap\nintroduced in PTX ISA version 1.5.\nAdditional clamp modifiers and cache operations introduced in PTX ISA version 2.0.\nsuld.b.3d\nand\nsuld.b.{a1d,a2d}\nintroduced in PTX ISA version 3.0.\nIndirect surface access introduced in PTX ISA version 3.1.\nTarget ISA Notes\nsuld.b\nsupported on all target architectures.\nsm_1x\ntargets support only the\n.trap\nclamping modifier.\nsuld.3d\nand\nsuld.{a1d,a2d}\nrequire\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nExamples\nsuld.b.1d.v4.b32.trap  {s1,s2,s3,s4}, [surf_B, {x}];\nsuld.b.3d.v2.b64.trap  {r1,r2}, [surf_A, {x,y,z,w}];\nsuld.b.a1d.v2.b32      {r0,r1}, [surf_C, {idx,x}];\nsuld.b.a2d.b32         r0, [surf_D, {idx,x,y,z}];  // z ignored\n9.7.11.2.\nSurface Instructions:\nsust\n\nsust\nStore to surface memory.\nSyntax\nsust.b.{1d,2d,3d}{.cop}.vec.ctype.clamp  [a, b], c;  // unformatted\nsust.p.{1d,2d,3d}.vec.b32.clamp          [a, b], c;  // formatted\n\nsust.b.{a1d,a2d}{.cop}.vec.ctype.clamp   [a, b], c;  // unformatted\n\n.cop   = { .wb, .cg, .cs, .wt };                     // cache operation\n.vec   = { none, .v2, .v4 };\n.ctype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero };\nDescription\nsust.{1d,2d,3d}\nStore to surface memory using a surface coordinate vector. The instruction stores data from operand\nc\nto the surface named by operand\na\nat coordinates given by operand\nb\n. Operand\na\nis\na\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a scalar or singleton tuple for 1d\nsurfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces,\nwhere the fourth element is ignored. Coordinate elements are of type\n.s32\n.\nsust.b\nperforms an unformatted store of binary data. The lowest dimension coordinate represents\na byte offset into the surface and is not scaled. The size of the data transfer matches the size of\nsource operand\nc\n.\nsust.p\nperforms a formatted store of a vector of 32-bit data values to a surface sample. The\nsource vector elements are interpreted left-to-right as\nR\n,\nG\n,\nB\n, and\nA\nsurface\ncomponents. These elements are written to the corresponding surface sample components. Source\nelements that do not occur in the surface sample are ignored. Surface sample components that do not\noccur in the source vector will be written with an unpredictable value. The lowest dimension\ncoordinate represents a sample offset rather than a byte offset.\nThe source data interpretation is based on the surface sample format as follows: If the surface\nformat contains\nUNORM\n,\nSNORM\n, or\nFLOAT\ndata, then\n.f32\nis assumed; if the surface\nformat contains\nUINT\ndata, then\n.u32\nis assumed; if the surface format contains\nSINT\ndata, then\n.s32\nis assumed. The source data is then converted from this type to the surface\nsample format.\nsust.b.{a1d,a2d}\nSurface layer selection, followed by an unformatted store to the selected surface. The instruction\nfirst selects a surface layer from the surface array named by operand\na\nusing the index given by\nthe first element of the array coordinate vector\nb\n. The instruction then stores the data in\noperand\nc\nto the selected surface at coordinates given by the remaining elements of operand\nb\n. Operand\na\nis a .surfref variable or\n.u64\nregister. Operand\nb\nis a bit-size type\nvector or tuple containing an index into the array of surfaces followed by coordinates within the\nselected surface, as follows:\nFor 1d surface arrays, operand\nb\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the second element is interpreted as\na 1d surface coordinate of type\n.s32\n.\nFor 2d surface arrays, operand\nb\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the next two elements are\ninterpreted as 2d surface coordinates of type\n.s32\n. The fourth element is ignored.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nstores data at the nearest surface location (sized appropriately)\n.zero\ndrops stores to out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nsust.b.trap\nintroduced in PTX ISA version 1.5.\nsust.p\n, additional clamp modifiers, and\ncache operations introduced in PTX ISA version 2.0.\nsust.b.3d\nand\nsust.b.{a1d,a2d}\nintroduced in PTX ISA version 3.0.\nIndirect surface access introduced in PTX ISA version 3.1.\nTarget ISA Notes\nsust.b\nsupported on all target architectures.\nsm_1x\ntargets support only the\n.trap\nclamping modifier.\nsust.3d\nand\nsust.{a1d,a2d}\nrequire\nsm_20\nor higher.\nsust.p\nrequires\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nExamples\nsust.p.1d.v4.b32.trap  [surf_B, {x}], {f1,f2,f3,f4};\nsust.b.3d.v2.b64.trap  [surf_A, {x,y,z,w}], {r1,r2};\nsust.b.a1d.v2.b64      [surf_C, {idx,x}], {r1,r2};\nsust.b.a2d.b32         [surf_D, {idx,x,y,z}], r0;  // z ignored\n9.7.11.3.\nSurface Instructions:\nsured\n\nsured\nReduce surface memory.\nSyntax\nsured.b.op.geom.ctype.clamp  [a,b],c; // byte addressing\nsured.p.op.geom.ctype.clamp  [a,b],c; // sample addressing\n\n.op    = { .add, .min, .max, .and, .or };\n.geom  = { .1d, .2d, .3d };\n.ctype = { .u32, .u64, .s32, .b32, .s64 };  // for sured.b\n.ctype = { .b32, .b64 };                    // for sured.p\n.clamp = { .trap, .clamp, .zero };\nDescription\nReduction to surface memory using a surface coordinate vector. The instruction performs a reduction\noperation with data from operand\nc\nto the surface named by operand\na\nat coordinates given by\noperand\nb\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a\nscalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a\nfour-element vector for 3d surfaces, where the fourth element is ignored. Coordinate elements are of\ntype\n.s32\n.\nsured.b\nperforms an unformatted reduction on\n.u32\n,\n.s32\n,\n.b32\n,\n.u64\n, or\n.s64\ndata. The lowest dimension coordinate represents a byte offset into the surface and is not\nscaled. Operation\nadd\napplies to\n.u32\n,\n.u64\n, and\n.s32\ntypes;\nmin\nand\nmax\napply to\n.u32\n,\n.s32\n,\n.u64\nand\n.s64\ntypes; operations\nand\nand\nor\napply to\n.b32\ntype.\nsured.p\nperforms an unformatted reduction on sample-addressed data. The lowest dimension coordinate\nrepresents a sample offset rather than a byte offset and is scaled by the data-size given by\n.ctype\n.\nThe instruction type\n.b64\nis restricted to\nmin\nand\nmax\noperations. For type\n.b32\n,\nthe data is interpreted as\n.u32\nor\n.s32\nbased on the surface sample format as follows:\nif the surface format contains\nUINT\ndata, then\n.u32\nis assumed; if the surface format contains\nSINT\ndata, then\n.s32\nis assumed. For type\n.b64\n, if the surface format contains\nUINT\ndata,\nthen\n.u64\nis assumed; if the surface format contains\nSINT\ndata, then\n.s64\nis assumed.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nstores data at the nearest surface location (sized appropriately)\n.zero\ndrops stores to out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nIndirect surface access introduced in PTX ISA version 3.1.\n.u64\n/\n.s64\n/\n.b64\ntypes with\n.min\n/\n.max\noperations introduced in PTX ISA version\n8.1.\nTarget ISA Notes\nsured requires\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\n.u64\n/\n.s64\n/\n.b64\ntypes with\n.min\n/\n.max\noperations requires\nsm_50\nor higher.\nExamples\nsured.b.add.2d.u32.trap  [surf_A, {x,y}], r1;\nsured.p.min.1d.u32.trap  [surf_B, {x}], r1;\nsured.b.max.1d.u64.trap  [surf_C, {x}], r1;\nsured.p.min.1d.b64.trap  [surf_D, {x}], r1;\n9.7.11.4.\nSurface Instructions:\nsuq\n\nsuq\nQuery a surface attribute.\nSyntax\nsuq.query.b32   d, [a];\n\n.query = { .width, .height, .depth,\n           .channel_data_type, .channel_order,\n           .array_size, .memory_layout };\nDescription\nQuery an attribute of a surface. Operand\na\nis a\n.surfref\nvariable or a\n.u64\nregister.\nQuery\nReturns\n.width\n.height\n.depth\nvalue in elements\n.channel_data_type\nUnsigned integer corresponding to source language’s channel data\ntype enumeration. If the source language combines channel data\ntype and channel order into a single enumeration type, that value\nis returned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.channel_order\nUnsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and\nchannel order into a single enumeration type, that value is\nreturned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.array_size\nFor a surface array, number of surfaces in array, 0 otherwise.\n.memory_layout\n1\nfor surface with linear memory layout;\n0\notherwise\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.5.\nChannel data type and channel order queries added in PTX ISA version 2.1.\nIndirect surface access introduced in PTX ISA version 3.1.\nThe\n.array_size\nquery was added in PTX ISA version 4.1.\nThe\n.memory_layout\nquery was added in PTX ISA version 4.2.\nTarget ISA Notes\nSupported on all target architectures.\nIndirect surface access requires\nsm_20\nor higher.\nExamples\nsuq.width.b32       %r1, [surf_A];\n9.7.12.\nControl Flow Instructions\n\nThe following PTX instructions and syntax are for controlling execution in a PTX program:\n{}\n@\nbra\ncall\nret\nexit\n9.7.12.1.\nControl Flow Instructions:\n{}\n\n{}\nInstruction grouping.\nSyntax\n{ instructionList }\nDescription\nThe curly braces create a group of instructions, used primarily for defining a function body. The\ncurly braces also provide a mechanism for determining the scope of a variable: any variable declared\nwithin a scope is not available outside the scope.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n{ add.s32  a,b,c; mov.s32  d,a; }\n9.7.12.2.\nControl Flow Instructions:\n@\n\n@\nPredicated execution.\nSyntax\n@{!}p    instruction;\nDescription\nExecute an instruction or instruction block for threads that have the guard predicate\nTrue\n. Threads with a\nFalse\nguard predicate do nothing.\nSemantics\nIf\n{!}p\nthen instruction\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsetp.eq.f32  p,y,0;     // is y zero?\n@!p div.f32      ratio,x,y  // avoid division by zero\n\n@q  bra L23;                // conditional branch\n9.7.12.3.\nControl Flow Instructions:\nbra\n\nbra\nBranch to a target and continue execution there.\nSyntax\n@p   bra{.uni}  tgt;           // tgt is a label\n     bra{.uni}  tgt;           // unconditional branch\nDescription\nContinue execution at the target. Conditional branches are specified by using a guard predicate. The\nbranch target must be a label.\nbra.uni\nis guaranteed to be non-divergent, i.e. all active threads in a warp that are currently\nexecuting this instruction have identical values for the guard predicate and branch target.\nSemantics\nif (p) {\n    pc = tgt;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nUnimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nbra.uni  L_exit;    // uniform unconditional jump\n@q  bra      L23;   // conditional branch\n9.7.12.4.\nControl Flow Instructions:\nbrx.idx\n\nbrx.idx\nBranch to a label indexed from a list of potential branch targets.\nSyntax\n@p    brx.idx{.uni} index, tlist;\n      brx.idx{.uni} index, tlist;\nDescription\nIndex into a list of possible destination labels, and continue execution from the chosen\nlabel. Conditional branches are specified by using a guard predicate.\nbrx.idx.uni\nguarantees that the branch is non-divergent, i.e. all active threads in a warp that\nare currently executing this instruction have identical values for the guard predicate and the\nindex\nargument.\nThe\nindex\noperand is a\n.u32\nregister. The\ntlist\noperand must be the label of a\n.branchtargets\ndirective. It is accessed as a zero-based sequence using\nindex\n. Behaviour is\nundefined if the value of\nindex\nis greater than or equal to the length of\ntlist\n.\nThe\n.branchtargets\ndirective must be defined in the local function scope before it is used. It\nmust refer to labels within the current function.\nSemantics\nif (p) {\n    if (index < length(tlist)) {\n      pc = tlist[index];\n    } else {\n      pc = undefined;\n    }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\n.function foo () {\n    .reg .u32 %r0;\n    ...\n    L1:\n    ...\n    L2:\n    ...\n    L3:\n    ...\n    ts: .branchtargets L1, L2, L3;\n    @p brx.idx %r0, ts;\n    ...\n}\n9.7.12.5.\nControl Flow Instructions:\ncall\n\ncall\nCall a function, recording the return location.\nSyntax\n// direct call to named function, func is a symbol\ncall{.uni} (ret-param), func, (param-list);\ncall{.uni} func, (param-list);\ncall{.uni} func;\n\n// indirect call via pointer, with full list of call targets\ncall{.uni} (ret-param), fptr, (param-list), flist;\ncall{.uni} fptr, (param-list), flist;\ncall{.uni} fptr, flist;\n\n// indirect call via pointer, with no knowledge of call targets\ncall{.uni} (ret-param), fptr, (param-list), fproto;\ncall{.uni} fptr, (param-list), fproto;\ncall{.uni} fptr, fproto;\nDescription\nThe\ncall\ninstruction stores the address of the next instruction, so execution can resume at that\npoint after executing a\nret\ninstruction. A\ncall\nis assumed to be divergent unless the\n.uni\nsuffix is present. The\n.uni\nsuffix indicates that the\ncall\nis guaranteed to be\nnon-divergent, i.e. all active threads in a warp that are currently executing this instruction have\nidentical values for the guard predicate and\ncall\ntarget.\nFor direct calls, the called location\nfunc\nmust be a symbolic function name; for indirect calls,\nthe called location\nfptr\nmust be an address of a function held in a register. Input arguments\nand return values are optional. Arguments may be registers, immediate constants, or variables in\n.param\nspace. Arguments are pass-by-value.\nIndirect calls require an additional operand,\nflist\nor\nfproto\n, to communicate the list of\npotential\ncall\ntargets or the common function prototype of all\ncall\ntargets,\nrespectively. In the first case,\nflist\ngives a complete list of potential\ncall\ntargets and\nthe optimizing backend is free to optimize the calling convention. In the second case, where the\ncomplete list of potential\ncall\ntargets may not be known, the common function prototype is given\nand the\ncall\nmust obey the ABI’s calling convention.\nThe\nflist\noperand is either the name of an array (call table) initialized to a list of function\nnames; or a label associated with a\n.calltargets\ndirective, which declares a list of potential\ncall\ntargets. In both cases the fptr register holds the address of a function listed in the call\ntable or\n.calltargets\nlist, and the\ncall\noperands are type-checked against the type\nsignature of the functions indicated by\nflist\n.\nThe fproto operand is the name of a label associated with a\n.callprototype\ndirective. This\noperand is used when a complete list of potential targets is not known. The\ncall\noperands are\ntype-checked against the prototype, and code generation will follow the ABI calling convention. If a\nfunction that doesn’t match the prototype is called, the behavior is undefined.\nCall tables may be declared at module scope or local scope, in either the constant or global state\nspace. The\n.calltargets\nand\n.callprototype\ndirectives must be declared within a function\nbody. All functions must be declared prior to being referenced in a\ncall\ntable initializer or\n.calltargets\ndirective.\nPTX ISA Notes\nDirect\ncall\nintroduced in PTX ISA version 1.0. Indirect\ncall\nintroduced in PTX ISA version 2.1.\nTarget ISA Notes\nDirect\ncall\nsupported on all target architectures. Indirect\ncall\nrequires\nsm_20\nor higher.\nExamples\n// examples of direct call\n    call     init;    // call function 'init'\n    call.uni g, (a);  // call function 'g' with parameter 'a'\n@p  call     (d), h, (a, b);  // return value into register d\n\n// call-via-pointer using jump table\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n\n.global .u32 jmptbl[5] = { foo, bar, baz };\n      ...\n@p    ld.global.u32  %r0, [jmptbl+4];\n@p    ld.global.u32  %r0, [jmptbl+8];\n      call  (retval), %r0, (x, y), jmptbl;\n\n// call-via-pointer using .calltargets directive\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n      ...\n@p    mov.u32  %r0, foo;\n@q    mov.u32  %r0, baz;\nFtgt: .calltargets foo, bar, baz;\n      call  (retval), %r0, (x, y), Ftgt;\n\n// call-via-pointer using .callprototype directive\n.func dispatch (.reg .u32 fptr, .reg .u32 idx)\n{\n...\nFproto: .callprototype _ (.param .u32 _, .param .u32 _);\n      call  %fptr, (x, y), Fproto;\n...\n9.7.12.6.\nControl Flow Instructions:\nret\n\nret\nReturn from function to instruction after call.\nSyntax\nret{.uni};\nDescription\nReturn execution to caller’s environment. A divergent return suspends threads until all threads are\nready to return to the caller. This allows multiple divergent\nret\ninstructions.\nA\nret\nis assumed to be divergent unless the\n.uni\nsuffix is present, indicating that the\nreturn is guaranteed to be non-divergent.\nAny values returned from a function should be moved into the return parameter variables prior to\nexecuting the\nret\ninstruction.\nA return instruction executed in a top-level entry routine will terminate thread execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nret;\n@p  ret;\n9.7.12.7.\nControl Flow Instructions:\nexit\n\nexit\nTerminate a thread.\nSyntax\nexit;\nDescription\nEnds execution of a thread.\nBarriers exclusively waiting on arrivals from exited threads are always released.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nexit;\n@p  exit;\n9.7.13.\nParallel Synchronization and Communication Instructions\n\nThese instructions are:\nbar{.cta}\n,\nbarrier{.cta}\nbar.warp.sync\nbarrier.cluster\nmembar\natom\nred\nred.async\nvote\nmatch.sync\nactivemask\nredux.sync\ngriddepcontrol\nelect.sync\nmbarrier.init\nmbarrier.inval\nmbarrier.arrive\nmbarrier.arrive_drop\nmbarrier.test_wait\nmbarrier.try_wait\nmbarrier.pending_count\ncp.async.mbarrier.arrive\ntensormap.cp_fenceproxy\nclusterlaunchcontrol.try_cancel\nclusterlaunchcontrol.query_cancel\n9.7.13.1.\nParallel Synchronization and Communication Instructions:\nbar\n,\nbarrier\n\nbar\n,\nbar.cta\n,\nbarrier\n,\nbarrier.cta\nBarrier synchronization.\nSyntax\nbarrier{.cta}.sync{.aligned}      a{, b};\nbarrier{.cta}.arrive{.aligned}    a, b;\n\nbarrier{.cta}.red.popc{.aligned}.u32  d, a{, b}, {!}c;\nbarrier{.cta}.red.op{.aligned}.pred   p, a{, b}, {!}c;\n\nbar{.cta}.sync      a{, b};\nbar{.cta}.arrive    a, b;\n\nbar{.cta}.red.popc.u32  d, a{, b}, {!}c;\nbar{.cta}.red.op.pred   p, a{, b}, {!}c;\n\n.op = { .and, .or };\nDescription\nPerforms barrier synchronization and communication within a CTA. Each CTA instance has sixteen\nbarriers numbered\n0..15\n.\nbarrier{.cta}\ninstructions can be used by the threads within the CTA for synchronization and\ncommunication.\nOperands\na\n,\nb\n, and\nd\nhave type\n.u32\n; operands\np\nand\nc\nare predicates. Source\noperand\na\nspecifies a logical barrier resource as an immediate constant or register with value\n0\nthrough\n15\n. Operand\nb\nspecifies the number of threads participating in the barrier. If\nno thread count is specified, all threads in the CTA participate in the barrier. When specifying a\nthread count, the value must be a multiple of the warp size. Note that a non-zero thread count is\nrequired for\nbarrier{.cta}.arrive\n.\nDepending on operand\nb\n, either specified number of threads (in multiple of warp size) or all\nthreads in the CTA participate in\nbarrier{.cta}\ninstruction. The\nbarrier{.cta}\ninstructions\nsignal the arrival of the executing threads at the named barrier.\nbarrier{.cta}\ninstruction causes executing thread to wait for all non-exited threads from its\nwarp and marks warps’ arrival at barrier. In addition to signaling its arrival at the barrier, the\nbarrier{.cta}.red\nand\nbarrier{.cta}.sync\ninstructions causes executing thread to wait for\nnon-exited threads of all other warps participating in the barrier to\narrive.\nbarrier{.cta}.arrive\ndoes not cause executing thread to wait for threads of other\nparticipating warps.\nWhen a barrier completes, the waiting threads are restarted without delay, and the barrier is\nreinitialized so that it can be immediately reused.\nThe\nbarrier{.cta}.sync\nor\nbarrier{.cta}.red\nor\nbarrier{.cta}.arrive\ninstruction\nguarantees that when the barrier completes, prior memory accesses requested by this thread are\nperformed relative to all threads participating in the barrier. The\nbarrier{.cta}.sync\nand\nbarrier{.cta}.red\ninstruction further guarantees that no new memory access is requested by this\nthread before the barrier completes.\nA memory read (e.g., by\nld\nor\natom\n) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread participating in the barrier. A\nmemory write (e.g., by\nst\n,\nred\nor\natom\n) has been performed when the value written has\nbecome visible to other threads participating in the barrier, that is, when the previous value can\nno longer be read.\nbarrier{.cta}.red\nperforms a reduction operation across threads. The\nc\npredicate (or its\ncomplement) from all threads in the CTA are combined using the specified reduction operator. Once\nthe barrier count is reached, the final value is written to the destination register in all threads\nwaiting at the barrier.\nThe reduction operations for\nbarrier{.cta}.red\nare population-count (\n.popc\n),\nall-threads-True (\n.and\n), and any-thread-True (\n.or\n). The result of\n.popc\nis the number of\nthreads with a\nTrue\npredicate, while\n.and\nand\n.or\nindicate if all the threads had a\nTrue\npredicate or if any of the threads had a\nTrue\npredicate.\nInstruction\nbarrier{.cta}\nhas optional\n.aligned\nmodifier. When specified, it indicates that\nall threads in CTA will execute the same\nbarrier{.cta}\ninstruction. In conditionally executed\ncode, an aligned\nbarrier{.cta}\ninstruction should only be used if it is known that all threads\nin CTA evaluate the condition identically, otherwise behavior is undefined.\nDifferent warps may execute different forms of the\nbarrier{.cta}\ninstruction using the same\nbarrier name and thread count. One example mixes\nbarrier{.cta}.sync\nand\nbarrier{.cta}.arrive\nto implement producer/consumer models. The producer threads execute\nbarrier{.cta}.arrive\nto\nannounce their arrival at the barrier and continue execution without delay to produce the next\nvalue, while the consumer threads execute the\nbarrier{.cta}.sync\nto wait for a resource to be\nproduced. The roles are then reversed, using a different barrier, where the producer threads execute\na\nbarrier{.cta}.sync\nto wait for a resource to consumed, while the consumer threads announce\nthat the resource has been consumed with\nbarrier{.cta}.arrive\n. Care must be taken to keep a warp\nfrom executing more\nbarrier{.cta}\ninstructions than intended (\nbarrier{.cta}.arrive\nfollowed\nby any other\nbarrier{.cta}\ninstruction to the same barrier) prior to the reset of the\nbarrier.\nbarrier{.cta}.red\nshould not be intermixed with\nbarrier{.cta}.sync\nor\nbarrier{.cta}.arrive\nusing the same active barrier. Execution in this case is unpredictable.\nThe optional\n.cta\nqualifier simply indicates CTA-level applicability of the barrier and it\ndoesn’t change the semantics of the instruction.\nbar{.cta}.sync\nis equivalent to\nbarrier{.cta}.sync.aligned\n.\nbar{.cta}.arrive\nis\nequivalent to\nbarrier{.cta}.arrive.aligned\n.\nbar{.cta}.red\nis equivalent to\nbarrier{.cta}.red.aligned\n.\nNote\nFor .target\nsm_6x\nor below,\nbarrier{.cta}\ninstruction without\n.aligned\nmodifier is equivalent to\n.aligned\nvariant and has the same restrictions as of\n.aligned\nvariant.\nAll threads in warp (except for those have exited) must execute\nbarrier{.cta}\ninstruction\nin convergence.\nPTX ISA Notes\nbar.sync\nwithout a thread count introduced in PTX ISA version 1.0.\nRegister operands, thread count, and\nbar.{arrive,red}\nintroduced in PTX ISA version 2.0.\nbarrier\ninstruction introduced in PTX ISA version 6.0.\n.cta\nqualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nRegister operands, thread count, and\nbar{.cta}.{arrive,red}\nrequire\nsm_20\nor higher.\nOnly\nbar{.cta}.sync\nwith an immediate barrier number is supported for\nsm_1x\ntargets.\nbarrier{.cta}\ninstruction requires\nsm_30\nor higher.\nExamples\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for all threads in CTA to also arrive:\n    st.shared [r0],r1;  // write my result to shared memory\n    bar.cta.sync  1;    // arrive, wait for others to arrive\n    ld.shared r2,[r3];  // use shared results from other threads\n\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for fixed number of cooperating threads to arrive:\n    #define CNT1 (8*12) // Number of cooperating threads\n\n    st.shared [r0],r1;     // write my result to shared memory\n    bar.cta.sync  1, CNT1; // arrive, wait for others to arrive\n    ld.shared r2,[r3];     // use shared results from other threads\n\n// Use bar.red.and to compare results across the entire CTA:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.and.pred r3,1,p; // r3=AND(p) forall threads in CTA\n\n// Use bar.red.popc to compute the size of a group of threads\n// that have a specific condition True:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.popc.u32 r3,1,p; // r3=SUM(p) forall threads in CTA\n\n// Examples of barrier.cta.sync\n    st.shared         [r0],r1;\n    barrier.cta.sync  0;\n    ld.shared         r1, [r0];\n\n/* Producer/consumer model. The producer deposits a value in\n * shared memory, signals that it is complete but does not wait\n * using bar.arrive, and begins fetching more data from memory.\n * Once the data returns from memory, the producer must wait\n * until the consumer signals that it has read the value from\n * the shared memory location. In the meantime, a consumer\n * thread waits until the data is stored by the producer, reads\n * it, and then signals that it is done (without waiting).\n */\n    // Producer code places produced value in shared memory.\n    st.shared   [r0],r1;\n    bar.arrive  0,64;\n    ld.global   r1,[r2];\n    bar.sync    1,64;\n    ...\n\n    // Consumer code, reads value from shared memory\n    bar.sync   0,64;\n    ld.shared  r1,[r0];\n    bar.arrive 1,64;\n    ...\n9.7.13.2.\nParallel Synchronization and Communication Instructions:\nbar.warp.sync\n\nbar.warp.sync\nBarrier synchronization for threads in a warp.\nSyntax\nbar.warp.sync      membermask;\nDescription\nbar.warp.sync\nwill cause executing thread to wait until all threads corresponding to\nmembermask\nhave executed a\nbar.warp.sync\nwith the same\nmembermask\nvalue before resuming\nexecution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s\nlaneid\n.\nThe behavior of\nbar.warp.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nbar.warp.sync\nalso guarantee memory ordering among threads participating in barrier. Thus,\nthreads within warp that wish to communicate via memory can store to memory, execute\nbar.warp.sync\n, and then safely read values stored by other threads in warp.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nbar.warp.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when the\nbar.warp.sync\ninstruction is executed. Otherwise, the behavior is\nundefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nst.shared.u32 [r0],r1;         // write my result to shared memory\nbar.warp.sync  0xffffffff;     // arrive, wait for others to arrive\nld.shared.u32 r2,[r3];         // read results written by other threads\n9.7.13.3.\nParallel Synchronization and Communication Instructions:\nbarrier.cluster\n\nbarrier.cluster\nBarrier synchronization within a cluster.\nSyntax\nbarrier.cluster.arrive{.sem}{.aligned};\nbarrier.cluster.wait{.acquire}{.aligned};\n\n.sem = {.release, .relaxed}\nDescription\nPerforms barrier synchronization and communication within a cluster.\nbarrier.cluster\ninstructions can be used by the threads within the cluster for synchronization\nand communication.\nbarrier.cluster.arrive\ninstruction marks warps’ arrival at barrier without causing executing\nthread to wait for threads of other participating warps.\nbarrier.cluster.wait\ninstruction causes the executing thread to wait for all non-exited threads\nof the cluster to perform\nbarrier.cluster.arrive\n.\nIn addition,\nbarrier.cluster\ninstructions cause the executing thread to wait for all non-exited\nthreads from its warp.\nWhen all non-exited threads in the cluster have executed\nbarrier.cluster.arrive\n, the barrier\ncompletes and is automatically reinitialized. After using\nbarrier.cluster.wait\nto detect completion\nof the barrier, a thread may immediately arrive at the barrier once again.\nEach thread must arrive at the barrier only once before the barrier completes.\nThe\nbarrier.cluster.wait\ninstruction guarantees that when it completes the execution, memory\naccesses (except asynchronous operations) requested, in program order, prior to the preceding\nbarrier.cluster.arrive\nby all threads in the cluster are complete and visible to the executing\nthread.\nThere is no memory ordering and visibility guarantee for memory accesses requested by the executing\nthread, in program order, after\nbarrier.cluster.arrive\nand prior to\nbarrier.cluster.wait\n.\nThe optional\n.relaxed\nqualifier on\nbarrier.cluster.arrive\nspecifies that there are no memory\nordering and visibility guarantees provided for the memory accesses performed prior to\nbarrier.cluster.arrive\n.\nThe optional\n.sem\nand\n.acquire\nqualifiers on instructions\nbarrier.cluster.arrive\nand\nbarrier.cluster.wait\nspecify the memory synchronization as described in the\nMemory Consistency Model\n. If the optional\n.sem\nqualifier is absent for\nbarrier.cluster.arrive\n,\n.release\nis assumed by default. If the optional\n.acquire\nqualifier is absent for\nbarrier.cluster.wait\n,\n.acquire\nis assumed by default.\nThe optional\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nbarrier.cluster\ninstruction. In conditionally executed code, an aligned\nbarrier.cluster\ninstruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nSupport for\n.acquire\n,\n.relaxed\n,\n.release\nqualifiers introduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n// use of arrive followed by wait\nld.shared::cluster.u32 r0, [addr];\nbarrier.cluster.arrive.aligned;\n...\nbarrier.cluster.wait.aligned;\nst.shared::cluster.u32 [addr], r1;\n\n// use memory fence prior to arrive for relaxed barrier\n@cta0 ld.shared::cluster.u32 r0, [addr];\nfence.cluster.acq_rel;\nbarrier.cluster.arrive.relaxed.aligned;\n...\nbarrier.cluster.wait.aligned;\n@cta1 st.shared::cluster.u32 [addr], r1;\n9.7.13.4.\nParallel Synchronization and Communication Instructions:\nmembar\n/\nfence\n\nmembar\n,\nfence\nEnforce an ordering of memory operations.\nSyntax\n// Thread fence:\nfence{.sem}.scope;\n\n// Thread fence:\nfence.acquire.sync_restrict::shared::cluster.cluster;\nfence.release.sync_restrict::shared::cta.cluster;\n\n// Operation fence:\nfence.op_restrict.release.cluster;\n\n// Proxy fence (bi-directional):\nfence.proxy.proxykind;\n\n// Proxy fence (uni-directional):\nfence.proxy.to_proxykind::from_proxykind.release.scope;\nfence.proxy.to_proxykind::from_proxykind.acquire.scope  [addr], size;\nfence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\nfence.proxy.async::generic.release.sync_restrict::shared::cta.cluster;\n\n// Old style membar:\nmembar.level;\nmembar.proxy.proxykind;\n\n.sem       = { .sc, .acq_rel, .acquire, .release };\n.scope     = { .cta, .cluster, .gpu, .sys };\n.level     = { .cta, .gl, .sys };\n.proxykind = { .alias, .async, .async.global, .async.shared::{cta, cluster} };\n.op_restrict = { .mbarrier_init };\n.to_proxykind::from_proxykind = {.tensormap::generic};\nDescription\nThe\nmembar\ninstruction guarantees that prior memory accesses requested by this thread (\nld\n,\nst\n,\natom\nand\nred\ninstructions) are performed at the specified\nlevel\n, before later\nmemory operations requested by this thread following the\nmembar\ninstruction. The\nlevel\nqualifier specifies the set of threads that may observe the ordering effect of this operation.\nA memory read (e.g., by\nld\nor\natom\n) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread at the indicated level. A memory\nwrite (e.g., by\nst\n,\nred\nor\natom\n) has been performed when the value written has become\nvisible to other threads at the specified level, that is, when the previous value can no longer be\nread.\nThe\nfence\ninstruction establishes an ordering between memory accesses requested by this thread\n(\nld\n,\nst\n,\natom\nand\nred\ninstructions) as described in the\nMemory Consistency Model\n. The scope qualifier specifies the set of threads that may\nobserve the ordering effect of this operation.\nfence.acq_rel\nis a light-weight fence that is sufficient for memory synchronization in most\nprograms. Instances of\nfence.acq_rel\nsynchronize when combined with additional memory operations\nas described in\nacquire\nand\nrelease\npatterns in the\nMemory Consistency Model\n.\nIf the optional\n.sem\nqualifier is absent,\n.acq_rel\nis assumed by default.\nfence.sc\nis a slower fence that can restore\nsequential consistency\nwhen used in sufficient\nplaces, at the cost of performance. Instances of\nfence.sc\nwith sufficient scope always\nsynchronize by forming a total order per scope, determined at runtime. This total order can be\nconstrained further by other synchronization in the program.\nQualifiers\n.op_restrict\nand\n.sync_restrict\nrestrict the class of memory operations\nfor which the\nfence\ninstruction provides the memory ordering guarantees. When\n.op_restrict\nis\n.mbarrier_init\n, the synchronizing effect of the fence only applies to the prior\nmbarrier.init\noperations executed by the same thread on\nmbarrier objects\nin\n.shared::cta\nstate space. When\n.sync_restrict\nis\n.sync_restrict::shared::cta\n,\n.sem\nmust be\n.release\n, and the effect of the fence only applies to operations performed on objects in\n.shared::cta\nstate space. Likewise, when\n.sync_restrict\nis\n.sync_restrict::shared::cluster\n,\n.sem\nmust be\n.acquire\n, and the effect of the fence only applies to operations performed on\nobjects in\n.shared::cluster\nstate space. When either\n.sync_restrict::shared::cta\nor\n.sync_restrict::shared::cluster\nis present, the\n.scope\nmust be specified as\n.cluster\n.\nThe address operand\naddr\nand the operand\nsize\ntogether specify the memory range\n[addr,\naddr+size-1]\non which the ordering guarantees on the memory accesses across the proxies is to be\nprovided. The only supported value for the\nsize\noperand is 128, which must be a constant integer literal.\nGeneric Addressing\nis used unconditionally, and the address specified by\nthe operand\naddr\nmust fall within the\n.global\nstate space. Otherwise, the behavior is undefined.\nOn\nsm_70\nand higher\nmembar\nis a synonym for\nfence.sc\n1\n, and the\nmembar\nlevels\ncta\n,\ngl\nand\nsys\nare synonymous with the\nfence\nscopes\ncta\n,\ngpu\nand\nsys\nrespectively.\nmembar.proxy\nand\nfence.proxy\ninstructions establish an ordering between memory accesses that\nmay happen through different\nproxies\n.\nA\nuni-directional\nproxy ordering from the\nfrom-proxykind\nto the\nto-proxykind\nestablishes\nordering between a prior memory access performed via the\nfrom-proxykind\nand a subsequent memory access\nperformed via the\nto-proxykind\n.\nA\nbi-directional\nproxy ordering between two proxykinds establishes two\nuni-directional\nproxy orderings\n: one from the first proxykind to the second proxykind and the other from the second proxykind to the first\nproxykind.\nThe\n.proxykind\nqualifier indicates the\nbi-directional\nproxy ordering that is established between the memory\naccesses done between the generic proxy and the proxy specified by\n.proxykind\n.\nValue\n.alias\nof the\n.proxykind\nqualifier refers to memory accesses performed using virtually\naliased addresses to the same memory location. Value\n.async\nof the\n.proxykind\nqualifier specifies\nthat the memory ordering is established between the async proxy and the generic proxy. The memory\nordering is limited only to operations performed on objects in the state space specified. If no state space\nis specified, then the memory ordering applies on all state spaces.\nA\n.release\nproxy fence can form a release sequence that synchronizes with an acquire\nsequence that contains a\n.acquire\nproxy fence. The\n.to_proxykind\nand\n.from_proxykind\nqualifiers indicate the\nuni-directional\nproxy ordering that is established.\nOn\nsm_70\nand higher,\nmembar.proxy\nis a synonym for\nfence.proxy\n.\n1\nThe semantics of\nfence.sc\nintroduced with\nsm_70\nis a superset of the semantics of\nmembar\nand the two are compatible; when executing on\nsm_70\nor later architectures,\nmembar\nacquires the full semantics of\nfence.sc\n.\nPTX ISA Notes\nmembar.{cta,gl}\nintroduced in PTX ISA version 1.4.\nmembar.sys\nintroduced in PTX ISA version 2.0.\nfence\nintroduced in PTX ISA version 6.0.\nmembar.proxy\nand\nfence.proxy\nintroduced in PTX ISA version 7.5.\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\n.op_restrict\nqualifier introduced in PTX ISA version 8.0.\nfence.proxy.async\nis introduced in PTX ISA version 8.0.\n.to_proxykind::from_proxykind\nqualifier introduced in PTX ISA version 8.3.\n.acquire\nand\n.release\nqualifiers for\nfence\ninstruction introduced in PTX ISA version 8.6.\n.sync_restrict\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nmembar.{cta,gl}\nsupported on all target architectures.\nmembar.sys\nrequires\nsm_20\nor higher.\nfence\nrequires\nsm_70\nor higher.\nmembar.proxy\nrequires\nsm_60\nor higher.\nfence.proxy\nrequires\nsm_70\nor higher.\n.cluster\nscope qualifier requires\nsm_90\nor higher.\n.op_restrict\nqualifier requires\nsm_90\nor higher.\nfence.proxy.async\nrequires\nsm_90\nor higher.\n.to_proxykind::from_proxykind\nqualifier requires\nsm_90\nor higher.\n.acquire\nand\n.release\nqualifiers for\nfence\ninstruction require\nsm_90\nor higher..\n.sync_restrict\nqualifier requires\nsm_90\nor higher..\nExamples\nmembar.gl;\nmembar.cta;\nmembar.sys;\nfence.sc.cta;\nfence.sc.cluster;\nfence.proxy.alias;\nmembar.proxy.alias;\nfence.mbarrier_init.release.cluster;\nfence.proxy.async;\nfence.proxy.async.shared::cta;\nfence.proxy.async.shared::cluster;\nfence.proxy.async.global;\n\ntensormap.replace.tile.global_address.global.b1024.b64   [gbl], new_addr;\nfence.proxy.tensormap::generic.release.gpu;\ncvta.global.u64  tmap, gbl;\nfence.proxy.tensormap::generic.acquire.gpu [tmap], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [tmap, {tc0}], [mbar0];\n\n// Acquire remote barrier state via async proxy.\nbarrier.cluster.wait.acquire;\nfence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\n\n// Release local barrier state via generic proxy.\nmbarrier.init [bar];\nfence.mbarrier_init.release.cluster;\nbarrier.cluster.arrive.relaxed;\n\n// Acquire local shared memory via generic proxy.\nmbarrier.try_wait.relaxed.cluster.shared::cta.b64 complete, [addr], parity;\nfence.acquire.sync_restrict::shared::cluster.cluster;\n\n// Release local shared memory via generic proxy.\nfence.release.sync_restrict::shared::cta.cluster;\nmbarrier.arrive.relaxed.cluster.shared::cluster.b64 state, [bar];\n9.7.13.5.\nParallel Synchronization and Communication Instructions:\natom\n\natom\nAtomic reduction operations for thread-to-thread communication.\nSyntax\nAtomic operation with scalar type:\natom{.sem}{.scope}{.space}.op{.level::cache_hint}.type d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.op.type d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b16 d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b128 d, [a], b, c;\natom{.sem}{.scope}{.space}.exch{.level::cache_hint}.b128 d, [a], b {, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16     d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2   d, [a], b{, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16    d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2  d, [a], b{, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                { .relaxed, .acquire, .release, .acq_rel };\n.scope =              { .cta, .cluster, .gpu, .sys };\n\n.op =                 { .and, .or, .xor,\n                        .cas, .exch,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };\nAtomic operation with vector type:\natom{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32                  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_16_bit.half_word_type  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type     d, [a], b{, cache-policy};\n\n.sem =               { .relaxed, .acquire, .release, .acq_rel };\n.scope =             { .cta, .cluster, .gpu, .sys };\n.op =                { .add, .min, .max };\n.half_word_type =    { .f16, .bf16 };\n.packed_type =       { .f16x2, .bf16x2 };\n.vec_16_bit =        { .v2, .v4, .v8 }\n.vec_32_bit =        { .v2, .v4 };\n.level::cache_hint = { .L2::cache_hint }\nDescription\nAtomically loads the original value at location\na\ninto destination register\nd\n, performs a\nreduction operation with operand\nb\nand the value in location\na\n, and stores the result of the\nspecified operation at location\na\n, overwriting the original value. Operand\na\nspecifies a\nlocation in the specified state space. If no state space is given, perform the memory accesses using\nGeneric Addressing\n.\natom\nwith scalar type may be used only\nwith\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\natom\nwith vector type may be used only with\n.global\nspace\nand with generic addressing where the address points to\n.global\nspace.\nFor\natom\nwith vector type, operands\nd\nand\nb\nare brace-enclosed vector expressions, size\nof which is equal to the size of vector qualifier.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.relaxed\nis assumed by default.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is absent,\n.gpu\nscope is\nassumed by default.\nFor\natom\nwith vector type, the supported combinations of vector qualifier and types, and atomic\noperations supported on these combinations are depicted in the following table:\nVector qualifier\nTypes\n.f16\n/\nbf16\n.f16x2\n/\nbf16x2\n.f32\n.v2\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v4\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v8\n.add\n,\n.min\n,\n.max\nNot supported\nNot Supported\nTwo atomic operations (\natom\nor\nred\n) are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write.\natom\ninstruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch cases, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire\natom\nis not guaranteed to be atomic as a single access.\nFor\nsm_6x\nand earlier architectures,\natom\noperations on\n.shared\nstate space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory atomic\ninstructions, e.g., by inserting barriers between normal stores and atomic operations to a common\naddress, or by using atom.exch to store to locations accessed by other atomic operations.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe bit-size operations are\n.and\n,\n.or\n,\n.xor\n,\n.cas\n(compare-and-swap), and\n.exch\n(exchange).\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n,\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\nThe floating-point operation\n.add\noperation rounds to nearest even. Current implementation of\natom.add.f32\non global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas\natom.add.f32\non shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero.\natom.add.f16\n,\natom.add.f16x2\n,\natom.add.bf16\nand\natom.add.bf16x2\noperation requires\nthe\n.noftz\nqualifier; it preserves subnormal inputs and results, and does not flush them to\nzero.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\natomic {\n    d = *a;\n    *a = (operation == cas) ? operation(*a, b, c)\n                            : operation(*a, b);\n}\nwhere\n    inc(r, s)  = (r >= s) ? 0 : r+1;\n    dec(r, s)  = (r==0 || r > s)  ? s : r-1;\n    exch(r, s) =  s;\n    cas(r,s,t) = (r == s) ? t : r;\nNotes\nSimple reductions may be specified by using the\nbit bucket\ndestination operand\n_\n.\nPTX ISA Notes\n32-bit atom.global introduced in PTX ISA version 1.1.\natom.shared\nand 64-bit\natom.global.{add,cas,exch}\nintroduced in PTX ISA 1.2.\natom.add.f32\nand 64-bit\natom.shared.{add,cas,exch}\nintroduced in PTX ISA 2.0.\n64-bit\natom.{and,or,xor,min,max}\nintroduced in PTX ISA 3.1.\natom.add.f64\nintroduced in PTX ISA 5.0.\n.scope\nqualifier introduced in PTX ISA 5.0.\n.sem\nqualifier introduced in PTX ISA version 6.0.\natom.add.noftz.f16x2\nintroduced in PTX ISA 6.2.\natom.add.noftz.f16\nand\natom.cas.b16\nintroduced in PTX ISA 6.3.\nPer-element atomicity of\natom.f16x2\nclarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2.\nSupport for\n.level::cache_hint\nqualifier introduced in PTX ISA version 7.4.\natom.add.noftz.bf16\nand\natom.add.noftz.bf16x2\nintroduced in PTX ISA 7.8.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for vector types introduced in PTX ISA version 8.1.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nTarget ISA Notes\natom.global\nrequires\nsm_11\nor higher.\natom.shared\nrequires\nsm_12\nor higher.\n64-bit\natom.global.{add,cas,exch}\nrequire\nsm_12\nor higher.\n64-bit\natom.shared.{add,cas,exch}\nrequire\nsm_20\nor higher.\n64-bit\natom.{and,or,xor,min,max}\nrequire\nsm_32\nor higher.\natom.add.f32\nrequires\nsm_20\nor higher.\natom.add.f64\nrequires\nsm_60\nor higher.\n.scope\nqualifier requires\nsm_60\nor higher.\n.sem\nqualifier requires\nsm_70\nor higher.\nUse of generic addressing requires\nsm_20\nor higher.\natom.add.noftz.f16x2\nrequires\nsm_60\nor higher.\natom.add.noftz.f16\nand\natom.cas.b16\nrequires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\natom.add.noftz.bf16\nand\natom.add.noftz.bf16x2\nrequire\nsm_90\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for vector types requires\nsm_90\nor higher.\nSupport for\n.b128\ntype requires\nsm_90\nor higher.\nExamples\natom.global.add.s32  d,[a],1;\natom.shared::cta.max.u32  d,[x+4],0;\n@p  atom.global.cas.b32  d,[p],my_val,my_new_val;\natom.global.sys.add.u32 d, [a], 1;\natom.global.acquire.sys.inc.u32 ans, [gbl], %r0;\natom.add.noftz.f16x2 d, [a], b;\natom.add.noftz.f16   hd, [ha], hb;\natom.global.cas.b16  hd, [ha], hb, hc;\natom.add.noftz.bf16   hd, [a], hb;\natom.add.noftz.bf16x2 bd, [b], bb;\natom.add.shared::cluster.noftz.f16   hd, [ha], hb;\natom.shared.b128.cas d, a, b, c; // 128-bit atom\natom.global.b128.exch d, a, b;   // 128-bit atom\n\natom.global.cluster.relaxed.add.u32 d, [a], 1;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\natom.global.add.L2::cache_hint.s32  d, [a], 1, cache-policy;\n\natom.global.v8.f16.max.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v8.bf16.add.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v2.f16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v2.bf16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v4.b16x2.min.noftz  {%hd0, %hd1, %hd2, %hd3}, [gbl], {%h0, %h1, %h2, %h3};\natom.global.v4.f32.add  {%f0, %f1, %f2, %f3}, [gbl], {%f0, %f1, %f2, %f3};\natom.global.v2.f16x2.min.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.bf16x2.max.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};\n9.7.13.6.\nParallel Synchronization and Communication Instructions:\nred\n\nred\nReduction operations on global and shared memory.\nSyntax\nReduction operation with scalar type:\nred{.sem}{.scope}{.space}.op{.level::cache_hint}.type          [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16    [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2  [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16\n                                                      [a], b {, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2\n                                                      [a], b {, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                {.relaxed, .release};\n.scope =              {.cta, .cluster, .gpu, .sys};\n\n.op =                 { .and, .or, .xor,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };\nReduction operation with vector type:\nred{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32 [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}. vec_16_bit.half_word_type [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy};\n\n.sem =                { .relaxed, .release };\n.scope =              { .cta, .cluster, .gpu, .sys };\n.op =                 { .add, .min, .max };\n.half_word_type =     { .f16, .bf16 };\n.packed_type =        { .f16x2,.bf16x2 };\n.vec_16_bit =         { .v2, .v4, .v8 }\n.vec_32_bit =         { .v2, .v4 };\n.level::cache_hint =  { .L2::cache_hint }\nDescription\nPerforms a reduction operation with operand\nb\nand the value in location\na\n, and stores the\nresult of the specified operation at location\na\n, overwriting the original value. Operand\na\nspecifies a location in the specified state space. If no state space is given, perform the memory\naccesses using\nGeneric Addressing\n.\nred\nwith scalar type may\nbe used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address\npoints to\n.global\nor\n.shared\nspace.\nred\nwith vector type may be used only with\n.global\nspace and with generic addressing where the address points to\n.global\nspace.\nFor\nred\nwith vector type, operand\nb\nis brace-enclosed vector expressions, size of which is\nequal to the size of vector qualifier.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.relaxed\nis assumed by default.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is absent,\n.gpu\nscope is\nassumed by default.\nFor\nred\nwith vector type, the supported combinations of vector qualifier, types and reduction\noperations supported on these combinations are depicted in following table:\nVector qualifier\nTypes\n.f16\n/\nbf16\n.f16x2\n/\nbf16x2\n.f32\n.v2\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v4\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v8\n.add\n,\n.min\n,\n.max\nNot supported\nNot Supported\nTwo atomic operations (\natom\nor\nred\n) are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write.\nred\ninstruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch case, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire\nred\nis not guaranteed to be atomic as a single access.\nFor\nsm_6x\nand earlier architectures,\nred\noperations on\n.shared\nstate space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory reduction\ninstructions, e.g., by inserting barriers between normal stores and reduction operations to a common\naddress, or by using\natom.exch\nto store to locations accessed by other reduction operations.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n,\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\nThe floating-point operation\n.add\noperation rounds to nearest even. Current implementation of\nred.add.f32\non global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas\nred.add.f32\non shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero.\nred.add.f16\n,\nred.add.f16x2\n,\nred.add.bf16\nand\nred.add.bf16x2\noperation requires the\n.noftz\nqualifier; it preserves subnormal inputs and results, and does not flush them to zero.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\n*a = operation(*a, b);\n\nwhere\n    inc(r, s) = (r >= s) ? 0 : r+1;\n    dec(r, s) = (r==0 || r > s)  ? s : r-1;\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2.\nred.add.f32\nand\nred.shared.add.u64\nintroduced in PTX ISA 2.0.\n64-bit\nred.{and,or,xor,min,max}\nintroduced in PTX ISA 3.1.\nred.add.f64\nintroduced in PTX ISA 5.0.\n.scope\nqualifier introduced in PTX ISA 5.0.\n.sem\nqualifier introduced in PTX ISA version 6.0.\nred.add.noftz.f16x2\nintroduced in PTX ISA 6.2.\nred.add.noftz.f16\nintroduced in PTX ISA 6.3.\nPer-element atomicity of\nred.f16x2\nclarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2\nSupport for\n.level::cache_hint\nqualifier introduced in PTX ISA version 7.4.\nred.add.noftz.bf16\nand\nred.add.noftz.bf16x2\nintroduced in PTX ISA 7.8.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for vector types introduced in PTX ISA version 8.1.\nTarget ISA Notes\nred.global\nrequires\nsm_11\nor higher\nred.shared\nrequires\nsm_12\nor higher.\nred.global.add.u64\nrequires\nsm_12\nor higher.\nred.shared.add.u64\nrequires\nsm_20\nor higher.\n64-bit\nred.{and,or,xor,min,max}\nrequire\nsm_32\nor higher.\nred.add.f32\nrequires\nsm_20\nor higher.\nred.add.f64\nrequires\nsm_60\nor higher.\n.scope\nqualifier requires\nsm_60\nor higher.\n.sem\nqualifier requires\nsm_70\nor higher.\nUse of generic addressing requires\nsm_20\nor higher.\nred.add.noftz.f16x2\nrequires\nsm_60\nor higher.\nred.add.noftz.f16\nrequires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nred.add.noftz.bf16\nand\nred.add.noftz.bf16x2\nrequire\nsm_90\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for vector types requires\nsm_90\nor higher.\nExamples\nred.global.add.s32  [a],1;\nred.shared::cluster.max.u32  [x+4],0;\n@p  red.global.and.b32  [p],my_val;\nred.global.sys.add.u32 [a], 1;\nred.global.acquire.sys.add.u32 [gbl], 1;\nred.add.noftz.f16x2 [a], b;\nred.add.noftz.bf16   [a], hb;\nred.add.noftz.bf16x2 [b], bb;\nred.global.cluster.relaxed.add.u32 [a], 1;\nred.shared::cta.min.u32  [x+4],0;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nred.global.and.L2::cache_hint.b32 [a], 1, cache-policy;\n\nred.global.v8.f16.add.noftz  [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v8.bf16.min.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v2.f16.add.noftz [gbl], {%h0, %h1};\nred.global.v2.bf16.add.noftz [gbl], {%h0, %h1};\nred.global.v4.f16x2.max.noftz [gbl], {%h0, %h1, %h2, %h3};\nred.global.v4.f32.add  [gbl], {%f0, %f1, %f2, %f3};\nred.global.v2.f16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.bf16x2.add.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};\n9.7.13.7.\nParallel Synchronization and Communication Instructions:\nred.async\n\nred.async\nAsynchronous reduction operation.\nSyntax\n// Increment and Decrement reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  =                 { .relaxed };\n.scope =                { .cluster };\n.ss   =                 { .shared::cluster };\n.op   =                 { .inc, .dec };\n.type =                 { .u32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n\n// MIN and MAX reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.op   = { .min, .max };\n.type = { .u32, .s32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// Bitwise AND, OR and XOR reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.op   = { .and, .or, .xor };\n.type = { .b32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// ADD reductions\nred.async.sem.scope{.ss}.completion_mechanism.add.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.type = { .u32, .s32, .u64 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\nred.async{.mmio}.sem.scope{.ss}.add.type [a], b;\n\n.sem  = { .release };\n.scope = { .gpu, .cluster };\n.ss   = { .global };\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nred.async\nis a non-blocking instruction which initiates an asynchronous reduction operation\nspecified by\n.op\n, with the operand\nb\nand the value at destination shared memory location\nspecified by operand\na\n.\nOperands\na\nis a destination address, and must be either a register, or of the form\nregister\n+\nimmOff\n, as described in\nAddresses as Operands\n.\nb\nis a source value, of the type indicated by qualifier\n.type\n.\nmbar\nis an mbarrier object address.\nQualifiers\n.mmio\nindicates whether this is an\nmmio Operation\n.\n.sem\nspecifies the memory ordering semantics as described in the\nMemory Consistency Model\n.\n.scope\nspecifies the set of threads with which this instruction can\ndirectly synchronize.\n.ss\nspecifies the state space of the destination operand\na\nand the\nmbarrier operand\nmbar\n.\nIf\n.ss\nis not specified,\nGeneric Addressing\nis used.\n.completion_mechanism\nspecifies the mechanism for observing the\ncompletion of the asynchronous operation.\nWhen\n.completion_mechanism\nis\n.mbarrier::complete_tx::bytes\n: upon\ncompletion of the asynchronous operation, a\ncomplete-tx\noperation will be performed on the mbarrier object specified by the operand\nmbar\n,\nwith\ncompleteCount\nargument equal to the amount of data stored in bytes.\nWhen\n.completion_mechanism\nis not specified: the completion of the store\nsynchronizes with the end of the CTA.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\n.op\nspecifies the reduction operation.\nThe\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\n.type\nspecifies the type of the source operand\nb\n.\nConditions\nWhen\n.sem\nis\n.relaxed\n:\nThe reduce operation is a relaxed memory operation.\nThe complete-tx operation on the mbarrier has\n.release\nsemantics at\n.cluster\nscope.\nThe shared-memory addresses of the destination operand\na\nand the\nmbarrier operand\nmbar\nmust meet all of the following conditions:\nThey belong to the same CTA.\nThe CTA to which they belong is different from the CTA of the executing thread,\nbut must be within the same cluster.\nOtherwise, the behavior is undefined.\n.mmio\nmust not be specified.\nIf\n.ss\nis specified, it must be\n.shared::cluster\n.\nIf\n.ss\nis not specified, generic addressing is used for operands\na\nand\nmbar\n.\nIf the generic addresses specified do not fall within the address window of\n.shared::cluster\nstate space, the behavior is undefined.\nIf\n.completion_mechanism\nis specified, it must be\n.mbarrier::complete_tx::bytes\n.\nIf\n.completion_mechanism\nis not specified, it defaults to\n.mbarrier::complete_tx::bytes\n.\nWhen\n.sem\nis\n.release\n:\nThe reduce operation is a strong memory operation with\n.release\nsemantics\nat the scope specified by\n.scope\n.\nIf\n.mmio\nis specified,\n.scope\nmust be\n.sys\n.\nIf\n.ss\nis specified, it must be\n.global\n.\nIf\n.ss\nis not specified, generic addressing is used for operand\na\n.\nIf the generic address specified does not fall within the address window of\n.global\nstate space, the behavior is undefined.\n.completion_mechanism\nmust not be specified.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space,\nand\n.gpu\nand\n.sys\nscopes introduced in PTX ISA version 8.7.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space,\nand\n.gpu\nand\n.sys\nscopes require\nsm_100\nor higher.\nExamples\nred.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr];\n\nred.async.release.sys.global.add.u32 [addr], b;\n9.7.13.8.\nParallel Synchronization and Communication Instructions:\nvote\n(deprecated)\n\nvote\n(deprecated)\nVote across thread group.\nSyntax\nvote.mode.pred  d, {!}a;\nvote.ballot.b32 d, {!}a;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni };\nDeprecation Note\nThe\nvote\ninstruction without a\n.sync\nqualifier is deprecated in PTX ISA version 6.0.\nSupport for this instruction with\n.target\nlower than\nsm_70\nmay be removed in a future PTX\nISA version.\nRemoval Note\nSupport for\nvote\ninstruction without a\n.sync\nqualifier is removed in PTX ISA version 6.4 for\n.target\nsm_70\nor higher.\nDescription\nPerforms a reduction of the source predicate across all active threads in a warp. The destination\npredicate value is the same across all threads in the warp.\nThe reduction modes are:\n.all\nTrue\nif source predicate is\nTrue\nfor all active threads in warp. Negate the source\npredicate to compute\n.none\n.\n.any\nTrue\nif source predicate is\nTrue\nfor some active thread in warp. Negate the source\npredicate to compute\n.not_all\n.\n.uni\nTrue\nif source predicate has the same value in all active threads in warp. Negating the\nsource predicate also computes\n.uni\n.\nIn the\nballot\nform,\nvote.ballot.b32\nsimply copies the predicate from each thread in a warp\ninto the corresponding bit position of destination register\nd\n, where the bit position\ncorresponds to the thread’s lane id.\nAn inactive thread in warp will contribute a 0 for its entry when participating in\nvote.ballot.b32\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2.\nDeprecated in PTX ISA version 6.0 in favor of\nvote.sync\n.\nNot supported in PTX ISA version 6.4 for .target\nsm_70\nor higher.\nTarget ISA Notes\nvote\nrequires\nsm_12\nor higher.\nvote.ballot.b32\nrequires\nsm_20\nor higher.\nvote\nis not supported on\nsm_70\nor higher starting PTX ISA version 6.4.\nRelease Notes\nNote that\nvote\napplies to threads in a single warp, not across an entire CTA.\nExamples\nvote.all.pred    p,q;\nvote.uni.pred    p,q;\nvote.ballot.b32  r1,p;  // get 'ballot' across warp\n9.7.13.9.\nParallel Synchronization and Communication Instructions:\nvote.sync\n\nvote.sync\nVote across thread group.\nSyntax\nvote.sync.mode.pred  d, {!}a, membermask;\nvote.sync.ballot.b32 d, {!}a, membermask;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni };\nDescription\nvote.sync\nwill cause executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nvote.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s\nlaneid\n. Operand\na\nis a\npredicate register.\nIn the\nmode\nform,\nvote.sync\nperforms a reduction of the source predicate across all non-exited\nthreads in\nmembermask\n. The destination operand\nd\nis a predicate register and its value is\nthe same across all threads in\nmembermask\n.\nThe reduction modes are:\n.all\nTrue\nif source predicate is\nTrue\nfor all non-exited threads in\nmembermask\n. Negate the\nsource predicate to compute\n.none\n.\n.any\nTrue\nif source predicate is\nTrue\nfor some thread in\nmembermask\n. Negate the source\npredicate to compute\n.not_all\n.\n.uni\nTrue\nif source predicate has the same value in all non-exited threads in\nmembermask\n. Negating the source predicate also computes\n.uni\n.\nIn the\nballot\nform, the destination operand\nd\nis a\n.b32\nregister. In this form,\nvote.sync.ballot.b32\nsimply copies the predicate from each thread in\nmembermask\ninto the\ncorresponding bit position of destination register\nd\n, where the bit position corresponds to the\nthread’s lane id.\nA thread not specified in\nmembermask\nwill contribute a 0 for its entry in\nvote.sync.ballot.b32\n.\nThe behavior of\nvote.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nvote.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when\nthe\nvote.sync\ninstruction is executed. Otherwise, the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nvote.sync.all.pred    p,q,0xffffffff;\nvote.sync.ballot.b32  r1,p,0xffffffff;  // get 'ballot' across warp\n9.7.13.10.\nParallel Synchronization and Communication Instructions:\nmatch.sync\n\nmatch.sync\nBroadcast and compare a value across threads in warp.\nSyntax\nmatch.any.sync.type  d, a, membermask;\nmatch.all.sync.type  d[|p], a, membermask;\n\n.type = { .b32, .b64 };\nDescription\nmatch.sync\nwill cause executing thread to wait until all non-exited threads from\nmembermask\nhave executed\nmatch.sync\nwith the same qualifiers and same\nmembermask\nvalue before resuming\nexecution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid.\nmatch.sync\nperforms broadcast and compare of operand\na\nacross all non-exited threads in\nmembermask\nand sets destination\nd\nand optional predicate\np\nbased on mode.\nOperand\na\nhas instruction type and\nd\nhas\n.b32\ntype.\nDestination\nd\nis a 32-bit mask where bit position in mask corresponds to thread’s laneid.\nThe matching operation modes are:\n.all\nd\nis set to mask corresponding to non-exited threads in\nmembermask\nif all non-exited\nthreads in\nmembermask\nhave same value of operand\na\n; otherwise\nd\nis set\nto 0. Optionally predicate\np\nis set to true if all non-exited threads in\nmembermask\nhave\nsame value of operand\na\n; otherwise\np\nis set to false. The sink symbol ‘_’ may be used in\nplace of any one of the destination operands.\n.any\nd\nis set to mask of non-exited threads in\nmembermask\nthat have same value of operand\na\n.\nThe behavior of\nmatch.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_70\nor higher.\nRelease Notes\nNote that\nmatch.sync\napplies to threads in a single warp, not across an entire CTA.\nExamples\nmatch.any.sync.b32    d, a, 0xffffffff;\nmatch.all.sync.b64    d|p, a, mask;\n9.7.13.11.\nParallel Synchronization and Communication Instructions:\nactivemask\n\nactivemask\nQueries the active threads within a warp.\nSyntax\nactivemask.b32 d;\nDescription\nactivemask\nqueries predicated-on active threads from the executing warp and sets the destination\nd\nwith 32-bit integer mask where bit position in the mask corresponds to the thread’s\nlaneid\n.\nDestination\nd\nis a 32-bit destination register.\nAn active thread will contribute 1 for its entry in the result and exited or inactive or\npredicated-off thread will contribute 0 for its entry in the result.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.2.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nactivemask.b32  %r1;\n9.7.13.12.\nParallel Synchronization and Communication Instructions:\nredux.sync\n\nredux.sync\nPerform reduction operation on the data from each predicated active thread in the thread group.\nSyntax\nredux.sync.op.type dst, src, membermask;\n.op   = {.add, .min, .max}\n.type = {.u32, .s32}\n\nredux.sync.op.b32 dst, src, membermask;\n.op   = {.and, .or, .xor}\n\nredux.sync.op{.abs.}{.NaN}.f32 dst, src, membermask;\n.op   = { .min, .max }\nDescription\nredux.sync\nwill cause the executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nredux.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s\nlaneid\n.\nredux.sync\nperforms a reduction operation\n.op\nof the 32 bit source register\nsrc\nacross\nall non-exited threads in the\nmembermask\n. The result of the reduction operation is written to\nthe 32 bit destination register\ndst\n.\nReduction operation can be one of the bitwise operation in\n.and\n,\n.or\n,\n.xor\nor arithmetic\noperation in\n.add\n,\n.min\n,\n.max\n.\nFor the\n.add\noperation result is truncated to 32 bits.\nFor\n.f32\ninstruction type, if the input value is 0.0 then +0.0 > -0.0.\nIf\n.abs\nqualifier is specified, then the absolute value of the input is considered for the\nreduction operation.\nIf the\n.NaN\nqualifier is specified, then the result of the reduction operation is canonical NaN\nif the input to the reduction operation from any participating thread is NaN.\nIn the absence of\n.NaN\nqualifier, only non-NaN values are considered for the reduction operation\nand the result will be canonical NaN when all inputs are NaNs.\nThe behavior of\nredux.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for\n.f32\ntype is introduced in PTX ISA version 8.6.\nSupport for\n.abs\nand\n.NaN\nqualifiers is introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\n.f32\ntype requires\nsm_100a\nand is supported on\nsm_100f\nfrom PTX ISA version 8.8.\nQualifiers\n.abs\nand\n.NaN\nrequire\nsm_100a\nand are supported on\nsm_100f\nor\nhigher in the same family from PTX ISA version 8.8.\nRelease Notes\nNote that\nredux.sync\napplies to threads in a single warp, not across an entire CTA.\nExamples\n.reg .b32 dst, src, init, mask;\nredux.sync.add.s32 dst, src, 0xff;\nredux.sync.xor.b32 dst, src, mask;\n\nredux.sync.min.abs.NaN.f32 dst, src, mask;\n9.7.13.13.\nParallel Synchronization and Communication Instructions:\ngriddepcontrol\n\ngriddepcontrol\nControl execution of dependent grids.\nSyntax\ngriddepcontrol.action;\n\n.action   = { .launch_dependents, .wait }\nDescription\nThe\ngriddepcontrol\ninstruction allows the dependent grids and prerequisite grids as defined by\nthe runtime, to control execution in the following way:\n.launch_dependents\nmodifier signals that specific dependents the runtime system designated to\nreact to this instruction can be scheduled as soon as all other CTAs in the grid issue the same\ninstruction or have completed. The dependent may launch before the completion of the current\ngrid. There is no guarantee that the dependent will launch before the completion of the current\ngrid. Repeated invocations of this instruction by threads in the current CTA will have no additional\nside effects past that of the first invocation.\n.wait\nmodifier causes the executing thread to wait until all prerequisite grids in flight have\ncompleted and all the memory operations from the prerequisite grids are performed and made visible\nto the current grid.\nNote\nIf the prerequisite grid is using\ngriddepcontrol.launch_dependents\n, then the dependent grid\nmust use\ngriddepcontrol.wait\nto ensure correct functional execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ngriddepcontrol.launch_dependents;\ngriddepcontrol.wait;\n9.7.13.14.\nParallel Synchronization and Communication Instructions:\nelect.sync\n\nelect.sync\nElect a leader thread from a set of threads.\nSyntax\nelect.sync d|p, membermask;\nDescription\nelect.sync\nelects one predicated active leader thread from among a set of threads specified by\nmembermask\n.\nlaneid\nof the elected thread is returned in the 32-bit destination operand\nd\n. The sink symbol ‘_’ can be used for destination operand\nd\n. The predicate destination\np\nis set to\nTrue\nfor the leader thread, and\nFalse\nfor all other threads.\nOperand\nmembermask\nspecifies a 32-bit integer indicating the set of threads from which a leader\nis to be elected. The behavior is undefined if the executing thread is not in\nmembermask\n.\nElection of a leader thread happens deterministically, i.e. the same leader thread is elected for\nthe same\nmembermask\nevery time.\nThe mandatory\n.sync\nqualifier indicates that\nelect\ncauses the executing thread to wait until\nall threads in the\nmembermask\nexecute the\nelect\ninstruction before resuming execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nelect.sync    %r0|%p0, 0xffffffff;\n9.7.13.15.\nParallel Synchronization and Communication Instructions:\nmbarrier\n\nmbarrier\nis a barrier created in shared memory that supports :\nSynchronizing any subset of threads within a CTA\nOne-way synchronization of threads across CTAs of a cluster. As noted in\nmbarrier support with shared memory\n, threads can\nperform only\narrive\noperations but not\n*_wait\non an mbarrier located in\nshared::cluster\nspace.\nWaiting for completion of asynchronous memory operations initiated by a thread and making them\nvisible to other threads.\nAn\nmbarrier object\nis an opaque object in memory which can be initialized and invalidated using :\nmbarrier.init\nmbarrier.inval\nOperations supported on\nmbarrier object\ns are :\nmbarrier.expect_tx\nmbarrier.complete_tx\nmbarrier.arrive\nmbarrier.arrive_drop\nmbarrier.test_wait\nmbarrier.try_wait\nmbarrier.pending_count\ncp.async.mbarrier.arrive\nPerforming any\nmbarrier\noperation except\nmbarrier.init\non an uninitialized\nmbarrier object\nresults in undefined behavior.\nPerforming any\nnon-mbarrier\nor\nmbarrier.init\noperations on an initialized\nmbarrier object\nresults in undefined behavior.\nUnlike\nbar{.cta}\n/\nbarrier{.cta}\ninstructions which can access a limited number of barriers\nper CTA,\nmbarrier objects\nare user defined and are only limited by the total shared memory size\navailable.\nmbarrier\noperations enable threads to perform useful work after the arrival at the\nmbarrier\nand\nbefore waiting for the\nmbarrier\nto complete.\n9.7.13.15.1.\nSize and alignment of mbarrier object\n\nAn mbarrier object is an opaque object with the following type and alignment requirements :\nType\nAlignment (bytes)\nMemory space\n.b64\n8\n.shared\n9.7.13.15.2.\nContents of the mbarrier object\n\nAn opaque\nmbarrier object\nkeeps track of the following information :\nCurrent phase of the\nmbarrier object\nCount of pending arrivals for the current phase of the\nmbarrier object\nCount of expected arrivals for the next phase of the\nmbarrier object\nCount of pending asynchronous memory operations (or transactions) tracked by the current phase of\nthe\nmbarrier object\n. This is also referred to as\ntx-count\n.\nAn\nmbarrier object\nprogresses through a sequence of phases where each phase is defined by threads\nperforming an expected number of\narrive-on\noperations.\nThe valid range of each of the counts is as shown below:\nCount name\nMinimum value\nMaximum value\nExpected arrival count\n1\n2\n20\n- 1\nPending arrival count\n0\n2\n20\n- 1\ntx-count\n-(2\n20\n- 1)\n2\n20\n- 1\n9.7.13.15.3.\nLifecycle of the mbarrier object\n\nThe\nmbarrier object\nmust be initialized prior to use.\nAn\nmbarrier object\nis used to synchronize threads and asynchronous memory operations.\nAn\nmbarrier object\nmay be used to perform a sequence of such synchronizations.\nAn\nmbarrier object\nmust be invalidated to repurpose its memory for any purpose,\nincluding repurposing it for another mbarrier object.\n9.7.13.15.4.\nPhase of the mbarrier object\n\nThe phase of an\nmbarrier object\nis the number of times the\nmbarrier object\nhas been used to\nsynchronize threads and\nasynchronous\noperations. In each phase {0, 1, 2, …}, threads perform in program order :\narrive-on\noperations to complete the current phase and\ntest_wait\n/\ntry_wait\noperations to check for the completion of the current phase.\nAn\nmbarrier object\nis automatically reinitialized upon completion of the current phase for\nimmediate use in the next phase. The current phase is incomplete and all prior phases are complete.\nFor each phase of the mbarrier object, at least one\ntest_wait\nor\ntry_wait\noperation must be\nperformed which returns\nTrue\nfor\nwaitComplete\nbefore an\narrive-on\noperation\nin the subsequent phase.\n9.7.13.15.5.\nTracking asynchronous operations by the mbarrier object\n\nStarting with the Hopper architecture (\nsm_9x\n),\nmbarrier object\nsupports a new count, called\ntx-count\n, which is used for tracking the completion of asynchronous memory operations or\ntransactions.\ntx-count\ntracks the number of asynchronous transactions, in units specified by the\nasynchronous memory operation, that are outstanding and yet to be complete.\nThe\ntx-count\nof an\nmbarrier object\nmust be set to the total amount of asynchronous memory\noperations, in units as specified by the asynchronous operations, to be tracked by the current\nphase. Upon completion of each of the asynchronous operations, the\ncomplete-tx\noperation will be performed on the\nmbarrier object\nand thus progress the mbarrier towards the\ncompletion of the current phase.\n9.7.13.15.5.1.\nexpect-tx operation\n\nThe\nexpect-tx\noperation, with an\nexpectCount\nargument, increases the\ntx-count\nof an\nmbarrier object\nby the value specified by\nexpectCount\n. This sets the current phase of the\nmbarrier object\nto expect and track the completion of additional asynchronous transactions.\n9.7.13.15.5.2.\ncomplete-tx operation\n\nThe\ncomplete-tx\noperation, with an\ncompleteCount\nargument, on an\nmbarrier object\nconsists of the following:\nmbarrier signaling\nSignals the completion of asynchronous transactions that were tracked by the current phase. As a\nresult of this,\ntx-count\nis decremented by\ncompleteCount\n.\nmbarrier potentially completing the current phase\nIf the current phase has been completed then the mbarrier transitions to the next phase. Refer to\nPhase Completion of the mbarrier object\nfor details on phase completion requirements and phase transition process.\n9.7.13.15.6.\nPhase Completion of the mbarrier object\n\nThe requirements for completion of the current phase are described below. Upon completion of the\ncurrent phase, the phase transitions to the subsequent phase as described below.\nCurrent phase completion requirements\nAn\nmbarrier object\ncompletes the current phase when all of the following conditions are met:\nThe count of the pending arrivals has reached zero.\nThe\ntx-count\nhas reached zero.\nPhase transition\nWhen an\nmbarrier\nobject completes the current phase, the following actions are performed\natomically:\nThe\nmbarrier object\ntransitions to the next phase.\nThe pending arrival count is reinitialized to the expected arrival count.\n9.7.13.15.7.\nArrive-on operation on mbarrier object\n\nAn\narrive-on\noperation, with an optional\ncount\nargument, on an\nmbarrier object\nconsists of the\nfollowing 2 steps :\nmbarrier signalling:\nSignals the arrival of the executing thread OR completion of the asynchronous instruction which\nsignals the arrive-on operation initiated by the executing thread on the\nmbarrier object\n. As a\nresult of this, the pending arrival count is decremented by\ncount\n. If the\ncount\nargument is\nnot specified, then it defaults to 1.\nmbarrier potentially completing the current phase:\nIf the current phase has been completed then the mbarrier transitions to the next phase. Refer to\nPhase Completion of the mbarrier object\nfor details on phase completion requirements and phase transition process.\n9.7.13.15.8.\nmbarrier support with shared memory\n\nThe following table summarizes the support of various mbarrier operations on\nmbarrier objects\nlocated at different shared memory locations:\nmbarrier operations\n.shared::cta\n.shared::cluster\nmbarrier.arrive\nSupported\nSupported, cannot return result\nmbarrier.expect_tx\nSupported\nSupported\nmbarrier.complete_tx\nSupported\nSupported\nOther mbarrier operations\nSupported\nNot supported\n9.7.13.15.9.\nParallel Synchronization and Communication Instructions:\nmbarrier.init\n\nmbarrier.init\nInitialize the\nmbarrier object\n.\nSyntax\nmbarrier.init{.shared{::cta}}.b64 [addr], count;\nDescription\nmbarrier.init\ninitializes the\nmbarrier object\nat the location specified by the address operand\naddr\nwith the unsigned 32-bit integer\ncount\n. The value of operand count must be in the range\nas specified in\nContents of the mbarrier object\n.\nInitialization of the\nmbarrier object\ninvolves :\nInitializing the current phase to 0.\nInitializing the expected arrival count to\ncount\n.\nInitializing the pending arrival count to\ncount\n.\nInitializing the\ntx-count\nto 0.\nThe valid range of values for the operand\ncount\nis [1, …, 2\n20\n- 1].\nRefer\nContents of the mbarrier object\nfor the\nvalid range of values for the various constituents of the mbarrier.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe behavior of performing an\nmbarrier.init\noperation on a memory location containing a\nvalid\nmbarrier object\nis undefined; invalidate the\nmbarrier object\nusing\nmbarrier.inval\nfirst, before repurposing the memory location for any other purpose, including another\nmbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.shared .b64 shMem, shMem2;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n\ncvta.shared.u64          addr, shMem2;\nmbarrier.init.b64        [addr],   %r1;\nbar.cta.sync             0;\n// ... other mbarrier operations on addr\n\nmbarrier.init.shared::cta.b64 [shMem], 12;\nbar.sync                 0;\n// ... other mbarrier operations on shMem\n9.7.13.15.10.\nParallel Synchronization and Communication Instructions:\nmbarrier.inval\n\nmbarrier.inval\nInvalidates the\nmbarrier object\n.\nSyntax\nmbarrier.inval{.shared{::cta}}.b64 [addr];\nDescription\nmbarrier.inval\ninvalidates the\nmbarrier object\nat the location specified by the address\noperand\naddr\n.\nAn\nmbarrier object\nmust be invalidated before using its memory location for any other purpose.\nPerforming any\nmbarrier\noperation except\nmbarrier.init\non a memory location that does not\ncontain a valid\nmbarrier object\n, results in undefined behaviour.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.shared .b64 shmem;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n.reg    .pred t0;\n\n// Example 1 :\nbar.sync                      0;\n@t0 mbarrier.init.b64     [addr], %r1;\n// ... other mbarrier operations on addr\nbar.sync                      0;\n@t0 mbarrier.inval.b64    [addr];\n\n\n// Example 2 :\nbar.cta.sync                  0;\nmbarrier.init.shared.b64           [shmem], 12;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared.b64      [shmem];\n\n// shmem can be reused here for unrelated use :\nbar.cta.sync                  0;\nst.shared.b64                      [shmem], ...;\n\n// shmem can be re-initialized as mbarrier object :\nbar.cta.sync                  0;\n@t0 mbarrier.init.shared.b64       [shmem], 24;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared::cta.b64 [shmem];\n9.7.13.15.11.\nParallel Synchronization and Communication Instructions:\nmbarrier.expect_tx\n\nmbarrier.expect_tx\nPerfoms\nexpect-tx\noperation on the\nmbarrier object\n.\nSyntax\nmbarrier.expect_tx{.sem.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster }\nDescription\nA thread executing\nmbarrier.expect_tx\nperforms an\nexpect-tx\noperation on the\nmbarrier object\nat the location specified by the address operand\naddr\n. The\n32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to the\nexpect-tx\noperation.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nare as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmbarrier.expect_tx.b64                       [addr], 32;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj1], 512;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj2], 512;\n9.7.13.15.12.\nParallel Synchronization and Communication Instructions:\nmbarrier.complete_tx\n\nmbarrier.complete_tx\nPerfoms\ncomplete-tx\noperation on the\nmbarrier object\n.\nSyntax\nmbarrier.complete_tx{.sem.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster }\nDescription\nA thread executing\nmbarrier.complete_tx\nperforms a\ncomplete-tx\noperation on the\nmbarrier object\nat the location specified by the address operand\naddr\n. The\n32-bit unsigned integer operand\ntxCount\nspecifies the\ncompleteCount\nargument to the\ncomplete-tx\noperation.\nmbarrier.complete_tx\ndoes not involve any asynchronous memory operations and only simulates the\ncompletion of an asynchronous memory operation and its side effect of signaling to the\nmbarrier\nobject\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nare as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmbarrier.complete_tx.b64             [addr],     32;\nmbarrier.complete_tx.shared.b64      [mbarObj1], 512;\nmbarrier.complete_tx.relaxed.cta.b64 [addr2],    32;\n9.7.13.15.13.\nParallel Synchronization and Communication Instructions:\nmbarrier.arrive\n\nmbarrier.arrive\nPerforms\narrive-on operation\non the\nmbarrier object\n.\nSyntax\nmbarrier.arrive{.sem.scope}{.shared{::cta}}.b64           state, [addr]{, count};\nmbarrier.arrive{.sem.scope}{.shared::cluster}.b64         _, [addr] {,count}\nmbarrier.arrive.expect_tx{.sem.scope}{.shared{::cta}}.b64 state, [addr], txCount;\nmbarrier.arrive.expect_tx{.sem.scope}{.shared::cluster}.b64   _, [addr], txCount;\nmbarrier.arrive.noComplete{.release.cta}{.shared{::cta}}.b64  state, [addr], count;\n\n.sem   = { .release, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nA thread executing\nmbarrier.arrive\nperforms an\narrive-on\noperation\non the\nmbarrier object\nat the location specified by the address operand\naddr\n. The 32-bit\nunsigned integer operand\ncount\nspecifies the\ncount\nargument to the\narrive-on\noperation.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional qualifier\n.expect_tx\nspecifies that an\nexpect-tx\noperation is performed prior to the\narrive-on\noperation. The 32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to\nthe\nexpect-tx\noperation. When both qualifiers\n.arrive\nand\n.expect_tx\nare specified, then\nthe count argument of the\narrive-on\noperation is assumed to be 1.\nA\nmbarrier.arrive\noperation with\n.noComplete\nqualifier must not cause the\nmbarrier\nto\ncomplete its current phase, otherwise the behavior is undefined.\nThe value of the operand\ncount\nmust be in the range as specified in\nContents of the mbarrier object\n.\nNote: for\nsm_8x\n, when the argument\ncount\nis specified, the modifier\n.noComplete\nis\nrequired.\nmbarrier.arrive\noperation on an\nmbarrier object\nlocated in\n.shared::cta\nreturns an opaque\n64-bit register capturing the phase of the\nmbarrier object\nprior to the\narrive-on operation\nin the\ndestination operand\nstate.\nContents of the\nstate\noperand are implementation\nspecific. Optionally, sink symbol\n'_'\ncan be used for the\nstate\nargument.\nmbarrier.arrive\noperation on an\nmbarrier object\nlocated in\n.shared::cluster\nbut not in\n.shared::cta\ncannot return a value. Sink symbol ‘_’ is mandatory for the destination operand for\nsuch cases.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.release\nis assumed by default.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is not specified then it\ndefaults to\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state space where the\nmbarrier resides.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sink symbol ‘_’ as the destination operand is introduced in PTX ISA version 7.1.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\ncount\nargument without the modifier\n.noComplete\nintroduced in PTX ISA version\n7.8.\nSupport for sub-qualifier\n::cluster\nintroduced in PTX ISA version 8.0.\nSupport for qualifier\n.expect_tx\nis introduced in PTX ISA version 8.0.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSupport for\ncount\nargument without the modifier\n.noComplete\nrequires\nsm_90\nor higher.\nQualifier\n.expect_tx\nrequires\nsm_90\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n.reg .b32 cnt, remoteAddr32, remoteCTAId, addr32;\n.reg .b64 %r<5>, addr, remoteAddr64;\n.shared .b64 shMem, shMem2;\n\ncvta.shared.u64            addr, shMem2;\nmov.b32                    addr32, shMem2;\nmapa.shared::cluster.u32   remoteAddr32, addr32, remoteCTAId;\nmapa.u64                   remoteAddr64, addr,   remoteCTAId;\n\ncvta.shared.u64          addr, shMem2;\n\nmbarrier.arrive.shared.b64                       %r0, [shMem];\nmbarrier.arrive.shared::cta.b64                  %r0, [shMem2];\nmbarrier.arrive.release.cta.shared::cluster.b64  _, [remoteAddr32];\nmbarrier.arrive.release.cluster.b64              _, [remoteAddr64], cnt;\nmbarrier.arrive.expect_tx.release.cluster.b64    _, [remoteAddr64], tx_count;\nmbarrier.arrive.noComplete.b64                   %r1, [addr], 2;\nmbarrier.arrive.relaxed.cta.b64                  %r2, [addr], 4;\nmbarrier.arrive.b64                              %r2, [addr], cnt;\n9.7.13.15.14.\nParallel Synchronization and Communication Instructions:\nmbarrier.arrive_drop\n\nmbarrier.arrive_drop\nDecrements the expected count of the\nmbarrier object\nand performs\narrive-on operation\n.\nSyntax\nmbarrier.arrive_drop{.sem.scope}{.shared{::cta}}.b64 state,           [addr]{, count};\nmbarrier.arrive_drop{.sem.scope}{.shared::cluster}.b64           _,   [addr] {,count};\nmbarrier.arrive_drop.expect_tx{.sem.scope}{.shared{::cta}}.b64 state, [addr], tx_count;\nmbarrier.arrive_drop.expect_tx{.sem.scope}{.shared::cluster}.b64   _, [addr], tx_count;\nmbarrier.arrive_drop.noComplete{.release.cta}{.shared{::cta}}.b64 state,  [addr], count;\n\n.sem   = { .release, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nA thread executing\nmbarrier.arrive_drop\non the\nmbarrier object\nat the location specified by\nthe address operand\naddr\nperforms the following steps:\nDecrements the expected arrival count of the\nmbarrier object\nby the value specified by the\n32-bit integer operand\ncount\n. If\ncount\noperand is not specified, it defaults to 1.\nPerforms an\narrive-on operation\non the\nmbarrier object\n. The operand\ncount\nspecifies the\ncount\nargument to the\narrive-on operation\n.\nThe decrement done in the expected arrivals count of the\nmbarrier object\nwill be for all the\nsubsequent phases of the\nmbarrier object\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional qualifier\n.expect_tx\nspecifies that an\nexpect-tx\noperation is performed prior to the\narrive_drop\noperation, i.e. the decrement of arrival count and\narrive-on\noperation. The 32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to\nthe\nexpect-tx\noperation. When both qualifiers\n.arrive_drop\nand\n.expect_tx\nare specified, then\nthe count argument of the\narrive-on\noperation is assumed to be 1.\nmbarrier.arrive_drop\noperation with\n.release\nqualifier forms the\nrelease\npattern as\ndescribed in the Memory Consistency Model and synchronizes with the\nacquire\npatterns.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.release\nis assumed by default.  The\n.relaxed\nqualifier does not provide any memory\nordering semantics and visibility guarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that an\nmbarrier.arrive_drop\ninstruction can directly synchronize. If the\n.scope\nqualifier is not specified then it defaults\nto\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state space where the mbarrier\nresides.\nA\nmbarrier.arrive_drop\nwith\n.noComplete\nqualifier must not complete the\nmbarrier,\notherwise the behavior is undefined.\nThe value of the operand\ncount\nmust be in the range as specified in\nContents of the mbarrier object\n.\nNote: for\nsm_8x\n, when the argument\ncount\nis specified, the modifier\n.noComplete\nis\nrequired.\nA thread that wants to either exit or opt out of participating in the\narrive-on operation\ncan use\nmbarrier.arrive_drop\nto drop itself from the\nmbarrier\n.\nmbarrier.arrive_drop\noperation on an\nmbarrier object\nlocated in\n.shared::cta\nreturns an\nopaque 64-bit register capturing the phase of the\nmbarrier object\nprior to the\narrive-on\noperation\nin the destination operand\nstate\n. Contents of the returned state are implementation\nspecific. Optionally, sink symbol\n'_'\ncan be used for the\nstate\nargument.\nmbarrier.arrive_drop\noperation on an\nmbarrier\nobject located in\n.shared::cluster\nbut not\nin\n.shared::cta\ncannot return a value. Sink symbol ‘_’ is mandatory for the destination operand\nfor such cases.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\ncount\nargument without the modifier\n.noComplete\nintroduced in PTX ISA version\n7.8.\nSupport for qualifier\n.expect_tx\nis introduced in PTX ISA version 8.0.\nSupport for sub-qualifier\n::cluster\nintroduced in PTX ISA version 8.0.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSupport for\ncount\nargument without the modifier\n.noComplete\nrequires\nsm_90\nor higher.\nQualifier\n.expect_tx\nrequires\nsm_90\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n.reg .b32 cnt;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\n// Example 1\n@p mbarrier.arrive_drop.shared.b64 _, [shMem];\n@p exit;\n@p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a;\n@p2 exit;\n..\n@!p mbarrier.arrive.shared.b64   %r1, [shMem];\n@!p mbarrier.test_wait.shared.b64  q, [shMem], %r1;\n\n// Example 2\nmbarrier.arrive_drop.shared::cluster.b64 _, [addr];\nmbarrier.arrive_drop.shared::cta.release.cluster.b64     _, [addr], cnt;\n\n// Example 3\nmbarrier.arrive_drop.expect_tx.shared::cta.relaxed.cluster.b64 state, [addr], tx_count;\n9.7.13.15.15.\nParallel Synchronization and Communication Instructions:\ncp.async.mbarrier.arrive\n\ncp.async.mbarrier.arrive\nMakes the\nmbarrier object\ntrack all prior\ncp.async\noperations initiated by the\nexecuting thread.\nSyntax\ncp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr];\nDescription\nCauses an\narrive-on operation\nto be\ntriggered by the system on the\nmbarrier object\nupon the completion of all prior\ncp.async\noperations initiated by the\nexecuting thread. The\nmbarrier object\nis at the location specified by the operand\naddr\n. The\narrive-on operation\nis\nasynchronous to execution of\ncp.async.mbarrier.arrive\n.\nWhen\n.noinc\nmodifier is not specified, the pending count of the mbarrier object is incremented\nby 1 prior to the asynchronous\narrive-on operation\n. This\nresults in a zero-net change for the pending count from the asynchronous\narrive-on\noperation\nduring the current phase. The pending count of the\nmbarrier object\nafter the increment should not\nexceed the limit as mentioned in\nContents of the mbarrier object\n. Otherwise,\nthe behavior is undefined.\nWhen the\n.noinc\nmodifier is specified, the increment to the pending count of the\nmbarrier\nobject\nis not performed. Hence the decrement of the pending count done by the asynchronous\narrive-on operation\nmust be\naccounted for in the initialization of the\nmbarrier object\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example 1: no .noinc\nmbarrier.init.shared.b64 [shMem], threadCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n....\n// Absence of .noinc accounts for arrive-on from completion of prior cp.async operations.\n// So mbarrier.init must only account for arrive-on from mbarrier.arrive.\ncp.async.mbarrier.arrive.shared.b64 [shMem];\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n\n\n\n// Example 2: with .noinc\n\n// Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive.\n\n// All threads participating in the mbarrier perform cp.async\nmov.b32 copyOperationCnt, threadCount;\n\n// 3 arrive-on operations will be triggered per-thread\nmul.lo.u32 copyArrivalCnt, copyOperationCnt, 3;\n\nadd.u32 totalCount, threadCount, copyArrivalCnt;\n\nmbarrier.init.shared.b64 [shMem], totalCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n...\n// Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 1st instance\n....\ncp.async.ca.shared.global [shard3], [gbl3], 4;\ncp.async.ca.shared.global [shard4], [gbl4], 16;\ncp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem]; // 2nd instance\n....\ncp.async.ca.shared.global [shard5], [gbl5], 4;\ncp.async.cg.shared.global [shard6], [gbl6], 16;\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 3rd and last instance\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n9.7.13.15.16.\nParallel Synchronization and Communication Instructions:\nmbarrier.test_wait\n/\nmbarrier.try_wait\n\nmbarrier.test_wait\n,\nmbarrier.try_wait\nChecks whether the\nmbarrier object\nhas completed the phase.\nSyntax\nmbarrier.test_wait{.sem.scope}{.shared{::cta}}.b64        waitComplete, [addr], state;\nmbarrier.test_wait.parity{.sem.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity;\n\nmbarrier.try_wait{.sem.scope}{.shared{::cta}}.b64         waitComplete, [addr], state\n                                                            {, suspendTimeHint};\n\nmbarrier.try_wait.parity{.sem.scope}{.shared{::cta}}.b64  waitComplete, [addr], phaseParity\n                                                            {, suspendTimeHint};\n\n.sem   = { .acquire, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nThe\ntest_wait\nand\ntry_wait\noperations test for the completion of the current or the immediately\npreceding phase of an\nmbarrier object\nat the location specified by the operand\naddr\n.\nmbarrier.test_wait\nis a non-blocking instruction which tests for the completion of the phase.\nmbarrier.try_wait\nis a potentially blocking instruction which tests for the completion of the\nphase. If the phase is not complete, the executing thread may be suspended. Suspended thread resumes\nexecution when the specified phase completes OR before the phase completes following a\nsystem-dependent time limit. The optional 32-bit unsigned integer operand\nsuspendTimeHint\nspecifies the time limit, in nanoseconds, that may be used for the time limit instead of the\nsystem-dependent limit.\nmbarrier.test_wait\nand\nmbarrier.try_wait\ntest for completion of the phase :\nSpecified by the 64-bit unsigned integer operand\nstate\n, which was returned by an\nmbarrier.arrive\ninstruction on the same\nmbarrier object\nduring the current or the\nimmediately preceding phase. Or\nIndicated by the 32-bit unsigned integer operand\nphaseParity\n, which is the integer parity\nof either the current phase or the immediately preceding phase of the\nmbarrier object\n.\nThe\n.parity\nvariant of the instructions test for the completion of the phase indicated by the\noperand\nphaseParity\n, which is the integer parity of either the current phase or the immediately\npreceding phase of the\nmbarrier object\n. An even phase has integer parity 0 and an odd phase has\ninteger parity of 1. So the valid values of\nphaseParity\noperand are 0 and 1.\nNote: the use of the\n.parity\nvariants of the instructions requires tracking the phase of an\nmbarrier object\nthroughout its lifetime.\nThe\ntest_wait\nand\ntry_wait\noperations are valid only for :\nthe current incomplete phase, for which\nwaitComplete\nreturns\nFalse\n.\nthe immediately preceding phase, for which\nwaitComplete\nreturns\nTrue\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nWhen\nmbarrier.test_wait\nand\nmbarrier.try_wait\noperations with\n.acquire\nqualifier\nreturns\nTrue\n, they form the\nacquire\npattern as described in the\nMemory Consistency Model\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.acquire\nis assumed by default.  The\n.relaxed\nqualifier does not provide any memory\nordering semantics and visibility guarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that the\nmbarrier.test_wait\nand\nmbarrier.try_wait\ninstructions can directly synchronize. If the\n.scope\nqualifier is not\nspecified then it defaults to\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state\nspace where the mbarrier resides.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nThe following ordering of memory operations hold for the executing thread when\nmbarrier.test_wait\nor\nmbarrier.try_wait\nhaving acquire semantics returns\nTrue\n:\nAll memory accesses (except\nasync operations\n) requested prior, in program\norder, to\nmbarrier.arrive\nhaving release semantics during the completed phase by\nthe participating threads of the CTA are performed and are visible to the executing thread.\nAll\ncp.async\noperations\nrequested prior, in program order, to\ncp.async.mbarrier.arrive\nduring the completed phase by\nthe participating threads of the CTA are performed and made visible to the executing thread.\nAll\ncp.async.bulk\nasynchronous operations using the same\nmbarrier object\nrequested prior,\nin program order, to\nmbarrier.arrive\nhaving release semantics during the completed\nphase by the participating threads of the CTA are performed and made visible to the executing thread.\nAll memory accesses requested after the\nmbarrier.test_wait\nor\nmbarrier.try_wait\n, in\nprogram order, are not performed and not visible to memory accesses performed prior to\nmbarrier.arrive\nhaving release semantics, in program order, by other threads\nparticipating in the\nmbarrier\n.\nThere is no ordering and visibility guarantee for memory accesses requested by the thread after\nmbarrier.arrive\nhaving release semantics and prior to\nmbarrier.test_wait\n,\nin program order.\nPTX ISA Notes\nmbarrier.test_wait\nintroduced in PTX ISA version 7.0.\nModifier\n.parity\nis introduced in PTX ISA version 7.1.\nmbarrier.try_wait\nintroduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nmbarrier.test_wait\nrequires\nsm_80\nor higher.\nmbarrier.try_wait\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n// Example 1a, thread synchronization with test_wait:\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.test_wait.shared.b64    complete, [shMem], %r1;\n@!complete nanosleep.u32 20;\n@!complete bra waitLoop;\n\n// Example 1b, thread synchronization with try_wait :\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.try_wait.relaxed.cluster.shared.b64    complete, [shMem], %r1;\n@!complete bra waitLoop;\n\n\n// Example 2, thread synchronization using phase parity :\n\n.reg .b32 i, parArg;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmov.b32 i, 0;\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nloopStart :                           // One phase per loop iteration\n    ...\n    mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads\n    ...\n    and.b32 parArg, i, 1;\n    waitLoop:\n    mbarrier.test_wait.parity.shared.b64  complete, [shMem], parArg;\n    @!complete nanosleep.u32 20;\n    @!complete bra waitLoop;\n    ...\n    add.u32 i, i, 1;\n    setp.lt.u32 p, i, IterMax;\n@p bra loopStart;\n\n\n// Example 3, Asynchronous copy completion waiting :\n\n.reg .b64 state;\n.shared .b64 shMem2;\n.shared .b64 shard1, shard2;\n.global .b64 gbl1, gbl2;\n\nmbarrier.init.shared.b64 [shMem2], threadCount;\n...\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n\n// Absence of .noinc accounts for arrive-on from prior cp.async operation\ncp.async.mbarrier.arrive.shared.b64 [shMem2];\n...\nmbarrier.arrive.shared.b64 state, [shMem2];\n\nwaitLoop:\nmbarrier.test_wait.shared::cta.b64 p, [shMem2], state;\n@!p bra waitLoop;\n\n// Example 4, Synchronizing the CTA0 threads with cluster threads\n.reg .b64 %r1, addr, remAddr;\n.shared .b64 shMem;\n\ncvta.shared.u64          addr, shMem;\nmapa.u64                 remAddr, addr, 0;     // CTA0's shMem instance\n\n// One thread from CTA0 executing the below initialization operation\n@p0 mbarrier.init.shared::cta.b64 [shMem], N;  // N = no of cluster threads\n\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Entire cluster executing the below arrive operation\nmbarrier.arrive.release.cluster.b64              _, [remAddr];\n\n// computation not requiring mbarrier synchronization ...\n\n// Only CTA0 threads executing the below wait operation\nwaitLoop:\nmbarrier.try_wait.parity.acquire.cluster.shared::cta.b64  complete, [shMem], 0;\n@!complete bra waitLoop;\n9.7.13.15.17.\nParallel Synchronization and Communication Instructions:\nmbarrier.pending_count\n\nmbarrier.pending_count\nQuery the pending arrival count from the opaque mbarrier state.\nSyntax\nmbarrier.pending_count.b64 count, state;\nDescription\nThe pending count can be queried from the opaque mbarrier state using\nmbarrier.pending_count\n.\nThe\nstate\noperand is a 64-bit register that must be the result of a prior\nmbarrier.arrive.noComplete\nor\nmbarrier.arrive_drop.noComplete\ninstruction. Otherwise, the\nbehavior is undefined.\nThe destination register\ncount\nis a 32-bit unsigned integer representing the pending count of\nthe\nmbarrier object\nprior to the\narrive-on operation\nfrom\nwhich the\nstate\nregister was obtained.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.reg .b32 %r1;\n.reg .b64 state;\n.shared .b64 shMem;\n\nmbarrier.arrive.noComplete.b64 state, [shMem], 1;\nmbarrier.pending_count.b64 %r1, state;\n9.7.13.16.\nParallel Synchronization and Communication Instructions:\ntensormap.cp_fenceproxy\n\ntensormap.cp_fenceproxy\nA fused copy and fence operation.\nSyntax\ntensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned  [dst], [src], size;\n\n.cp_qualifiers    = { .global.shared::cta }\n.fence_qualifiers = { .to_proxy::from_proxy.release.scope }\n.to_proxy::from_proxy  = { .tensormap::generic }\n.scope            = { .cta, .cluster, .gpu , .sys }\nDescription\nThe\ntensormap.cp_fenceproxy\ninstructions perform the following operations in order :\nCopies data of size specified by the\nsize\nargument, in bytes, from the location specified\nby the address operand\nsrc\nin shared memory to the location specified by the address operand\ndst\nin the global memory, in the generic proxy.\nEstablishes a\nuni-directional\nproxy release pattern on the ordering from the copy operation\nto the subsequent access performed in the tensormap proxy on the address\ndst\n.\nThe valid value of immediate operand\nsize\nis 128.\nThe operands\nsrc\nand\ndst\nspecify non-generic addresses in\nshared::cta\nand\nglobal\nstate space respectively.\nThe\n.scope\nqualifier specifies the set of threads that can directly observe the proxy\nsynchronizing effect of this operation, as described in\nMemory Consistency Model\n.\nThe mandatory\n.sync\nqualifier indicates that\ntensormap.cp_fenceproxy\ncauses the executing\nthread to wait until all threads in the warp execute the same\ntensormap.cp_fenceproxy\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\ntensormap.cp_fenceproxy\ninstruction. In conditionally executed code, an aligned\ntensormap.cp_fenceproxy\ninstruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n// Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor\n\n.reg .b64 new_addr;\n.global .align 128 .b8 gbl[128];\n.shared .align 128 .b8 sMem[128];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar];\n...\ntry_wait_loop:\nmbarrier.try_wait.shared.b64 p, [mbar], state;\n@!p bra try_wait loop;\n\ntensormap.replace.tile.global_address.shared.b1024.b64   [sMem], new_addr;\ntensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.release.gpu.sync.aligned\n                                                         [gbl], [sMem], 128;\nfence.proxy.tensormap::generic.acquire.gpu [gbl], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [gbl, {tc0}], [mbar0];\n9.7.13.17.\nParallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.try_cancel\n\nclusterlaunchcontrol.try_cancel\nRequests cancellation of cluster which is not launched yet.\nSyntax\nclusterlaunchcontrol.try_cancel.async{.space}.completion_mechanism{.multicast::cluster::all}.b128 [addr], [mbar];\n\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n.space = { .shared::cta };\nDescription\nThe\nclusterlaunchcontrol.try_cancel\ninstruction requests atomically cancelling the launch of\na cluster that has not started running yet. It asynchronously writes an opaque response to shared\nmemory indicating whether the operation succeeded or failed. The completion of the asynchronous\noperation is tracked using the mbarrier completion mechanism at\n.cluster\nscope.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nOn success, the opaque response contains the\nctaid\nof the first CTA of the canceled cluster; no\nother successful response from other\nclusterlaunchcontrol.try_cancel\noperations from the same\ngrid will contain that id.\nThe mandatory\n.async\nqualifier indicates that the instruction will initiate the cancellation\noperation asynchronously and control will return to the executing thread before the requested\noperation is complete.\nThe\n.space\nqualifier is specified, both operands\naddr\nand\nmbar\nmust be in the\n.shared::cta\nstate space. Otherwise, generic addressing will be assumed for both. The result\nis undefined if any of address operands do not fall within the address window of\n.shared::cta\n.\nThe qualifier\n.completion_mechanism\nspecifies that upon completion of the asynchronous operation,\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data stored in bytes, will be performed\non the mbarrier object specified by the operand\nmbar\n.\nThe executing thread can then use\nmbarrier instructions\nto wait for completion\nof the asynchronous operation. No other synchronization mechanisms described in\nMemory Consistency Model\ncan be used to guarantee the completion of the asynchronous copy operations.\nThe\n.multicast::cluster::all\nqualifier indicates that the response is asynchronously written using\nweak async-proxy writes to the corresponding local shared memory\naddr\nof each CTA in the requesting\ncluster. The completion of the writes to\naddr\nof a particular CTA is signaled via a complete-tx operation\nto the mbarrier object on the shared memory of that CTA.\nThe behavior of instruction with\n.multicast::cluster::all\nqualifier is undefined if any CTA in the\ncluster is exited.\nOperand\naddr\nspecifies the naturally aligned address of the 16-byte wide shared memory location where\nthe request’s response is written.\nThe response of\nclusterlaunchcontrol.try_cancel\ninstruction will be 16-byte opaque value and will be\nit available at location specified by operand\naddr\n. After loading this response into 16-byte register,\ninstruction\nclusterlaunchcontrol.query_cancel\ncan be used to check if request was successful and to\nretrieve\nctaid\nof the first CTA of the canceled cluster.\nIf the executing CTA has already observed the completion of a\nclusterlaunchcontrol.try_cancel\ninstruction\nas failed, then the behavior of issuing a subsequent\nclusterlaunchcontrol.try_cancel\ninstruction is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nQualifier\n.multicast::cluster::all\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Assumption: 1D cluster (cluster_ctaid.y/.z == 1) with 1 thread per CTA.\n\n// Current Cluster to be processed: initially the launched cluster:\nmov.b32 xctaid, %ctaid.x;\n\n// Establish full synchronization across all CTAs of the cluster for the first iteration.\n// Weaker synchronization may suffice depending on initialization sequence.\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Iteration loop over all cluster CTAs\nprocessCluster:\n  mov.u32  %r0, %tid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  // Elect a leader thread (thread idx 0) for each CTA to arrive and expect_tx at\n  // each CTA local shared memory barrier:\n  mov.u32  %r0, %tid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  // All other threads skip to processing the work of the current cluster:\n  @!p0 bra processCurrentCluster;\n\n  // All CTAs in the cluster arrive at their local SMEM barrier and set 16B handle tx count:\n  mbarrier.arrive.expect_tx.cluster.relaxed.shared::cta.b64 state, [mbar], 16;\n\n  // First CTA in Cluster attempts to cancel a not-yet-started cluster:\n  mov.u32  %r0, %cluster_ctaid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  @p0 clusterlaunchcontrol.try_cancel.async.mbarrier::complete_tx::bytes.multicast::cluster::all.b128 [addr], [mbar];\n\n  processCurrentCluster:\n    // ...process current cluster (\"xctaid\") while cancellation request for next cluster runs asynchronously...\n\n  // After processing current cluster, wait on cancellation request response for next cluster via specified mbarrier:\n  waitLoop:\n    // .acquire prevents weak handle read (\"ld.shared handle, [addr]\") from overtaking this mbarrier.try_wait:\n    mbarrier.try_wait.cluster.acquire.shared::cta.b64   complete, [mbar], state;\n    @!complete bra waitLoop;\n   // Cancellation request has completed.\n\n  // Generic-proxy weak read of cancellation request into 16-byte wide register:\n  ld.shared.b128 handle, [addr];\n\n  // Check whether cancellation succeeded:\n  clusterlaunchcontrol.query_cancel.is_canceled.pred.b128 p, handle;\n  // If cancellation request failed, we couldn't cancel any other cluster, so all current cluster CTAs exit.\n  @!p ret;\n\n  // Otherwise, cancellation request succeeded.\n  // Extract \"ctaid\" of first cancelled-cluster CTA which we'll process in next \"processCluster\" loop iteration:\n  @p clusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xctaid, _, _, _},  handle;\n\n  // Release current iteration generic-proxy weak read of handle (\"ld.shared handle, [addr]\")\n  // before next iteration async-proxy write to handle (\"clusterlaunchcontrol.try_cancel [addr]\")\n  fence.proxy.async::generic.release.sync_restrict::shared::cta.cluster;\n\n  // Arrive and wait at the next iteration cluster barrier with relaxed semantics.\n  barrier.cluster.arrive.relaxed;\n  barrier.cluster.wait;\n\n  // Acquire prior iteration generic-proxy weak read of handle (\"ld.shared handle, [addr]\")\n  // before current iteration async-proxy write to handle (\"clusterlaunchcontrol.try_cancel [addr]\")\n  fence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\n\n  bra processCluster;\n9.7.13.18.\nParallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.query_cancel\n\nclusterlaunchcontrol.query_cancel\nQueries response of\nclusterlaunchcontrol.try_cancel\noperation.\nSyntax\nclusterlaunchcontrol.query_cancel.is_canceled.pred.b128 pred, try_cancel_response;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xdim, ydim, zdim, _},  try_cancel_response;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid{::dimension}.b32.b128 reg, try_cancel_response;\n\n::dimension = { ::x, ::y, ::z };\nDescription\nInstruction\nclusterlaunchcontrol.query_cancel\ncan be used to decode opaque response\nwritten by instruction\nclusterlaunchcontrol.try_cancel\n.\nAfter loading response from\nclusterlaunchcontrol.try_cancel\ninstruction into 16-byte\nregister it can be further queried using\nclusterlaunchcontrol.query_cancel\ninstruction\nas follows:\nclusterlaunchcontrol.query_cancel.is_canceled.pred.b128\n: If the cluster is canceled\nsuccessfully, predicate\np\nis set to\ntrue\n; otherwise, it is set to\nfalse\n.\nIf the request succeeded, the instruction\nclusterlaunchcontrol.query_cancel.get_first_ctaid\nextracts the CTA id of the first CTA in the canceled cluster. By default, the instruction\nreturns a\n.v4\nvector whose first three elements are the\nx\n,\ny\nand\nz\ncoordinate\nof first CTA in canceled cluster. The contents of the 4th element are unspecified. The\nexplicit\n.get_first_ctaid::x\n,\n.get_first_ctaid::y\n, or\n.get_first_ctaid::z\nqualifiers can be used to extract individual\nx\n,\ny\nor\nz\ncoordinates into a 32-bit\nregister.\nIf the request fails the behavior of\nclusterlaunchcontrol.query_cancel.get_first_ctaid\nis undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nExamples\nclusterlaunchcontrol.query_cancel.is_canceled pred.b128 p, handle;\n\n@p clusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xdim, ydim, zdim, ignr}  handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::x.b32.b128 reg0, handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::y.b32.b128 reg1, handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::z.b32.b128 reg2, handle;\n9.7.14.\nWarp Level Matrix Multiply-Accumulate Instructions\n\nThe matrix multiply and accumulate operation has the following form:\nD = A * B + C\nwhere\nD\nand\nC\nare called accumulators and may refer to the same matrix.\nPTX provides two ways to perform matrix multiply-and-accumulate computation:\nUsing\nwmma\ninstructions:\nThis warp-level computation is performed collectively by all threads in the warp as follows:\nLoad matrices A, B and C from memory into registers using the\nwmma.load\noperation. When\nthe operation completes, the destination registers in each thread hold a fragment of the\nloaded matrix.\nPerform the matrix multiply and accumulate operation using the\nwmma.mma\noperation on the\nloaded matrices. When the operation completes, the destination registers in each thread hold\na fragment of the result matrix returned by the\nwmma.mma\noperation.\nStore result Matrix D back to memory using the\nwmma.store\noperation. Alternately, result\nmatrix D can also be used as argument C for a subsequent\nwmma.mma\noperation.\nThe\nwmma.load\nand\nwmma.store\ninstructions implicitly handle the organization of matrix\nelements when loading the input matrices from memory for the\nwmma.mma\noperation and when\nstoring the result back to memory.\nUsing\nmma\ninstruction:\nSimilar to\nwmma\n,\nmma\nalso requires computation to be performed collectively by all\nthreads in the warp however distribution of matrix elements across different threads in warp\nneeds to be done explicitly before invoking the\nmma\noperation. The\nmma\ninstruction\nsupports both dense as well as sparse matrix A. The sparse variant can be used when A is a\nstructured sparse matrix as described in\nSparse matrix storage\n.\n9.7.14.1.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and C. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\n, where A is an\nMxK\nmatrix, B is a\nKxN\nmatrix, while C and D are\nMxN\nmatrices.\nThe following matrix shapes are supported for the specified types:\nInstruction\nScale\nSparsity\nMultiplicand Data-type\nShape\nPTX ISA version\nwmma\nNA\nDense\nFloating-point -\n.f16\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 6.0\nwmma\nDense\nAlternate floating-point format -\n.bf16\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 7.0\nwmma\nDense\nAlternate floating-point format -\n.tf32\n.m16n16k8\nPTX ISA version 7.0\nwmma\nDense\nInteger -\n.u8\n/\n.s8\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 6.3\nwmma\nDense\nSub-byte integer -\n.u4\n/\n.s4\n.m8n8k32\nPTX ISA version 6.3\nwmma\nDense\nSingle-bit -\n.b1\n.m8n8k128\nPTX ISA version 6.3\nmma\nNA\nDense\nFloating-point -\n.f64\n.m8n8k4\nPTX ISA version 7.0\n.m16n8k4\n,\n.m16n8k8\n,\nand\n.m16n8k16\nPTX ISA version 7.8\nmma\nDense\nFloating-point -\n.f16\n.m8n8k4\nPTX ISA version 6.4\n.m16n8k8\nPTX ISA version 6.5\n.m16n8k16\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.bf16\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.tf32\n.m16n8k4\nand\n.m16n8k8\nPTX ISA version 7.0\nmma\nDense\nInteger -\n.u8\n/\n.s8\n.m8n8k16\nPTX ISA version 6.5\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.0\nmma\nDense\nSub-byte integer -\n.u4\n/\n.s4\n.m8n8k32\nPTX ISA version 6.5\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 7.0\nmma\nDense\nSingle-bit -\n.b1\n.m8n8k128\n,\n.m16n8k128\n,\nand\n.m16n8k256\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k32\nPTX ISA version 8.4\nmma\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k16\nPTX ISA version 8.7\nmma\nDense\nAlternate floating-point format -\n.e3m2\n/\n.e2m3\n/\n.e2m1\n.m16n8k32\nPTX ISA version 8.7\nmma\nYes\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nX\n(Scale)\n.ue8m0\n.m16n8k32\nPTX ISA version 8.7\nmma\nDense\nAlternate floating-point format -\n.e2m1\nX\n(Scale)\n.ue8m0\n/\n.ue4m3\n.m16n8k64\nPTX ISA version 8.7\nmma\nNA\nSparse\nFloating-point -\n.f16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.bf16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.tf32\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 7.1\nmma\nSparse\nInteger -\n.u8\n/\n.s8\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 7.1\nmma\nSparse\nSub-byte integer -\n.u4\n/\n.s4\n.m16n8k64\nand\n.m16n8k128\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k64\nPTX ISA version 8.4\nmma\nSparse\nwith\nordered\nmetadata\nFloating-point -\n.f16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.bf16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.tf32\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nInteger -\n.u8\n/\n.s8\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nSub-byte integer -\n.u4\n/\n.s4\n.m16n8k64\nand\n.m16n8k128\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k64\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e3m2\n/\n.e2m3\n/\n.e2m1\n.m16n8k64\nPTX ISA version 8.7\nmma\nYes\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nX\n(Scale)\n.ue8m0\n.m16n8k64\nPTX ISA version 8.7\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e2m1\nX\n(Scale)\n.ue8m0\n/\n.ue4m3\n.m16n8k128\nPTX ISA version 8.7\n9.7.14.2.\nMatrix Data-types\n\nThe matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point.\nFor floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later.\nData-type\nMultiplicands (A or B)\nAccumulators (C or D)\nInteger\n.u8\n,\n.s8\n.s32\nFloating Point\n.f16\n.f16\n,\n.f32\nAlternate floating Point\n.bf16\n.f32\nAlternate floating Point\n.tf32\n.f32\nAlternate floating Point\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n.f16\n,\n.f32\nAlternate floating Point\nwith scale\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\nX (Scale)\n.ue8m0\n.f32\nAlternate floating Point\nwith scale\n.e2m1\nX (Scale)\n.ue8m0\nor\n.ue4m3\n.f32\nFloating Point\n.f64\n.f64\nSub-byte integer\nboth\n.u4\nor both\n.s4\n.s32\nSingle-bit integer\n.b1\n.s32\n9.7.14.3.\nBlock Scaling for\nmma.sync\n\nThe\nmma\ninstruction with the following\n.kind\nqualifier:\n.kind::mxf8f6f4\n.kind::mxf4\n.kind::mxf4nvf4\nperform matrix multiplication with block scaling. This operation has the following form:\nD\n=\n(A\n*\nscale_A)\n*\n(B\n*\nscale_B)\n+\nC\n.\nFor a\nscale_A\nmatrix of shape\nM x SFA_N\n, each row of matrix\nA\nis divided into\nSFA_N\nnumber of chunks and each chunk of a row is multiplied with the corresponding\nelement (henceforth referred as\nSF_A\n) from the same row of\nscale_A\n.\nSimilarly, for a\nscale_B\nmatrix of shape\nSFB_M x N\n, each column of matrix\nB\nis\ndivided into the\nSFB_M\nnumber of chunks and each chunk of a column is multiplied with\nthe corresponding element (henceforth referred as\nSF_B\n) from the same column of\nscale_B\n.\nFigure 42\nshows an example of\nmma\nwith block scaling of\nscale_vec::2X\n.\nFigure 42\nmma\nwith block scaling of\n.scale_vec::2X\n\nThe shapes for\nscale_A\nand\nscale_B\nmatrices depend upon the qualifier\n.scale_vec_size\nas shown in\nTable 35\n.\nTable 35\nShapes for scale matrices depending upon\n.scale_vec_size\nqualifier\n\n.scale_vec_size\nShape of scale_A\nShape of scale_B\n.scale_vec::1X\nM x 1\n1 x N\n.scale_vec::2X\nM x 2\n2 x N\n.scale_vec::4X\nM x 4\n4 x N\nThe valid combination of the exact element types and the\n.scale_vec_size\nare listed in\nTable 36\n.\nTable 36\nValid combinations of\n.scale_vec_size\nand\n.kind\nqualifier\n\n.kind::*\nElement Data Type\n.atype and .btype\nScale Data Type\n.stype\n.scale_vec_size\n.kind::mxf8f6f4\n.e4m3\n,\n.e5m2\n.e3m2\n,\n.e2m3\n.e2m1\n.ue8m0\n.scale_vec::1X\n.kind::mxf4\n.e2m1\n.ue8m0\n.scale_vec::2X\n.kind::mxf4nvf4\n.e2m1\n.ue8m0\n.scale_vec::2X\n,\n.scale_vec::4X\n.e2m1\n.ue4m3\n.scale_vec::4X\nThe\nscale-a-data\nand\nscale-b-data\nargument provides metadata for\nscale_A\nand\nscale_B\nmatrices respectively. The tuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nprovides the selector information to choose elements\nSF_A\nand\nSF_B\nfrom corresponding metadata arguments\nscale-a-data\nand\nscale-b-data\n.\nThe tuple\n{byte-id-a,\nthread-id-a}\nallows to select the scale matrix element\nSF_A\nfrom\nscale-a-data\n. Similarly, the tuple\n{byte-id-b,\nthread-id-b}\nallows to select\nthe scale matrix element\nSF_B\nfrom\nscale-b-data\n.\nThe components\nthread-id-a\n,\nthread-id-b\ndecides which threads among the quad\ncontribute the\nSF_A\nand\nSF_B\nvalues. The following listing describes the impact\nof thread selector component\nthread-id-a\n,\nthread-id-b\n:\nOne thread-pair within the quad determined by\nthread-id-a\n, contributes the\nSF_A\nvalues. The value of 0 selects lower two threads whereas value of 1 selects upper two\nthreads from the quad. In other words, when\nthread-id-a\nset to 0, thread-pair\nsatisfying:\n%laneid\n% 4 == 0 or 1 provides the\nSF_A\n. In contrast when\nthread-id-a\nset to 1, thread-pair satisfying:\n%laneid\n% 4 == 2 or 3 provides\nthe\nSF_A\n. Refer\nFigure 43\nfor more details.\nFigure 43\nSelection of set of values for\nSF_A\nbased on\nthread-id-a\n\nOne thread within the quad, determined by\nthread-id-b\n, contributes the\nSF_B\nvalue. In other words, each thread satisfying:\n%laneid\n% 4 ==\nthread-id-b\nprovides the\nSF_B\n. Refer\nFigure 44\nfor more details.\nFigure 44\nSelection of set of values for\nSF_B\nbased on\nthread-id-b\n\nThe arguments\nbyte-id-a\n,\nbyte-id-b\nselects which bytes from the\nscale-a-data\n,\nscale-b-data\ncontribute the\nSF_A\nand\nSF_B\nvalues. The following listing describes\nimplications of\n.scale_vec_size\nqualifier on byte selector component\nbyte-id-a\n,\nbyte-id-b\n:\nWhen\n.scale_vec_size\nis\n.scale_vec::1X\nOne byte each within\nscale-a-data\nand\nscale-b-data\ndetermined by\nbyte-id-a\n,\nbyte-id-b\nrespectively contributes the\nSF_A\nand\nSF_B\nvalues.\nWhen\n.scale_vec_size\nis\n.scale_vec::2X\nOne byte-pair (two bytes) within\nscale-a-data\nand\nscale-b-data\ndetermined by\nbyte-id-a\nand\nbyte-id-b\ncontributes the\nSF_A\nand\nSF_B\nvalues. The value\nof 0 selects lower two bytes whereas value of 2 selects upper two bytes from the\ncorresponding metadata value.\nWhen\n.scale_vec_size\nis\n.scale_vec::4X\nAll four bytes within\nscale-a-data\nand\nscale-b-data\ncontribute the values.\nHence,\nbyte-id-a\n,\nbyte-id-b\nmust be zero.\nRefer\nFigure 45\nfor more details.\nFigure 45\nSelection of set of values for\nSF_A\nor\nSF_B\nbased on\nbyte-id-a\nor\nbyte-id-b\n\nTable 37\nenumerates the valid values for\nvarious selector components. Any other value results in an undefined behavior.\nTable 37\nValid values for various selector components\n\n.scale_vec_size\nSelector Components\nbyte-id-a\nthread-id-a\nbyte-id-b\nthread-id-b\nscale_vec::1X\n[0, 1, 2, 3]\n[0, 1]\n[0, 1, 2, 3]\n[0, 1, 2, 3]\nscale_vec::2X\n[0, 2]\n[0, 2]\nscale_vec::4X\n0\n0\n9.7.14.4.\nMatrix multiply-accumulate operation using\nwmma\ninstructions\n\nThis section describes warp level\nwmma.load,\nwmma.mma\nand\nwmma.store\ninstructions and the\norganization of various matrices invovled in these instruction.\n9.7.14.4.1.\nMatrix Fragments for WMMA\n\nEach thread in the warp holds a fragment of the matrix. The distribution of fragments loaded by the\nthreads in a warp is unspecified and is target architecture dependent, and hence the identity of the\nfragment within the matrix is also unspecified and is target architecture dependent. The fragment\nreturned by a\nwmma\noperation can be used as an operand for another\nwmma\noperation if the\nshape, layout and element type of the underlying matrix matches. Since fragment layout is\narchitecture dependent, using the fragment returned by a\nwmma\noperation in one function as an\noperand for a\nwmma\noperation in a different function may not work as expected if the two\nfunctions are linked together but were compiled for different link-compatible SM architectures. Note\npassing\nwmma\nfragment to a function having\n.weak\nlinkage is unsafe since at link time\nreferences to such function may get resolved to a function in different compilation module.\nEach fragment is a vector expression whose contents are determined as follows. The identity of\nindividual matrix elements in the fragment is unspecified.\nInteger fragments\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.u8\nor\n.s8\n.m16n16k16\nA\nA vector expression of two\n.b32\nregisters, with each\nregister containing four elements from the matrix.\nB\nA vector expression of two\n.b32\nregisters, with each\nregister containing four elements from the matrix.\n.m8n32k16\nA\nA vector expression containing a single\n.b32\nregister\ncontaining four elements from the matrix.\nB\nA vector expression of four\n.b32\nregisters, with each\nregister containing four elements from the matrix.\n.m32n8k16\nA\nA vector expression of four\n.b32\nregisters, with each\nregister containing four elements from the matrix.\nB\nA vector expression containing single\n.b32\nregister,\nwith each containing four elements from the matrix.\nAccumulators (C or D):\nData-type\nShape\nFragment\n.s32\n.m16n16k16\nA vector expression of eight\n.s32\nregisters.\n.m8n32k16\n.m32n8k16\nFloating point fragments\nData-type\nMatrix\nFragment\n.f16\nA or B\nA vector expression of eight\n.f16x2\nregisters.\n.f16\nC or D\nA vector expression of four\n.f16x2\nregisters.\n.f32\nA vector expression of eight\n.f32\nregisters.\nFloating point fragments for\n.bf16\ndata format\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.bf16\n.m16n16k16\nA\nA vector expression of four\n.b32\nregisters, with each\nregister containing two elements from the matrix.\nB\n.m8n32k16\nA\nA vector expression containing a two\n.b32\nregisters,\nwith containing two elements from the matrix.\nB\nA vector expression of eight\n.b32\nregisters, with\neach register containing two elements from the matrix.\n.m32n8k16\nA\nA vector expression of eight\n.b32\nregisters, with\neach register containing two elements from the matrix.\nB\nA vector expression containing two\n.b32\nregisters,\nwith each containing two elements from the matrix.\nAccumulators (C or D):\nData-type\nMatrix\nFragment\n.f32\nC or D\nA vector expression containing eight\n.f32\nregisters.\nFloating point fragments for\n.tf32\ndata format\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.tf32\n.m16n16k8\nA\nA vector expression of four\n.b32\nregisters.\nB\nA vector expression of four\n.b32\nregisters.\nAccumulators (C or D):\nData-type\nShape\nMatrix\nFragment\n.f32\n.m16n16k8\nC or D\nA vector expression containing eight\n.f32\nregisters.\nDouble precision floating point fragments\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.f64\n.m8n8k4\nA or B\nA vector expression of single\n.f64\nregister.\nAccumulators (C or D):\nData-type\nShape\nMatrix\nFragment\n.f64\n.m8n8k4\nC or D\nA vector expression containing single\n.f64\nregister.\nSub-byte integer and single-bit fragments\nMultiplicands (A or B):\nData-type\nShape\nFragment\n.u4\nor\n.s4\n.m8n8k32\nA vector expression containing a single\n.b32\nregister, containing eight elements from the matrix.\n.b1\n.m8n8k128\nA vector expression containing a single\n.b32\nregister, containing 32 elements from the matrix.\nAccumulators (C or D):\nData-type\nShape\nFragment\n.s32\n.m8n8k32\nA vector expression of two\n.s32\nregisters.\n.m8n8k128\nA vector expression of two\n.s32\nregisters.\nManipulating fragment contents\nThe contents of a matrix fragment can be manipulated by reading and writing to individual\nregisters in the fragment, provided the following conditions are satisfied:\nAll matrix element in the fragment are operated on uniformly across threads, using the same\nparameters.\nThe order of the matrix elements is not changed.\nFor example, if each register corresponding to a given matrix is multiplied by a uniform constant\nvalue, then the resulting matrix is simply the scaled version of the original matrix.\nNote that type conversion between\n.f16\nand\n.f32\naccumulator fragments is not supported in\neither direction. The result is undefined even if the order of elements in the fragment remains\nunchanged.\n9.7.14.4.2.\nMatrix Storage for WMMA\n\nEach matrix can be stored in memory with a\nrow-major\nor\ncolumn-major\nlayout. In a\nrow-major\nformat, consecutive elements of each row are stored in contiguous memory locations, and the row is\ncalled the\nleading dimension\nof the matrix. In a\ncolumn-major\nformat, consecutive elements of\neach column are stored in contiguous memory locations and the column is called the\nleading\ndimension\nof the matrix.\nConsecutive instances of the\nleading dimension\n(rows or columns) need not be stored contiguously\nin memory. The\nwmma.load\nand\nwmma.store\noperations accept an optional argument\nstride\nthat specifies the offset from the beginning of each row (or column) to the next, in terms of matrix\nelements (and not bytes). For example, the matrix being accessed by a\nwmma\noperation may be a\nsubmatrix from a larger matrix stored in memory. This allows the programmer to compose a\nmultiply-and-accumulate operation on matrices that are larger than the shapes supported by the\nwmma\noperation.\nAddress Alignment\nThe starting address of each instance of the leading dimension (row or column) must be aligned\nwith the size of the corresponding fragment in bytes. Note that the starting address is\ndetermined by the base pointer and the optional\nstride\n.\nConsider the following instruction as an example:\nwmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s;\nFragment size in bytes = 32 (eight elements of type\n.f16x2\n)\nActual\nstride\nin bytes = 2 *\ns\n(since\nstride\nis specified in terms of\n.f16\nelements, not bytes)\nFor each row of this matrix to be aligned at fragment size the following must be true:\np\nis a multiple of 32.\n2*s\nis a multiple of 32.\nDefault value for stride\nThe default value of the\nstride\nis the size of the\nleading dimension\nof the matrix. For\nexample, for an\nMxK\nmatrix, the\nstride\nis\nK\nfor a\nrow-major\nlayout and\nM\nfor a\ncolumn-major\nlayout. In particular, the default strides for the supported matrix shapes are as\nfollows:\nShape\nA (row)\nA (column)\nB (row)\nB (column)\nAccumulator (row)\nAccumulator (column)\n16x16x16\n16\n16\n16\n16\n16\n16\n8x32x16\n16\n8\n32\n16\n32\n8\n32x8x16\n16\n32\n8\n16\n8\n32\n8x8x32\n32\n8\n8\n32\n8\n8\n8x8x128\n128\n8\n8\n128\n8\n8\n16x16x8\n8\n16\n16\n8\n16\n16\n8x8x4\n4\n8\n8\n4\n8\n8\n9.7.14.4.3.\nWarp-level Matrix Load Instruction:\nwmma.load\n\nwmma.load\nCollectively load a matrix from memory for WMMA\nSyntax\nFloating point format\n.f16\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride};\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride};\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f16, .s8, .u8};\n.btype  = {.f16, .s8, .u8};\n.ctype  = {.f16, .f32, .s32};\nAlternate floating point format\n.bf16\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.bf16 };\n.btype  = {.bf16 };\n.ctype  = {.f32 };\nAlternate floating point format\n.tf32\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.tf32 };\n.btype  = {.tf32 };\n.ctype  = {.f32 };\nDouble precision Floating point\n.f64\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f64 };\n.btype  = {.f64 };\n.ctype  = {.f64 };\nSub-byte loads:\nwmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\n.ctype  = {.s32};\nSingle-bit loads:\nwmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.b1};\n.btype  = {.b1};\n.ctype  = {.s32};\nDescription\nCollectively load a matrix across all threads in a warp from the location indicated by address\noperand\np\nin the specified state space into destination register\nr\n.\nIf no state space is given, perform the memory accesses using\nGeneric Addressing\n.\nwmma.load\noperation may be used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\nThe mutually exclusive qualifiers\n.a\n,\n.b\nand\n.c\nindicate whether matrix A, B or C is\nbeing loaded respectively for the\nwmma\ncomputation.\nThe destination operand\nr\nis a brace-enclosed vector expression that can hold the fragment\nreturned by the load operation, as described in\nMatrix Fragments for WMMA\n.\nThe\n.shape\nqualifier indicates the dimensions of all the matrix arguments involved in the\nintended\nwmma\ncomputation.\nThe\n.layout\nqualifier indicates whether the matrix to be loaded is stored in\nrow-major\nor\ncolumn-major\nformat.\nstride\nis an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the\nleading dimension\n(rows or columns). The default\nvalue of\nstride\nis described in\nMatrix Storage for WMMA\nand must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior.\nThe required alignment for address\np\nand\nstride\nis described in the\nMatrix Storage for WMMA\n.\nThe mandatory\n.sync\nqualifier indicates that\nwmma.load\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.load\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.load\ninstruction. In conditionally executed code, a\nwmma.load\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.load\nis undefined if all threads do not use the same qualifiers and the\nsame values of\np\nand\nstride\n, or if any thread in the warp has exited.\nwmma.load\nis treated as a\nweak\nmemory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\n.m8n8k4\nand\n.m16n16k8\non\nwmma\nintroduced in PTX ISA version 7.0.\nDouble precision and alternate floating point precision\nwmma\nintroduced in PTX ISA version 7.0.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision and alternate floating point precision\nwmma\nrequires\nsm_80\nor higher.\nExamples\n// Load elements from f16 row-major matrix B\n.reg .b32 x<8>;\n\nwmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr];\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from f32 column-major matrix C and scale the values:\n.reg .b32 x<8>;\n\nwmma.load.c.sync.aligned.m16n16k16.col.f32\n                 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr];\n\nmul.f32 x0, x0, 0.1;\n// repeat for all registers x<8>;\n...\nmul.f32 x7, x7, 0.1;\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from integer matrix A:\n.reg .b32 x<4>\n// destination registers x<4> contain four packed .u8 values each\nwmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr];\n\n// Load elements from sub-byte integer matrix A:\n.reg .b32 x0;\n// destination register x0 contains eight packed .s4 values\nwmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr];\n\n// Load elements from .bf16 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k16.row.bf16\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .tf32 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k8.row.tf32\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .f64 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m8n8k4.row.f64\n                {x0}, [ptr];\n9.7.14.4.4.\nWarp-level Matrix Store Instruction:\nwmma.store\n\nwmma.store\nCollectively store a matrix into memory for WMMA\nSyntax\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f16, .f32, .s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32, .m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.type   = {.s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.type   = {.f64};\nDescription\nCollectively store a matrix across all threads in a warp at the location indicated by address\noperand\np\nin the specified state space from source register\nr\n.\nIf no state space is given, perform the memory accesses using\nGeneric Addressing\n.\nwmma.load\noperation may be used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\nThe source operand\nr\nis a brace-enclosed vector expression that matches the shape of the\nfragment expected by the store operation, as described in\nMatrix Fragments for WMMA\n.\nThe\n.shape\nqualifier indicates the dimensions of all the matrix arguments involved in the\nintended\nwmma\ncomputation. It must match the\n.shape\nqualifier specified on the\nwmma.mma\ninstruction that produced the D matrix being stored.\nThe\n.layout\nqualifier indicates whether the matrix to be loaded is stored in\nrow-major\nor\ncolumn-major\nformat.\nstride\nis an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the\nleading dimension\n(rows or columns). The default\nvalue of\nstride\nis described in\nMatrix Storage for WMMA\nand must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior.\nThe required alignment for address\np\nand\nstride\nis described in the\nMatrix Storage for WMMA\n.\nThe mandatory\n.sync\nqualifier indicates that\nwmma.store\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.store\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.store\ninstruction. In conditionally executed code, a\nwmma.store\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.store\nis undefined if all threads do not use the same qualifiers and the\nsame values of\np\nand\nstride\n, or if any thread in the warp has exited.\nwmma.store\nis treated as a\nweak\nmemory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\n.m16n16k8\nintroduced in PTX ISA version 7.0.\nDouble precision\nwmma\nintroduced in PTX ISA version 7.0.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision\nwmma\nand shape\n.m16n16k8\nrequires\nsm_80\nor higher.\nExamples\n// Storing f32 elements computed by a wmma.mma\n.reg .b32 x<8>;\n\nwmma.mma.sync.m16n16k16.row.col.f32.f32\n              {d0, d1, d2, d3, d4, d5, d6, d7}, ...;\nwmma.store.d.sync.m16n16k16.row.f32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m16n16k16 shape:\n.reg .b32 d<8>;\nwmma.store.d.sync.aligned.m16n16k16.row.s32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m8n8k128 shape:\n.reg .b32 d<2>\nwmma.store.d.sync.aligned.m8n8k128.row.s32\n[ptr], {d0, d1};\n\n// Store f64 accumulator for m8n8k4 shape:\n.reg .f64 d<2>;\nwmma.store.d.sync.aligned.m8n8k4.row.f64\n              [ptr], {d0, d1};\n9.7.14.4.5.\nWarp-level Matrix Multiply-and-Accumulate Instruction:\nwmma.mma\n\nwmma.mma\nPerform a single matrix multiply-and-accumulate operation across a warp\nSyntax\n// Floating point (.f16 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype d, a, b, c;\n\n// Integer (.u8/.s8 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape  =  {.m16n16k16, .m8n32k16, .m32n8k16};\n.dtype   = {.f16, .f32};\n.atype   = {.s8, .u8};\n.btype   = {.s8, .u8};\n.ctype   = {.f16, .f32};\nFloating point format\n.bf16\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k16, .m8n32k16, .m32n8k16};\n.atype   = {.bf16 };\n.btype   = {.bf16};\nFloating point format\n.tf32\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k8 };\n.atype   = {.tf32 };\n.btype   = {.tf32};\nFloating point Double precision\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape{.rnd}.f64.f64.f64.f64 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m8n8k4 };\n.rnd = { .rn, .rz, .rm, .rp };\nSub-byte (\n.u4\n/\n.s4\nmultiplicands)\nwmma.mma\n:\nwmma.mma.sync.aligned.row.col.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n.shape  = {.m8n8k32};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\nSingle-bit (\n.b1\nmultiplicands)\nwmma.mma\n:\nwmma.mma.op.popc.sync.aligned.row.col.shape.s32.atype.btype.s32 d, a, b, c;\n.shape  = {.m8n8k128};\n.atype  = {.b1};\n.btype  = {.b1};\n.op     = {.xor, .and}\nDescription\nPerform a warp-level matrix multiply-and-accumulate computation\nD\n=\nA\n*\nB\n+\nC\nusing matrices A,\nB and C loaded in registers\na\n,\nb\nand\nc\nrespectively, and store the result matrix in\nregister\nd\n. The register arguments\na\n,\nb\n,\nc\nand\nd\nhold unspecified fragments of\nthe corresponding matrices as described in\nMatrix Fragments for WMMA\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively.\nFor\nwmma.mma\nwithout explicit\n.atype\nand\n.btype\n:\n.atype\nand\n.btype\nare\nimplicitly set to\n.f16\n.\nFor integer\nwmma\n,\n.ctype\nand\n.dtype\nmust be specified as\n.s32\n. Also, the values for\n.atype\nand\n.btype\nmust be the same, i.e., either both are\n.s8\nor both are\n.u8\n.\nFor sub-byte single-bit\nwmma\n,\n.ctype\nand\n.dtype\nmust be specified as\n.s32\n. Also, the\nvalues for\n.atype\nand\n.btype\nmust be the same; i.e., either both are\n.s4\n, both are\n.u4\n, or both are\n.b1\n.\nFor single-bit\nwmma\n, multiplication is replaced by a sequence of logical operations;\nspecifically,\nwmma.xor.popc\nand\nwmma.and.popc\ncomputes the XOR, AND respectively of a\n128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result\n(\npopc\n). This result is added to the corresponding element of C and written into D.\nThe qualifiers\n.alayout\nand\n.blayout\nmust match the layout specified on the\nwmma.load\ninstructions that produce the contents of operands\na\nand\nb\nrespectively. Similarly, the\nqualifiers\n.atype\n,\n.btype\nand\n.ctype\nmust match the corresponding qualifiers on the\nwmma.load\ninstructions that produce the contents of operands\na\n,\nb\nand\nc\nrespectively.\nThe\n.shape\nqualifier must match the\n.shape\nqualifier used on the\nwmma.load\ninstructions\nthat produce the contents of all three input operands\na\n,\nb\nand\nc\nrespectively.\nThe destination operand\nd\nis a brace-enclosed vector expression that matches the\n.shape\nof\nthe fragment computed by the\nwmma.mma\ninstruction.\nSaturation at the output:\nThe optional qualifier\n.satfinite\nindicates that the final values in the destination register\nare saturated as follows:\nThe output is clamped to the minimum or maximum 32-bit signed integer value. Otherwise, if the\naccumulation would overflow, the value wraps.\nPrecision and rounding for\n.f16\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with\nat least single precision. When both\n.ctype\nand\n.dtype\nare specified as\n.f16\n, the\naccumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs is unspecified.\nPrecision and rounding for\n.bf16\n,\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding and handling of subnormal inputs is unspecified.\nRounding modifiers on double precision\nwmma.mma\n(default is\n.rn\n):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe mandatory\n.sync\nqualifier indicates that\nwmma.mma\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.mma\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.mma\ninstruction. In conditionally executed code, a\nwmma.mma\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.mma\nis undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\nDouble precision and alternate floating point precision\nwmma\nintroduced in PTX ISA version 7.0.\nSupport for\n.and\noperation in single-bit\nwmma\nintroduced in PTX ISA version 7.1.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n.satfinite\non floating point\nwmma.mma\nis deprecated in PTX ISA version 6.4 and\nis removed from PTX ISA version 6.5.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision, alternate floating point precision\nwmma\nrequire\nsm_80\nor higher.\n.and\noperation in single-bit\nwmma\nrequires\nsm_80\nor higher.\nExamples\n.global .align 32 .f16 A[256], B[256];\n.global .align 32 .f32 C[256], D[256];\n.reg .b32 a<8> b<8> c<8> d<8>;\n\nwmma.load.a.sync.aligned.m16n16k16.global.row.f16\n        {a0, a1, a2, a3, a4, a5, a6, a7}, [A];\nwmma.load.b.sync.aligned.m16n16k16.global.col.f16\n        {b0, b1, b2, b3, b4, b5, b6, b7}, [B];\n\nwmma.load.c.sync.aligned.m16n16k16.global.row.f32\n        {c0, c1, c2, c3, c4, c5, c6, c7}, [C];\n\nwmma.mma.sync.aligned.m16n16k16.row.col.f32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3, a4, a5, a6, a7},\n        {b0, b1, b2, b3, b4, b5, b6, b7},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\nwmma.store.d.sync.aligned.m16n16k16.global.col.f32\n        [D], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Compute an integer WMMA:\n.reg .b32  a, b<4>;\n.reg .b32 c<8>, d<8>;\nwmma.mma.sync.aligned.m8n32k16.row.col.s32.s8.s8.s32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a}, {b0, b1, b2,  b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\n// Compute sub-byte WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute single-bit type WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.xor.popc.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute double precision wmma\n.reg .f64 a, b, c<2>, d<2>;\nwmma.mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute alternate floating point precision wmma\n.reg .b32 a<2>, b<2>, c<8>, d<8>;\nwmma.mma.sync.aligned.m16n16k8.row.col.f32.tf32.tf32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3}, {b0, b1, b2, b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n9.7.14.5.\nMatrix multiply-accumulate operation using\nmma\ninstruction\n\nThis section describes warp-level\nmma\n,\nldmatrix\n,\nstmatrix\n, and\nmovmatrix\ninstructions and the organization of various matrices involved in these instructions.\n9.7.14.5.1.\nMatrix Fragments for\nmma.m8n8k4\nwith\n.f16\nfloating point type\n\nA warp executing\nmma.m8n8k4\nwith\n.f16\nfloating point type will compute 4 MMA operations of shape\n.m8n8k4\n.\nElements of 4 matrices need to be distributed across the threads in a warp. The following table\nshows distribution of matrices for MMA operations.\nMMA Computation\nThreads participating in MMA computation\nMMA computation 1\nThreads with\n%laneid\n0-3 (low group) and 16-19 (high group)\nMMA computation 2\nThreads with\n%laneid\n4-7 (low group) and 20-23 (high group)\nMMA computation 3\nThreads with\n%laneid\n8-11 (low group) and 24-27 (high group)\nMMA computation 4\nThreads with\n%laneid\n12-15 (low group) and 28-31 (high group)\nFor each of the individual MMA computation shown above, each of the required thread holds a fragment\nof the matrix for performing mma operation as follows:\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters,\nwith each register containing two\n.f16\nelements from\nthe matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown below:\nFragment layout for Row Major matrix A is shown in\nFigure 46\n.\nFigure 46\nMMA .m8n8k4 fragment layout for row-major matrix A with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\ncol\n=\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nFragment layout for Column Major matrix A is shown in\nFigure 47\n.\nThe layout of the fragments held by different threads is shown below:\nFigure 47\nMMA .m8n8k4 fragment layout for column-major matrix A with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\ni\n%\n4\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nif\n%\nlaneid\n<\n16\n(\ni\n%\n4\n)\n+\n4\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\notherwise\ncol\n=\n%\nlaneid\n%\n4\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown below:\nFragment layout for Row Major matrix B is shown in\nFigure 48\n.\nFigure 48\nMMA .m8n8k4 fragment layout for row-major matrix B with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\ncol\n=\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nif\n%\nlaneid\n<\n16\ni\n+\n4\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\notherwise\nFragment layout for Column Major matrix B is shown in\nFigure 49\n.\nFigure 49\nMMA .m8n8k4 fragment layout for column-major matrix B with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\nAccumulators C (or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing four\n.f16x2\nregisters, with\neach register containing two\n.f16\nelements from the matrix\nC (or D).\nc0, c1, c2, c3, c4, c5, c6, c7\n.f32\nA vector expression of eight\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown below:\nFragment layout for accumulator matrix when\n.ctype\nis\n.f16\nis shown in\nFigure 50\n.\nFigure 50\nMMA .m8n8k4 fragment layout for matrix C/D with\n.ctype\n=\n.f16\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\ncol\n=\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nFragment layout for accumulator matrix when\n.ctype\nis\n.f32\nis shown in\nFigure 51\nand\nFigure 52\n.\nFigure 51\nMMA .m8n8k4 computation 1 and 2 fragment layout for matrix C/D with\n.ctype\n=\n.f32\n\nFigure 52\nMMA .m8n8k4 computation 3 and 4 fragment layout for matrix C/D with\n.ctype\n=\n.f32\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\nX\nif\n%\nlaneid\n<\n16\nX\n+\n4\notherwise\nwhere\nX\n=\n(\n%\nlaneid\n&\n0b1\n)\n+\n(\ni\n&\n0b10\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\n(\ni\n&\n0b100\n)\n+\n(\n%\nlaneid\n&\n0b10\n)\n+\n(\ni\n&\n0b1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\n9.7.14.5.2.\nMatrix Fragments for\nmma.m8n8k4\nwith\n.f64\nfloating point type\n\nA warp executing\nmma.m8n8k4\nwith\n.f64\nfloating point type will compute an MMA operation of\nshape\n.m8n8k4\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing a single\n.f64\nregister, containing\nsingle\n.f64\nelement from the matrix A.\na0\nThe layout of the fragments held by different threads is shown in\nFigure 53\n.\nFigure 53\nMMA .m8n8k4 fragment layout for matrix A  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n>>\n2\ncol\n=\n%\nlaneid\n%\n4\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression containing a single\n.f64\nregister, containing a single\n.f64\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 54\n.\nFigure 54\nMMA .m8n8k4 fragment layout for matrix B  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\ncol\n=\n%\nlaneid\n>>\n2\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing of two\n.f64\nregisters containing two\n.f64\nelements from the matrix C.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 55\n.\nFigure 55\nMMA .m8n8k4 fragment layout for accumulator matrix C/D  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.3.\nMatrix Fragments for\nmma.m8n8k16\n\nA warp executing\nmma.m8n8k16\nwill compute an MMA operation of shape\n.m8n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.s8\nor\n.u8\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 56\n.\nFigure 56\nMMA .m8n8k16 fragment layout for matrix A with\n.u8\n/\n.s8\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.s8\nor\n.u8\nelements from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 57\n.\nFigure 57\nMMA .m8n8k16 fragment layout for matrix B with\n.u8\n/\n.s8\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing of two\n.s32\nregisters.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 58\n.\nFigure 58\nMMA .m8n8k16 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.4.\nMatrix Fragments for\nmma.m8n8k32\n\nA warp executing\nmma.m8n8k32\nwill compute an MMA operation of shape\n.m8n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister, containing\neight\n.s4\nor\n.u4\nelements from the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 59\n.\nFigure 59\nMMA .m8n8k32 fragment layout for matrix A with\n.u4\n/\n.s4\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister, containing\neight\n.s4\nor\n.u4\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 60\n.\nFigure 60\nMMA .m8n8k32 fragment layout for matrix B with\n.u4\n/\n.s4\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression of two\n.s32\nregisters.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 61\n:\nFigure 61\nMMA .m8n8k32 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.5.\nMatrix Fragments for\nmma.m8n8k128\n\nA warp executing\nmma.m8n8k128\nwill compute an MMA operation of shape\n.m8n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister, containing thirty\ntwo\n.b1\nelements from the matrix A.\na0, a1, … a30, a31\nThe layout of the fragments held by different threads is shown in\nFigure 62\n.\nFigure 62\nMMA .m8n8k128 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n31\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister, containing thirty\ntwo\n.b1\nelements from the matrix B.\nb0, b1, …, b30, b31\nThe layout of the fragments held by different threads is shown in\nFigure 63\n.\nFigure 63\nMMA .m8n8k128 fragment layout for matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n31\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing two\n.s32\nregisters, containing two\n.s32\nelements from the matrix C (or D).\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 64\n.\nFigure 64\nMMA .m8n8k128 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.6.\nMatrix Fragments for\nmma.m16n8k4\n\nA warp executing\nmma.m16n8k4\nwill compute an MMA operation of shape\n.m16n8k4\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.tf32\n:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing two\n.b32\nregisters, containing two\n.tf32\nelements from the matrix A.\na0, a1\nThe layout of the fragments held by different threads is shown in\nFigure 65\n.\nFigure 65\nMMA .m16n8k4 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\nthreadID_in_group\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing two\n.f64\nregisters, containing two\n.f64\nelements from the matrix A.\na0, a1\nThe layout of the fragments held by different threads is shown in\nFigure 66\n.\nFigure 66\nMMA .m16n8k4 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\nthreadID_in_group\nMultiplicand B:\n.tf32\n:\n.btype\nFragment\nElements (low to high)\n.tf32\nA vector expression of a single\n.b32\nregister, containing a single\n.tf32\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 67\n.\nFigure 67\nMMA .m16n8k4 fragment layout for matrix B with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\ncol\n=\ngroupID\n.f64\n:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression of a single\n.f64\nregister, containing a single\n.f64\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 68\n.\nFigure 68\nMMA .m16n8k4 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\ncol\n=\ngroupID\nAccumulators (C or D):\n.tf32\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f32\nA vector expression containing four\n.f32\nregisters, containing four\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 69\n.\nFigure 69\nMMA .m16n8k4 fragment layout for accumulator matrix C/D with\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.f64\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, containing four\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 70\n.\nFigure 70\nMMA .m16n8k4 fragment layout for accumulator matrix C/D with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.7.\nMatrix Fragments for\nmma.m16n8k8\n\nA warp executing\nmma.m16n8k8\nwill compute an MMA operation of shape\n.m16n8k8\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.f16\nand\n.bf16\n:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\n/\n.bf16\nelements from the\nmatrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 71\n.\nFigure 71\nMMA .m16n8k8 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na1\ngroupID\n+\n8\nfor\na2\nand\na3\ncol\n=\nthreadID_in_group\n*\n2\n+\n(\ni\n&\n0x1\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.tf32\n:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters, containing four\n.tf32\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 72\n.\nFigure 72\nMMA .m16n8k8 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\nthreadID_in_group\nfor\na0\nand\na1\nthreadID_in_group\n+\n4\nfor\na2\nand\na3\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, containing four\n.f64\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 73\n.\nFigure 73\nMMA .m16n8k8 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\nthreadID_in_group\nfor\na0\nand\na1\nthreadID_in_group\n+\n4\nfor\na2\nand\na3\nMultiplicand B:\n.f16\nand\n.bf16\n:\n.btype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing a single\n.f16x2\nregister, containing\ntwo\n.f16\n/\n.bf16\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 74\n.\nFigure 74\nMMA .m16n8k8 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,\n1\n}\ncol\n=\ngroupID\n.tf32\n:\n.btype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing two\n.b32\nregisters, containing two\n.tf32\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 75\n.\nFigure 75\nMMA .m16n8k8 fragment layout for matrix B with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\nfor\nb0\nthreadID_in_group\n+\n4\nfor\nb1\ncol\n=\ngroupID\n.f64\n:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression containing two\n.f64\nregisters, containing two\n.f64\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 76\n.\nFigure 76\nMMA .m16n8k8 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\nfor\nb0\nthreadID_in_group\n+\n4\nfor\nb1\ncol\n=\ngroupID\nAccumulators (C or D):\n.f16\n,\n.bf16\nand\n.tf32\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression of four\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown in\nFigure 77\n.\nFigure 77\nMMA .m16n8k8 fragment layout for accumulator matrix C/D with\n.f16x2\n/\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.f64\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression of four\n.f64\nregisters containing four\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 78\n.\nFigure 78\nMMA .m16n8k8 fragment layout for accumulator matrix C/D with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.8.\nMatrix Fragments for\nmma.m16n8k16\nwith floating point type\n\nA warp executing\nmma.m16n8k16\nfloating point types will compute an MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.f16\nand\n.bf16\n:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.f16x2\nregisters, with\neach register containing two\n.f16\n/\n.bf16\nelements\nfrom the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 79\n.\nFigure 79\nMMA .m16n8k16 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n2\n||\n4\n<=\ni\n<\n6\ngroupID\n+\n8\nOtherwise\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nai\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\n+\n8\nfor\nai\nwhere\ni\n>=\n4\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing eight\n.f64\nregisters, with each\nregister containing one\n.f64\nelement from the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 80\n.\nFigure 80\nMMA .m16n8k16 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n%\n2\n=\n0\ngroupID\n+\n8\nOtherwise\ncol\n=\n(\ni\n*\n2\n)\n+\nthreadID_in_group\nfor\nai\nwhere\ni\n%\n2\n=\n0\n(\ni\n*\n2\n)\n-\n2\n+\n(\nthreadID_in_group\nOtherwise\nMultiplicand B:\n.f16\nand\n.bf16\n:\n.btype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing two\n.f16x2\nregisters, with\neach register containing two\n.f16\n/\n.bf16\nelements\nfrom the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 81\n.\nFigure 81\nMMA .m16n8k16 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nwhere the row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nbi\nwhere\ni\n<\n2\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\n+\n8\nfor\nbi\nwhere\ni\n>=\n2\ncol\n=\ngroupID\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, with each\nregister containing one\n.f64\nelement from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 82\n.\nFigure 82\nMMA .m16n8k16 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\n+\n(\ni\n*\n4\n)\nfor\nbi\nwhere\ni\n<\n4\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters containing\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters containing\nfour\n.f32\nelements from the matrix C (or D).\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nThe layout of the fragments held by different threads is shown in\nFigure 83\n.\nFigure 83\nMMA .m16n8k16 fragment layout for accumulator matrix matrix C/D.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.9.\nMatrix Fragments for\nmma.m16n8k16\nwith integer type\n\nA warp executing\nmma.m16n8k16\nwill compute an MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.u8\n/\n.s8\nelements from the\nmatrix A.\na0, a1, a2, a3, a4, a5, a6, a7\n.e4m3\n/\n.e5m2\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\nelements from the\nmatrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 84\n.\nFigure 84\nMMA .m16n8k16 fragment layout for matrix A with\n.u8\n/\n.s8\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n4\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n4\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.u8\n/\n.s8\nelements from the matrix B.\nb0, b1, b2, b3\n.e4m3\n/\n.e5m2\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.e4m3\n/\n.e5m2\nelements from the matrix B.\nb0, b1. b2. b3\nThe layout of the fragments held by different threads is shown in\nFigure 85\n.\nFigure 85\nMMA .m16n8k16 fragment layout for matrix B with\n.u8\n/\n.s8\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing\nfour\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c1, c2\nThe layout of the fragments held by different threads is shown in\nFigure 86\n.\nFigure 86\nMMA .m16n8k16 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.10.\nMatrix Fragments for\nmma.m16n8k32\n\nA warp executing\nmma.m16n8k32\nwill compute an MMA operation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.s4\nor\n.u4\n:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.u4\n/\n.s4\nelements from the\nmatrix A.\na0, a1, …, a14, a15\nThe layout of the fragments held by different threads is shown in\nFigure 87\n.\nFigure 87\nMMA .m16n8k32 fragment layout for matrix A with\n.u4\n/\n.s4\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n8\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n8\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n15\n}\n.s8\nor\n.u8\nor\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.s8\n/\n.u8\nelements from the\nmatrix A.\na0, a1, .., a14, a15\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from the matrix A.\na0, a1, …, a14, a15\nThe layout of the fragments held by different threads is shown in\nFigure 88\n.\nFigure 88\nMMA .m16n8k32 fragment layout for matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\n||\n8\n<=\ni\n<\n12\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nai\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\n+\n16\nfor\nai\nwhere\ni\n>=\n8\nMultiplicand B:\n.s4\nor\n.u4\n:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister,\ncontaining eight\n.s4\n/\n.u4\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 89\n.\nFigure 89\nMMA .m16n8k32 fragment layout for matrix B with\n.u4\n/\n.s4\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\ngroupID\n.s8\nor\n.u8\nor\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n:\n.btype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.s8\n/\n.u8\nelements from the\nmatrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 90\nand\nFigure 91\n.\nFigure 90\nMMA .m16n8k32 fragment layout for rows 0–15 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 91\nMMA .m16n8k32 fragment layout for rows 16–31 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nbi\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\n+\n16\nfor\nbi\nwhere\ni\n>=\n4\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing\nfour\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 92\n.\nFigure 92\nMMA .m16n8k32 fragment layout for accumulator matrix C/D with\n.s32\n/\n.f32\n/\n.f16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.11.\nMatrix Fragments for\nmma.m16n8k64\n\nA warp executing\nmma.m16n8k64\nwill compute an MMA operation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight\n.s4\n/\n.u4\nelements from the\nmatrix A.\na0, a1, …, a30, a31\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight\n.e2m1\nelements from the matrix A.\na0, a1, …, a30, a31\nThe layout of the fragments held by different threads is shown in\nFigure 93\n.\nFigure 93\nMMA .m16n8k64 fragment layout for matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\n||\n16\n<=\ni\n<\n24\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nai\nwhere\ni\n<\n16\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\n+\n32\nfor\nai\nwhere\ni\n>=\n16\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.s4\n/\n.u4\nelements from the\nmatrix B.\nb0, b1, …, b14, b15\n.e2m1\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.e2m1\nelements from the matrix B.\nb0, b1, …, b14, b15\nThe layout of the fragments held by different threads is shown in\nFigure 94\nand\nFigure 95\n.\nFigure 94\nMMA .m16n8k64 fragment layout for rows 0–31 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 95\nMMA .m16n8k64 fragment layout for rows 32–63 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nbi\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\n+\n32\nfor\nbi\nwhere\ni\n>=\n8\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing four\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing four\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 96\n.\nFigure 96\nMMA .m16n8k64 fragment layout for accumulator matrix C/D with\n.s32\n/\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.12.\nMatrix Fragments for\nmma.m16n8k128\n\nA warp executing\nmma.m16n8k128\nwill compute an MMA operation of shape\n.m16n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing two\n.b32\nregisters, with each register containing\nthirty two\n.b1\nelements from the matrix A.\na0, a1, …, a62, a63\nThe layout of the fragments held by different threads is shown in\nFigure 97\n.\nFigure 97\nMMA .m16n8k128 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n32\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n32\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,\n...,\n63\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister containing thirty\ntwo\n.b1\nelements from the matrix B.\nb0, b1, … , b30, b31\nThe layout of the fragments held by different threads is shown in\nFigure 98\n.\nFigure 98\nMMA .m16n8k128 fragment layout for matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,...,\n31\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 99\n.\nFigure 99\nMMA .m16n8k128 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n,\n2\n,\n3\n}\n9.7.14.5.13.\nMatrix Fragments for\nmma.m16n8k256\n\nA warp executing\nmma.m16n8k256\nwill compute an MMA operation of shape\n.m16n8k256\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing four\n.b32\nregisters, with each register\ncontaining thirty two\n.b1\nelements from the matrix A.\na0, a1, …, a126, a127\nThe layout of the fragments held by different threads is shown in\nFigure 100\n.\nFigure 100\nMMA .m16n8k256 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n32\n||\n64\n<=\ni\n<\n96\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nai\nwhere\ni\n<\n64\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\n+\n128\nfor\nai\nwhere\ni\n>=\n64\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing two\n.b32\nregisters, with each register\ncontaining thirty two\n.b1\nelements from the matrix B.\nb0, b1, …, b62, b63\nThe layout of the fragments held by different threads is shown in\nFigure 101\nand\nFigure 102\n.\nFigure 101\nMMA .m16n8k256 fragment layout for rows 0–127 of matrix B with\n.b1\ntype.\n\nFigure 102\nMMA .m16n8k256 fragment layout for rows 128–255 of matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\nfor\nbi\nwhere\ni\n<\n32\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\n+\n128\nfor\nbi\nwhere\ni\n>=\n32\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 103\n.\nFigure 103\nMMA .m16n8k256 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n,\n2\n,\n3\n}\n9.7.14.5.14.\nMultiply-and-Accumulate Instruction:\nmma\n\nmma\nPerform matrix multiply-and-accumulate operation\nSyntax\nHalf precision floating point type:\nmma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.ctype   = {.f16, .f32};\n.dtype   = {.f16, .f32};\nAlternate floating point type:\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32        d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.f32.atype.btype.f32      d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32       d, a, b, c;\nmma.sync.aligned.shape.row.col.dtype.f8type.f8type.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k32.row.col.kind.dtype.f8f6f4type.f8f6f4type.ctype d, a, b, c;\n\n.atype      = {.bf16, .tf32};\n.btype      = {.bf16, .tf32};\n.f8type     = {.e4m3, .e5m2};\n.f8f6f4type = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.ctype      = {.f16, .f32};\n.dtype      = {.f16, .f32};\n.shape      = {.m16n8k16, .m16n8k32};\n.kind       = {.kind::f8f6f4};\nAlternate floating point type with block scaling:\nmma.sync.aligned.m16n8k64.row.col.kind.block_scale{.scale_vec_size}.f32.e2m1.e2m1.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf4};\n.scale_vec_size = {.scale_vec::2X};\n.stype          = {.ue8m0};\n\nmma.sync.aligned.m16n8k64.row.col.kind.block_scale.scale_vec_size.f32.e2m1.e2m1.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf4nvf4};\n.scale_vec_size = {.scale_vec::2X, .scale_vec::4X};\n.stype          = {.ue8m0, .ue4m3};\n\nmma.sync.aligned.m16n8k32.row.col.kind.block_scale{.scale_vec_size}.f32.f8f6f4type.f8f6f4type.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf8f6f4};\n.scale_vec_size = {.scale_vec::1X};\n.f8f6f4type     = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.stype          = {.ue8m0};\nDouble precision floating point type:\nmma.sync.aligned.shape.row.col.f64.f64.f64.f64 d, a, b, c;\n\n.shape   = {.m8n84, .m16n8k4, .m16n8k8, .m16n8k16};\nInteger type:\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k16, .m16n8k16, .m16n8k32}\n.atype   = {.u8, .s8};\n.btype   = {.u8, .s8};\n\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k32, .m16n8k32, .m16n8k64}\n.atype   = {.u4, .s4};\n.btype   = {.u4, .s4};\nSingle bit:\nmma.sync.aligned.shape.row.col.s32.b1.b1.s32.bitOp.popc d, a, b, c;\n\n.bitOp = {.xor, .and}\n.shape = {.m8n8k128, .m16n8k128, .m16n8k256}\nDescription\nPerform a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+C\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the C and D matrices are\nMxN\n.\nQualifier\n.block_scale\nspecifies that the matrices A and B are scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply and accumulate operation\nas specified in the section\nBlock Scaling\n. The data type\ncorresponding to each of the element within\nscale_A\nand\nScale_B\nmatrices is specified\nby\n.stype\n. Qualifier\n.scale_vec_size\nspecifies the number of columns of\nscale_A\nmatrix\nand number of rows in the matrix\nscale_B\n.\nThe valid combinations of\n.kind\n,\n.stype\nand\n.scale_vec_size\nare described in\nTable 36\n. For\nmma\nwith\n.kind::mxf4\nwhen the\nqualifier\n.scale_vec_size\nis not specified, then it defaults to\n2X\n. In contrast, when\n.kind\nis specified as\n.kind::mxf8f6f4\nthen the qualifier\n.scale_vec_size\ndefaults\nto\n1X\n. However, for\n.kind::mxf4nvf4\n, it is mandatory to provide valid\n.scale_vec_size\n.\nA warp executing\nmma.sync.m8n8k4\ninstruction computes 4 matrix multiply and accumulate\noperations. Rest of the\nmma.sync\noperations compute a single matrix mutliply and accumulate\noperation per warp.\nFor single-bit\nmma.sync\n, multiplication is replaced by a sequence of logical operations;\nspecifically,\nmma.xor.popc\nand\nmma.and.popc\ncomputes the XOR, AND respectively of a k-bit\nrow of A with a k-bit column of B, then counts the number of set bits in the result (\npopc\n). This\nresult is added to the corresponding element of C and written into D.\nOperands\na\nand\nb\nrepresent two multiplicand matrices A and B, while\nc\nand\nd\nrepresent the accumulator and destination matrices, distributed across the threads in warp.\nWhen\n.block_scale\nqualifier is specified, operand\nscale-a-data\n,\nscale-b-data\nrepresents\nthe scale matrix metadata corresponding to\nscale_A\nand\nscale_B\nmatrices respectively. The\ntuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nrepresent selectors for matrices\nscale_A\nand\nscale_B\nrespectively from their corresponding metadata arguments\nscale-a-data\n,\nscale-b-data\n. The operands\nscale-a-data\n,\nscale-b-data\nare of type\n.b32\n. The operands\nbyte-id-a\n,\nthread-id-a\n,\nbyte-id-b\n,\nthread-id-b\nare unsigned 16-bit integer values.\nFor more details on selector arguments refer\nBlock Scaling\nsection.\nThe registers in each thread hold a fragment of matrix as described in\nMatrix multiply-accumulate operation using mma instruction\n.\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively. The qualifier\n.stype\nindicate the data-type\nof the elements in the matrices\nscale_A\nand\nscale_B\n. Specific shapes have type restrictions :\n.m8n8k4\n: When\n.ctype\nis\n.f32\n,\n.dtype\nmust also be\n.f32\n.\n.m16n8k8\n:\n.dtype\nmust be the same as\n.ctype\n.\n.atype\nmust be the same as\n.btype\n.\n.m16n8k16\nand\n.m16n8k32\n:\n.dtype\nmust be the same as\n.ctype\n.\nThe qualifiers\n.alayout\nand\n.blayout\nindicate the row-major or column-major layouts of\nmatrices A and B respectively.\nWhen\n.kind\nis either of\n.kind::mxf8f6f4\nor\n.kind::f8f6f4\n, the individual 4-bit and the\n6-bit floating point type elements must be packed in an 8-bit container. The matrix element of type\n.e2m1\nresides in central 4 bits of the 8-bit container with padding in the upper 2 bits and\nlower 2 bits of the container. When the matrix element is of type\n.e3m2\nor\n.e2m3\n, the\nmatrix element resides in the lower 6 bits of the 8-bit container with padding in the upper 2 bits\nof the container. In contrast, note that when using\nmma\nwith\n.kind::mxf4\nor\n.kind::mxf4nvf4\n, no explicit padding is necessary even though matrix elements are of type\n.e2m1\n.\nPrecision and rounding :\n.f16\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single\nprecision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values\nis performed with at least single precision. When both\n.ctype\nand\n.dtype\nare specified\nas\n.f16\n, the accumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.e4m3\n,\n.e5m2\n,\n.e3m2\n,\n.e2m3\n,\n.e2m1\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.f64\nfloating point operations :\nPrecision of the element-wise multiplication and addition operation is identical to that of\n.f64\nprecision fused multiply-add. Supported rounding modifiers are :\n.rn\n: mantissa LSB rounds to nearest even. This is the default.\n.rz\n: mantissa LSB rounds towards zero.\n.rm\n: mantissa LSB rounds towards negative infinity.\n.rp\n: mantissa LSB rounds towards positive infinity.\nInteger operations :\nThe integer\nmma\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the range\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nmma\ninstruction causes the executing thread to\nwait until all threads in the warp execute the same\nmma\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmma\ninstruction. In conditionally executed code, a\nmma\ninstruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined.\nThe behavior of\nmma\ninstruction is undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited.\nNotes\nPrograms using double precision floating point\nmma\ninstruction with shapes\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nrequire at least 64 registers for compilation.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.4.\n.f16\nfloating point type\nmma\noperation with\n.m8n8k4\nshape introduced in PTX ISA version\n6.4.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k8\nshape introduced in PTX ISA version\n6.5.\n.u8/.s8\ninteger type\nmma\noperation with\n.m8n8k16\nshape introduced in PTX ISA version\n6.5.\n.u4/.s4\ninteger type\nmma\noperation with\n.m8n8k32\nshape introduced in PTX ISA version\n6.5.\n.f64\nfloating point type\nmma\noperation with\n.m8n8k4\nshape introduced in PTX ISA version\n7.0.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k16\nshape introduced in PTX ISA\nversion 7.0.\n.bf16\nalternate floating point type\nmma\noperation with\n.m16n8k8\nand\n.m16n8k16\nshapes\nintroduced in PTX ISA version 7.0.\n.tf32\nalternate floating point type\nmma\noperation with\n.m16n8k4\nand\n.m16n8k8\nshapes\nintroduced in PTX ISA version 7.0.\n.u8/.s8\ninteger type\nmma\noperation with\n.m16n8k16\nand\n.m16n8k32\nshapes introduced in\nPTX ISA version 7.0.\n.u4/.s4\ninteger type\nmma\noperation with\n.m16n8k32\nand\n.m16n8k64\nshapes introduced in\nPTX ISA version 7.0.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m8n8k128\n,\n.m16n8k128\nand\n.m16n8k256\nshapes introduced in PTX ISA version 7.0.\nSupport for\n.and\noperation in single-bit\nmma\nintroduced in PTX ISA version 7.1.\n.f64\nfloating point type\nmma\noperation with\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nshapes introduced in PTX ISA version 7.8.\nSupport for\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation introduced in\nPTX ISA version 8.4.\nSupport for shape\n.m16n8k16\nand\n.f16\ndtype\n/\nctype\nwith\n.e4m3\n/\n.e5m2\nalternate\nfloating point type mma operation introduced in PTX ISA version 8.7.\nSupport for\n.e3m2\n,\n.e2m3\n,\n.e2m1\nalternate floating point type\nmma\noperation introduced\nin PTX ISA version 8.7.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier introduced in PTX ISA version 8.7.\nSupport for\n.scale_vec::4X\non\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nis introduced in\nPTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_70\nor higher.\n.f16\nfloating point type\nmma\noperation with\n.m8n8k4\nshape requires\nsm_70\nor higher.\nNote\nmma.sync.m8n8k4\nis optimized for target architecture\nsm_70\nand may have substantially\nreduced performance on other target architectures.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k8\nshape requires\nsm_75\nor higher.\n.u8/.s8\ninteger type\nmma\noperation with\n.m8n8k16\nshape requires\nsm_75\nor higher.\n.u4/.s4\ninteger type\nmma\noperation with\n.m8n8k32\nshape\nsm_75\nor higher.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m8n8k128\nshape\nsm_75\nor higher.\n.f64\nfloating point type\nmma\noperation with\n.m8n8k4\nshape requires\nsm_80\nor higher.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k16\nshape requires\nsm_80\nor\nhigher.\n.bf16\nalternate floating point type\nmma\noperation with\n.m16n8k8\nand\n.m16n8k16\nshapes\nrequires\nsm_80\nor higher.\n.tf32\nalternate floating point type\nmma\noperation with\n.m16n8k4\nand\n.m16n8k8\nshapes\nrequires\nsm_80\nor higher.\n.u8/.s8\ninteger type\nmma\noperation with\n.m16n8k16\nand\n.m16n8k32\nshapes requires\nsm_80\nor higher.\n.u4/.s4\ninteger type\nmma\noperation with\n.m16n8k32\nand\n.m16n8k64\nshapes requires\nsm_80\nor higher.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m16n8k128\nand\n.m16n8k256\nshapes\nrequires\nsm_80\nor higher.\n.and\noperation in single-bit\nmma\nrequires\nsm_80\nor higher.\n.f64\nfloating point type\nmma\noperation with\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nshapes require\nsm_90\nor higher.\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation requires\nsm_89\nor higher.\n.e3m2\n,\n.e2m3\nand\n.e2m1\nalternate floating point type\nmma\noperation requires\nsm_120a\nand is supported on\nsm_120f\nfrom PTX ISA version 8.8.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier requires\nsm_120a\nand are\nsupported on\nsm_120f\nor higher in the same family from PTX ISA version 8.8.\nExamples of half precision floating point type\n// f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4> %Rd<4>\nmma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n\n// f16 elements in C and f32 elements in D\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4>\n.reg .f32 %Rd<8>\nmma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n // f32 elements in C and D\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<4>, %Rb<2>, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .f16 %Ra<4>, %Rb<2>;\n.reg .f32 %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of alternate floating point type\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Rb2, %Rb3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f32.e4m3.e5m2.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f16.e4m3.e5m2.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f16.e5m2.e5m2.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.kind::f8f6f4.f32.e3m2.e2m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.kind::f8f6f4.f16.e2m3.e2m1.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\nExamples of integer type\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\n\n// s8 elements in A and u8 elements in B\nmma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// u4 elements in A and B matrix\nmma.sync.aligned.m8n8k32.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// s8 elements in A and u8 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u4 elements in A and s4 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.u4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// s8 elements in A and s8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u8 elements in A and u8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k64.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1 },\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of single bit type\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of\n.f64\nfloating point type\n.reg .f64 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .f64 %Ra<8>, %Rb<4>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k8.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k16.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3, %Ra4, %Ra5, %Ra6, %Ra7},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of\nmma\nwith block scale\n.reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n mma.sync.aligned.m16n8k64.row.col.kind::mxf4.block_scale.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {2, 1}, scaleBData, {2, 3};\n\n .reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .u16 bidA, bidB, tidA, tidB;\n mma.sync.aligned.m16n8k64.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue4m3\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .u16 bidA, bidB, tidA, tidB;\nmma.sync.aligned.m16n8k64.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\nmma.sync.aligned.m16n8k32.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e3m2.e2m1.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  scaleAData, {0, 1}, scaleBData, {0, 1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\nmma.sync.aligned.m16n8k32.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e4m3.e5m2.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2,  %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  scaleAData, {0, 1}, scaleBData, {0, 0};\n9.7.14.5.15.\nWarp-level matrix load instruction:\nldmatrix\n\nldmatrix\nCollectively load one or more matrices from shared memory for\nmma\ninstruction\nSyntax\nldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p];\n\nldmatrix.sync.aligned.m8n16.num{.ss}.dst_fmt.src_fmt        r, [p];\nldmatrix.sync.aligned.m16n16.num.trans{.ss}.dst_fmt.src_fmt r, [p];\n\n.shape   = {.m8n8, .m16n16};\n.num     = {.x1, .x2, .x4};\n.ss      = {.shared{::cta}};\n.type    = {.b16, .b8};\n.dst_fmt = { .b8x16 };\n.src_fmt = { .b6x16_p32, .b4x16_p64 };\nDescription\nCollectively load one or more matrices across all threads in a warp from the location indicated by\nthe address operand\np\n, from\n.shared\nstate space into destination register\nr\n. If no state\nspace is provided, generic addressing is used, such that the address in\np\npoints into\n.shared\nspace. If the generic address doesn’t fall in\n.shared\nstate space, then the behavior\nis undefined.\nThe\n.shape\nqualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit or 8-bit or 6-bit or 4-bit data.\nFollowing table shows the matrix load case for each\n.shape\n.\n.shape\nMatrix shape\nElement size\n.m8n8\n8x8\n16-bit\n.m16n16\n16x16\n8-bit or 6-bit or 4-bit\n.m8n16\n8x16\n6-bit or 4-bit\nFollowing table shows the valid use of 6-bit or 4-bit data load.\n.src_fmt\n.shape\nSource data\nPadding\n.dst_fmt\n.b6x16_p32\n.m8n16\n16 6-bit elements\n32 bits\n.b8x16\n(16 8-bit\nelements)\n.m16n16\n.b4x16_p64\n.m8n16\n16 4-bit elements\n64 bits\n.m16n16\nFor\n.b6x16_p32\nformat source data is 16 unsigned 6-bit elements with 32 bits padding.\nFor\n.b4x16_p64\nformat source data is 16 unsigned 4-bit elements with 64 bits padding.\nThe values\n.x1\n,\n.x2\nand\n.x4\nfor\n.num\nindicate one, two or four matrices\nrespectively. When\n.shape\nis\n.m16n16\n, only\n.x1\nand\n.x2\nare valid values for\n.num\n.\nThe mandatory\n.sync\nqualifier indicates that\nldmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nldmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nldmatrix\ninstruction. In conditionally executed code, an\nldmatrix\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined.\nThe behavior of\nldmatrix\nis undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited.\nThe destination operand\nr\nis a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of\n.num\n. Each component of the vector expression holds a fragment\nfrom the corresponding matrix.\nSupported addressing modes for\np\nare described in\nAddresses as Operands\n.\nConsecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of\n.num\nas shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on.\n.num\nThreads 0–7\nThreads 8–15\nThreads 16–23\nThreads 24–31\n.x1\naddr0–addr7\n–\n–\n–\n.x2\naddr0–addr7\naddr8–addr15\n–\n–\n.x4\naddr0–addr7\naddr8–addr15\naddr16–addr23\naddr24–addr31\nNote\nFor .target\nsm_75\nor below, all threads must contain valid addresses. Otherwise, the behavior\nis undefined. For\n.num\n=\n.x1\nand\n.num\n=\n.x2\n, addresses contained in lower threads can be\ncopied to higher threads to achieve the expected behavior.\nWhen reading 8x8 matrices, a group of four consecutive threads loads 16 bytes. The matrix addresses\nmust be naturally aligned accordingly.\nEach thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its\nregister\nr\n, and so on. A group of four threads loads an entire row of the matrix as shown in\nFigure 104\n.\nFigure 104\nldmatrix fragment layout for one 8x8 Matrix with 16-bit elements\n\nWhen\n.num\n=\n.x2\n, the elements of the second matrix are loaded in the next destination\nregister in each thread as per the layout in above table. Similarly, when\n.num\n=\n.x4\n,\nelements of the third and fourth matrices are loaded in the subsequent destination registers in each\nthread.\nFor matrix shape 16x16, two destination registers\nr0\nand\nr1\nof type\n.b32\nmust be\nspecified and in each register four 8-bit elements are loaded. For 4-bit or 6-bit data, 8-bit\nelement will have 4 bits or 2 bits of padding respectively.\nRefer\nOptional Decompression\nfor more details\non these formats.\nAn entire row of the matrix can be loaded by a group of four consecutive and aligned threads.\nEach thread in a warp loads 4 consecutive columns across 2 rows as shown in the\nFigure 105\n.\nFigure 105\nldmatrix fragment layout for one 16x16 matrix with 8-bit elements\n\nFor matrix shape 8x16, one destination register\nr0\nof type\n.b32\nmust be specified where four\n8-bit elements are loaded in the register. For 4-bit or 6-bit data, 8-bit element will have 4 bits\nor 2 bits of padding respectively.\nAn entire row of the matrix can be loaded by a group of four consecutive and aligned threads.\nEach thread in a warp loads 4 consecutive columns as shown in\nFigure 106\n.\nFigure 106\nldmatrix fragment layout for one 8x16 matrix with 8-bit elements containing 4-bit/6-bit data\n\nOptional qualifier\n.trans\nindicates that the matrix is loaded in column-major format. However,\nfor 16x16 matrices,\n.trans\nis mandatory.\nThe\nldmatrix\ninstruction is treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nSupport for\n.m16n16\n,\n.m8n16\nshapes introduced in PTX ISA version 8.6.\nSupport for\n.b8\ntype with\nldmatrix\nis introduced in PTX ISA version 8.6.\nSupport for\n.src_fmt\n,\n.dst_fmt\nqualifiers introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nShapes\n.m16n16\n,\n.m8n16\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nType\n.b8\nwith\nldmatrix\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nQualifiers\n.src_fmt\n,\n.dst_fmt\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Load a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 d;\nldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr];\n\n// Load two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr];\n\n// Load four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr];\n\n// Load one 16x16 matrices of 64-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m16n16.x1.trans.shared.b8 {d0, d1}, [addr];\n\n// Load two 16x16 matrices of 64-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m16n16.x2.trans.shared::cta.b8 {d0, d1, d2, d3}, [addr];\n\n// Load two 16x16 matrices of 6-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m16n16.x2.trans.shared::cta.b8x16.b6x16_p32 {d0, d1, d2, d3}, [addr];\n9.7.14.5.16.\nWarp-level matrix store instruction:\nstmatrix\n\nstmatrix\nCollectively store one or more matrices to shared memory.\nSyntax\nstmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r;\n\n.shape  = {.m8n8, .m16n8};\n.num    = {.x1, .x2, .x4};\n.ss     = {.shared{::cta}};\n.type   = {.b16, .b8};\nDescription\nCollectively store one or more matrices across all threads in a warp to the location indicated by\nthe address operand\np\n, in\n.shared\nstate space. If no state space is provided, generic\naddressing is used, such that the address in\np\npoints into\n.shared\nspace. If the generic\naddress doesn’t fall in\n.shared\nstate space, then the behavior is undefined.\nThe\n.shape\nqualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit or 8-bit data as indicated by the\n.type\nqualifier.\n.m16n8\nshape is valid only for\n.b8\ntype.\nThe values\n.x1\n,\n.x2\nand\n.x4\nfor\n.num\nindicate one, two or four matrices\nrespectively.\nThe mandatory\n.sync\nqualifier indicates that\nstmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nstmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nstmatrix\ninstruction. In conditionally executed code, an\nstmatrix\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined.\nThe behavior of\nstmatrix\nis undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited.\nThe source operand\nr\nis a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of\n.num\n. Each component of the vector expression holds a fragment\nfrom the corresponding matrix.\nSupported addressing modes for\np\nare described in\nAddresses as Operands\n.\nConsecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of\n.num\nas shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on.\n.num\nThreads 0–7\nThreads 8–15\nThreads 16–23\nThreads 24–31\n.x1\naddr0–addr7\n–\n–\n–\n.x2\naddr0–addr7\naddr8–addr15\n–\n–\n.x4\naddr0–addr7\naddr8–addr15\naddr16–addr23\naddr24–addr31\nWhen storing 8x8 matrices, a group of four consecutive threads stores 16 bytes. The matrix addresses\nmust be naturally aligned accordingly.\nEach thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its\nregister\nr\n, and so on. A group of four threads stores an entire row of the matrix as shown in\nFigure 107\n.\nFigure 107\nstmatrix fragment layout for one 8x8 matrix with 16-bit elements\n\nWhen\n.num\n=\n.x2\n, the elements of the second matrix are storedd from the next source register\nin each thread as per the layout in above table. Similarly, when\n.num\n=\n.x4\n, elements of the\nthird and fourth matrices are stored from the subsequent source registers in each thread.\nFor 16x8 matrix shape, each of the 32 threads in the warp provides four elements of data per matrix.\nEach element in the source operand\nr\nis of type\n.b32\nand contains four 8 bit elements\ne0\n,\ne1\n,\ne2\n,\ne3\nwith\ne0\nand\ne3\ncontaining the LSB and MSB respectively of register\nr\n.\nFigure 108\nstmatrix fragment layout for one 16x8 matrix with 8 bit elements\n\nOptional qualifier\n.trans\nindicates that the matrix is stored in column-major format. However,\nfor 16x8 matrices,\n.trans\nis mandatory.\nThe\nstmatrix\ninstruction is treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nSupport for\n.m16n8\nshape is introduced in PTX ISA version 8.6.\nSupport for\n.b8\ntype with\nstmatrix\nis introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nShape\n.m16n8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nType\n.b8\nwith\nstmatrix\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Store a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r};\n\n// Store two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1};\n\n// Store four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3};\n\n// Store a single 16x8 matrix using generic addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m16n8.x1.trans.shared.b8 [addr], {r};\n\n// Store two 16x8 matrices\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m16n8.x2.trans.shared::cta.b8 [addr],{r0, r1};\n\n// Store four 16x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m16n8.x4.b8 [addr], {r0, r1, r2, r3};\n9.7.14.5.17.\nWarp-level matrix transpose instruction:\nmovmatrix\n\nmovmatrix\nTranspose a matrix in registers across the warp.\nSyntax\nmovmatrix.sync.aligned.shape.trans.type d, a;\n\n.shape  = {.m8n8};\n.type   = {.b16};\nDescription\nMove a row-major matrix across all threads in a warp, reading elements from source\na\n, and\nwriting the transposed elements to destination\nd\n.\nThe\n.shape\nqualifier indicates the dimensions of the matrix being transposed. Each matrix\nelement holds 16-bit data as indicated by the\n.type\nqualifier.\nThe mandatory\n.sync\nqualifier indicates that\nmovmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nmovmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmovmatrix\ninstruction. In conditionally executed code, a\nmovmatrix\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nthe behavior is undefined.\nOperands\na\nand\nd\nare 32-bit registers containing fragments of the input matrix and the\nresulting matrix respectively. The mandatory qualifier\n.trans\nindicates that the resulting\nmatrix in\nd\nis a transpose of the input matrix specified by\na\n.\nEach thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first\nfragment in register\na\n, and so on. A group of four threads holds an entire row of the input\nmatrix as shown in\nFigure 109\n.\nFigure 109\nmovmatrix source matrix fragment layout\n\nEach thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the\nfirst fragment in register\nd\n, and so on. A group of four threads holds an entire column of the\nresult matrix as shown in\nFigure 110\n.\nFigure 110\nmovmatrix result matrix fragment layout\n\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\n.reg .b32 d, a;\nmovmatrix.sync.aligned.m8n8.trans.b16 d, a;\n9.7.14.6.\nMatrix multiply-accumulate operation using\nmma.sp\ninstruction with sparse matrix A\n\nThis section describes warp-level\nmma.sp{::ordered_metadata}\ninstruction with sparse matrix A.\nThis variant of the\nmma\noperation can be used when A is a structured sparse matrix with 50%\nzeros in each row distributed in a shape-specific granularity. For an\nMxNxK\nsparse\nmma.sp{::ordered_metadata}\noperation, the\nMxK\nmatrix A is packed into\nMxK/2\nelements.\nFor each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements\nare packed in the operand representing matrix A. The mapping of these K/2 elements to the\ncorresponding K-wide row is provided explicitly as metadata.\n9.7.14.6.1.\nSparse matrix storage\n\nGranularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a\n16x16\nmatrix A, sparsity is expected to be at 2:4\ngranularity, i.e. each 4-element vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row\ncontains 2 zeros. Index of each non-zero element in a sub-chunk is stored in the metadata\noperand. Values\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nare invalid values for metadata and\nwill result in undefined behavior. In a group of four consecutive threads, one or more threads store\nthe metadata for the whole group depending upon the matrix shape. These threads are specified using\nan additional\nsparsity selector\noperand.\nFigure 111\nshows an example of a 16x16 matrix A represented in sparse format and sparsity\nselector indicating which thread in a group of four consecutive threads stores the metadata.\nFigure 111\nSparse MMA storage example\n\nGranularities for different matrix shapes and data types are described below.\nSparse\nmma.sp{::ordered_metadata}\nwith half-precision and\n.bf16\ntype\nFor the\n.m16n8k16\nand\n.m16n8k32\nmma.sp{::ordered_metadata}\noperations, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements\nin a row of matrix A has two zeros and two non-zero elements. Only the two non-zero elements are\nstored in the operand representing matrix A and their positions in the four-wide chunk in matrix\nA are indicated by two 2-bit indices in the metadata operand. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values\nof indices; any other values result in an undefined behavior.\nFigure 112\nSparse MMA metadata example for\n.f16\n/\n.bf16\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k16\n: One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}.\nm16n8k32\n: A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior.\nSparse\nmma.sp{::ordered_metadata}\nwith\n.tf32\ntype\nWhen matrix A has\n.tf32\nelements, matrix A is structured sparse at a granularity of 1:2. In\nother words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero\nelement. Only the non-zero elements are stored in the operand for matrix A and their positions in a\ntwo-wide chunk in matrix A are indicated by the 4-bit index in the metadata.\n0b1110\nand\n0b0100\nare the only meaningful index values; any other values result in an undefined behavior.\nFigure 113\nSparse MMA metadata example for\n.tf32\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k8\n: One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}.\nm16n8k16\n: A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior.\nSparse\nmma.sp{::ordered_metadata}\nwith integer type\nWhen matrices A and B have\n.u8\n/\n.s8\nelements, matrix A is structured sparse at a granularity\nof 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes\nand two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their\npositions in the four-wide chunk are indicated by two 2-bit indices in the metadata. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 114\nSparse MMA metadata example for\n.u8\n/\n.s8\ntype.\n\nwhen matrices A and B have\n.u4\n/\n.s4\nelements, matrix A is pair-wise structured sparse at a\ngranularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has\nfour zeroes and four non-zero values. Further, the zero and non-zero values are clustered in\nsub-chunks of two elements each within the eight-wide chunk. i.e., each two-wide sub-chunk within\nthe eight-wide chunk must be all zeroes or all non-zeros. Only the four non-zero values are stored\nin sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the\neight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 115\nSparse MMA metadata example for\n.u4\n/\n.s4\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k32\nwith\n.u8\n/\n.s8\ntype and\nm16n8k64\nwith\n.u4\n/\n.s4\ntype: A thread-pair\nwithin a group of four consecutive threads contributes the sparsity metadata. Hence, the sparsity\nselector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an\nundefined behavior.\nm16n8k64\nwith\n.u8\n/\n.s8\ntype and\nm16n8k128\nwith\n.u4\n/\n.s4\ntype: All threads\nwithin a group of four consecutive threads contribute the sparsity metadata. Hence, the sparsity\nselector in this case must be 0. Any other value of sparsity selector results in an undefined\nbehavior.\nSparse\nmma.sp{::ordered_metadata}\noperating on\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype with\n.kind::f8f6f4\nor\n.kind::mxf8f6f4\nWhen matrices A and B have\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a\nrow of matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored\nin sparse matrix and their positions in the four-wide chunk are indicated by two 2-bit indices in the\nmetadata.\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful\nvalues of indices; any other values result in an undefined behavior.\nFigure 116\nSparse MMA metadata example for\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k64\n: All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior.\nSparse\nmma.sp::ordered_metadata\noperating on\n.e2m1\ntype with\n.kind::mxf4\nor\n.kind::mxf4nvf4\nWhen matrices A and B have\n.e2m1\nelements, matrix A is pair-wise structured sparse at a granularity\nof 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has four zeroes and\nfour non-zero values. Further, the zero and non-zero values are clustered in sub-chunks of two elements\neach within the eight-wide chunk. i.e., each two-wide sub-chunk within the eight-wide chunk must be all\nzeroes or all non-zeros. Only the four non-zero values are stored in sparse matrix and the positions of\nthe two two-wide sub-chunks with non-zero values in the eight-wide chunk of a row of matrix A are\nindicated by two 2-bit indices in the metadata.\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 117\nSparse MMA metadata example for\n.e2m1\ntype with\n.kind::mxf4\nor\n.kind::mxf4nvf4\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k128\n: All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior.\n9.7.14.6.2.\nMatrix fragments for multiply-accumulate operation with sparse matrix A\n\nIn this section we describe how the contents of thread registers are associated with fragments of\nvarious matrices and the sparsity metadata. The following conventions are used throughout this\nsection:\nFor matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data.\nFor matrix B, when the combination of matrix dimension and the supported data type is not already\ncovered in\nMatrix multiply-accumulate operation using mma instruction\n, a pictorial representation of matrix\nfragments is provided.\nFor matrices C and D, since the matrix dimension - data type combination is the same for all\nsupported shapes, and is already covered in\nMatrix multiply-accumulate operation using mma instruction\n, the pictorial representations\nof matrix fragments are not included in this section.\nFor the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included.\nTk:\n[m..n]\npresent\nin cell\n[x][y..z]\nindicates that bits\nm\nthrough\nn\n(with\nm\nbeing higher) in the\nmetadata operand of thread with\n%laneid=k\ncontains the indices of the non-zero elements from\nthe chunk\n[x][y]..[x][z]\nof matrix A.\n9.7.14.6.2.1.\nMatrix Fragments for sparse\nmma.m16n8k16\nwith\n.f16\nand\n.bf16\ntypes\n\nA warp executing sparse\nmma.m16n8k16\nwith\n.f16\n/\n.bf16\nfloating point type will compute\nan MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.f16\n/\n.bf16\nA vector expression containing two\n.b32\nregisters,\nwith each register containing two non-zero\n.f16\n/\n.bf16\nelements out of 4 consecutive elements from\nmatrix A.\nMapping of the non-zero elements is as\ndescribed in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 118\n.\nFigure 118\nSparse MMA .m16n8k16 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na1\ngroupID\n+\n8\nfor\na2\nand\na3\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n4\nlastcol\n=\nfirstcol\n+\n3\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\nfor\n.f16\n/\n.b16\nformats.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors each storing the index of a non-zero\nelement of a 4-wide chunk of matrix A as shown in\nFigure 119\n.\nFigure 119\nSparse MMA .m16n8k16 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.14.6.2.2.\nMatrix Fragments for sparse\nmma.m16n8k32\nwith\n.f16\nand\n.bf16\ntypes\n\nA warp executing sparse\nmma.m16n8k32\nwith\n.f16\n/\n.bf16\nfloating point type will compute\nan MMA operation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters,\nwith each register containing two non-zero\n.f16\n/\n.bf16\nelements out of 4 consecutive elements from\nmatrix A.\nMapping of the non-zero elements is as\ndescribed in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 120\n.\nFigure 120\nSparse MMA .m16n8k32 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n2\n||\n4\n<=\ni\n<\n6\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n4\nFor\nai\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n4\n)\n+\n16\nfor\nai\nwhere\ni\n>=\n4\nlastcol\n=\nfirstcol\n+\n3\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters, each\ncontaining two\n.f16\n/\n.bf16\nelements from matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 121\n.\nFigure 121\nSparse MMA .m16n8k32 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\nfor\n.f16\n/\n.b16\nformats.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero element from a 4-wide chunk of matrix A as shown in\nFigure 122\n.\nFigure 122\nSparse MMA .m16n8k32 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.14.6.2.3.\nMatrix Fragments for sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type\n\nA warp executing sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type will compute an MMA\noperation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.tf32\nA vector expression containing four\n.b32\nregisters, with each\nregister containing one non-zero\n.tf32\nelement out of 2\nconsecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 123\n.\nFigure 123\nSparse MMA .m16n8k16 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n2\nfor\na0\nand\na1\n(\nthreadID_in_group\n*\n2\n)\n+\n8\nfor\na2\nand\na3\nlastcol\n=\nfirstcol\n+\n1\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters, each\ncontaining four\n.tf32\nelements from matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 124\n.\nFigure 124\nSparse MMA .m16n8k16 fragment layout for matrix B with\n.tf32\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\n.\nMetadata: A\n.b32\nregister containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in\nFigure 125\n.\nFigure 125\nSparse MMA .m16n8k16 metadata layout for\n.tf32\ntype.\n\n9.7.14.6.2.4.\nMatrix Fragments for sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type\n\nA warp executing sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type will compute an MMA\noperation of shape\n.m16n8k8\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.tf32\nA vector expression containing two\n.b32\nregisters, each\ncontaining one non-zero\n.tf32\nelement out of 2\nconsecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 126\n.\nFigure 126\nSparse MMA .m16n8k8 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n2\nlastcol\n=\nfirstcol\n+\n1\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k8\nfor\n.tf32\nformat.\nMetadata: A\n.b32\nregister containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in\nFigure 127\n.\nFigure 127\nSparse MMA .m16n8k8 metadata layout for\n.tf32\ntype.\n\n9.7.14.6.2.5.\nMatrix Fragments for sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type\n\nA warp executing sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type will compute an MMA\noperation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u8\n/\n.s8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four non-zero\n.u8\n/\n.s8\nelements out\nof 8 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 128\n.\nFigure 128\nSparse MMA .m16n8k32 fragment layout for matrix A with\n.u8\n/\n.s8\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n8\nlastcol\n=\nfirstcol\n+\n7\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k32\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in\nFigure 129\n.\nFigure 129\nSparse MMA .m16n8k32 metadata layout for\n.u8\n/\n.s8\ntype.\n\n9.7.14.6.2.6.\nMatrix Fragments for sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\ntype\n\nA warp executing sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype will compute an MMA operation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u8\n/\n.s8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four non-zero\n.u8\n/\n.s8\nelements out\nof 8 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four non-zero\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements out of 8 consecutive\nelements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 130\nand\nFigure 131\n.\nFigure 130\nSparse MMA .m16n8k64 fragment layout for columns 0–31 of matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 131\nSparse MMA .m16n8k64 fragment layout for columns 32–63 of matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\n||\n8\n<=\ni\n<\n12\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n8\nFor\nai\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n8\n)\n+\n32\nFor\nai\nwhere\ni\n>=\n8\nlastcol\n=\nfirstcol\n+\n7\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing four\n.b32\nregisters,\neach containing four\n.u8\n/\n.s8\nelements from\nmatrix B.\nb0, b1, b2, b3, …, b15\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters,\neach containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from matrix B.\nThe layout of the fragments held by different threads is shown in\nFigure 132\n,\nFigure 133\n,\nFigure 134\nand\nFigure 135\n.\nFigure 132\nSparse MMA .m16n8k64 fragment layout for rows 0–15 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 133\nSparse MMA .m16n8k64 fragment layout for rows 16–31 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 134\nSparse MMA .m16n8k64 fragment layout for rows 32–47 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 135\nSparse MMA .m16n8k64 fragment layout for rows 48–63 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with integer type\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in\nFigure 136\nand\nFigure 137\n.\nFigure 136\nSparse MMA .m16n8k64 metadata layout for columns 0–31 for\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 137\nSparse MMA .m16n8k64 metadata layout for columns 32–63 for\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\n9.7.14.6.2.7.\nMatrix Fragments for sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type\n\nA warp executing sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type will compute an MMA\noperation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u4\n/\n.s4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight non-zero\n.u4\n/\n.s4\nelements\nout of 16 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 138\n.\nFigure 138\nSparse MMA .m16n8k64 fragment layout for matrix A with\n.u4\n/\n.s4\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n16\nlastcol\n=\nfirstcol\n+\n15\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k64\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in\nFigure 139\n.\nFigure 139\nSparse MMA .m16n8k64 metadata layout for\n.u4\n/\n.s4\ntype.\n\n9.7.14.6.2.8.\nMatrix Fragments for sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\ninteger type\n\nA warp executing sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\n/\n.e2m1\ninteger type will compute an MMA\noperation of shape\n.m16n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u4\n/\n.s4\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight non-zero\n.u4\n/\n.s4\nelements out\nof 16 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight non-zero\n.e2m1\nelements out\nof 16 consecutive elements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 140\nand\nFigure 141\n.\nFigure 140\nSparse MMA .m16n8k128 fragment layout for columns 0–63 of matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 141\nSparse MMA .m16n8k128 fragment layout for columns 64–127 of matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\n||\n16\n<=\ni\n<\n24\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n16\nFor\nai\nwhere\ni\n<\n16\n(\nthreadID_in_group\n*\n16\n)\n+\n64\nFor\nai\nwhere\ni\n>=\n16\nlastcol\n=\nfirstcol\n+\n15\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.u4\n/\n.s4\nA vector expression containing four\n.b32\nregisters, each containing\neight\n.u4\n/\n.s4\nelements from matrix B.\nb0, b1, b2, b3, …, b31\n.e2m1\nA vector expression containing four\n.b32\nregisters, each containing\neight\n.e2m1\nelements from matrix B.\nThe layout of the fragments held by different threads is shown in\nFigure 142\n,\nFigure 143\n,\nFigure 144\n,\nFigure 145\n.\nFigure 142\nSparse MMA .m16n8k128 fragment layout for rows 0–31 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 143\nSparse MMA .m16n8k128 fragment layout for rows 32–63 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 144\nSparse MMA .m16n8k128 fragment layout for rows 64–95 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 145\nSparse MMA .m16n8k128 fragment layout for rows 96–127 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k64\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in\nFigure 146\nand\nFigure 147\n.\nFigure 146\nSparse MMA .m16n8k128 metadata layout for  columns 0–63 for\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 147\nSparse MMA .m16n8k128 metadata layout for  columns 64–127 for\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\n9.7.14.6.3.\nMultiply-and-Accumulate Instruction:\nmma.sp\n/\nmma.sp::ordered_metadata\n\nmma.sp\n,\nmma.sp::ordered_metadata\nPerform matrix multiply-and-accumulate operation with sparse matrix A\nSyntax\nHalf precision floating point type:\nmma.spvariant.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\n\n.ctype     = {.f16, .f32};\n.dtype     = {.f16, .f32};\n.spvariant = {.sp, .sp::ordered_metadata};\nAlternate floating point type:\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32      d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.tf32.tf32.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k64.row.col.f32.f8type.f8type.f32 d, a, b, c, e, f;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind.dtype.f8f6f4type.f8f6f4type.ctype d, a, b, c, e, f;\n\n.f8type     = {.e4m3, .e5m2};\n.spvariant  = {.sp, .sp::ordered_metadata};\n.f8f6f4type = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.kind       = {kind::f8f6f4};\n.ctype      = {.f16, .f32};\n.dtype      = {.f16, .f32};\nAlternate floating point type with block scaling:\nmma.spvariant.sync.aligned.m16n8k128.row.col.kind.block_scale{.scale_vec_size}.f32.e2m1.e2m1.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf4};\n.scale_vec_size = {.scale_vec::2X};\n.stype          = {.ue8m0};\n\nmma.spvariant.sync.aligned.m16n8k128.row.col.kind.block_scale.scale_vec_size.f32.e2m1.e2m1.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf4nvf4};\n.scale_vec_size = {.scale_vec::2X, .scale_vec::4X};\n.stype          = {.ue8m0, .ue4m3};\n\nmma.spvariant.sync.aligned.m16n8k64.row.col.kind.block_scale{.scale_vec_size}.f32.f8f6f4type.f8f6f4type.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf8f6f4};\n.scale_vec_size = {.scale_vec::1X};\n.f8f6f4type     = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.stype          = {.ue8m0};\nInteger type:\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k32, .m16n8k64}\n.atype     = {.u8, .s8};\n.btype     = {.u8, .s8};\n.spvariant = {.sp, .sp::ordered_metadata};\n\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k64, .m16n8k128}\n.atype     = {.u4, .s4};\n.btype     = {.u4, .s4};\n.spvariant = {.sp, .sp::ordered_metadata};\nDescription\nPerform a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+C\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the C and D matrices are\nMxN\n.\nA warp executing\nmma.sp.sync/mma.sp::ordered_metadata.sync\ninstruction compute a single matrix\nmultiply and accumulate operation.\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply and accumulate operation\nas specified in the section\nBlock Scaling\n. The data type corresponding\nto each of the element within\nscale_A\nand\nscale_B\nmatrices is specified by\n.stype\n.\nQualifier\n.scale_vec_size\nspecifies the number of columns of\nscale_A\nmatrix and number of\nrows in the matrix\nscale_B\n.\nThe valid combinations of\n.kind\n,\n.stype\nand\n.scale_vec_size\nare described in\nTable 36\n. For\nmma\nwith\n.kind::mxf4\nwhen the\nqualifier\n.scale_vec_size\nis not specified, then it defaults to\n2X\n. In contrast,\nwhen\n.kind\nis specified as\n.kind::mxf8f6f4\nthen the qualifier\n.scale_vec_size\ndefaults to\n1X\n. However, for\n.kind::mxf4nvf4\n, it is mandatory to provide valid\n.scale_vec_size\n.\nOperands\na\nand\nb\nrepresent two multiplicand matrices A and B, while\nc\nand\nd\nrepresent the accumulator and destination matrices, distributed across the threads in warp. Matrix A\nis structured sparse as described in\nSparse matrix storage\nOperands\ne\nand\nf\nrepresent sparsity\nmetadata and sparsity selector respectively. Operand\ne\nis a 32-bit integer and operand\nf\nis\na 32-bit integer constant with values in the range 0..3.\nWhen\n.block_scale\nqualifier is specified, operand\nscale-a-data\n,\nscale-b-data\nrepresents\nthe scale matrix metadata corresponding to\nscale_A\nand\nscale_B\nmatrices respectively.\nThe tuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nrepresent selectors for\nmatrices\nscale_A\nand\nscale_B\nrespectively from their corresponding metadata arguments\nscale-a-data\n,\nscale-b-data\n. The operands\nscale-a-data\n,\nscale-b-data\nare of type\n.b32\n. The operands\nbyte-id-a\n,\nthread-id-a\n,\nbyte-id-b\n,\nthread-id-b\nare unsigned\n16-bit integer values. For more details on selector arguments refer\nBlock Scaling for mma.sync\nsection.\nInstruction\nmma.sp::ordered_metadata\nrequires the indices in the sparsity metadata to be sorted\nin an increasing order starting from LSB, otherwise behavior is undefined.\nThe registers in each thread hold a fragment of matrix as described in\nMatrix fragments for multiply-accumulate operation with sparse matrix A\n.\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively. The qualifier\n.stype\nindicate the\ndata-type of the elements in the matrices\nscale_A\nand\nscale_B\n. In case of shapes\n.m16n8k16\n,\n.m16n8k32\nand\n.m16n8k64\n,\n.dtype\nmust be the same as\n.ctype\n.\nWhen\n.kind\nis either of\n.kind::mxf8f6f4\nor\n.kind::f8f6f4\n, the individual 4-bit and\nthe 6-bit floating point type elements must be packed in an 8-bit container. The matrix element\nof type\n.e2m1\nresides in central 4 bits of the 8-bit container with padding in the upper 2\nbits and lower 2 bits of the container. When the matrix element is of type\n.e3m2\nor\n.e2m3\n,\nthe matrix element resides in the lower 6 bits of the 8-bit container with padding in the upper\n2 bits of the container. In contrast, note that when using\nmma\nwith\n.kind::mxf4\nor\n.kind::mxf4nvf4\n, no explicit padding is necessary even though matrix elements are of type\n.e2m1\n.\nPrecision and rounding :\n.f16\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with at least single\nprecision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values\nis performed with at least single precision. When both\n.ctype\nand\n.dtype\nare specified\nas\n.f16\n, the accumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.e4m3\n,\n.e5m2\n,\n.e3m2\n,\n.e2m3\n,\n.e2m1\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations :\nThe integer\nmma.sp/mma.sp::ordered_metadata\noperation is performed with\n.s32\naccumulators.\nThe\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the range\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nmma.sp/mma.sp::ordered_metadata\ninstruction causes\nthe executing thread to wait until all threads in the warp execute the same\nmma.sp/mma.sp::ordered_metadata\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmma.sp/mma.sp::ordered_metadata\ninstruction. In conditionally executed code, a\nmma.sp/mma.sp::ordered_metadata\ninstruction should only be used if it is known that all threads in the warp evaluate the condition identically,\notherwise behavior is undefined.\nThe behavior of\nmma.sp/mma.sp::ordered_metadata\ninstruction is undefined if all threads in the same warp\ndo not use the same qualifiers, or if any thread in the warp has exited.\nNotes\nmma.sp\ninstruction may have substantially reduced performance on some target architectures.\nHence, it is advised to use\nmma.sp::ordered_metadata\ninstruction.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.1.\nSupport for\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation introduced in\nPTX ISA version 8.4.\nmma.sp::ordered_metadata\nintroduced in PTX ISA version 8.5.\nSupport for shape\n.m16n8k32\nand\n.f16\ndtype/ctype with\n.e4m3\n/\n.e5m2\nalternate floating\npoint type\nmma\noperation introduced in PTX ISA version 8.7.\nSupport for\n.e3m2\n,\n.e2m3\n,\n.e2m1\nalternate floating point type\nmma\noperation introduced\nin PTX ISA version 8.7.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier introduced in PTX ISA version 8.7.\nSupport for\n.scale_vec::4X\non\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nis introduced in\nPTX ISA version 9.1\nTarget ISA Notes\nRequires\nsm_80\nor higher.\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation requires\nsm_89\nor higher.\nmma.sp::ordered_metadata\nrequires\nsm_80\nor higher.\nSupport for shape\n.m16n8k32\nand\n.f16\ndtype/ctype with\n.e4m3\n/\n.e5m2\nalternate floating\npoint type\nmma\noperation requires\nsm_120\n.\n.e3m2\n,\n.e2m3\nand\n.e2m1\nalternate floating point type\nmma\noperation requires\nsm_120a\nand are supported on\nsm_120f\nor higher in the same family from PTX ISA version 8.8.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier requires\nsm_120a\nand are\nsupported on\nsm_120f\nand later generation targets in the same family from PTX ISA version 8.8 except for\n.kind::mxf4nvf4\n/\n.kind::mxf4\n.\nQualifiers\n.kind::mxf4nvf4\nand\n.kind::mxf4\nare supported on following architectures:\nsm_120a\nsm_121a\nExamples of half precision floating point type\n// f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\n\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\n\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\nExamples of alternate floating point type\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k64.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::f8f6f4.f32.e3m2.e2m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::f8f6f4.f16.e2m3.e2m1.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1}, %Re, 0;\nExamples of integer type\n.reg .b32 %Ra<4>, %Rb<4>, %Rc<4>, %Rd<4>;\n.reg .u32 %Re;\n\n// u8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k32.row.col.satfinite.s32.u8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// s8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// s8 elements in A and B matrix with ordered metadata\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.s32.s4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k128.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\nExamples of mma with block scale\n.reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .b32 %Re;\n mma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4.block_scale.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1, %Rb2, %Rb3},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   %Re, 0,\n   scaleAData, {2, 1}, scaleBData, {2, 3};\n\n .reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .u16 bidA, bidB, tidA, tidB;\n .reg .b32 %Re;\n mma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue4m3\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1, %Rb2, %Rb3},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   %Re, 0,\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .u16 bidA, bidB, tidA, tidB;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue8m0\n {%Rd0, %Rd1, %Rd2, %Rd3},\n {%Ra0, %Ra1, %Ra2, %Ra3},\n {%Rb0, %Rb1, %Rb2, %Rb3},\n {%Rc0, %Rc1, %Rc2, %Rc3},\n %Re, 0,\n scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e3m2.e2m1.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  %Re, 0,\n  scaleAData, {0, 1}, scaleBData, {0, 1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e4m3.e5m2.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2,  %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  %Re, 0,\n  scaleAData, {0, 1}, scaleBData, {0, 0};\n9.7.15.\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions\n\nThe warpgroup level matrix multiply and accumulate operation has either of the following forms,\nwhere matrix\nD\nis called accumulator:\nD\n=\nA\n*\nB\n+\nD\nD\n=\nA\n*\nB\n, where the input from accumulator D is disabled.\nThe\nwgmma\ninstructions perform warpgroup level matrix multiply-and-accumulate operation by\nhaving all threads in a warpgroup collectively perform the following actions:\nLoad matrices A, B and D into registers or into shared memory.\nPerform the following\nfence\noperations:\nwgmma.fence\noperations to indicate that the register/shared-memory across the warpgroup\nhave been written into.\nfence.proxy.async\noperation to make the generic proxy operations visible to the async\nproxy.\nIssue the asynchronous matrix multiply and accumulate operations using the\nwgmma.mma_async\noperation on the input matrices. The\nwgmma.mma_async\noperation is performed in the async\nproxy.\nCreate a wgmma-group and commit all the prior outstanding\nwgmma.mma_async\noperations into the\ngroup, by using\nwgmma.commit_group\noperation.\nWait for the completion of the required wgmma-group.\nOnce the wgmma-group completes, all the\nwgmma.mma_async\noperations have been performed and\ncompleted.\n9.7.15.1.\nWarpgroup\n\nA warpgroup is a set of four contiguous warps such that the\nwarp-rank\nof the first warp is a\nmultiple of 4.\nwarp-rank of a warp is defined as:\n(%tid.x + %tid.y * %ntid.x  + %tid.z * %ntid.x * %ntid.y) / 32\n9.7.15.2.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\n, where A is an\nMxK\nmatrix, B is a\nKxN\nmatrix, while D is a\nMxN\nmatrix.\nThe following matrix shapes are supported for the specified types for the\nwgmma.mma_async\noperation:\nMultiplicand Data type\nSparsity\nShape\nFloating-point -\n.f16\nDense\n.m64n8k16\n,\n.m64n16k16\n,\n.m64n24k16\n,\n.m64n32k16\n,\n.m64n40k16\n,\n.m64n48k16\n,\n.m64n56k16\n,\n.m64n64k16\n,\n.m64n72k16\n,\n.m64n80k16\n,\n.m64n88k16\n,\n.m64n96k16\n,\n.m64n104k16\n,\n.m64n112k16\n,\n.m64n120k16\n,\n.m64n128k16\n,\n.m64n136k16\n,\n.m64n144k16\n,\n.m64n152k16\n,\n.m64n160k16\n,\n.m64n168k16\n,\n.m64n176k16\n,\n.m64n184k16\n,\n.m64n192k16\n,\n.m64n200k16\n,\n.m64n208k16\n,\n.m64n216k16\n,\n.m64n224k16\n,\n.m64n232k16\n,\n.m64n240k16\n,\n.m64n248k16\n,\n.m64n256k16\nAlternate floating-point\nformat -\n.bf16\nAlternate floating-point\nformat -\n.tf32\nSparse\nAlternate floating-point\nformat -\n.tf32\nDense\n.m64n8k8\n,\n.m64n16k8\n,\n.m64n24k8\n,\n.m64n32k8\n,\n.m64n40k8\n,\n.m64n48k8\n,\n.m64n56k8\n,\n.m64n64k8\n,\n.m64n72k8\n,\n.m64n80k8\n,\n.m64n88k8\n,\n.m64n96k8\n,\n.m64n104k8\n,\n.m64n112k8\n,\n.m64n120k8\n,\n.m64n128k8\n,\n.m64n136k8\n,\n.m64n144k8\n,\n.m64n152k8\n,\n.m64n160k8\n,\n.m64n168k8\n,\n.m64n176k8\n,\n.m64n184k8\n,\n.m64n192k8\n,\n.m64n200k8\n,\n.m64n208k8\n,\n.m64n216k8\n,\n.m64n224k8\n,\n.m64n232k8\n,\n.m64n240k8\n,\n.m64n248k8\n,\n.m64n256k8\nAlternate floating-point\nformat -\n.e4m3\n/\n.e5m2\nDense\n.m64n8k32\n,\n.m64n16k32\n,\n.m64n24k32\n,\n.m64n32k32\n,\n.m64n40k32\n,\n.m64n48k32\n,\n.m64n56k32\n,\n.m64n64k32\n,\n.m64n72k32\n,\n.m64n80k32\n,\n.m64n88k32\n,\n.m64n96k32\n,\n.m64n104k32\n,\n.m64n112k32\n,\n.m64n120k32\n,\n.m64n128k32\n,\n.m64n136k32\n,\n.m64n144k32\n,\n.m64n152k32\n,\n.m64n160k32\n,\n.m64n168k32\n,\n.m64n176k32\n,\n.m64n184k32\n,\n.m64n192k32\n,\n.m64n200k32\n,\n.m64n208k32\n,\n.m64n216k32\n,\n.m64n224k32\n,\n.m64n232k32\n,\n.m64n240k32\n,\n.m64n248k32\n,\n.m64n256k32\nFloating point -\n.f16\nSparse\nAltername floating-point\nformat -\n.bf16\nInteger -\n.u8\n/\n.s8\nDense\n.m64n8k32\n,\n.m64n16k32\n,\n.m64n24k32\n,\n.m64n32k32\n,\n.m64n48k32\n,\n.m64n64k32\n,\n.m64n80k32\n,\n.m64n96k32\n,\n.m64n112k32\n,\n.m64n128k32\n,\n.m64n144k32\n,\n.m64n160k32\n,\n.m64n176k32\n,\n.m64n192k32\n,\n.m64n208k32\n,\n.m64n224k32\n,\n.m64n240k32\n,\n.m64n256k32\nAlternate floating-point\nformat -\n.e4m3\n/\n.e5m2\nSparse\n.m64n8k64\n,\n.m64n16k64\n,\n.m64n24k64\n,\n.m64n32k64\n,\n.m64n40k64\n,\n.m64n48k64\n,\n.m64n56k64\n,\n.m64n64k64\n,\n.m64n72k64\n,\n.m64n80k64\n,\n.m64n88k64\n,\n.m64n96k64\n,\n.m64n104k64\n,\n.m64n112k64\n,\n.m64n120k64\n,\n.m64n128k64\n,\n.m64n136k64\n,\n.m64n144k64\n,\n.m64n152k64\n,\n.m64n160k64\n,\n.m64n168k64\n,\n.m64n176k64\n,\n.m64n184k64\n,\n.m64n192k64\n,\n.m64n200k64\n,\n.m64n208k64\n,\n.m64n216k64\n,\n.m64n224k64\n,\n.m64n232k64\n,\n.m64n240k64\n,\n.m64n248k64\n,\n.m64n256k64\nInteger -\n.u8\n/\n.s8\nSparse\n.m64n8k64\n,\n.m64n16k64\n,\n.m64n24k64\n,\n.m64n32k64\n,\n.m64n48k64\n,\n.m64n64k64\n,\n.m64n80k64\n,\n.m64n96k64\n,\n.m64n112k64\n,\n.m64n128k64\n,\n.m64n144k64\n,\n.m64n160k64\n,\n.m64n176k64\n,\n.m64n192k64\n,\n.m64n208k64\n,\n.m64n224k64\n,\n.m64n240k64\n,\n.m64n256k64\nSingle-bit -\n.b1\nDense\n.m64n8k256\n,\n.m64n16k256\n,\n.m64n24k256\n,\n.m64n32k256\n,\n.m64n48k256\n,\n.m64n64k256\n,\n.m64n80k256\n,\n.m64n96k256\n,\n.m64n112k256\n,\n.m64n128k256\n,\n.m64n144k256\n,\n.m64n160k256\n,\n.m64n176k256\n,\n.m64n192k256\n,\n.m64n208k256\n,\n.m64n224k256\n,\n.m64n240k256\n,\n.m64n256k256\n9.7.15.3.\nMatrix Data-types\n\nThe matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point.\nFor floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later.\nFor integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have\nelements of the same data-type, e.g. both signed integer or both unsigned integer.\nData-type\nMultiplicands (A or B)\nAccumulator (D)\nInteger\nboth\n.u8\nor both\n.s8\n.s32\nFloating Point\n.f16\n.f16\n,\n.f32\nAlternate floating Point\n.bf16\n.f32\nAlternate floating Point\n.tf32\n.f32\nAlternate floating Point\n.e4m3\n,\n.e5m2\n.f16\n,\n.f32\nSingle-bit integer\n.b1\n.s32\n9.7.15.4.\nAsync Proxy\n\nThe\nwgmma.mma_async\noperations are performed in the asynchronous proxy (or async proxy).\nAccessing the same memory location across multiple proxies needs a cross-proxy fence. For the async\nproxy,\nfence.proxy.async\nshould be used to synchronize memory between generic proxy and the\nasync proxy.\nThe completion of a\nwgmma.mma_async\noperation is followed by an implicit generic-async proxy\nfence. So the result of the asynchronous operation is made visible to the generic proxy as soon as\nits completion is observed.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used\nto wait for the completion of the\nwgmma.mma_async\ninstructions.\n9.7.15.5.\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using\nwgmma.mma_async\ninstruction\n\nThis section describes warpgroup level\nwgmma.mma_async\ninstruction and the organization of\nvarious matrices involved in this instruction.\n9.7.15.5.1.\nRegister Fragments and Shared Memory Matrix Layouts\n\nThe input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared\nmemory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This\nsection describes the layouts of register fragments and shared memory expected by the warpgroup MMA\ninstructions.\nWhen the matrices are in shared memory, their starting addresses must be aligned to 16 bytes.\n9.7.15.5.1.1.\nRegister Fragments\n\nThis section describes the organization of various matrices located in register operands of the\nwgmma.mma_async\ninstruction.\n9.7.15.5.1.1.1.\nMatrix Fragments for\nwgmma.mma_async.m64nNk16\n\nA warpgroup executing\nwgmma.mma_async.m64nNk16\nwill compute an MMA operation of shape\n.m64nNk16\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.f16x2\nregisters, with each\nregister containing two\n.f16\n/\n.bf16\nelements from matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 148\n.\nFigure 148\nWGMMA .m64nNk16 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing N/4 number of\n.f16x2\nregisters, with each register containing two\n.f16\nelements from matrix D.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\n.f32\nA vector expression containing N/2 number of\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown in\nFigure 149\n.\nFigure 149\nWGMMA .m64nNk16 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.2.\nMatrix Fragments for\nwgmma.mma_async.m64nNk8\n\nA warpgroup executing\nwgmma.mma_async.m64nNk8\nwill compute an MMA operation of shape\n.m64nNk8\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters containing\nfour\n.tf32\nelements from matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 150\n.\nFigure 150\nWGMMA .m64nNk8 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.f32\nA vector expression containing N/2 number of\n.f32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\nThe layout of the fragments held by different threads is shown in\nFigure 151\n.\nFigure 151\nWGMMA .m64nNk8 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.3.\nMatrix Fragments for\nwgmma.mma_async.m64nNk32\n\nA warpgroup executing\nwgmma.mma_async.m64nNk32\nwill compute an MMA operation of shape\n.m64nNk32\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.u8\n/\n.s8\nelements from matrix A.\na0, a1, a2, a3, … , a14, a15\n.e4m3\n/\n.e5m2\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\nelements from\nmatrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 152\n.\nFigure 152\nWGMMA .m64nNk32 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\nMiscellaneous Information\n.s32\nA vector expression containing\nN/2 number of\n.s32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\ndepends on .dtype, as\ndescribed in the next column.\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n3,\n4}\n=\n16*i\nwhere\ni\n=\n{3,\n4,\n...,\n15,\n16}\n.f32\nA vector expression containing\nN/2 number of\n.f32\nregisters.\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\n.f16\nA vector expression containing\nN/4 number of\n.f16x2\nregisters, with each register\ncontaining two\n.f16\nelements from matrix D.\nThe layout of the fragments held by different threads is shown in\nFigure 153\n.\nFigure 153\nWGMMA .m64nNk32 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.4.\nMatrix Fragments for\nwgmma.mma_async.m64nNk256\n\nA warpgroup executing\nwgmma.mma_async.m64nNk256\nwill compute an MMA operation of shape\n.m64nNk256\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing thirty two\n.b1\nelement from matrix A.\na0, a1, a2, …, a127\nThe layout of the fragments held by different threads is shown in\nFigure 154\n.\nFigure 154\nWGMMA .m64nNk256 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing N/2 number of\n.s32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n3,\n4}\n=\n16*i\nwhere\ni\n=\n{3,\n4,\n...,\n15,\n16}\nThe layout of the fragments held by different threads is shown in\nFigure 155\n.\nFigure 155\nWGMMA .m64nNk256 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.2.\nShared Memory Matrix Layout\n\nIf the argument\nimm-trans-a\n/\nimm-trans-b\nof the instruction\nwgmma.mma_async{.sp}\nis 0, then\nK-major\nis used for matrix\nA\n/\nB\nrespectively. If the value of argument\nimm-trans-a\nis 1 then\nM-major\nis used for matrix\nA\n. If the value of the argument\nimm-trans-b\nis 1, then\nN-major\nis used for matrix\nB\n.\nIn a column-major default BLAS library such as cuBLAS, the matrices\nA\nand\nB\nwith and\nwithout transpose can be classified as either\nK-Major\nor\nM-or-N-Major\nas shown in the\nfollowing table:\nNon-Transposed\nTransposed\nA\nK-major\nM-major\nB\nK-major\nN-major\nTo avoid confusion with\nA\n,\nB\n,\nrow-major\n,\ncol-major\n,\ntranspose\n, and\nnon-transpose\n, we will use\nMN-Major\nand\nK-Major\nthroughout this section.\nThe matrices in the shared memory are made up of one or more “swizzle layout atom”.\nThe exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,\nand the leading dimension. The layout of the swizzle are shown in\nTable 38\n.\nTable 38\nVarious combinations of swizzling mode, leading dimension and swizzle-atom layout\n\nSwizzling mode\nLeading Dimension\n/ Major-ness\nSwizzle atom layout\n(128b element)\n128B Swizzling Mode\nM/N\n8x8\nK\n8x8\n64B Swizzling Mode\nM/N\n4x8\nK\n8x4\n32B Swizzling Mode\nM/N\n2x8\nK\n8x2\nNone\nM/N\n1x8\nK\n8x1\nThe above shapes are for elements of size 128 bits. For smaller elements sizes, the same\nshapes would get multiplied along the leading dimension by a factor of\n128/sizeof_bits(Element)\n.\nFor example, 128B MN major swizzle atom would have a shape of\n(8*(128/32))x8\n=\n32x8\nfor\ntf32\ntensor core inputs.\nExamples\nThe following are some example layouts of\nMxK\nor\nKxN\nmatrices with various swizzling modes,\nand are in units of 128b elements as shown\nby each colored cell as shown in\nFigure 156\n,\nFigure 157\n,\nFigure 158\n,\nFigure 159\n,\nFigure 160\n,\nFigure 161\n,\nFigure 162\n,\nFigure 163\n.\nFigure 156\nMN major 128B swizzling\n\nFigure 157\nK major 128B swizzling\n\nFigure 158\nMN major 64B swizzling\n\nFigure 159\nK major 64B swizzling\n\nFigure 160\nMN major 32B swizzling\n\nFigure 161\nK major 32B swizzling\n\nFigure 162\nMN major interleaved\n\nFigure 163\nK major interleaved\n\nFollowing are some of the examples of the 128B swizzling layout for\ntf32\nelement type.\nK-Major:\nFigure 164\nFigure 164\nK major\n\nMN-Major:\nFigure 165\nFigure 165\nMN major\n\n9.7.15.5.1.2.1.\nMajor-ness supported by Strides\n\nThere are two strides involved while accessing a matrix from shared memory:\nLeading dimension byte offset\nStride dimension byte offset\n9.7.15.5.1.2.1.1.\nLeading Dimension Byte Offset\n\nThe leading dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The leading byte offset is defined as follows for matrices whose element types are\nnormalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nNo-Swizzling: the offset from the first column to the second columns\nof the 8x2 tile in the 128-bit element type normalized matrix.\nSwizzled layouts: not used, assumed to be 1.\nMN-Major\nInterleave: offset from the first 8 columns to the next 8 columns.\nSwizzled layouts: offset from the first (swizzle-byte-size/16) rows\nto the next (swizzle-byte-size/16) rows.\n9.7.15.5.1.2.1.2.\nStride Dimension Byte Offset\n\nThe stride dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The stride dimension byte offset is defined as follows for matrices whose element\ntypes are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nThe offset from the first 8 rows to the next 8 rows.\nMN-Major\nInterleave: offset from the first row to the next row.\nSwizzled layout: offset from the first 8 columns to the next 8\ncolumns\n9.7.15.5.1.2.1.3.\nCanonical Layouts\n\nIn terms of\nCuTe layouts\nthe canonical layout can be expressed as follows:\nMajor-\nness\nSwizzling mode\nCanonical Layout without swizzling\nSwizzling\non the previous column\nMN-\nmajor\nNo-swizzling or\nInterleaved\n((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nSwizzle<1, 4, 3>\n64B Swizzling\n((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nSwizzle<2, 4, 3>\n128B Swizzling\n((T,8,m),(8,k)):((1,T,LBO),(8T,SBO))\nSwizzle<3, 4, 3>\nK-\nmajor\nNo-swizzling or\nInterleaved\n((8,m),(T,2k)):((1T,SBO),(1,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((8,m),(T,2k)):((2T,SBO),(1,T))\nSwizzle<1, 4, 3>\n64B Swizzling\n((8,m),(T,2k)):((4T,SBO),(1,T))\nSwizzle<2, 4, 3>\n128B Swizzling\n((8,m),(T,2k)):((8T,SBO),(1,T))\nSwizzle<3, 4, 3>\nwhere\nT = 128 / sizeof-elements-in-bits\nT represents scale factor which normalizes matrix element types to 128-bits.\nm represents the number of repeating patterns across rows.\nk represents the number of repeating patterns across columns.\nExamples\nK-Major, no-swizzling and tf32 type:\nFigure 166\nFigure 166\nK major, no-swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))\nCanonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO\n64*sizeof(tf32)\nSBO\n32*sizeof(tf32)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nK-Major, 32B swizzling and tf32 type:\nFigure 167\nFigure 167\nK major, 32B swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))\nCanonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO\nNA\nSBO\n64*sizeof(tf32)\nEncoding of LBO in descriptor\n1 (assumed)\nEncoding of SBO in descriptor\n(SBO) >> 4 = 16\nMN-Major, no-swizzling and bf16 type:\nFigure 168\nFigure 168\nMN major, no-swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))\nCanonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n128*sizeof(bf16)\nSBO\n64*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nMN-Major, 32B swizzling and bf16 type:\nFigure 169\nFigure 169\nMN major, 32B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))\nCanonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n128*sizeof(bf16)\nSBO\n256*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 32\nMN-Major, 64B swizzling and bf16 type:\nFigure 170\nFigure 170\nMN major, 64B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))\nCanonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n256*sizeof(bf16)\nSBO\n512*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 32\nEncoding of SBO in descriptor\n(SBO) >> 4 = 64\n9.7.15.5.1.2.2.\nMatrix Descriptor Format\n\nMatrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in\nthe matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the\nfollowing layout:\nBit-field\nSize in bits\nDescription\n13–0\n14\nmatrix-descriptor-encode(Matrix start address)\n29–16\n14\nmatrix-descriptor-encode\n(\nLeading dimension byte offset\n)\n45–32\n14\nmatrix-descriptor-encode\n(\nStride dimension byte offset\n)\n51–49\n3\nMatrix base offset. This is valid for all swizzling modes except the no-swizzle mode.\n63–62\n2\nSpecifies the swizzling mode to be used:\n0: No swizzle\n1: 128-Byte swizzle\n2: 64-Byte swizzle\n3: 32-Byte swizzle\nwhere\nmatrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4\nThe value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as\nper the below table:\nSwizzling mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the base offset must be a non-zero value, computed using the following formula:\nbase offset = (pattern start addr >> 0x7) & 0x7\n9.7.15.5.2.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async\n\nwgmma.mma_async\nPerform matrix multiply-and-accumulate operation across warpgroup\nSyntax\nHalf precision floating point type:\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype   = {.f16, .f32};\nAlternate floating point type :\n.bf16 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8,\n            .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8,\n            .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8,\n            .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8,\n            .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,\n            .m64n168k8, .m64n176k8, .m64n184k8, .m64n192k8,\n            .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8,\n            .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32};\nInteger type:\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32,\n            .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32,\n            .m64n176k32, .m64n192k32, .m64n208k32, .m64n224k32};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8};\nSingle bit:\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256,\n            .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256,\n            .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256,\n            .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256,\n            .m64n240k256, .m64n256k256};\n.op  = {.and};\nDescription\nInstruction\nwgmma.mma_async\nissues a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the D matrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nscale-d\nis\nfalse.\nwgmma.fence\ninstruction must be used to fence the register accesses of\nwgmma.mma_async\ninstruction from their prior accesses. Otherwise, the behavior is undefined.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed.\nRegister operand\nd\nrepresents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand\na\nrepresents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands\na-desc\nand\nb-desc\nare the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the warps\nin the warpgroup. The format of the matrix descriptor is described in\nMatrix Descriptor Format\n.\nMatrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments\nimm-trans-a\nand\nimm-trans-b\nrespectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of\nimm-trans-a\nand\nimm-trans-b\nare 0\nand 1. The transpose operation is only supported for the\nwgmma.mma_async\nvariants with\n.f16\n/\n.bf16\ntypes on matrices accessed from shared memory using matrix descriptors.\nFor the floating point variants of the\nwgmma.mma_async\noperation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands\nimm-scale-a\nand\nimm-scale-b\nrespectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of\nimm-scale-a\nand\nimm-scale-b\nare -1 and 1.\nThe qualifiers\n.dtype\n,\n.atype\nand\n.btype\nindicate the data type of the elements in\nmatrices D, A and B respectively.\n.atype\nand\n.btype\nmust be the same for all floating point\nwgmma.mma_async\nvariants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the\nwgmma.mma_async\noperation are as follows:\nMatrices A and B have 8-bit data elements when\n.atype\n/\n.btype\nis\n.e4m3\n/\n.e5m2\n.\nMatrices A and B have 16-bit data elements when\n.atype\n/\n.btype\nis\n.bf16\n.\nMatrices A and B have 32-bit data elements when\n.atype\n/\n.btype\nis\n.tf32\n.\nPrecision and rounding:\nFloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with at least single\nprecision. When\n.dtype\nis\n.f16\n, the accumulation is performed with at least half\nprecision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified\nprecision.\nwgmma.mma_async\noperation involving type\n.tf32\nwill truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations:\nThe integer\nwgmma.mma_async\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the\nrange\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.mma_async\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.mma_async\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.mma_async\ninstruction. In conditionally executed code, a\nwgmma.mma_async\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.u8.s8\nand\n.s8.u8\nas .atype.btype introduced in PTX ISA version 8.4.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples of half precision floating point type\n.reg .f16x2 f16a<40>, f16d<40>;\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n.reg .pred  scaleD;\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16\n  {f16d0, f16d1,  f16d2,  f16d3,  f16d4,  f16d5,  f16d6,  f16d7,  f16d8,\n   f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17},\n  descA,\n  descB,\n  scaleD, -1, 1, 1, 0;\nExamples of alternate floating point type\n.reg .f32   f32d<40>;\n.reg .b32   bf16a<40>\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9,\n   f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19,\n   f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29,\n   f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39,\n   f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49,\n   f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59},\n  {bf16a0, bf16a1, bf16a2, bf16a3},\n  descB,\n  scaleD, -1, -1, 0;\n\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7},\n  descA,\n  descB,\n  0, -1, -1;\n\n.reg .b32 f16d<8>, f16a<8>;\n.reg .f32 f32d<8>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2\n  {f16d0, f16d1},\n  descA,\n  descB,\n  scaleD, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1;\nExamples of integer type\n.reg .s32 s32d<8>, s32a<8>;\n.reg .u32 u32a<8>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\nExamples of single bit type\n.reg .s32 s32d<4>;\n.reg .b32 b32a<4>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\n\nwgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc\n  {s32d0, s32d1, s32d2, s32d3},\n  {b32a0, b32a1, b32a2, b32a3},\n  descB,\n  scaleD;\n9.7.15.6.\nAsynchronous Warpgroup Level Multiply-and-Accumulate Operation using\nwgmma.mma_async.sp\ninstruction\n\nThis section describes warp-level\nwgmma.mma_async.sp\ninstruction with sparse matrix A. This\nvariant of the\nwgmma.mma_async\noperation can be used when A is a structured sparse matrix with\n50% zeros in each row distributed in a shape-specific granularity. For an\nMxNxK\nsparse\nwgmma.mma_async.sp\noperation, the\nMxK\nmatrix A is packed into\nMxK/2\nelements. For each\nK-wide row of matrix A, 50% elements are zeros and the remaining\nK/2\nnon-zero elements are\npacked in the operand representing matrix A. The mapping of these\nK/2\nelements to the\ncorresponding K-wide row is provided explicitly as metadata.\n9.7.15.6.1.\nSparse matrix storage\n\nGranularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a\n64x32\nmatrix A used in floating point\nwgmma.mma_async\noperations, sparsity is expected to be at 2:4 granularity, i.e. each 4-element\nvector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each\nnon-zero element in a sub-chunk is stored in the metadata operand. Values\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nare invalid values for metadata and will result in undefined behavior. In a\ngroup of four consecutive threads, one or more threads store the metadata for the whole group\ndepending upon the matrix shape. These threads are specified using an additional sparsity selector operand.\nMatrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in\nFigure 111\n, with an appropriate matrix size.\nGranularities for different matrix shapes and data types are described below.\nSparse\nwgmma.mma_async.sp\nwith half-precision and\n.bf16\ntype\nFor\n.f16\nand\n.bf16\ntypes, for all supported\n64xNx32\nshapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand.\nFigure 171\nSparse WGMMA metadata example for\n.f16\n/\n.bf16\ntype.\n\nThe sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith\n.tf32\ntype\nFor\n.tf32\ntype, for all supported\n64xNx16\nshapes, matrix A is structured sparse at a\ngranularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have\none zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and\nthe 4-bit index in the metadata indicates the position of the non-zero element in the two-wide\nchunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in\nan undefined behavior.\nFigure 172\nSparse WGMMA metadata example for\n.tf32\ntype.\n\nThe sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith\n.e4m3\nand\n.e5m2\nfloating point type\nFor\n.e4m3\nand\n.e5m2\ntypes, for all supported\n64xNx64\nshapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand.\nFigure 173\nSparse WGMMA metadata example for\n.e4m3\n/\n.e5m2\ntype.\n\nAll threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith integer type\nFor the integer type, for all supported\n64xNx64\nshapes, matrix A is structured sparse at a\ngranularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have\ntwo zeroes and two non-zero elements. Only the two non-zero elements are stored in matrix A and two\n2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide\nchunk.\nFigure 174\nSparse WGMMA metadata example for\n.u8\n/\n.s8\ntype.\n\nAll threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior.\n9.7.15.6.2.\nMatrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A\n\nIn this section we describe how the contents of thread registers are associated with fragments of A\nmatrix and the sparsity metadata.\nEach warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following\ntable shows the assignment of warps to rows of matrix A:\nWarp\nSparsity information for rows of matrix A\n%warpid\n% 4 = 3\n48-63\n%warpid\n% 4 = 2\n32-47\n%warpid\n% 4 = 1\n16-31\n%warpid\n% 4 = 0\n0-15\nThe following conventions are used throughout this section:\nFor matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data.\nFor matrix D, since the matrix dimension - data type combination is the same for all supported\nshapes, and is already covered in\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction\n, the pictorial\nrepresentations of matrix fragments are not included in this section.\nFor the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included.\nTk:\n[m..n]\npresent\nin cell\n[x][y..z]\nindicates that bits\nm\nthrough\nn\n(with\nm\nbeing higher) in the\nmetadata operand of thread with\n%laneid=k\ncontains the indices of the non-zero elements from\nthe chunk\n[x][y]..[x][z]\nof matrix A.\n9.7.15.6.2.1.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk32\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk32\nwill compute an MMA operation of shape\n.m64nNk32\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nShared Memory Matrix Layout\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters, with each register containing two\nnon-zero\n.f16\n/\n.bf16\nelements out of 4\nconsecutive elements from matrix A.\nNon-zero elements:\na0, a1, a2, a3, a4, a5, a6, a7\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\nThe layout of the fragments held by different threads is shown in\nFigure 175\n.\nFigure 175\nSparse WGMMA .m64nNk32 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk32\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nShared Memory Matrix Layout\n.\nMetadata operand is a\n.b32\nregister containing 16 2-bit vectors each storing the index of a\nnon-zero element of a 4-wide chunk of matrix A.\nFigure 176\nshows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable\ni\nrepresents the value of the sparsity\nselector operand.\nFigure 176\nSparse WGMMA .m64nNk32 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.15.6.2.2.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk16\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk16\nwill compute an MMA operation of shape\n.m64nNk16\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nShared Memory Matrix Layout\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.tf32\nA vector expression containing four\n.b32\nregisters, containing four non-zero\n.tf32\nelements out of eight consecutive elements\nfrom matrix A.\nNon-zero elements:\na0, a1, a2, a3\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\nThe layout of the fragments held by different threads is shown in\nFigure 177\n.\nFigure 177\nSparse WGMMA .m64nNk16 fragment layout for matrix A with\n.tf32\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk8\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nShared Memory Matrix Layout\n.\nMetadata operand is a\n.b32\nregister containing eight 4-bit vectors each storing the index of a\nnon-zero element of a 2-wide chunk of matrix A.\nFigure 178\nshows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable\ni\nrepresents the value of the sparsity\nselector operand.\nFigure 178\nSparse WGMMA .m64nNk16 metadata layout for\n.tf32\ntype.\n\n9.7.15.6.2.3.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk64\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk64\nwill compute an MMA operation of shape\n.m64nNk64\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nMatrix Fragments for sparse wgmma.mma_async.m64nNk64\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.e4m3\n/\n.e5m2\nA vector expression containing four\n.b32\nregisters, with each register containing four\nnon-zero\n.e4m3\n/\n.e5m2\nelements out of\neight consecutive elements from matrix A.\nNon-zero elements:\na0, a1, a2, … , a15\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each register containing four\nnon-zero\n.s8\n/\n.u8\nelements out of\neight consecutive elements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 179\n.\nFigure 179\nSparse WGMMA .m64nNk64 fragment layout for matrix A with\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk32\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nMatrix Fragments for sparse wgmma.mma_async.m64nNk64\n.\nMetadata operand is a\n.b32\nregister containing 16 4-bit vectors each storing the indices of\ntwo non-zero elements of a 4-wide chunk of matrix A.\nFigure 180\nshows the mapping of the metadata\nbits to the elements of columns 0–31 of matrix A.\nFigure 180\nSparse WGMMA .m64nNk64 metadata layout for\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype for columns 0–31\n\nFigure 181\nshows the mapping of the metadata\nbits to the elements of columns 32–63 of matrix A.\nFigure 181\nSparse WGMMA .m64nNk64 metadata layout for\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype for columns 32–63\n\n9.7.15.6.3.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async.sp\n\nwgmma.mma_async.sp\nPerform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup\nSyntax\nHalf precision floating point type:\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype   = {.f16, .f32};\nAlternate floating point type :\n.bf16 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64,\n            .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64,\n            .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64,\n            .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,\n            .m64n168k64, .m64n176k64, .m64n184k64, .m64n192k64,\n            .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64,\n            .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32};\nInteger type:\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d;\n\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64,\n            .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64,\n            .m64n176k64, .m64n192k64, .m64n208k64, .m64n224k64,\n            .m64n240k64, .m64n256k64};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8};\nDescription\nInstruction\nwgmma.mma_async\nissues a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the D matrix is\nMxN\n.\nThe matrix A is stored in the packed format Mx(K/2) as described in\nSparse matrix storage\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nscale-d\nis\nfalse.\nwgmma.fence\ninstruction must be used to fence the register accesses of\nwgmma.mma_async\ninstruction from their prior accesses. Otherwise, the behavior is undefined.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed.\nRegister operand\nd\nrepresents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand\na\nrepresents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands\na-desc\nand\nb-desc\nare the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the\nwarps in the warpgroup. The format of the matrix descriptor is described in\nMatrix Descriptor Format\n. Matrix A is\nstructured sparse as described in\nSparse matrix storage\n.  Operands\nsp-meta\nand\nsp-sel\nrepresent sparsity metadata and sparsity selector respectively. Operand\nsp-meta\nis a 32-bit\ninteger and operand\nsp-sel\nis a 32-bit integer constant with values in the range 0..3.\nThe valid values of\nsp-meta\nand\nsp-sel\nfor each shape is specified in\nSparse matrix storage\nand are summarized here :\nMatrix shape\n.atype\nValid values of\nsp-meta\nValid values of\nsp-sel\n.m64nNk16\n.tf32\n0b1110 , 0b0100\n0 (threads T0, T1) or 1 (threads T2, T3)\n.m64nNk32\n.f16\n/\n.bf16\n0b00, 0b01, 0b10, 0b11\n0 (threads T0, T1) or 1 (threads T2, T3)\n.m64nNk64\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\n0b00, 0b01, 0b10, 0b11\n0 (all threads contribute)\nMatrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments\nimm-trans-a\nand\nimm-trans-b\nrespectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of\nimm-trans-a\nand\nimm-trans-b\nare 0\nand 1. The transpose operation is only supported for the\nwgmma.mma_async\nvariants with\n.f16\n/\n.bf16\ntypes on matrices accessed from shared memory using matrix descriptors.\nFor the floating point variants of the\nwgmma.mma_async\noperation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands\nimm-scale-a\nand\nimm-scale-b\nrespectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of\nimm-scale-a\nand\nimm-scale-b\nare -1 and 1.\nThe qualifiers\n.dtype\n,\n.atype\nand\n.btype\nindicate the data type of the elements in\nmatrices D, A and B respectively.\n.atype\nand\n.btype\nmust be the same for all floating point\nwgmma.mma_async\nvariants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the\nwgmma.mma_async\noperation are as follows:\nMatrices A and B have 8-bit data elements when\n.atype\n/\n.btype\nis\n.e4m3\n/\n.e5m2\n.\nMatrices A and B have 16-bit data elements when\n.atype\n/\n.btype\nis\n.bf16\n.\nMatrices A and B have 32-bit data elements when\n.atype\n/\n.btype\nis\n.tf32\n.\nPrecision and rounding:\nFloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with at least single\nprecision. When\n.dtype\nis\n.f16\n, the accumulation is performed with at least half\nprecision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified\nprecision.\nwgmma.mma_async\noperation involving type\n.tf32\nwill truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations:\nThe integer\nwgmma.mma_async\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the\nrange\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.mma_async\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.mma_async\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.mma_async\ninstruction. In conditionally executed code, a\nwgmma.mma_async\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.2.\nSupport for\n.u8.s8\nand\n.s8.u8\nas .atype.btype introduced in PTX ISA version 8.4.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples of integer type\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n9.7.15.7.\nAsynchronous\nwgmma\nProxy Operations\n\nThis section describes warpgroup level\nwgmma.fence\n,\nwgmma.commit_group\nand\nwgmma.wait_group\ninstructions.\n9.7.15.7.1.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.fence\n\nwgmma.fence\nEnforce an ordering of register accesses between\nwgmma.mma_async\nand other operations.\nSyntax\nwgmma.fence.sync.aligned;\nDescription\nwgmma.fence\ninstruction establishes an ordering between prior accesses to any warpgroup\nregisters and subsequent accesses to the same registers by a\nwgmma.mma_async\ninstruction. Only\nthe accumulator register and the input registers containing the fragments of matrix A require this\nordering.\nThe\nwgmma.fence\ninstruction must be issued by all warps of the warpgroup at the following\nlocations:\nBefore the first\nwgmma.mma_async\noperation in a warpgroup.\nBetween a register access by a thread in the warpgroup and any\nwgmma.mma_async\ninstruction\nthat accesses the same registers, either as accumulator or input register containing fragments of\nmatrix A, except when these are accumulator register accesses across multiple\nwgmma.mma_async\ninstructions of the same shape. In the latter case, an ordering guarantee is provided by default.\nOtherwise, the behavior is undefined.\nAn async proxy fence must be used to establish an ordering between prior writes to shared memory\nmatrices and subsequent reads of the same matrices in a\nwgmma.mma_async\ninstruction.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.fence\ninstruction causes the executing\nthread to wait until all threads in the warp execute the same\nwgmma.fence\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.fence\ninstruction. In conditionally executed code, an\nwgmma.fence\ninstruction\nshould only be used if it is known that all threads in the warpgroup evaluate the condition\nidentically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\n// Example 1, first use example:\nwgmma.fence.sync.aligned;    // Establishes an ordering w.r.t. prior accesses to the registers s32d<0-3>\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n\n// Example 2, use-case with the input value updated in between:\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\n...\nmov.b32 s32d0, new_val;\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d4, s32d5, s32d6, s32d7},\n                                                 {s32d0, s32d1, s32d2, s32d3},\n                                                  descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n9.7.15.7.2.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.commit_group\n\nwgmma.commit_group\nCommits all prior uncommitted\nwgmma.mma_async\noperations into a\nwgmma-group\n.\nSyntax\nwgmma.commit_group.sync.aligned;\nDescription\nwgmma.commit_group\ninstruction creates a new wgmma-group per warpgroup and batches all prior\nwgmma.mma_async\ninstructions initiated by the executing warp but not committed to any\nwgmma-group into the new wgmma-group. If there are no uncommitted\nwgmma.mma_async\ninstructions\nthen\nwgmma.commit_group\nresults in an empty wgmma-group.\nAn executing thread can wait for the completion of all\nwgmma.mma_async\noperations in a\nwgmma-group by using\nwgmma.wait_group\n.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.commit_group\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.commit_group\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.commit_group\ninstruction. In conditionally executed code, an\nwgmma.commit_group\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\nwgmma.commit_group.sync.aligned;\n9.7.15.7.3.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.wait_group\n\nwgmma.wait_group\nSignal the completion of a preceding warpgroup operation.\nSyntax\nwgmma.wait_group.sync.aligned N;\nDescription\nwgmma.wait_group\ninstruction will cause the executing thread to wait until only N or fewer of\nthe most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing\nthreads are complete. For example, when N is 0, the executing thread waits on all the prior\nwgmma-groups to complete. Operand N is an integer constant.\nAccessing the accumulator register or the input register containing the fragments of matrix A of a\nwgmma.mma_async\ninstruction without first performing a\nwgmma.wait_group\ninstruction that\nwaits on a\nwgmma-group\nincluding that\nwgmma.mma_async\ninstruction is undefined behavior.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.wait_group\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.wait_group\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.wait_group\ninstruction. In conditionally executed code, an\nwgmma.wait_group\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\nwgmma.fence.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3},\n                                                  {f16a0, f16a1, f16a2, f16a3},\n                                                   descB, 1, -1, -1, 1;\nwgmma.commit_group.sync.aligned;\n\nwgmma.wait_group.sync.aligned 0;\n9.7.16.\nTensorCore 5th Generation Family Instructions\n\n9.7.16.1.\nTensor Memory\n\nThe 5\nth\ngeneration TensorCore has dedicated on-chip memory that is specialized for use by\nTensorCore operations. This Tensor Memory is organized as a two-dimensional matrix where\nthe horizontal rows are called lanes and the vertical columns are called columns.\nOn architecture\nsm_100a\n/\nsm_100f\n, the 5\nth\ngeneration TensorCore’s Tensor Memory has a\ntwo-dimensional structure of 512 columns and 128 rows per CTA, with each cell being 32-bits in size.\nRestrictions on threads accessing the Tensor Memory via the load and store operations\nare specified in\nAccess restrictions\n.\n9.7.16.1.1.\nTensor Memory Addressing\n\nTensor Memory addresses are 32-bit wide and specify two components.\nLane index\nColumn index\nThe layout is as follows:\n31                      16\n15                       0\nLane index\nColumn index\nFigure 182\nshows the view of the Tensor Memory Layout within CTA.\nFigure 182\nTensor Memory Layout and Addressing\n\n9.7.16.1.2.\nTensor Memory Allocation\n\nThe Tensor Memory is dynamically allocated. The Tensor Memory must be allocated by a single\nwarp in a CTA using the\nTensor Memory Allocation and Management Instructions\n.\nThe allocation and deallocation of\nTensor Memory\nis performed in terms of\ncolumns. The unit of allocation is 32 columns and the number of columns being allocated must be\na power of 2. When a column is allocated, all 128 lanes of the column are allocated.\nAll of the Tensor Memory that was allocated in a kernel, must be explicitly deallocated\nbefore the kernel exits.\n9.7.16.2.\nMatrix and Data Movement Shape\n\nThere are two kinds of shapes involved.\nShapes in the data movement operations\nShapes in the MMA operations\n9.7.16.2.1.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand matrices\nA\n,\nB\nand\nD\n. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\nwhere\nA\nis\nMxK\nmatrix,\nB\nis a\nKxN\nmatrix, and\nD\nis a\nMxN\nmatrix.\nTable 39\nshows matrix shapes that are supported for the specified types for the\ntcgen05.mma\noperation.\nTable 39\nVarious combinations of .kind and shapes\n\nVarious Combinations\nShapes Supported\n.kind::*\nHas .ws\nCTA Group\nSparsity\ndtype\natype/btype\nkind::f16\nNo\n.ws\n1\nDense\n.f16\n.f16\n64xNxK\n128xNxK\nN = {8, 16, 24, … 256} steps of 8\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nK = 32\n.f32\n.f16\n,\n.bf16\n2\nDense\n.f16\n.f16\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nK = 32\n.f32\n.f16\n,\n.bf16\n.ws\n1\nDense\n.f16\n.f16\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nN = {64, 128}\nK = 32\n.f32\n.f16\n,\n.bf16\n2\nEither\n.f16\n.f16\nInvalid\n.f32\n.f16\n,\n.bf16\n.kind::tf32\nNo\n.ws\n1\nDense\n.f32\n.tf32\n64xNxK\n128xNxK\nN = {8, 16, 24, … 256} steps of 8\nK = 8\nSparse\nK = 16\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 8\nSparse\nK = 16\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 8\nSparse\nN = {64, 128}\nK = 16\n2\nDense\nInvalid\nSparse\n.kind::f8f6f4\nNo\n.ws\n1\nDense\n.f32\n.f16\n.e4m3\n,\n.e5m2\n,\n.e2m3\n,\n.e3m2\n,\n.e2m1\n64xNxK\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 32\nSparse\nK = 64\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 32\nSparse\nN = {64, 128}\nK = 64\n2\nDense\nInvalid\nSparse\n.kind::mxf8f6f4\nNo\n.ws\n1\nDense\n.f32\n.e4m3\n,\n.e5m2\n,\n.e2m3\n,\n.e3m2\n,\n.e2m1\nX\n(Scale)\n.ue8m0\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 32\nSparse\n256xNxK\nK = 64\n.ws\n1\nDense\nInvalid\nSparse\n2\nDense\nSparse\n.kind::i8\nNo\n.ws\n1\nDense\n.s32\n.s8\n,\n.u8\n64xNxK\n128xNxK\nN = {8, 16, 24, 32, 48, … 256}\nsteps of 16 after N > 32\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {32, 64, … 256} steps of 32\nK = 32\nSparse\nK = 64\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 32\nSparse\nN = {64, 128}\nK = 64\n2\nDense\nInvalid\nSparse\n.kind::mxf4\nNo\n.ws\n1\nDense\n.f32\n.e2m1\nX\n(Scale)\n.ue8m0\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 64\nSparse\nK = 128\n2\nDense\n128xNxK\n256xNxK\n256xNxK\n1\nN = {16, 32, … 256} steps of 16\nK = 64\nK\n1\n= 96\nSparse\n256xNxK\nK = 128\n.ws\n1 / 2\nEither\nInvalid\n.kind::mxf4nvf4\nNo\n.ws\n1\nDense\n.f32\n.e2m1\nX\n(Scale)\n.ue8m0\n,\n.ue4m3\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 64\nSparse\nK = 128\n2\nDense\n128xNxK\n256xNxK\n256xNxK\n1\nN = {16, 32, … 256} steps of 16\nK = 64\nK\n1\n= 96\nSparse\n256xNxK\nK = 128\n.ws\n1 / 2\nEither\nInvalid\n9.7.16.2.1.1.\nTarget ISA Note\n\nK = 96 is only supported for target architecture\nsm_103a\n.\n9.7.16.2.2.\nSpecifying Matrix Shape\n\nM\nand\nN\ncan be specified in the\nInstruction descriptor\n.\nK\ncannot be explicitly specified but is implicitly determined by the MMA-kind\nand the sparsity, as shown in the\nTable 39\n.\n9.7.16.2.3.\nData Movement Shape\n\nThe data movement shape indicates the dimension of the data to be moved to or from the\nTensor Memory\n. These shapes are described as a tuple\nlane\nx\nsize\nwhere:\nlane\nindicates the number of rows in the\nTensor Memory\n; and\nsize\nindicates the amount of data, in units of bits (b), across the columns in the\nTensor Memory\n.\nThe following shapes are supported by various tcgen05 operations:\nShape\ntcgen05.<op>\n.16x64b\n,\n.16x128b\n,\n.16x256b\n,\n.16x32bx2\n,\n.32x32b\n.ld\n/\n.st\n.4x256b\n,\n.32x128b\n,\n.64x128b\n,\n.128x256b\n,\n.128x128b\n.cp\n.31x256b\n(implicit)\n.shift\n9.7.16.2.3.1.\nMemory Layout\n\nThe following shows the layout of the matrix fragments across threads of the warp.\n9.7.16.2.3.1.1.\nMatrix fragments for shape .32x32b\n\nA\ntcgen05{.ld,.st}.32x32b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.32x32b\nwill access 32 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (32 * .num)-bits of data as shown in\nFigure 183\n.\nFigure 183\nMatrix Fragment for shape .32x32b\n\n9.7.16.2.3.1.2.\nMatrix fragments for shape .16x64b\n\nA\ntcgen05{.ld,.st}.16x64b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x64b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (64 * .num)-bits of data as shown in\nFigure 184\n.\nFigure 184\nMatrix Fragment for shape .16x64b\n\n9.7.16.2.3.1.3.\nMatrix fragments for shape .16x128b\n\nA\ntcgen05{.ld,.st}.16x128b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x128b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (128 * .num)-bits of data as shown in\nFigure 185\n.\nFigure 185\nMatrix Fragment for shape .16x128b\n\n9.7.16.2.3.1.4.\nMatrix fragments for shape .16x256b\n\nA\ntcgen05{.ld,.st}.16x256b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, r2, r3, …\nA warp executing\ntcgen05{.ld,.st}.16x256b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (256 * .num)-bits of data as shown in\nFigure 186\n.\nFigure 186\nMatrix Fragment for shape .16x256b\n\n9.7.16.2.3.1.5.\nMatrix fragments for shape .16x32bx2\n\nA\ntcgen05{.ld,.st}.16x32bx2\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x32bx2\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (32 * .num)-bits of data as shown in\nFigure 187\n.\nFigure 187\nMatrix Fragment for shape .16x32bx2\n\n9.7.16.3.\nMajor-ness supported by Strides\n\nThere are two strides involved while accessing a matrix from shared memory:\nLeading dimension stride (byte offset or absolute address)\nStride dimension byte offset\n9.7.16.3.1.\nLeading Dimension Stride: relative offset or absolute address\n\nThere are two modes of Leading Dimension Strides as described below.\nBit #52 in the\nShared memory descriptor\nis used to distinguish between two modes.\n9.7.16.3.1.1.\nRelative offset mode\n\nIn this mode, the leading dimension stride is specified as a relative byte offset between the\ncolumns as explained in the below table.\nThe leading dimension stride can either be specified as a relative offset between the columns\nor as an absolute byte address of next buffer. The leading dimension stride is defined\ndifferently for transposed and non-transposed matrices. The leading dimension stride is defined\nas follows for matrices whose element types are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nNo-Swizzling: the stride from the first column to the second column\nof the 8x2 tile in the 128-bit element type normalized matrix.\nSwizzled layouts: not used, assumed to be 1.\nMN-Major\nInterleave: stride from the first 8 columns to the next 8 columns.\nSwizzled layouts: stride from the first (swizzle-byte-size/16) rows\nto the next (swizzle-byte-size/16) rows.\n9.7.16.3.1.2.\nAbsolute address mode for K dimension being 48B\n\nThe\ntcgen05.mma\ninstruction with\nK-dimension\nof 48B would overflow the 128B\nshared memory boundary if the data is packed contiguously.\nIn this case, the absolute address mode can be used to break up the data in the\nshared memory into two chunks such that both these chunks are laid out within\nthe aligned 128-byte address boundary.\nThe leading dimension absolute address can point to the second data chunk in the shared memory.\n9.7.16.3.1.2.1.\nRestrictions on the Leading Dimension Absolute Address Stride\n\nFollowing are the restrictions on the absolute address stride mode:\nOnly 128B swizzle (with 16B atomicity) is supported.\nOnly K-Major mode is supported. That is, the transpose bits(bits #15 and #16) in\nInstruction descriptor\nmust be 0.\nThe matrix base offset must be 0.\n9.7.16.3.2.\nStride Dimension Byte Offset\n\nThe stride dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The stride dimension byte offset is defined as follows for matrices whose element\ntypes are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nThe offset from the first 8 rows to the next 8 rows.\nMN-Major\nInterleave: offset from the first row to the next row.\nSwizzled layout: offset from the first 8 columns to the next 8\ncolumns\n9.7.16.3.3.\nCanonical Layouts\n\nIn terms of\nCuTe layouts\nthe canonical layout can be expressed as follows:\nMajor-\nness\nSwizzling mode\nCanonical Layout without swizzling\nSwizzling\non the previous column\nMN-\nmajor\nNo-swizzling or\nInterleaved\n((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nSwizzle<1, 4, 3>\n64B Swizzling\n((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nSwizzle<2, 4, 3>\n128B Swizzling\n((T,8,m),(8,k)):((1,T,LBO),(8T,SBO))\nSwizzle<3, 4, 3>\nK-\nmajor\nNo-swizzling or\nInterleaved\n((8,m),(T,2k)):((1T,SBO),(1,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((8,m),(T,2k)):((2T,SBO),(1,T))\nSwizzle<1, 4, 3>\n64B Swizzling\n((8,m),(T,2k)):((4T,SBO),(1,T))\nSwizzle<2, 4, 3>\n128B Swizzling\n((8,m),(T,2k)):((8T,SBO),(1,T))\nSwizzle<3, 4, 3>\nwhere\nT = 128 / sizeof-elements-in-bits\nT represents scale factor which normalizes matrix element types to 128-bits.\nm represents the number of repeating patterns across rows.\nk represents the number of repeating patterns across columns.\nExamples\nK-Major, no-swizzling and tf32 type:\nFigure 188\nFigure 188\nK major, no-swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))\nCanonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO (relative offset)\n64*sizeof(tf32)\nSBO\n32*sizeof(tf32)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nK-Major, 32B swizzling and tf32 type:\nFigure 189\nFigure 189\nK major, 32B swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))\nCanonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO (relative offset)\nNA\nSBO\n64*sizeof(tf32)\nEncoding of LBO in descriptor\n1 (assumed)\nEncoding of SBO in descriptor\n(SBO) >> 4 = 16\nMN-Major, no-swizzling and bf16 type:\nFigure 190\nFigure 190\nMN major, no-swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))\nCanonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n128*sizeof(bf16)\nSBO\n64*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nMN-Major, 32B swizzling and bf16 type:\nFigure 191\nFigure 191\nMN major, 32B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))\nCanonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n128*sizeof(bf16)\nSBO\n256*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 32\nMN-Major, 64B swizzling and bf16 type:\nFigure 192\nFigure 192\nMN major, 64B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))\nCanonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n256*sizeof(bf16)\nSBO\n512*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 32\nEncoding of SBO in descriptor\n(SBO) >> 4 = 64\n9.7.16.4.\nMatrix Descriptors\n\nThere are three kinds of matrix descriptors used by the\ntcgen05\nfamily of instructions.\n9.7.16.4.1.\nShared memory descriptor\n\nThe shared memory descriptor describes the properties of multiplicand matrix in shared\nmemory including its location in the shared memory of the current\nCTA\n. It is a 64-bit\nvalue contained in a register with the following layout:\nTable 40\nShared memory descriptor layout\n\nBit-field\nSize in bits\nDescription\n0-13\n14\nmatrix-descriptor-encode (Matrix start address)\n16-29\n14\nmatrix-descriptor-encode\n(\nLeading dimension byte offset relative\n)\nOR\nmatrix-descriptor-encode\n(\nLeading dimension byte address absolute\n)\n32-45\n14\nmatrix-descriptor-encode\n(\nStride dimension byte offset\n)\n46-48\n3\nFixed constant value of 0b001\n49-51\n3\nMatrix base offset\n52\n1\nLeading dimension stride mode:\n- 0: byte offset relative\n- 1: byte address absolute\n53-60\n8\nFixed constant value of 0xb00000000\n61-63\n3\nSpecifies the swizzling mode to be used:\n0. No swizzling\n1. 128-Byte with 32B atomic swizzling\n2. 128-Byte swizzling\n4. 64-Byte swizzling\n6. 32-Byte swizzling\nNote: Values 3, 5 and 7 are invalid\nwhere matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4\nThe value of base offset is 0 when the repeating pattern of the specified swizzling mode\nstarts as per shown in\nTable 41\n.\nTable 41\nStarting address of repeating pattern for various swizzling modes\n\nSwizzling mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the base offset must be a non-zero value, computed using the following formula:\nbase\noffset\n=\n(pattern\nstart\naddr\n>>\n0x7)\n&\n0x7\nThe following must be 16-byte aligned:\nMatrix start address\nLeading dimension byte offset\nStride dimension byte offset\n9.7.16.4.1.1.\nTarget ISA Note\n\nThe byte address mode for the leading dimension stride is supported on\nsm_103a\n.\n9.7.16.4.2.\nInstruction descriptor\n\nThe instruction descriptor describes the shapes, types and other details of all the matrices\nand the matrix-multiplication-and-accumulation operation. It is a 32-bit value in registers\nand the exact layout is dependent on the MMA-Kind:\nTable 42\nInstruction descriptor format for .kind::tf32, .kind::f16, .kind::f8f6f4 and .kind::i8\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::tf32\n.kind::f16\n.kind::f8f6f4\n.kind::i8\n0-1\n2\nSparsity selector\n,\nif Sparsity is enabled\n0-3\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nSaturate for integer types\n0 (NA)\nNo Saturate = 0\nSaturate = 1\n4-5\n2\ndtype (Matrix D type)\nF32 = 1\nF16 = 0\nF32 = 1\nS32 = 2\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nTF32 = 2\nF16 = 0\nBF16 = 1\nE4M3 = 0\nE5M2 = 1\nE2M3 = 3\nE3M2 = 4\nE2M1 = 5\nUnsigned 8b = 0\nSigned 8b = 1\n10-12\n3\nbtype (Matrix B type)\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\nNo Negate = 0\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\nTranspose = 1\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nReserved\n0\n24-28\n5\nM, Dimension of Matrix A\n(4 LSBs not included)\nM >> 4\n29\n1\nReserved\n0\n30-31\n2\nMaximum shift while attempting\nB matrix -reuse in\n.ws\nno shift = 0\nmaximum shift of 8 = 1\nmaximum shift of 16 = 2\nmaximum shift of 32 = 3\nTable 43\nInstruction descriptor format for .kind::mxf8f6f4\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::mxf8f6f4\n0-1\n2\nReserved\n0\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nReserved\n0\n4-5\n2\nMatrix B Scale Factor Data ID\n0-3\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nE4M3 = 0\nE5M2 = 1\nE2M3 = 3\nE3M2 = 4\nE2M1 = 5\n10-12\n3\nbtype (Matrix B type)\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\nTranspose = 1\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nScale Matrix Type, for both scale_A /\nscale_B\nUE8M0 = 1\n24-26\n3\nReserved\n0\n27-28\n2\nM, Dimension of Matrix A\n(7 LSBs not included)\nM >> 7\n29-30\n2\nMatrix A Scale Factor Data ID\n0-3\n31\n1\nReserved\n0\nTable 44\nInstruction descriptor format for .kind::mxf4 and .kind::mxf4nvf4\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::mxf4\n.kind::mxf4nvf4\n0-1\n2\nReserved\n0\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nReserved\n0\n4-5\n2\nMatrix B Scale Factor Data ID\n0 or 2\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nE2M1 = 1\n10-11\n2\nbtype (Matrix B type)\n12\n1\nReserved\n0\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nScale Matrix Type, for both scale_A /\nscale_B\nUE8M0 = 1\nUE4M3 = 0\n24-26\n3\nReserved\n0\n27-28\n2\nM, Dimension of Matrix A\n(7 LSBs not included)\nM >> 7\n29-30\n2\nMatrix A Scale Factor Data ID\n0 or 2\n31\n1\nK Dimension\n(Dense K=64 / Sparse K=128) = 0\n(Dense K=96) = 1\n9.7.16.4.3.\nZero-Column Mask Descriptor\n\nThe zero-column mask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix will have zero value for the MMA operation regardless of the values present in\nthe shared memory. The total size of the generated mask is N-bits.\nA 0-bit in the mask specifies that values of the corresponding column in matrix\nB\nshould\nbe used for the MMA operation. A 1-bit in the mask specifies 0s must be used for the entire\ncolumn for the MMA operation.\nThe zero-column mask descriptor is a 64-bit value in registers with the following layout:\nTable 45\nZero-Column Mask descriptor layout\n\nBits\nSize (bits)\nField Name\nDescription\n0-7\n8\nStart Count 0 (sc0)\nSpecifies the LSBs that must be skipped\nfor sub-mask mask-i\n8-15\n8\nStart Count 1 (sc1)\n16-23\n8\nStart Count 2 (sc2)\n24-31\n8\nStart Count 3 (sc3)\n32\n1\nFirst Span 0 (fs0)\nSpecifies the starting value for\nsub-mask mask-i\n33\n1\nFirst Span 1 (fs1)\n34\n1\nFirst Span 2 (fs2)\n35\n1\nFirst Span 3 (fs3)\n36-38\n3\nReserved\n39\n1\nNon-Zero Mask\nValue 0 indicates generated mask will have all 0s\nValue 1 indicates the mask has to be generated\n40-47\n8\nSkip Span\n(Count of consecutive columns where B matrix is used) - 1\n48-55\n8\nUse Span\n(Count of consecutive columns where 0s ar used) - 1\n56-61\n6\nColumn Shift\nShifts column by specified amount.\nThus allows MMA on non-0 starting column.\nMax shift amount = 16 for M=32\nMax shift amount = 32 otherwise\nThe zero-column mask is made up of one or more sub-mask depending on M, as shown in the table:\nM\nZero-Column Mask breakup\nSub-masks\nFirst Span used\nStart Column used\n128\nSingle sub-mask of size N-bits\nmask0\nfs0\nsc0\n64\nTwo sub-masks, each with size of N/2 bits\nmask0, mask1\nfs0, fs1\nsc0, sc1\n32\nFour sub-masks, each with size of N/4 bits\nmask0, mask1\nmask2, mask3\nfs0, fs1, fs2,\nfs3\nsc0, sc1, sc2,\nsc3\nThe following table shows the coverage of the sub-masks across N-dimension:\nSub-mask\nM\n128\n64\n32\nmask0\nColumns [0, N-1]\nColumns [0, N/2-1]\nColumns [0, N/4-1]\nmask1\n–\nColumns [N/2, N-1]\nColumns [N/4, N/2-1]\nmask2\n–\n–\nColumns [N/2, (N/4*3)-1]\nmask3\n–\n–\nColumns [(N/4*3), N-1]\nThe following examples shows zero-column mask descriptor and their corresponding mask generated:\nExample 1: M = 128\nInput zero-column mask descriptor:\nStart count\nFirst span\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{0, 0, 0, 0}\n{0, 0, 0, 0}\n0\n4\n3\n0\nOutput zero-column mask: 0x0.\nAs Non-Zero Mask field is 0, the mask is 0x0. All the columns of the matrix\nB\nwill be used\nfor the MMA operation.\nExample 2: M = 128\nInput zero-column mask descriptor:\nStart count\nFirst span\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{-, -, -, 0}\n{-, -, -, 0}\n1\n2\n3\n0\nOutput mask0: 0b … 111 0000 111 0000 (size = N)\nExample 3: M = 64\nInput zero-column mask descriptor:\nStart count\n{.., sc1, sc0}\nFirst span\n{.., fs1, fs0}\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{-, -, 0, 0}\n{-, -, 0, 1}\n1\n2\n3\n0\nOutput mask0: 0b …   111 0000 111 0000 111\nOutput masl1: 0b … 0000 111 0000 111 0000\nExample 4: M = 32\nInput zero-column mask descriptor:\nStart count\n{sc3, sc2, sc1, sc0}\nFirst span\n{fs3, fs2, fs1, fs0}\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{1, 2, 1, 0}\n{0, 0, 1, 1}\n1\n2\n3\n2\nOutput mask0: 0b … 0000 111 0000 111\nOutput mask1: 0b … 0000 111 0000 11\nOutput mask2: 0b … 111 0000 111 00\nOutput mask3: 0b … 111 0000 111 000\nIf N = 128 then\nB\nMatrix with columns from 2 to 129 will be used for the MMA operation,\ndue to the shift of 2.\n9.7.16.5.\nIssue Granularity\n\nEach of the\ntcgen05\noperation has different requirements for the number of\nthreads/warps that needs to issue them.\nThe following table lists the execution granularity requirements of each of the\ntcgen05\noperation:\nTable 46\nExecution granularity requirements for tcgen05 operations\n\ntcgen05 operation\n.cta_group\nIssue Granularity\n.\nmma\n,\n.\ncp\n,\n.\nshift\n,\n.\ncommit\n::1\nAn issue from a single thread in the current\nCTA would initiate the base operation.\n::2\nIssue from a single thread from the\nCTA-Pair\nwould initiate\nthe base operation.\nWhen the current CTA issues the operation, the peer\nCTA should be active and should not have exited.\n.\nalloc\n,\n.\ndealloc\n,\n.\nrelinquish_alloc_permit\n::1\nIssue from a single warp in the current CTA\nwould initiate the allocation management instruction.\n::2\nIssue from two warps, one in each of the current CTA\nand its\nPeer CTA\n, in order to\ncollectively perform the operation, i.e., the first\nwarp to perform the operation could block until the\nthe second warp in the\nPeer CTA\nalso performs the operation (see examples below).\n.\nld\n,\n.\nst\n,\n.\nwait\n::\n{\nld\n,\nst\n}\nN/A\nIssue from a warp in the current CTA can access only\n1/4 of the Tensor Memory of the current CTA. So, a\nwarpgroup is needed to access the entire Tensor Memory\nof the current CTA.\n.\nfence\n::*\nN/A\nA thread needs to fence all its accesses to the tensor\nmemory that it wants to order with other accesses to\nthe tensor memory from other threads.\nThe following example shows that:\nBefore attempting to deallocate Tensor Memory, it suffices to ensure that there are no concurrent\nTensor Memory accesses from the\nPeer CTA\n.\nWarps can immediately exit after deallocating Tensor Memory; no extra synchronization required.\nTable 47\nExample of deallocating Tensor Memory before CTA exit.\n\nCTA0 Warp\nCTA1 Warp\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\nThis example uses a cluster barrier for illustration purposes but in practice other\nsynchronization mechanisms are often used.\nThe following example illustrates a scenario in which the program exhibits non-deterministic\nbehavior due to incorrect synchronizaton because\n.dealloc\nmay or may not block:\nTable 48\nExample of non-determinstic hang due to incorrect synchronization\n\nCTA0 Warp\nCTA1 Warp\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\ntcgen05.dealloc.2cta.sync.aligned;\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\nexit;\n9.7.16.5.1.\nCTA Pair\n\nAny 2 CTAs within the cluster whose\n%cluster_ctarank\ndiffers by the last bit only\nis said to form a CTA pair.\nWithin a CTA pair, the CTA whose last bit in the\n%cluster_ctarank\nis:\n0 is termed the even numbered CTA within the CTA pair.\n1 is termed as the odd numbered CTA within the CTA pair.\nMost of the\ntcgen05\noperations can either execute at a single CTA level granularity OR\nat a CTA pair level granularity. When a\ntcgen05\noperation is performed at CTA pair\ngranularity, the Tensor Memory of both the CTAs within the CTA pair are accessed. The set\nof threads that need to issue the\ntcgen05\noperation is listed in the\nIssue Granularity\n.\n9.7.16.5.2.\nPeer CTA\n\nThe peer CTA of the odd CTA within the CTA pair is the even CTA in the same pair.\nSimilarly, the peer CTA of the even CTA within the CTA pair is the odd CTA in the same pair.\n9.7.16.6.\nMemory Consistency Model for 5th generation of TensorCore operations\n\nOrdering of\ntcgen05\ninstructions is described in terms of two key concepts:\nPipelined tcgen05 instructions\nSpecialized tcgen05-specific inter-thread synchronization mechanisms.\nThese concepts combine to form four canonical synchronization patterns, as described further below.\n9.7.16.6.1.\nAsynchronous Operations\n\nThe tcgen05 family of instructions are divided into 2 categories:\nAsynchronous instructions:\nThese\ntcgen05\noperations are not inherently ordered with respect to\nother\ntcgen05\noperations in the same thread (unless pipelined as mentioned below).\nSynchronous instructions:\nThese\ntcgen05\noperations are inherently ordered with respect to other\ntcgen05\noperations in the same order.\nThe Tensor Memory allocation related instructions that access shared memory maintain\nsame-address ordering with respect to non-\ntcgen05\ninstructions.\nThe following table lists the category of each of the\ntcgen05\ninstruction:\ntcgen05.* operation\nCategory\n.alloc\nSynchronous\ninstructions\n.dealloc\n.relinquish_alloc_permit\n.fence::*\n.wait::*\n.commit\n.mma\nAsynchronous\ninstructions\n.cp\n.shift\n.ld\n.st\n9.7.16.6.2.\nPipelined tcgen05 Instructions\n\nThe asynchronous\ntcgen05\noperations may execute and complete in a different order than they\nwere issued. However, some specific pairs of the asynchronous\ntcgen05\ninstructions form\ntcgen05\npipelines, where in the two asynchronous operations are guaranteed to execute in\nthe same order as the instructions that issued them. The specific pairings are as follows:\ntcgen05.mma.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N and accumulator and shape)\ntcgen05.copy.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.cp.4x256b.cta_group::N\n(same N)\ntcgen05.mma.cta_group::N\n->\ntcgen05.shift.cta_group::N\n(same N)\n9.7.16.6.2.1.\nImplicitly pipelined tcgen05 Instructions\n\nInstructions\ntcgen05.commit\nand\ntcgen05.wait\nare implicitly pipelined with respect\nto previously issued\ntcgen05.{mma,cp,shift}\nand\ntcgen05.{ld,st}\ninstructions\nrespectively that they track from the same thread.\n9.7.16.6.2.1.1.\nmbarrier based completion mechanism\n\nCompletion of the following instruction’s asynchronous operations is observed\nthrough the mbarrier based waiting mechanism:\ntcgen05.mma\ntcgen05.cp\ntcgen05.shift\ntcgen05.commit\nis used to track the completion of the above asynchronous instructions.\nFollowing are the implicitly pipelined\ntcgen05\ninstruction pairing that uses mbarrier\nbased completion mechanism:\ntcgen05.mma.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\ntcgen05.cp.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\n9.7.16.6.2.1.2.\ntcgen05.wait\ninstruction based completion mechanism\n\nCompletion of the following instruction’s asynchronous operations is observed through\ntcgen05.wait\nbased waiting mechanism:\ntcgen05.ld\ntcgen05.st\ntcgen05.wait::ld\nand\ntcgen05.wait::st\nis used to track the completion of the\ntcgen05.ld\nand\ntcgen05.st\nasynchronous instructions.\nFollowing are the implicitly pipelined\ntcgen05\ninstruction pairing that uses\ntcgen05.wait\nbased completion mechanism:\ntcgen05.ld\n->\ntcgen05.wait::ld\ntcgen05.st\n->\ntcgen05.wait::st\n9.7.16.6.3.\nSpecialized Inter-thread Synchronization for tcgen05 instructions\n\nThe\ntcgen05\ninstructions support a specialized inter-thread synchronization which are\noptimized for\ntcgen05\nfamily of instructions. The standard memory consistency model\nsynchronization mechanisms also apply to the\ntcgen05\nfamily of instructions.\nThe\nTensorCore 5th Generation Specialized Synchronization Operations\nsection contains the specialized inter-thread\nsynchronization for tcgen05 instructions.\nThe\ntcgen05.fence::before_thread_sync\nand\ntcgen05.fence::after_thread_sync\ncomposes\nwith execution ordering instructions, like morally strong\nld\n/\nst\n/\natom\ninstructions,\nmbarrier\ninstruction,\nbarrier\ninstructions and so on, to establish an ordering between\nthe\ntcgen05\noperations across threads. The asynchronous\ntcgen05\ninstructions that are\nordered across threads also form a\ntcgen05\npipeline.\nAn asynchronous\ntcgen05\noperation prior to a\ntcgen05.fence::before_thread_sync\nis ordered\nbefore all subsequent\ntcgen05\nand the execution ordering operations.\nAn asynchronous\ntcgen05\noperation subsequent to a\ntcgen05.fence::after_thread_sync\nis\nordered after all the prior\ntcgen05\nand the execution ordering operations.\n9.7.16.6.4.\nCanonical synchronization patterns\n\nUsing the above rules, the following are the five canonical synchronization patterns:\n9.7.16.6.4.1.\nPipelined instructions, same thread\n\nIn this pattern, no explicit ordering mechanism is needed and the ordering guarantee is\nprovided by the pipelined instruction pairing.\nExample:\ntcgen05.mma\ntcgen05.mma (same shape and accumulator)\nThe two instructions will be executed in program order.\n9.7.16.6.4.2.\nNon-pipelined instructions, same thread\n\nIn this pattern, explicit waiting mechanisms are used to wait for the completion of the\nasynchronous\ntcgen05\noperations.\nExample 1:\ntcgen05.st\ntcgen05.wait::st\ntcgen05.ld\ntcgen05.wait::st\nis used to wait for the completion of the prior asynchronous\ninstruction\ntcgen05.st\n.\nExample 2:\ntcgen05.mma [d], ...\ntcgen05.commit.mbarrier::arrive::one\nmbarrier.try_wait.relaxed.cluster (loop until successful)\ntcgen05.fence::after_thread_sync\ntcgen05.ld [d], ...\nFor the completion of the asynchronous\ntcgen05.mma\n,\ntcgen05.commit\nis used.\nAs\ntcgen05.ld\nis an asynchronous operation, the instruction\ntcgen05.fence::after_thread_sync\nis needed.\nNo explicit\ntcgen05.fence::before_thread_sync\nis needed as this is implicitly performed by\ntcgen05.commit\n. The combination of\ntcgen05.mma\nand\ntcgen05.commit\nforms a\nconceptual asynchronous pipeline and establishes execution ordering.\ntcgen05.mma [d], ...\ntcgen05.fence::before_thread_sync\nmbarrier::arrive\n9.7.16.6.4.3.\nPipelined instructions, different thread\n\nIn this pattern, no explicit waiting mechanism is needed but proper synchronization between threads is needed.\nExample:\nThread 0\nThread 1\ntcgen05\n.\ncp\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nmma\n9.7.16.6.4.4.\nNon-pipelined instructions, different thread\n\nIn this pattern, the producer threads that issue the asynchronous\ntcgen05\ninstructions\nmust explicitly wait for the instructions’ completion before synchronizing with the consumer threads.\nExample 1:\nThread 0\nThread 1\ntcgen05\n.\nld\ntcgen05\n.\nwait\n::\nld\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nmma\nExample 1:\nThread 0\nThread 1\ntcgen05\n.\nmma\ntcgen05\n.\ncommit\n.\nmbarrier\n::\narrive\n::\none\n[\nmbar\n]\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nmbar\n]\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nld\nThe synchronization mechanisms can also be composed with each other. For example:\nThread 0\nThread 1\ntcgen05\n.\nmma\ntcgen05\n.\ncommit\n.\nmbarrier\n::\narrive\n::\none\n[\nbar1\n]\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nbar1\n]\n// loop\n...\ntcgen05\n.\nfence\n::\nafter_thread_sync\n...\n// completion is guaranteed\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\n[\nbar2\n]\n// loop\n...\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nbar2\n]\n// loop\n...\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nld\n9.7.16.6.4.5.\nRegister dependencies, same thread\n\nFor\ntcgen05.ld\n, an intra-thread ordering through true register dependency will be respected\nregardless of the presence or absence of other forms of synchronization. This form of register\ndependency does not imply any other form of ordering. For example, a register dependency does\nnot imply that a dependee instruction’s memory accesses will be performed before a dependent\ninstruction’s memory accesses. To enforce such memory orderings and avoiding anti-dependency\nhazards around\ntcgen05.ld\n,\ntcgen05.wait::ld\nmust be used.\nExample:\ntcgen05.ld %r1, ...;\ntcgen05.mma ..., %r1, ...;\n9.7.16.6.5.\nShared Memory Accesses\n\nThe shared memory accesses by\ntcgen05.mma\nand\ntcgen05.cp\noperations are performed\nin the asynchronous proxy (async proxy).\nAccessing the same memory location across miltiple proxies needs a cross-proxy fence.\nFor the async proxy,\nfence.proxy.async\nshould be used to synchronize memory between\ngeneric proxy and the async proxy.\n9.7.16.7.\nTensor Memory Allocation and Management Instructions\n\n9.7.16.7.1.\nTensorcore 5th Generation Instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\nDynamic\nTensor Memory\nallocation management instructions\nSyntax\ntcgen05.alloc.cta_group.sync.aligned{.shared::cta}.b32  [dst], nCols;\n\ntcgen05.dealloc.cta_group.sync.aligned.b32              taddr, nCols;\n\ntcgen05.relinquish_alloc_permit.cta_group.sync.aligned;\n\n.cta_group = { .cta_group::1, .cta_group::2 }\nDescription\ntcgen05.alloc\nis a potentially blocking instruction which dynamically allocates\nthe specified number of columns in the\nTensor Memory\nand writes\nthe address of the allocated\nTensor Memory\ninto shared memory\nat the location specified by address operand dst. The\ntcgen05.alloc\nblocks if the\nrequested amount of\nTensor Memory\nis not available and unblocks\nas soon as the requested amount of\nTensor Memory\nbecomes\navailable for allocation.\ntcgen05.dealloc\nis a potentially blocking instruction which deallocates the\nTensor Memory\nspecified by the\nTensor Memory\naddress\ntaddr\n. The operand\ntaddr\nmust point to a previous\nTensor Memory\nallocation.\nIf\n.cta_group::2\nis specified,\nissuing warp and\npeer CTA\nwarp must synchronize\nTensor Memory\naccesses\nbefore attempting to collectively deallocate the\nTensor Memory\n, and\ntcgen05.dealloc\nmay block to collectively performs the deallocation with\nthe other\npeer CTA\n’s warp.\nAll of the Tensor Memory that was allocated using\ntcgen05.alloc\ninstruction in a kernel,\nmust be explicitly deallocated using\ntcgen05.dealloc\nbefore the kernel exits.\nThe unsigned 32-bit operand\nnCols\nspecify the number of columns to be allocated or\nde-allocated. The unit of allocation and de-allocation is 32 columns and all of lanes\nper column. The number of columns must be a power of 2. The operand\nnCols\nmust be\nwithin the range [32, 512]. The number of columns allocated should not increase between\nany two allocations in the execution order within the CTA. Operand\nnCols\nmust be\npower of 2.\nInstruction\ntcgen05.relinquish_alloc_permit\nspecifies that the CTA of the executing\nthread is relinquishing the right to allocate\nTensor Memory\n. So,\nit is illegal for a CTA to perform\ntcgen05.alloc\nafter any of its constituent threads\nexecute\ntcgen05.relinquish_alloc_permit\n.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the address specified by\ndst\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nQualifier\n.cta_group\nspecifies the number of CTAs involved in the allocation and\nde-allocation operation. When\n.cta_group::1\nis specified, one warp from the CTA must\nperform the allocation and de-allocation. When\n.cta_group::2\nis specified, one warp\nfrom each of the\npeer CTAs\nmust collectively perform the allocation and\nde-allocation. Refer to the\nIssue Granularity\nsection.\nWhen\n.cta_group::2\nis specified, the issuing warp must make sure that peer CTA is launched\nand its warps eventually participate in collective operations.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe mandatory\n.sync\nqualifier indicates that the instruction causes the executing thread\nto wait until all threads in the warp execute the same instruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame instruction. In conditionally executed code, the instruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior\nis undefined.\nThe behavior of the instruction is undefined if all the threads in the warp do not use the\nsame values of\nnCols\n, or if any thread in the warp has exited.\nThe store operation in\ntcgen05.alloc\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n// Example 1:\n\ntcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sMemAddr1], 32;\nld.shared.b32 taddr, [sMemAddr1];\n// use taddr ...\n// more allocations and its usages ...\ntcgen05.dealloc.cta_group::1.sync.aligned.b32  taddr, 32;\n// more deallocations ...\ntcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;\n\n// Example 2:\n\n// Following instructions are performed by current warp and the warp in the peer-CTA:\ntcgen05.alloc.cta_group::2.sync.aligned.shared::cta.b32 [sMemAddr2], 32;\nld.shared.b32 taddr, [sMemAddr2];\n// use taddr ...\n// more allocations and its usages ...\ntcgen05.dealloc.cta_group::2.sync.aligned.b32  taddr, 32;\n// more deallocations ...\ntcgen05.relinquish_alloc_permit.cta_group::2.sync.aligned;\n9.7.16.8.\nTensor Memory and Register Load/Store Instructions\n\nThe threads of the CTA can perform the loads and stores to the\nTensor Memory\nof the CTA and move data between registers and Tensor Memory. The loads and stores of data\ncan be performed in certain shapes as specified in the\nMatrix and Data Movement Shape\nsection.\n9.7.16.8.1.\nAccess restrictions\n\nNot all threads of the CTA can access the entire Tensor Memory via the\ntcgen05.ld\nand\ntcgen05.st\noperations.\nThe Tensor Memory of a CTA is divided into 4 equal chunks such that each warp of a warpgroup\nin the CTA can access a chunk of the Tensor Memory. All the columns of the Tensor Memory can\nbe accessed by all the four warps of a warpgroup. A lane of the Tensor Memory can be accessed\nby a single warp in the warpgroup. The following table describes the access restriction.\nID of the warp within the warpgroup\nAccessible Lanes\n0\n0-31\n1\n32-63\n2\n64-95\n3\n96-127\n9.7.16.8.2.\nPacking and Unpacking\n\nOptionally, the following pack and unpack operations can be performed during the load and store:\nPacking: two 16-bit chunks can be packed into a single 32-bit chunk in the register in\ntcgen05.ld\nUnpacking: a single 32-bit chunk in the register can be unpacked into two 16-bit chunks in\ntcgen05.st\nas shown in the\nFigure 193\n.\nFigure 193\nPack/Unpack operations for tcgen05 ld/st\n\n9.7.16.8.3.\nTensorcore 5th Generation Instructions:\ntcgen05.ld\n\ntcgen05.ld\nAsynchronous collective load from tensor memory into registers.\nSyntax\n// Base load instruction:\n\ntcgen05.ld.sync.aligned.shape1.num{.pack}.b32    r, [taddr];\n\ntcgen05.ld.sync.aligned.shape2.num{.pack}.b32    r, [taddr], immHalfSplitoff;\n\n.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }\n.shape2 = { .16x32bx2 }\n.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }\n.pack   = { .pack::16b }\n\n// Floating point type load along with reduction :\n\ntcgen05.ld.red.sync.aligned.shape3.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr];\n\ntcgen05.ld.red.sync.aligned.shape4.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr], immHalfSplitoff;\n\n// Integer type load along with reduction :\n\ntcgen05.ld.red.sync.aligned.shape3.num.redOp.type r, redval, [taddr];\n\ntcgen05.ld.red.sync.aligned.shape4.num.redOp.type r, redval, [taddr], immHalfSplitoff;\n\n.shape3 = { .32x32b   }\n.shape4 = { .16x32bx2 }\n.redOp  = { .min, .max }\n.type   = { .u32, .s32 }\nDescription\nInstruction\ntcgen05.ld\nasynchronously loads data from the\nTensor Memory\nat the location specified by the 32-bit address operand\ntaddr\ninto the destination\nregister\nr\n, collectively across all threads of the warps.\nAll the threads in the warp must specify the same value of\ntaddr\n, which must be the\nbase address of the collective load operation. Otherwise, the behavior is undefined.\nThe\n.shape\nqualifier and the\n.num\nqualifier together determines the total\ndimension of the data which is loaded from the\nTensor Memory\n. The\n.shape\nqualifier indicates the base dimension of data to be accessed as described in the\nData Movement Shape\n. The\n.num\nqualifier indicates\nthe repeat factor on the base dimension resulting in the total dimension of the data that\nis accessed.\nThe shape\n.16x32bx2\nperforms two accesses into Tensor Memory of the shape\n.16x32b\n.\nThe base address of the first access is specified by taddr and the base address of the\nsecond access is specified by\ntaddr+immHalfSplitoff\n, where\nimmHalfSplitoff\nis an\nimmediate argument.\nThe destination operand\nr\nis a brace-enclosed vector expression consisting of one\nor more 32-bit registers as per the value of\n.shape\nand\n.num\n. The size of the\nvector for various combinations of\n.num\nand\n.shape\nis shown in\nTable 49\n.\nTable 49\nVarious-combinations of .num and .shape\n\n.num\n.shape\n.16x32bx2 / .16x64b / .32x32b\n.16x128b\n.16x256b\n.x1\n1\n2\n4\n.x2\n2\n4\n8\n.x4\n4\n8\n16\n.x8\n8\n16\n32\n.x16\n16\n32\n64\n.x32\n32\n64\n128\n.x64\n64\n128\nNA\n.x128\n128\nNA\nNA\nThe qualifier\n.red\nspecifies that the reduction operation specified by\n.redOp\nis\nperformed on the data that is loaded across columns in each lane. The result of the\nreduction operation is written into the corresponding thread’s 32-bit destination register\noperand\nredVal\n. When\n.red\nqualifier is specified,\n.num\nmodifier must be at least\n.x2\n.\nThe optional qualifier\n.pack::16b\ncan be used to pack two 16-bit elements from adjacent\ncolumns into a single 32-bit element during the load as shown in the section\nPacking and Unpacking\n.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.ld\ncauses the executing thread\nto wait until all threads in the warp execute the same\ntcgen05.ld\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame\ntcgen05.ld\ninstruction. In conditionally executed code, a\ntcgen05.ld\ninstruction\nshould only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nThe behavior of\ntcgen05.ld\nis undefined if all threads do not use the same values of\ntaddr\n,\nor if any thread in the warp has exited.\nThe instruction\ntcgen05.ld\nis performed asynchronously and more details are specified in the\nsection\nMemory Consistency Model for 5th generation of TensorCore operations\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\ntcgen05.ld.red\nis introduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\ntcgen05.ld.red\nis supported on following architectures:\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_103f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\ntcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr1];\n\ntcgen05.ld.sync.aligned.16x128b.x4.b32    {r0, r1, r2, r3, r4, r5, r6, r7}, [taddr2];\n\ntcgen05.ld.red.sync.aligned.16x32bx2.x8.u32.max {r0, r1, r2, r3, r4, r5, r6, r7},\n                                                 redVal, [taddr3], 16;\n9.7.16.8.4.\nTensorcore 5th Generation Instructions:\ntcgen05.st\n\ntcgen05.st\nAsynchronous collective store to tensor memory from registers.\nSyntax\ntcgen05.st.sync.aligned.shape1.num{.unpack}.b32    [taddr], r;\n\ntcgen05.st.sync.aligned.shape2.num{.unpack}.b32    [taddr], immHalfSplitoff, r;\n\n.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }\n.shape2 = { .16x32bx2 }\n.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }\n.unpack = { .unpack::16b }\nDescription\nInstruction\ntcgen05.st\nasynchronously stores data from the source register\nr\ninto\nthe\nTensor Memory\nat the location specified by the 32-bit address operand\ntaddr\n,\ncollectively across all threads of the warps.\nAll the threads in the warp must specify the same value of\ntaddr\n, which must be the base\naddress of the collective store operation. Otherwise, the behavior is undefined.\nThe\n.shape\nqualifier and the\n.num\nqualifier together determines the total dimension\nof the data which is stored to the Tensor Memory. The\n.shape\nqualifier indicates the base\ndimension of data to be accessed as described in the\nData Movement Shape\n. The\n.num\nqualifier indicates the repeat factor on the base dimension resulting in the total dimension of\nthe data that is accessed.\nThe shape\n.16x32bx2\nperforms two accesses into Tensor Memory of the shape\n.16x32b\n.\nThe base address of the first access is specified by\ntaddr\nand the base address of the\nsecond access is specified by\ntaddr+immHalfSplitoff\n, where\nimmHalfSplitoff\nis an\nimmediate argument.\nThe source operand\nr\nis a brace-enclosed vector expression consisting of one or more 32-bit\nregisters as per the value of\n.shape\nand\n.num\n. The size of the vector for various\ncombinations of\n.num\nand\n.shape\nis shown in\nTable 50\n.\nTable 50\nVarious-combinations of .num and .shape\n\n.num\n.shape\n.16x32bx2 / .16x64b / .32x32b\n.16x128b\n.16x256b\n.x1\n1\n2\n4\n.x2\n2\n4\n8\n.x4\n4\n8\n16\n.x8\n8\n16\n32\n.x16\n16\n32\n64\n.x32\n32\n64\n128\n.x64\n64\n128\nNA\n.x128\n128\nNA\nNA\nThe optional qualifier\n.unpack::16b\ncan be used to unpack a 32-bit element in the\nregister into two 16-bit elements and store them in adjacent columns as shown in the\nsection\nPacking and Unpacking\n.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.st\ncauses the executing\nthread to wait until all threads in the warp execute the same\ntcgen05.st\ninstruction\nbefore resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute\nthe same\ntcgen05.st\ninstruction. In conditionally executed code, a\ntcgen05.st\ninstruction should only be used if it is known that all threads in the warp evaluate\nthe condition identically, otherwise behavior is undefined.\nThe behavior of\ntcgen05.st\nis undefined if all threads do not use the same values of\ntaddr\n, or if any thread in the warp has exited.\nThe instruction\ntcgen05.st\nis performed asynchronously and more details are specified\nin the section\nMemory Consistency Model for 5th generation of TensorCore operations\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\ntcgen05.st.sync.aligned.16x64b.x4.b32               [taddr0], {r0,  r1,  r2,  r3};\n\ntcgen05.st.sync.aligned.16x128b.x1.unpack::16b.b32  [taddr1], {r0,  r1};\n9.7.16.8.5.\nTensorcore 5th Generation Instructions:\ntcgen05.wait\n\ntcgen05.wait\nWaits for the completion of all prior asynchronous\ntcgen05.ld\n/\ntcgen05.st\ninstructions.\nSyntax\ntcgen05.wait_operation.sync.aligned;\n\n.wait_operation = { .wait::ld, .wait::st }\nDescription\nInstruction\ntcgen05.wait::st\ncauses the executing thread to block until all prior\ntcgen05.st\noperations issued by the executing thread have completed.\nInstruction\ntcgen05.wait::ld\ncauses the executing thread to block until all prior\ntcgen05.ld\noperations issued by the executing thread have completed.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.wait_operation\ncauses the\nexecuting thread to wait until all threads in the warp execute the same\ntcgen05.wait_operation\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame\ntcgen05.wait_operation\ninstruction.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nExample 1:\n\ntcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr0];\n\n// Prevents subsequent tcgen05.mma from racing ahead of the tcgen05.ld\n\ntcgen05.wait::ld.sync.aligned;\n\ntcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;\n\nExample 2:\n\ntcgen05.st.sync.aligned.32x32b.x2.b32     [taddr0], {r0, r1};\n\n// Prevents the write to taddr0 in tcgen05.mma from racing ahead of the tcgen05.st\n\ntcgen05.wait::st.sync.aligned;\n\ntcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;\n9.7.16.9.\nTensor Memory Data Movement Instructions\n\nData from the shared memory can be copied asynchronously to the\nTensor Memory\nusing the\nTensorcore 5th Generation Instructions: tcgen05.cp\noperation.\n9.7.16.9.1.\nOptional Decompression\n\nOptionally, during the copy, a vector of 4-bit and 6-bit\ncustom floating point types can be decompressed into 8-bit types.\n9.7.16.9.1.1.\nDecompression of 4-bit floating point to 8-bit type\n\nA contiguous set of 16 elements of 4-bits each followed by 8 bytes of padding can be converted\ninto 16 elements of 8-bits each as shown in\nFigure 194\n.\nFigure 194\nDecompression from 4-bit to 8-bit\n\nThe individual 4-bit to 8-bit decompression would look like as shown in\nFigure 195\n.\nFigure 195\nIndividual decompression from 4-bit to 8-bit\n\n9.7.16.9.1.2.\nDecompression of 6-bit floating point to 8-bit type\n\nA contiguous set of 16 elements of 6-bits each followed by 4 bytes of padding is\ndecompressed into 16 elements of 8-bits each as shown in\nFigure 196\n.\nFigure 196\nDecompression from 6-bit to 8-bit\n\nThe individual 6-bit to 8-bit decompression for types\nE3M2\nand\nE2M3\nis shown in\nFigure 197\nand\nFigure 198\nrespectively.\nFigure 197\nIndividual decompression from 6-bit to 8-bit for E3M2 type\n\nFigure 198\nIndividual decompression from 6-bit to 8-bit for E2M3 type\n\n9.7.16.9.2.\nTensorcore 5th Generation Instructions:\ntcgen05.cp\n\ntcgen05.cp\nInitiates an asynchronous copy operation from shared memory to the\nTensor Memory\n.\nSyntax\ntcgen05.cp.cta_group.shape{.multicast}{.dst_fmt.src_fmt} [taddr], s-desc;\n\n.cta_group = { .cta_group::1, .cta_group::2 }\n.src_fmt   = { .b6x16_p32 , .b4x16_p64 }\n.dst_fmt   = { .b8x16 }\n.shape     = { .128x256b, .4x256b, .128x128b, .64x128b**, .32x128b*** }\n.multicast = { .warpx2::02_13** , .warpx2::01_23**, .warpx4*** }\nDescription\nInstruction\ntcgen05.cp\ninitiates an asynchronous copy operation from shared memory to the\nlocation specified by the address operand\ntaddr\nin the\nTensor Memory\n.\nThe 64-bit register operand\ns-desc\nis the matrix descriptor which represents the source\nmatrix in the shared memory that needs to be copied. The format of the matrix descriptor is\ndescribed in\nMatrix Descriptors\n.\nThe\n.shape\nqualifier indicates the dimension of data to be copied as described in the\nData Movement Shape\n.\nQualifier\n.cta_group\nspecifies the number of CTAs whose\nTensor Memory\nis\naccessed when a single thread of a single CTA executes the\ntcgen05.cp\ninstruction.\nWhen\n.cta_group::1\nis specified, the data is copied into the\nTensor Memory\nof the current CTA. When\n.cta_group::2\nis specified, the data is copied into the\nTensor Memory\nof both the current and the\npeer CTAs\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nWhen the qualifiers\n.dst_fmt\nand\n.src_fmt\nare specified, the data is decompressed\nfrom the source format\n.src_fmt\nin the shared memory to the destination format\n.dst_fmt\nin\nTensor Memory\nby the copy operation. The details of source\nand the destination formats as specified in the section\nOptional Decompression\n.\nSome of the\n.shape\nqualifiers require certain\n.multicast\nqualifiers.\n.64x128b\nrequires\n.warpx2::02_13\nor\n.warpx2::01_23\n.32x128b\nrequires\n.warpx4\nWhen the\n.multicast\nqualifier is specified as either\n.warpx2::02_13\nor\n.warpx2::01_23\nthen the data being copied is multicasted into warp pairs and each\nwarp in the warp pair receive half of the data. Warp pairs are formed as follows:\n.warpx2::02_13\n: warps 0 and 2 form a pair; warps 1 and 3 form a pair.\n.warpx2::01_23\n: warps 0 and 1 form a pair; warps 2 and 3 form a pair.\nWhen the\n.multicast\nmodifier is specified as\n.warpx4\nthen the data being\ncopied is multicasted into all 4 warps.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\ntcgen05.cp.cta_group::1.128x256b                 [taddr0], sdesc0;\ntcgen05.cp.cta_group::2.128x128b.b8x16.b6x16_p32 [taddr1], sdesc1;\ntcgen05.cp.cta_group::1.64x128b.warpx2::02_13    [taddr2], sdesc2;\n9.7.16.9.3.\nTensorcore 5th Generation Instructions:\ntcgen05.shift\n\ntcgen05.shift\nAsynchronously shift down the rows of the matrix in the\nTensor Memory\nfor a warp.\nSyntax\ntcgen05.shift.cta_group.down  [taddr];\n\n.cta_group = { .cta_group::1, .cta_group::2 }\nDescription\nInstruction\ntcgen05.shift\nis an asynchronous instruction which initiates the shifting of 32-byte\nelements downwards across all the rows, except the last, by one row. The address operand\ntaddr\nspecifies the base address of the matrix in the\nTensor Memory\nwhose rows must\nbe down shifted.\nThe lane of the address operand\ntaddr\nmust be aligned to 32.\nQualifier\n.cta_group\nspecifies the number of CTAs whose\nTensor Memory\nis touched when a single thread of a single CTA executes the\ntcgen05.shift\ninstruction.\nWhen\n.cta_group::1\nis specified, the shift operation is performed in the\nTensor Memory\nof the current CTA. When\n.cta_group::2\nis specified,\nthe shift operation is performed in the\nTensor Memory\nof both the current and the\npeer CTAs\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_103a\nsm_110a\nExamples\ntcgen05.shift.down.cta_group::1 [taddr0];\ntcgen05.shift.down.cta_group::2 [taddr1];\n9.7.16.10.\nTensorCore 5th Generation Matrix Multiply and accumulate Operations\n\nThe 5\nth\ngeneration of TensorCore operations of shape\nMxNxK\nperform matrix\nmultiplication and accumulation of the form:\nD\n=\nA*B+D\nwhere:\nthe\nA\nmatrix has shape\nMxK\n, in either Tensor Memory or Shared Memory\nthe\nB\nmatrix has shape\nKxN\n, in Shared Memory of the current CTA and optionally in peer CTA\nthe\nD\nmatrix is of the shape\nMxN\n, in Tensor Memory\nOptionally an input predicate can be used to disable the input from the accumulator\nmatrix and the following operation can be performed as\nD\n=\nA*B\nThe matrix multiplication and accumulation operations are categorized into various kinds\nbased on input types and the throughput of the multiplication operation. The following shows the\ndifferent kinds of MMA operations that are supported:\nf16\n: supports\nf16\nand\nbf16\ninput types.\ntf32\n: supports\ntf32\ninput types.\nf8f6f4\n: supports all input combinations of\nf8\n,\nf6\nand\nf4\ntypes.\ni8\n: supports signed and unsigned 8-bit integer input types.\nmxf8f6f4\n/\nmxf4\n: supports mx-floating points input types.\nmxf4nvf4\n: supports\nmxf4\ntype and a custom NVIDIA floating-point\ntype for inputs where the type of the vector elements is 4 bits and requires a common\nscaling factor to form the complete floating-point type, similar to other mx-types.\nOptionally, the 5\nth\ngeneration of TensorCore MMAs support dense and sparse matrix\nA\n.\nSparse Matrices\ndescribes the details of the sparse matrices.\nSome of the MMA-kinds requires scaling of input matrices from memory to form the matrix\nA\nand matrix\nB\nbefore performing the MMA operation.\nBlock Scaling for tcgen05.mma.sync\ndescribes the details of the scaling of matrices.\nThe following table show the various matrices involved in the MMA operations and the memory in\nwhich they can reside:\nMatrix Type\nMemory\nA\nTensor Memory OR Shared Memory\nB\nShared Memory\nD\nTensor Memory\nSparse\nMeta\nData\nA-Scale\n/\nB-Scale\nA sequence of MMA instructions may reuse the same\nA\nmatrix with a sequence of\nB\nmatrices or may reuse the same\nB\nmatrix with a sequence of\nA\nmatrices.\nIn these patterns the TensorCore may be able to laod the unchanged matrix once and reuse\nit through the sequence without multiple reloads. The\nA\nor\nB\nmatrices are loaded\ninto a TensorCore collector buffer (i.e., special cache).\nAn MMA instruction has an optional\ncollector\nqualifier to specify when an\nA\nor\nB\nmatrix is new to the sequence and should be loaded, unchanged within the sequence\nand should be reused, or the last use in the sequence and should be discarded.\nThe\ncollector\nqualifier is used to give the TensorCore permission to reuse a previously\nloaded\nA\nor\nB\nmatrix; however reuse is opportunistic in that the TensorCore may\nreload a matrix even when it has permission to reuse that matrix. Thus, the source\nmemory of an\nA\nor\nB\nmatrix must not be modified while the MMA instruction using those\nmatrices has not completed - regardless of\ncollector\nqualifier permissions.\nThe 5\nth\ngeneration of TensorCore MMAs can be used for general matrix multiplication OR for\nconvolution operations. In case of convolutions, the activations can be stored in either\nmatrix\nA\nor matrix\nB\nwhile the weights will be stored in the other matrix.\nActivation\nMatrix\nWeights\nMatrix\nName of the\nop\nInstruction\nName\nCollector Buffer Applicability\nA\nB\nActivation\nStationary\n(default\ntcgen05.mma\n)\nCollector buffer is applicable on matrix\nA\nB\nA\nWeights\nStationary\n.ws\nCollector buffer is applicable on matrix\nB\n9.7.16.10.1.\nTranspose and Negate operations\n\nThe matrices\nA\nand\nB\ncan be transposed by specifying the Tranpose\nA\nMatrix\nand Transpose\nB\nMatrix bits in the instruction descriptor respectively.\nThe elements of the matrices\nA\nand\nB\ncan be negated by specifying the Negate\nA\nMatrix and Negate\nB\nMatrix bits in the instruction descriptor respectively.\nThe support for Transpose and Negate operation for various MMA-Kind are shown in\nTable 51\n.\nTable 51\nTranspose and Negate operation for various MMA-Kind\n\nMMA-Kind\nIs Transpose A/B supported\nIs Negate A/B supported\n.kind::tf32\nYes\nYes\n.kind::f16\nYes\nYes\n.kind::f8f6f4\nYes\nYes\n.kind::mxf8f6f4\nYes\nYes\n.kind::i8\nYes\nNo\n.kind::mxf4\nNo\nYes\n.kind::mxf4nvf4\nNo\nYes\nFor\n.kind::tf32\n, the transpose operations on matrices\nA\nand\nB\nare supported\nonly with 128B swizzling mode with 32B swizzle-atomicity.\nFor all other MMA-Kinds, the transpose operations on matrices\nA\nand\nB\nare not supported\non 128B swizzling mode with 32B swizzle-atomicity.\nTable 52\nshows the valid combinations of N shape with\n.cta_group\nqualifier for 8bit transpose B.\nTable 52\nVarious combinations of N shape with .cta_group qualifier for 8bit transpose B\n\n.cta_group\nN shape\n1\n16 <= N <= 256, step 16\n2\n32 <= N <= 256, step 32\n9.7.16.10.2.\nMatrix Layout Organization\n\nTable 53\ndescribes the major-ness used for different matrices.\nTable 53\nMajor-ness for different matrices\n\nMatrix\nResiding in Memory\nDefault Major-ness\nD\nTensor Memory\nRow-Major\nA\nTensor Memory\nShared Memory\nDepends on swizzling mode.\nRefer\nShared Memory Layout and Swizzling\nB\nShared Memory\n9.7.16.10.3.\nValid Combinations of Type-Size, Major-ness and Swizzling\n\nTable 54\nValid Combinations of Type-Size, Major-ness and Swizzling\n\nType-Size\nMajor-ness\nMatrix\nSupported Swizzle\n4-bit,\n6-bit,\n8-bit,\n16-bit,\n32-bit\nRow\nA\nAll swizzling modes\nColumn\nB\n8-bit\n16-bit\nColumn (transpose)\nA\nAll except 128B\nswizzling with 32B\natomicity\nRow (transpose)\nB\n32-bit\nColumn (transpose)\nA\nOnly 128B swizzling\nwith 32B atomicity\nRow (transpose)\nB\n9.7.16.10.4.\nPacking formats of elements in Tensor and Shared memory\n\n9.7.16.10.4.1.\nPacking format for matrix D in Tensor Memory\n\nThe sub-word elements of matrix\nD\nare expected not to be packed within a 32-bit Tensor Memory word.\nFor example, if the type of elements of the matrix\nD\nis 16 bits then a Tensor Memory word\nwould contain a single 16-bit element in its lower 16 bits.\n9.7.16.10.4.2.\nPacking format for matrix A and B\n\nThe 6-bit and 4-bit floating point types have different packing format requirements for\ndifferent MMA kinds in both Tensor memory and Shared memory. The requirements are as follows.\n9.7.16.10.4.3.\nPacking format used for matrix A by\n.kind::mxf8f6f4\nin Tensor Memory\n\nThe individual 4-bit and the 6-bit floating point type elements must be packed in an 8-bit container\nin Tensor memory as shown below. The 8-bit containers must be contiguously packed in a 32-bit Tensor\nMemory word. For example, if the type of elements of the matrix\nA\nis 6 bits then 4 consecutive\nA\nelements should be packed in one 32-bit Tensor Memory word.\n4-bit packing format as shown in\nFigure 199\nFigure 199\n4-bit packing format with type E2M1\n\n6-bit packing format\nType E3M2 as shown in\nFigure 200\nFigure 200\n6-bit packing format with type E3M2\n\nType E2M3 as shown in\nFigure 201\nFigure 201\n6-bit packing format with type E2M3\n\n9.7.16.10.4.4.\nPacking format used for matrix A and B by\n.kind::mxf8f6f4\nin Shared Memory\n\nThe 4-bit and 6-bit floating point elements in shared memory must be contiguously packed along\nwith padding as follows.\n4-bit packing format as shown in\nFigure 202\nFigure 202\n4-bit packing format\n\n6-bit packing format as shown in\nFigure 203\nFigure 203\n6-bit packing format\n\n9.7.16.10.4.5.\nPacking format used for matrix A by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Tensor Memory\n\nTwo 4-bit floating point type elements must be packed in an 8-bit container in Tensor memory as\nshown in\nFigure 204\nfor\nmxf4\n.\nFigure 204\n4-bit packing format with type E2M1\n\n9.7.16.10.4.6.\nPacking format used for matrix A and B by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Shared Memory\n\nThe packing format for 4-bit floating point elements in shared memory is to pack two 4-bit\nelements in a 8-bit container, with no padding.\n9.7.16.10.5.\nData Path Layout Organization\n\nDifferent MMA variants access the tensor memory with different layout organization.\nThe following table lists the various layouts:\nM\ncta_group\nA-Sparsity\nIs .ws mode\nDatapath organization\nLayout ID\nTensor Memory Datapath Lane Alignment\n32\n::1\nEither\nYes\n1x4\nLayout G\n0\n64\n::1\nEither\nYes\n2x3\nLayout E\n0\n64\n::1\nEither\nNo\n4x1 (1/2 datapath utilized)\nLayout F\n0 or 16\n128\n::1\nEither\nEither\n4x1\nLayout D\n0\n128\n::2\nDense\nN/A\n2x2\nLayout B\n0\n128\n::2\nSparse\nN/A\n4x1 (1/2 datapath utilized)\nLayout C\n0 or 16\n256\n::2\nEither\nN/A\n4x1\nLayout A\n0\nThe layouts which utilize only half the datapath lanes, i.e.,\nLayout F\nand\nLayout C\n, must use the same Tensor Memory\nlane alignment across matrices\nA\n,\nD\nand the sparsity metadata matrix.\nThe following shows the warps that can access the Tensor Memory regions via\ntcgen05.ld\n/\ntcgen05.st\nalong with the addresses for various Tensor Memory Layouts.\n9.7.16.10.5.1.\nLayout A (M = 256)\n\nLayout organization for M = 256 is shown in\nFigure 205\n.\nFigure 205\nLayout organization for M = 256\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 206\nFigure 206\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.2.\nLayout B (M = 128 + cta-group::2 + Dense A matrix)\n\nLayout organization for M = 128 + .cta_group::2 + Dense A matrix is shown in\nFigure 207\n.\nFigure 207\nLayout organization for M = 128 + .cta_group::2 + Dense A matrix\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 208\nFigure 208\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.3.\nLayout C (M = 128 + cta-group::2 + Sparse A matrix)\n\nLayout organization for M = 128 + .cta_group::2 + Sparse A matrix is shown in\nFigure 209\n.\nFigure 209\nLayout organization for M = 128 + .cta_group::2 + Sparse A matrix\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 210\nFigure 210\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.4.\nLayout D (M = 128 + cta-group::1)\n\nLayout organization for M = 128 + .cta_group::1 is shown in\nFigure 211\n.\nFigure 211\nLayout organization for M = 128 + .cta_group::1\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 212\nFigure 212\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.5.\nLayout E (M = 64 + .ws mode)\n\nLayout organization for M = 64 + .ws mode is shown in\nFigure 213\n.\nFigure 213\nLayout organization for M = 64 + .ws mode\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 214\nFigure 214\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.6.\nLayout F (M = 64 + non .ws mode)\n\nLayout organization for M = 64 + non .ws mode is shown in\nFigure 215\n.\nFigure 215\nLayout organization for M = 64 + non .ws mode\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 216\nFigure 216\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.7.\nLayout G (M = 32)\n\nLayout organization for M = 32 is shown in\nFigure 217\n.\nFigure 217\nLayout organization for M = 32\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 218\nFigure 218\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.6.\nShared Memory Layout and Swizzling\n\nIf the bit\nTranspose\nA\nMatrix\n/\nTranspose\nB\nMatrix\nin the\nInstruction descriptor\nis 0, then\nK-major\nis\nused for matrix\nA\n/\nB\nrespectively. If the bit\nTranspose\nA\nMatrix\nin the\nInstruction descriptor\nis 1 then\nM-major\nis\nused for matrix\nA\n. If the bit\nTranspose\nB\nMatrix\nin the\nInstruction descriptor\nis 1, then\nN-major\nis\nused for matrix\nB\n.\nIn a column-major default BLAS library such as cuBLAS, the matrices\nA\nand\nB\nwith and\nwithout transpose can be classified as either\nK-Major\nor\nM-or-N-Major\nas shown in the\nfollowing table:\nNon-Transposed\nTransposed\nA\nK-major\nM-major\nB\nK-major\nN-major\nTo avoid confusion with\nA\n,\nB\n,\nrow-major\n,\ncol-major\n,\ntranspose\n, and\nnon-transpose\n, we will use\nMN-Major\nand\nK-Major\nthroughout this section.\nThe matrices in the shared memory are made up of one or more “swizzle layout atom”.\nThe exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,\nand the leading dimension. The layout of the swizzle are shown in\nTable 55\nTable 55\nLayout for swizzle atoms\n\nSwizzling mode and\nSwizzle-Atomicity\nLeading\nDimension\nSwizzle atom layout\n(128b element)\n128B Swizzling with\n32B atomicity\nM/N\n8x4\n–\n–\n128B Swizzling with\n16B atomicity\nM/N\n8x8\nK\n8x8\n64B Swizzling Mode\nM/N\n4x8\nK\n8x4\n32B Swizzling Mode\nM/N\n2x8\nK\n8x2\nNone\nM/N\n1x8\nK\n8x1\nThe above shapes are for elements of size 128 bits. For smaller element sizes, the same shapes\nwould get multiplied along the leading dimension by a factor of\n128\n/\nsizeof_bits(Element)\n.\nFor example, 128B MN major swizzle atom would have a shape of (8*(128/32))x8 = 32x8 for\ntf32 tensor core inputs.\nSome example Layouts of\nMxK\nor\nKxN\nmatrices with various swizzling modes, and are in units\nof 128b elements as shown by each colored cell as shown in\nFigure 219\n,\nFigure 220\n,\nFigure 221\n,\nFigure 222\n,\nFigure 223\n,\nFigure 224\n,\nFigure 225\n,\nFigure 226\n,\nFigure 227\n.\nFigure 219\nMN major 128B swizzling with 32B atomicity\n\nFigure 220\nMN major 128B swizzling\n\nFigure 221\nK major 128B swizzling\n\nFigure 222\nMN major 64B swizzling\n\nFigure 223\nK major 64B swizzling\n\nFigure 224\nMN major 32B swizzling\n\nFigure 225\nK major 32B swizzling\n\nFigure 226\nMN major no-swizzling mode\n\nFigure 227\nK major no-swizzling mode\n\nFollowing are some of the examples of the 128B swizzling layout for\ntf32\nelement type.\nK-Major:\nFigure 228\nFigure 228\nK major\n\nMN-Major:\nFigure 229\nFigure 229\nMN major\n\n9.7.16.10.7.\nBlock Scaling for\ntcgen05.mma.sync\n\nThe\ntcgen05.mma\ninstructions with the following\n.kind\nqualifier:\n.kind::mxf8f6f4\n.kind::mxf4\n.kind::mxf4nvf4\nperform matrix multiplication with block scaling. This operation has the following form:\n(A\n*\nscale_A)\n*\n(B\n*\nscale_B)\n+\nD\nwhere\nscale_A\nand\nscale_B\nare matrices residing in\nTensor Memory\n.\nFor a\nscale_A\nmatrix of shape\nM x SFA_N\n, each row of matrix\nA\nis divided into\nSFA_N\nnumber of chunks and each chunk of a row is multiplied with the corresponding\nelement in the\nSF_A\nof the same row.\nSimilarly, for a\nscale_B\nmatrix of shape\nSFB_M x N\n, each column of matrix\nB\nis\ndivided into the\nSFB_M\nnumber of chunks and each chunk of a column is multiplied with\nthe corresponding element in the\nSF_B\nof the same column.\nScale factors for\nA\nand\nB\nmatrices need to be duplicated to all 32 lane partitions\nof tensor memory.\nFigure 230\nshows an example of\ntcgen05.mma\nwith block scaling of\nscale_vec::2X\n.\nFigure 230\ntcgen05.mma\nwith block scaling of\nscale_vec::2X\n\n9.7.16.10.7.1.\nValid combinations of scale_vectorsize with types and MMA-Kind\n\nThe shape of\nscale_A\nand\nscale_B\nmatrices depend on the\n.scale_vectorsize\nas shown in\nTable 56\n.\nTable 56\nValid combinations of scale_vectorsize and shapes\n\n.scale_vectorsize\n.kind::*\nK\nShape of scale_A\nShape of scale_B\n.scale_vec::1X\n.kind::mxf8f6f4\nAll supported values of K\nM x 1\n1 x N\n.scale_vec::2X\n.kind::mxf4\n,\n.kind::mxf4nvf4\nAll supported values of K\nM x 2\n2 x N\n.scale_vec::4X\n.kind::mxf4nvf4\nAll supported values of K\nM x 4\n4 x N\n.block16\n.kind::mxf4nvf4\nK = 96\nM x 6\n6 x N\nAll supported values of K\nexcept 96\nM x 4\n4 x N\n.block32\n.kind::mxf4\n,\n.kind::mxf4nvf4\nK = 96\nM x 3\n3 x N\nAll supported values of K\nexcept 96\nM x 2\n2 x N\n.kind::mxf8f6f4\nAll supported values of K\nM x 1\n1 x N\nThe valid combination of the exact element types and the\n.scale_vectorsize\nare listed in\nTable 57\n.\nTable 57\nValid combinations of scale_vectorsize with types and MMA-Kind\n\n.kind::*\nElement Data Type\nScale Data Type\n.scale_vectorsize\n.kind::mxf8f6f4\nE4M3, E5M2, E2M3\nE3M2, E2M1\nUE8M0\n.scale_vec::1X\n/\n.block32\n.kind::mxf4\nE2M1\nUE8M0\n.scale_vec::2X\n/\n.block32\n.kind::mxf4nvf4\nE2M1\nUE8M0\n.scale_vec::2X\n/\n.block32\n,\n.scale_vec::4X\n/\n.block16\nE2M1\nUE4M3\n.scale_vec::4X\n/\n.block16\nNew\n.blockN\nqualifiers are aliases for\n.scale_vec::NX\nqualifiers as:\n.block32\nis alias for\n.scale_vec::1X\nor\n.scale_vec::2X\nbased on\n.kind\nand K dimension\n.block16\nis alias for\n.scale_vec::4X\n9.7.16.10.7.2.\nScale Factor A ID\n\nThe value of the scale factor\nA\nID\nselects the sub-columns in the Tensor Memory to\nform the scale factor\nA\nmatrix, which is used to scale the matrix\nA\n.\nThe following shows the scale factor matrix layout for various scale vector sizes:\n9.7.16.10.7.2.1.\nLayout of the Scale Factor A Matrix for scale_vec::1X/block32 with K=32/K=64\n\nThere is one scale factor per row of the\nA\nmatrix with block size as 32 and the scale factor must be provided in\n1-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies the byte offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 231\nshows which sub-columns get selected for\ndifferent values of\nSFA_ID\n.\nFigure 231\nLayout of scale factor A matrix with scale_vec::1X/block32 with K=32/K=64\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFA_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns,\nrespectively.\n9.7.16.10.7.2.2.\nLayout of the Scale Factor A Matrix for scale_vec::2X/block32 with K=64/K=128\n\nThere are two scale factors per row of the\nA\nmatrix with block size as 32 and the scale factor must be provided in\n2-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies the half word offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 232\nshows which sub-columns gets selected for different\nvalues of\nSFA_ID\n.\nFigure 232\nLayout of scale factor A matrix with scale_vec::2X/block32 with K=64/K=128\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFA_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.2.3.\nLayout of the Scale Factor A Matrix for scale_vec::4X/block16 with K=64/K=128\n\nThere are four scale factors per row of the\nA\nmatrix with block size as 16 and the scale factor must be provided in\n4-byte aligned sub-column of the Tensor Memory. The\nSFA_ID\nvalue must be 0 and this specifies\nthat all of the columns (in green) will be used for the scale factor matrix.\nFigure 233\nshows which sub-columns gets selected for different\nvalues of\nSFA_ID\n.\nFigure 233\nLayout of scale factor A matrix with scale_vec::4X/block16 with K=64/K=128\n\n9.7.16.10.7.2.4.\nLayout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n\nThere are three scale factors per row of the\nA\nmatrix with block size as 32 and the scale\nfactor must be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies\nthe byte offset in the Tensor Memory word that must be used for the scale factor matrix.\nFigure 234\n,\nFigure 235\n,\nFigure 236\nand\nFigure 237\nshow which sub-columns get selected for different values of\nSFA_ID\n.\nFigure 234\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=00\n\nFigure 235\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=01\n\nFigure 236\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=10\n\nFigure 237\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=11\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFA_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns,\nrespectively.\n9.7.16.10.7.2.5.\nLayout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n\nThere are six scale factors per row of the\nA\nmatrix with block size as 16 and the scale\nfactor must be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies\nthe byte offset in the Tensor Memory word that must be used for the scale factor matrix.\nFigure 238\nand\nFigure 239\nshow which sub-columns get selected for different values of\nSFA_ID\n.\nFigure 238\nLayout of scale factor A matrix with block16 with K=96 with SFA_ID=00\n\nFigure 239\nLayout of scale factor A matrix with block16 with K=96 with SFA_ID=10\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFA_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.3.\nScale Factor B ID\n\nThe value of the scale factor\nB\nID\nselects the sub-columns in the Tensor Memory to\nform the scale factor\nB\nmatrix, which is used to scale the matrix\nB\n.\nThe following shows the scale factor matrix layout for various scale vector sizes:\n9.7.16.10.7.3.1.\nLayout of the Scale Factor B Matrix for scale_vec::1X/block32 with K=32/K=64\n\nThere is one scale factor per row of the\nB\nmatrix with block size as 32 and the scale factor must be provided in\n1-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 240\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 240\nLayout of scale factor B matrix with scale_vec::1X/block32 with K=32/K=64\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFB_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.\n9.7.16.10.7.3.2.\nLayout of the Scale Factor B Matrix for scale_vec::2X/block32 with K=64/K=128\n\nThere are two scale factors per row of the\nB\nmatrix with block size as 32 and the scale factor must be provided in\n2-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the half word offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 241\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 241\nLayout of scale factor B matrix with scale_vec::2X/block32 with K=64/K=128\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFB_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.3.3.\nLayout of the Scale Factor B Matrix for scale_vec::4X/block16 with K=64/K=128\n\nThere are four scale factors per row of the\nB\nmatrix with block size as 16 and the scale factor must be provided in\n4-byte aligned sub-column of the Tensor Memory. The\nSFB_ID\nvalue must be 0 and this specifies\nthat all of the columns (in green) will be used for the scale factor matrix.\nFigure 242\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 242\nLayout of scale factor B matrix with scale_vec::4X/block16 with K=64/K=128\n\n9.7.16.10.7.3.4.\nLayout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n\nThere are three scale factors per row of the\nB\nmatrix with block size as 32 and the scale factor\nmust be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte\noffset in the Tensor Memory word that must be used for the scale factor matrix.\nFor N<=128,\nFigure 243\n,\nFigure 244\n,\nFigure 245\nand\nFigure 246\nshow which\nsub-columns get selected for different values of\nSFB_ID\n.\nFigure 243\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=00\n\nFigure 244\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=01\n\nFigure 245\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=10\n\nFigure 246\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=11\n\nFor N>128,\nFigure 247\n,\nFigure 248\n,\nFigure 249\n,\nFigure 250\n,\nFigure 251\nand\nFigure 252\nshow which\nsub-columns get selected for different values of\nSFB_ID\n.\nFigure 247\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=00\n\nFigure 248\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=01\n\nFigure 249\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10\n\nFigure 250\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10\n\nFigure 251\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11\n\nFigure 252\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the\nscale factor matrix. Similarly,\nSFB_ID\nvalues of 1, 2 and 3 would select the blue,\nyellow, and red columns, respectively.\n9.7.16.10.7.3.5.\nLayout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n\nThere are six scale factors per row of the\nB\nmatrix with block size as 16 and the scale factor\nmust be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte\noffset in the Tensor Memory word that must be used for the scale factor matrix.\nFor N<=128,\nFigure 253\nand\nFigure 254\nshow which sub-columns\nget selected for different values of\nSFB_ID\n.\nFigure 253\nLayout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=00\n\nFigure 254\nLayout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=10\n\nFor N>128,\nFigure 255\n,\nFigure 256\n,\nFigure 257\nand\nFigure 258\nshow which sub-columns\nget selected for different values of\nSFB_ID\n.\nFigure 255\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00\n\nFigure 256\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00\n\nFigure 257\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10\n\nFigure 258\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the\nscale factor matrix. Similarly, if\nSFB_ID\nis 2, then all of the blue columns are\nselected to form the scale factor matrix.\n9.7.16.10.8.\nSparse Matrices\n\nThis instruction\ntcgen05.mma.sp\ncan be used when the matrix\nA\nis a structured\nsparse matrix with 50% zeros in each row distributed as per its sparse granularity.\nIn a\nMxNxK\nsparse\ntcgen05.mma.sp\noperation, the matrix\nA\nof shape\nMxK\nis\nstored in a packed form as\nMx(K/2)\nin memory. For each\nK-wide\nrow of matrix\nA\n,\n50% of elements are zeros and the remaining\nK/2\nnon-zero elements are stored in\nmemory. The metadata specifies the mapping of the\nK/2\nnon-zero elements to the\nK\nelements before performing the MMA operation.\nGranularity of sparse matrix\nA\nis defined as the ratio of the number of non-zero\nelements in a sub-chunk of the matrix row to the total number of elements in that\nsub-chunk where the size of the sub-chunk is shape-specific. The following table lists\nthe granularity of different\ntcgen05.mma.sp\nvariants:\n.kind of tcgen05.mma\nSparse Granularity\n.kind::tf32\n1:2\n.kind::f16\n2:4\n.kind::f8f6f4\n.kind::mxf8f6f4\n.kind::i8\n.kind::mxf4\n4:8 (in pairs)\n9.7.16.10.8.1.\nSparse\ntcgen05.mma.sp\nwith\n.kind::tf32\n\nFor\n.kind::tf32\n, matrix\nA\nis structured sparse at a granularity of\n1:2\n.\nIn other words, each chunk of two adjacent elements in a row of matrix\nA\nhas one\nzero and one non-zero element. Only the non-zero element is stored in memory and the\n4-bit index in the metadata indicates the position of the non-zero element in the\ntwo-wide chunk. The only meaningful values of the index are:\n0b1110\n0b0100\nRest of the values result in undefined behavior.\nFigure 259\nSparse tcgen05.mma metadata example for tf32 kind\n\n9.7.16.10.8.2.\nSparse\ntcgen05.mma.sp\nwith\n.kind::f16\n,\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n\nFor the following\n.kind\nvariants of\ntcgen05.mma\n:\n.kind::f16\n.kind::f8f8f4\n.kind::mxf8f6f4\n.kind::i8\nmatrix\nA\nis structured sparse at a granularity of\n2:4\n. In other words, each chunk\nof four adjacent elements in a row of matrix\nA\nhas two zero and two non-zero elements.\nOnly the non-zero elements are stored in memory and the two 2-bit indices in the metadata\nindicates the position of the two non-zero elements in the four-wide chunk. The only\nmeaningful values of the index are:\n0b0100\n0b1000\n0b1100\n0b1001\n0b1101\n0b0110\n0b1110\nFigure 260\nSparse tcgen05.mma metadata example for f16/f8f6f4/mxf8f6f4 kind\n\n9.7.16.10.8.3.\nSparse\ntcgen05.mma.sp\nwith\n.kind::mxf4\nand\n.kind::mxf4nvf4\n\nFor\n.kind::mxf4\nand\n.kind::mxf4nvf4\n, matrix\nA\nis pair-wise structured\nsparse at a granularity of\n4:8\n. In other words, each chunk of eight adjacent\nelements in a row of matrix\nA\nhas four zero and four non-zero elements. The\nzero and non-zero elements are clustered in sub-chunks of two elements each within\nthe eight-wide chunk, so each two-wide sub-chunk within the eight-wide chunk must be\nall zeros or all non-zeros. Only the four non-zero elements are stored in memory and\nthe two 2-bit indices in the metadata indicates the position of the two two-wide\nsub-chunks with non-zero values in the eight-wide chunk of a row of matrix\nA\n.\nThe only meaningful values of the index are:\n0b0100\n0b1000\n0b1100\n0b1001\n0b1101\n0b0110\n0b1110\nRest of the values result in undefined behavior.\nFigure 261\nSparse tcgen05.mma metadata example for mxf4 kind\n\n9.7.16.10.8.4.\nSparsity selector\n\nThe value of the sparsity selector selects the sub-columns in the Tensor Memory\nto form the sparsity metadata matrix, which is used with matrix\nA\nto form the\nmultiplicand matrix.\nThe following shows the sparse metadata matrix layout in Tensor Memory for various MMA variants:\n9.7.16.10.8.4.1.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f16\n\nFigure 262\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 262\nSparsity Metadata Layout for M = 64 for\n.kind::f16\n\n9.7.16.10.8.4.2.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f16\n\nFigure 263\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 263\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::f16\n\n9.7.16.10.8.4.3.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::tf32\n\nFigure 264\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 264\nSparsity Metadata Layout for M = 64 for\n.kind::tf32\n\n9.7.16.10.8.4.4.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::tf32\n\nFigure 265\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 265\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::tf32\n\n9.7.16.10.8.4.5.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\nThe value of the sparsity selector:\nmust be 0 for\n.kind::i8\nand\n.kind::f8f6f4\nis assumed to be 0 for\n.kind::mxf8f6f4\n,\n.kind::mxf4\nand\n.kind::mxf4nvf4\nand all of the columns are selected as\nshown in\nFigure 266\nFigure 266\nSparsity Metadata Layout for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\n9.7.16.10.8.4.6.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\nThe value of the sparsity selector:\nmust be 0 for\n.kind::i8\nand\n.kind::f8f6f4\nis assumed to be 0 for\n.kind::mxf8f6f4\n,\n.kind::mxf4\nand\n.kind::mxf4nvf4\nand all of the columns are selected as\nshown in\nFigure 267\nFigure 267\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\n9.7.16.10.8.5.\nAlignment restriction\n\nThe layouts which utilize only half the datapath lanes as specified in\nData Path Layout Organization\n,\ni.e.\nLayout F\nand\nLayout C\n, must use the same alignment\nacross matrices A, D and the sparsity metadata matrix.\n9.7.16.10.9.\nTensorCore 5th Generation of MMA Instructions\n\n9.7.16.10.9.1.\nTensorCore 5th Generation Instructions:\ntcgen05.mma\n\ntcgen05.mma\nPerform the 5\nth\ngeneration of matrix multiply and accumulate operation.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.cta_group.kind   [d-tmem],  a-desc,  b-desc, idesc,\n                             { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind   [d-tmem], [a-tmem], b-desc, idesc,\n                             { disable-output-lane }, enable-input-d {, scale-input-d};\n\n.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.cta_group = { .cta_group::1, .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 2. Floating-point type with block scaling:\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}\n                                        [d-tmem],  a-desc,  b-desc, idesc,\n                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}\n                                        [d-tmem], [a-tmem], b-desc, idesc,\n                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.cta_group      = { .cta_group::1,   .cta_group::2 }\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n\n----------------------------------------------------------------------------------\n\n// 3. Convolution MMA for floating-point type without block scaling:\n\ntcgen05.mma.cta_group.kind.collector_usage [d-tmem],  a-desc,  b-desc, idesc,\n                                           { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,\n                                                    { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                    { disable-output-lane }, enable-input-d {, scale-input-d};\n\n.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.cta_group = { .cta_group::1,   .cta_group::2 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 4. Activation Stationary MMA for floating-point type with block scaling:\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                            [d-tmem],  a-desc,  b-desc, idesc,\n                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                            [d-tmem], [a-tmem], b-desc, idesc,\n                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.cta_group       = { .cta_group::1,   .cta_group::2 }\n.scale_vectorsize  = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.kind            = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 5. Integer type:\n\ntcgen05.mma.cta_group.kind::i8  [d-tmem],  a-desc,  b-desc, idesc,\n                                { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8  [d-tmem], [a-tmem], b-desc, idesc,\n                                { disable-output-lane }, enable-input-d;\n\n.cta_group = { .cta_group::1,   .cta_group::2  }\n\n----------------------------------------------------------------------------------\n\n// 6. Convolution MMA for integer type:\n\ntcgen05.mma.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\n.cta_group       = { .cta_group::1,   .cta_group::2  }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\nDescription\nInstruction\ntcgen05.mma\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix\nmultiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMxK\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe optional immediate argument\nscale-input-d\ncan be specified to scale the input\nmatrix\nD\nas follows:\nD\n=\nA*B+D\n*\n(2\n^\n-\nscale-input-d)\nThe valid range of values for argument\nscale-input-d\nis [0, 15]. The argument\nscale-input-d\nis only valid for\n.kind::tf32\nand\n.kind::f16\n.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described\nin\nInstruction descriptor\n, specifies\nthe shapes, exact types, sparsity and other details of the input matrices,\noutput matrix and the matrix multiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and\naccumulate operation is performed on the\nTensor Memory\nof the\nexecuting thread’s CTA only. The qualifier\n.cta_group::2\nspecifies that the matrix\nmultiply and accumulate operation is performed on the\nTensor Memory\nof the executing thread’s CTA and its\npeer CTA\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n.\nThe 64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which\nrepresent the matrices\nA\nand\nB\nin shared memory respectively. The format of the\nmatrix descriptor is described in\nMatrix Descriptors\n.\nThe vector operand\ndisable-output-lane\nspecifies the lane(s) in the\nTensor Memory\nthat should be not be updated with the resultant\nmatrix\nD\n. Elements of the vector operand\ndisable-output-lane\nforms a mask where\neach bit corresponds to a lane of the\nTensor Memory\n, with least\nsignificant bit of the first element of the vector (leftmost in syntax) corresponding\nto the lane 0 of the\nTensor Memory\n. If a bit in the mask is 1,\nthen the corresponding lane in the Tensor Memory for the resultant matrix\nD\nwill not\nbe updated. The size of the vector is as follows:\n.cta_group\nSize of the vector disable-output-lane\n::1\n4\n::2\n8\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply\nand accumulate operation as specified in the section\nBlock Scaling for tcgen05.mma.sync\n.\nThe address operand\nscale-A-tmem\nand\nscale-B-tmem\nspecify the base address the\nmatrices\nscale_A\nand\nscale_B\nrespectively in the\nTensor Memory\n.\nFor qualifier\n.scale_vectorsize\n,\nIf\n.scale_vec::NX\nis specified: N specifies the number of columns in\nscale_A\nmatrix and number of rows in\nscale_B\nmatrix.\nIf\n.blockN\nis specified: N specifies the block size for which single scale factor\nwill be applied. In this form, value of N is same as the K-dimension / (N of\n.scale_vec::NX\n).\nAliased\n.scale_vectorsize\nvariants:\n.block16\nis aliased with:\n.scale_vec::4X\nwhen\n.kind\n=\n.kind::mxf4nvf4\nand K = 64 or 128\n.block32\nis aliased with:\n.scale_vec::1X\nwhen\n.kind\n=\n.kind::mxf8f6f4\nfor all supported values of K\n.scale_vec::2X\nwhen\n.kind\n=\n.kind::mxf4\nor\n.kind::mxf4nvf4\nand K = 64 or 128\nThe valid combinations of MMA-kind and\n.scale_vectorsize\nare\ndescribed in\nTable 56\n. For\n.kind::mxf4\nwhen the qualifier\n.scale_vectorsize\nis not specified, then it defaults to\n.block32\n. For\n.kind::mxf4nvf4\n,\nthe qualifier\n.scale_vectorsize\nmust be explicitly specified.\nThe qualifier\n.ashift\nshifts the rows of the\nA\nmatrix down by one row, except for\nthe last row in the\nTensor Memory\n. Qualifier\n.ashift\nis only allowed\nwith\nM\n= 128 or\nM\n= 256.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for matrix\nA\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::a::fill\nSpecifies that the\nA\nmatrix read from the memory\nshould be filled in collector buffer.\n.collector::a::use\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer. This requires a previous fill to\nthe collector buffer to be still valid.\n.collector::a::lastuse\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer and the contents of the collector\nbuffer can be discarded. This requires a previous\nfill to the collector buffer to be valid till the\ncollector buffer is read.\n.collector::a::discard\nSpecifies that the contents of the collector buffer\nfor\nA\ncan be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::a::discard\n.\nIt is illegal to specify either of\n.collector::a::use\nor\n.collector::a::fill\nalong with\n.ashift\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nQualifier\n.kind::mxf4nvf4\nintroduced in PTX ISA version 8.7.\nQualifiers\n.block16\nand\n.block32\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nArgument\nscale-input-d\nrequires\nsm_100a\nand is supported on\nsm_100f\nor higher in the same family from PTX ISA version 8.8.\nFor\n.scale_vectorsize\n,\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\nrequires\nsm_100a\n.\n.block16\n,\n.block32\nrequires\nsm_100f\nor\nsm_110f\n.\nFor Target ISA details on matrix shape, check\nTarget ISA Note\n.\nFor Target ISA details on shared memory descriptor, check\nTarget ISA Note\n.\nExamples\ntcgen05.mma.cta_group::1.kind::tf32      [taddr0],  adesc,  bdesc, idesc, {m0, m1, m2, m3}, p;\ntcgen05.mma.cta_group::1.kind::mxf8f6f4  [taddr2],  [taddr1],  bdesc, idesc,\n                                         [tmem_scaleA], [tmem_scaleB], p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.2.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.sp\n\ntcgen05.mma.sp\nPerform the 5\nth\ngeneration of matrix multiply and accumulate operation with sparse\nA\nmatrix.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.sp.cta_group.kind  [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] ,  idesc,\n                               { disable-output-lane }, enable-input-d{, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind  [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,\n                               { disable-output-lane }, enable-input-d{, scale-input-d};\n\n.kind       = { .kind::f16, , .kind::tf32, .kind::f8f6f4 }\n.cta_group  = { .cta_group::1,  .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 2. Floating-point type with block scaling:\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}\n                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}\n                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.cta_group      = { .cta_group::1,  .cta_group::2 }\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n\n----------------------------------------------------------------------------------\n\n// 3. Convolution MMA with floating-point type without block scaling:\n\ntcgen05.mma.sp.cta_group.kind.collector_usage           [d-tmem],  a-desc,  b-desc,\n                                                        [sp-meta-tmem] ,  idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind.ashift{.collector_usage}  [d-tmem], [a-tmem], b-desc,\n                                                        [sp-meta-tmem] , idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind{.ashift}.collector_usage  [d-tmem], [a-tmem], b-desc,\n                                                        [sp-meta-tmem] , idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\n.kind            = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 4. Activation Stationary MMA with floating-point type with block scaling:\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 5. Integer type:\n\ntcgen05.mma.sp.cta_group.kind::i8 [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] , idesc,\n                                  { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8 [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,\n                                  { disable-output-lane }, enable-input-d;\n\n.cta_group      = { .cta_group::1,  .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 6. Convolution MMA with Integer type:\n\ntcgen05.mma.sp.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc,\n                                                           [sp-meta-tmem] , idesc,\n                                                           { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                           [sp-meta-tmem], idesc ,\n                                                           { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc,\n                                                           [sp-meta-tmem], idesc ,\n                                                           { disable-output-lane }, enable-input-d;\n\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\nDescription\nInstruction\ntcgen05.mma.sp\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix multiply and accumulate operation of the form\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMx(K/2)\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nSparse Matrices\ndescribes the details of the sparsity.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe optional immediate argument\nscale-input-d\ncan be specified to scale the\ninput matrix\nD\nas follows:\nD\n=\nA*B+D\n*\n(2\n^\n-\nscale-input-d)\nThe valid range of values for argument\nscale-input-d\nis [0, 15]. The argument\nscale-input-d\nis only valid for\n.kind::tf32\nand\n.kind::f16\n.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes,\nexact types, sparsity and other details of the input matrices, output matrix and the\nmatrix multiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate\noperation is performed on the\nTensor Memory\nof the executing\nthread’s CTA only. The qualifier\n.cta_group::2\nspecifies that the matrix\nmultiply and accumulate operation is performed on the\nTensor Memory\nof the executing thread’s CTA and its\npeer CTA\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.sp\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.sp\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n. The\n64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which represent\nthe matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor\nis described in\nMatrix Descriptors\n.\nThe vector operand\ndisable-output-lane\nspecifies the lane(s) in the\nTensor Memory\nthat should be not be updated with the resultant matrix\nD\n. Elements of the vector operand\ndisable-output-lane\nforms a mask where each bit corresponds to a lane of the\nTensor Memory\n. with least significant bit of the first element of\nthe vector (leftmost in syntax) corresponding to the lane 0 of the Tensor Memory. If a bit in\nthe mask is 1, then the corresponding lane in the Tensor Memory for the resultant matrix\nD\nwill not be updated. The size of the vector is as follows:\n.cta_group\nSize of the vector disable-output-lane\n::1\n4\n::2\n8\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply\nand accumulate operation as specified in the section\nBlock Scaling for tcgen05.mma.sync\n.\nThe address operand\nscale-A-tmem\nand\nscale-B-tmem\nspecify the base address the\nmatrices\nscale_A\nand\nscale_B\nrespectively in the\nTensor Memory\n.\nFor qualifier\n.scale_vectorsize\n,\nIf\n.scale_vec::NX\nis specified: N specifies the number of columns in\nscale_A\nmatrix and number of rows in\nscale_B\nmatrix.\nIf\n.blockN\nis specified: N specifies the block size for which single scale factor\nwill be applied. In this form, value of N is same as the K-dimension / (N of\n.scale_vec::NX\n).\nAliased\n.scale_vectorsize\nvariants:\n.block16\nis aliased with:\n.scale_vec::4X\nwhen\n.kind\n=\n.kind::mxf4nvf4\nand K = 64 or 128\n.block32\nis aliased with:\n.scale_vec::1X\nwhen\n.kind\n=\n.kind::mxf8f6f4\nfor all supported values of K\n.scale_vec::2X\nwhen\n.kind\n=\n.kind::mxf4\nor\n.kind::mxf4nvf4\nand K = 64 or 128\nThe valid combinations of MMA-kind and\n.scale_vectorsize\nare\ndescribed in\nTable 56\n. For\n.kind::mxf4\nwhen the qualifier\n.scale_vectorsize\nis not specified, then it defaults to\n.block32\n. For\n.kind::mxf4nvf4\n,\nthe qualifier\n.scale_vectorsize\nmust be explicitly specified.\nThe qualifier\n.ashift\nshifts the rows of the\nA\nmatrix down by one row, except for\nthe last row in the\nTensor Memory\n. Qualifier\n.ashift\nis only allowed\nwith\nM\n= 128 or\nM\n= 256.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for matrix\nA\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::a::fill\nSpecifies that the\nA\nmatrix read from the memory\nshould be filled in collector buffer.\n.collector::a::use\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer. This requires a previous fill to\nthe collector buffer to be still valid.\n.collector::a::lastuse\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer and the contents of the collector\nbuffer can be discarded. This requires a previous\nfill to the collector buffer to be valid till the\ncollector buffer is read.\n.collector::a::discard\nSpecifies that the contents of the collector buffer\nfor\nA\ncan be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::a::discard\n.\nIt is illegal to specify either of\n.collector::a::use\nor\n.collector::a::fill\nalong with\n.ashift\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nQualifier\n.kind::mxf4nvf4\nintroduced in PTX ISA version 8.7.\nQualifiers\n.block16\nand\n.block32\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n/\n.kind::mxf4nvf4\n/\n.kind::mxf4\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nQualifiers\n.kind::mxf4nvf4\nand\n.kind::mxf4\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_103a\nsm_110a\nArgument\nscale-input-d\nrequires\nsm_100a\nand is supported on\nsm_100f\nor higher in the same family from PTX ISA version 8.8.\nFor\n.scale_vectorsize\n,\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\nrequires\nsm_100a\n.\n.block16\n,\n.block32\nrequires\nsm_100f\nor\nsm_110f\n.\nFor Target ISA details on matrix shape, check\nTarget ISA Note\n.\nFor Target ISA details on shared memory descriptor, check\nTarget ISA Note\n.\nExamples\ntcgen05.mma.sp.cta_group::1.kind::f16      [taddr0],  adesc,  bdesc, [tmem_spmeta0], idesc, p;\n\ntcgen05.mma.sp.cta_group::1.kind::mxf8f6f4.collector::a:fill\n                                           [taddr2],  [taddr1],  bdesc, [tmem_spmeta1], idesc,\n                                           [tmem_scaleA], [tmem_scaleB], p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.3.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.ws\n\ntcgen05.mma.ws\nPerform the 5\nth\ngeneration of weight stationary convolution matrix multiply and accumulate\noperation.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem],  a-desc,  b-desc,  idesc,\n                                                      enable-input-d {, zero-column-mask-desc };\n\ntcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem], [a-tmem], b-desc, idesc,\n                                                      enable-input-d {, zero-column-mask-desc };\n\n.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n\n----------------------------------------------------------------------------------\n\n// 2. Integer type:\n\ntcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem],  a-desc,  b-desc, idesc,\n                                                       enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                       enable-input-d {, zero-column-mask-desc};\n\n.collector_usage = { .collector::buffer::op }\n::buffer = { ::b0, ::b1, ::b2, ::b3 }\n::op   = { ::fill, ::use, ::lastuse, ::discard}\nDescription\nInstruction\ntcgen05.mma.ws\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMxK\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes, exact\ntypes, sparsity and other details of the input matrices, output matrix and the matrix\nmultiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate operation\nis performed on the\nTensor Memory\nof the executing thread’s CTA only.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.ws\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.ws\nwill result in the initiation of the whole matrix multiply and accumulate\noperation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n. The\n64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which represent\nthe matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor\nis described in\nMatrix Descriptors\n.\nThe optional operand\nzero-column-mask-desc\nis a 64-bit register which specifies the\nZero-Column Mask Descriptor\n. The zero-column\nmask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix\nwill have zero value for the matrix multiply and accumulate operation regardless of the\nvalues present in the shared memory.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for Matrix\nB\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::bN::fill\nSpecifies that the\nB\nmatrix read from the memory\nshould be filled in collector buffer #N.\n.collector::bN::use\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N. This requires a previous fill\nto the collector buffer #N to be still valid.\n.collector::bN::lastuse\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N after which the contents of the\ncollector buffer #N can be discarded. This requires\na previous fill to the collector buffer #N to be\nvalid till the collector buffer #N is read.\n.collector::bN::discard\nSpecifies that the contents of the collector buffer\n#N can be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::b0::discard\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nExamples\ntcgen05.mma.ws.cta_group::1.kind::i8.collector::b2:use [taddr2], [taddr1], bdesc, idesc, p;\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.4.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.ws.sp\n\ntcgen05.mma.ws.sp\nPerform the 5\nth\ngeneration of weight stationary convolution matrix multiply and accumulate\noperation with sparse\nA\nmatrix.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem],  a-desc,  b-desc,\n                                                      [sp-meta-tmem] ,  idesc,\n                                                      enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                      [sp-meta-tmem] , idesc,\n                                                      enable-input-d {, zero-column-mask-desc};\n\n.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n\n----------------------------------------------------------------------------------\n\n// 2. Integer type:\n\ntcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], a-desc, b-desc,\n                                                          [sp-meta-tmem] , idesc,\n                                                          enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                          [sp-meta-tmem] , idesc,\n                                                          enable-input-d {, zero-column-mask-desc};\n\n.collector_usage = { .collector::buffer::op }\n::buffer = { ::b0, ::b1, ::b2, ::b3 }\n::op   = { ::fill, ::use, ::lastuse, ::discard}\nDescription\nInstruction\ntcgen05.mma.ws.sp\nis an asynchronous instruction which initiates\nan\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMx(K/2)\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix\nis\nMxN\n.\nSparse Matrices\ndescribes the details of the\nsparsity.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes, exact\ntypes, sparsity and other details of the input matrices, output matrix and the matrix\nmultiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate\noperation is performed on the Tensor Memory of the executing thread’s CTA only.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.ws.sp\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.ws.sp\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind are\nspecified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies\nthe address of the matrix\nA\nin the\nTensor Memory\n. The 64-bit register\noperand\na-desc\nand\nb-desc\nare the matrix descriptors which represent the matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor is described in\nMatrix Descriptors\n.\nThe optional operand\nzero-column-mask-desc\nis a 64-bit register which specifies the\nZero-Column Mask Descriptor\n. The zero-column\nmask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix\nwill have zero value for the matrix multiply and accumulate operation regardless of the\nvalues present in the shared memory.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for Matrix\nB\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::bN::fill\nSpecifies that the\nB\nmatrix read from the memory\nshould be filled in collector buffer #N.\n.collector::bN::use\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N. This requires a previous fill\nto the collector buffer #N to be still valid.\n.collector::bN::lastuse\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N after which the contents of the\ncollector buffer #N can be discarded. This requires\na previous fill to the collector buffer #N to be\nvalid till the collector buffer #N is read.\n.collector::bN::discard\nSpecifies that the contents of the collector buffer\n#N can be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::b0::discard\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nExamples\ntcgen05.mma.ws.sp.cta_group::1.kind::tf32.collector::b1::fill  [taddr1], [taddr0], bdesc,\n                                                               [tmem_spmeta0], idesc, p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.11.\nTensorCore 5th Generation Specialized Synchronization Operations\n\n9.7.16.11.1.\nTensorCore 5th Generation Instructions:\ntcgen05.fence\n\ntcgen05.fence\nSpecialized fence for the asynchronous tcgen05 operations.\nSyntax\ntcgen05.fence::before_thread_sync ;\ntcgen05.fence::after_thread_sync  ;\nDescription\nThe instruction\ntcgen05.fence::before_thread_sync\norders all the prior asynchronous\ntcgen05\noperations with respect to the subsequent\ntcgen05\nand the execution\nordering operations.\nThe instruction\ntcgen05.fence::after_thread_sync\norders all the subsequent asynchronous\ntcgen05\noperations with respect to the prior\ntcgen05\nand the execution ordering\noperations.\nThe\ntcgen05.fence::*\ninstructions compose with execution ordering instructions across\na thread scope and provide ordering between\ntcgen05\ninstructions across the same scope.\nThe\ntcgen05.fence::before_thread_sync\ninstructions behave as code motion fence for prior\ntcgen05\ninstructions as they cannot be hoisted across. The\ntcgen05.fence::after_thread_sync\ninstructions behave as code motion fence for subsequent\ntcgen05\ninstructions as they cannot\nbe hoisted across.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n// Producer thread:\n\ntcgen05.cp.cta_group::1.128x256b  [taddr0], sdesc0;\n\ntcgen05.fence::before_thread_sync;\nst.relaxed.b32 [flag], 1;\n\n// Consumer thread:\n\nloop:\nld.relaxed.b32 r, [flag];\nsetp.eq.u32 p, r, 1;\n@!p bra loop;\n\ntcgen05.fence::after_thread_sync;\ntcgen05.mma.cta_group.kind   [taddr0], adesc, bdesc, idesc, p;\n9.7.16.12.\nTensorCore 5th Generation Async Synchronization Operations\n\n9.7.16.12.1.\nTensorCore 5th Generation Instructions:\ntcgen05.commit\n\ntcgen05.commit\nMakes the mbarrier object track the completion of all prior async-tcgen05 operations initiated\nby the executing thread.\nSyntax\ntcgen05.commit.cta_group.completion_mechanism{.shared::cluster}{.multicast}.b64\n                                                            [mbar] {, ctaMask};\n\n.completion_mechanism = { .mbarrier::arrive::one }\n.cta_group            = { .cta_group::1, .cta_group::2 }\n.multicast            = { .multicast::cluster }\nDescription\nThe instruction\ntcgen05.commit\nis an asynchronous instruction which makes the mbarrier object,\nspecified by the address operand\nmbar\n, track the completion of all the prior asynchronous\ntcgen05\noperations, as listed in\nmbarrier based completion mechanism\n,\ninitiated by the executing thread. Upon the completion of the tracked asynchronous\ntcgen05\noperations, the signal specified by the\n.completion_mechanism\nis triggered by the system\non the mbarrier object.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe instruction\ntcgen05.commit.cta_group::1\ntracks for the completion of all prior\nasynchronous\ntcgen05\noperations with\n.cta_group::1\nissued by the current thread.\nSimilarly, the instruction\ntcgen05.commit.cta_group::2\ntracks for the completion of all\nprior asynchronous\ntcgen05\noperations with\n.cta_group::2\nissued by the current thread.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe qualifier\n.mbarrier::arrive::one\nindicates that upon the completion of the prior\nasynchronous\ntcgen05\noperation issued by the current thread, an arrive-on operation, with\nthe count argument of 1, is signaled on the mbarrier object. The scope of the arrive-on operation\nis the cluster scope.\nThe optional qualifier\n.multicast::cluster\nallows signaling on the mbarrier objects of multiple\nCTAs in the cluster. Operand\nctaMask\nspecifies the CTAs in the cluster such that each bit\nposition in the 16-bit\nctaMask\noperand corresponds to the\n%cluster_ctarank\nof the destination\nCTA. The mbarrier signal is multicast to the same offset as\nmbar\nin the shared memory of each\ndestination CTA.\nIf no state space is specified then\nGeneric Addressing\nis used. If the\naddress specified by\nmbar\ndoes not fall within the address window of\n.shared::cluster\nstate\nspace then the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nExample 1:\ntcgen05.cp.cta_group::1.128x256b                      [taddr0], sdesc0;\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj1];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj1], 0;\n@!p bra loop;\n\nExample 2:\ntcgen05.mma.cta_group::2.kind::tf32    [taddr0],  adesc,  bdesc, idesc, p;\ntcgen05.commit.cta_group::2.mbarrier::arrive::one.b64 [mbarObj2];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj2], 0;\n@!p bra loop;\n9.7.17.\nStack Manipulation Instructions\n\nThe stack manipulation instructions can be used to dynamically allocate and deallocate memory on the\nstack frame of the current function.\nThe stack manipulation instrucitons are:\nstacksave\nstackrestore\nalloca\n9.7.17.1.\nStack Manipulation Instructions:\nstacksave\n\nstacksave\nSave the value of stack pointer into a register.\nSyntax\nstacksave.type  d;\n\n.type = { .u32, .u64 };\nDescription\nCopies the current value of stack pointer into the destination register\nd\n. Pointer returned by\nstacksave\ncan be used in a subsequent\nstackrestore\ninstruction to restore the stack\npointer. If\nd\nis modified prior to use in\nstackrestore\ninstruction, it may corrupt data in\nthe stack.\nDestination operand\nd\nhas the same type as the instruction type.\nSemantics\nd = stackptr;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nstacksave\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 rd;\nstacksave.u32 rd;\n\n.reg .u64 rd1;\nstacksave.u64 rd1;\n9.7.17.2.\nStack Manipulation Instructions:\nstackrestore\n\nstackrestore\nUpdate the stack pointer with a new value.\nSyntax\nstackrestore.type  a;\n\n.type = { .u32, .u64 };\nDescription\nSets the current stack pointer to source register\na\n.\nWhen\nstackrestore\nis used with operand\na\nwritten by a prior\nstacksave\ninstruction, it\nwill effectively restore the state of stack as it was before\nstacksave\nwas executed. Note that\nif\nstackrestore\nis used with an arbitrary value of\na\n, it may cause corruption of stack\npointer. This implies that the correct use of this feature requires that\nstackrestore.type\na\nis\nused after\nstacksave.type\na\nwithout redefining the value of\na\nbetween them.\nOperand\na\nhas the same type as the instruction type.\nSemantics\nstackptr = a;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nstackrestore\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 ra;\nstacksave.u32 ra;\n// Code that may modify stack pointer\n...\nstackrestore.u32 ra;\n9.7.17.3.\nStack Manipulation Instructions:\nalloca\n\nalloca\nDynamically allocate memory on stack.\nSyntax\nalloca.type  ptr, size{, immAlign};\n\n.type = { .u32, .u64 };\nDescription\nThe\nalloca\ninstruction dynamically allocates memory on the stack frame of the current function\nand updates the stack pointer accordingly. The returned pointer\nptr\npoints to local memory and\ncan be used in the address operand of\nld.local\nand\nst.local\ninstructions.\nIf sufficient memory is unavailable for allocation on the stack, then execution of\nalloca\nmay\nresult in stack overflow. In such cases, attempting to access the allocated memory with\nptr\nwill\nresult in undefined program behavior.\nThe memory allocated by\nalloca\nis deallocated in the following ways:\nIt is automatically deallocated when the function exits.\nIt can be explicitly deallocated using\nstacksave\nand\nstackrestore\ninstructions:\nstacksave\ncan be used to save the value of stack pointer before executing\nalloca\n, and\nstackrestore\ncan be used after\nalloca\nto restore stack pointer to the original value which\nwas previously saved with\nstacksave\n. Note that accessing deallocated memory after executing\nstackrestore\nresults in undefined behavior.\nsize\nis an unsigned value which specifies the amount of memory in number of bytes to be\nallocated on stack.\nsize\n=\n0\nmay not lead to a valid memory allocation.\nBoth\nptr\nand\nsize\nhave the same type as the instruction type.\nimmAlign\nis a 32-bit value which specifies the alignment requirement in number of bytes for the\nmemory allocated by\nalloca\n. It is an integer constant, must be a power of 2 and must not exceed\n2^23.\nimmAlign\nis an optional argument with default value being 8 which is the minimum\nguaranteed alignment.\nSemantics\nalloca.type ptr, size, immAlign:\n\na = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment\n\n// Allocate size bytes of stack memory with alignment a and update the stack pointer.\n// Since the stack grows down, the updated stack pointer contains a lower address.\nstackptr = alloc_stack_mem(size, a);\n\n// Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory\n// allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).\nstacksave ptr;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nalloca\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 ra, stackptr, ptr, size;\n\nstacksave.u32 stackptr;     // Save the current stack pointer\nalloca ptr, size, 8;        // Allocate stack memory\nst.local.u32 [ptr], ra;     // Use the allocated stack memory\nstackrestore.u32 stackptr;  // Deallocate memory by restoring the stack pointer\n9.7.18.\nVideo Instructions\n\nAll video instructions operate on 32-bit register operands. However, the video instructions may be\nclassified as either scalar or SIMD based on whether their core operation applies to one or multiple\nvalues.\nThe video instructions are:\nvadd\n,\nvadd2\n,\nvadd4\nvsub\n,\nvsub2\n,\nvsub4\nvmad\nvavrg2\n,\nvavrg4\nvabsdiff\n,\nvabsdiff2\n,\nvabsdiff4\nvmin\n,\nvmin2\n,\nvmin4\nvmax\n,\nvmax2\n,\nvmax4\nvshl\nvshr\nvset\n,\nvset2\n,\nvset4\n9.7.18.1.\nScalar Video Instructions\n\nAll scalar video instructions operate on 32-bit register operands. The scalar video instructions\nare:\nvadd\nvsub\nvabsdiff\nvmin\nvmax\nvshl\nvshr\nvmad\nvset\nThe scalar video instructions execute the following stages:\nExtract and sign- or zero-extend byte, half-word, or word values from its source operands, to\nproduce signed 33-bit input values.\nPerform a scalar arithmetic operation to produce a signed 34-bit result.\nOptionally clamp the result to the range of the destination type.\nOptionally perform one of the following:\napply a second operation to the intermediate result and a third operand, or\ntruncate the intermediate result to a byte or half-word value and merge into a specified\nposition in the third operand to produce the final result.\nThe general format of scalar video instructions is as follows:\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}        d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.secop  d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}   d.dsel, a{.asel}, b{.bsel}, c;\n\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.secop = { .add, .min, .max };\nThe source and destination operands are all 32-bit registers. The type of each operand (\n.u32\nor\n.s32\n) is specified in the instruction type; all combinations of\ndtype\n,\natype\n, and\nbtype\nare valid. Using the\natype/btype\nand\nasel/bsel\nspecifiers, the input values are\nextracted and sign- or zero-extended internally to\n.s33\nvalues. The primary operation is then\nperformed to produce an\n.s34\nintermediate result. The sign of the intermediate result depends on\ndtype.\nThe intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging.\n.s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) {\n    if ( !sat )  return tmp;\n\n    switch ( dsel ) {\n        case .b0, .b1, .b2, .b3:\n            if ( sign )  return CLAMP( tmp, S8_MAX, S8_MIN );\n            else         return CLAMP( tmp, U8_MAX, U8_MIN );\n        case .h0, .h1:\n            if ( sign )  return CLAMP( tmp, S16_MAX, S16_MIN );\n            else         return CLAMP( tmp, U16_MAX, U16_MIN );\n        default:\n            if ( sign )  return CLAMP( tmp, S32_MAX, S32_MIN );\n            else         return CLAMP( tmp, U32_MAX, U32_MIN );\n    }\n}\nThis intermediate result is then optionally combined with the third source operand using a secondary\narithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the\nthird operand is based on\ndtype\n.\n.s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) {\n    switch ( secop ) {\n        .add:     return tmp + c;\n        .min:     return MIN(tmp, c);\n        .max      return MAX(tmp, c);\n        default:  return tmp;\n    }\n}\n.s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) {\n    switch ( dsel ) {\n        case .h0:  return ((tmp & 0xffff)        | (0xffff0000 & c);\n        case .h1:  return ((tmp & 0xffff) << 16) | (0x0000ffff & c);\n        case .b0:  return ((tmp & 0xff)          | (0xffffff00 & c);\n        case .b1:  return ((tmp & 0xff) <<  8)   | (0xffff00ff & c);\n        case .b2:  return ((tmp & 0xff) << 16)   | (0xff00ffff & c);\n        case .b3:  return ((tmp & 0xff) << 24)   | (0x00ffffff & c);\n        default:   return tmp;\n    }\n}\nThe lower 32-bits are then written to the destination operand.\n9.7.18.1.1.\nScalar Video Instructions:\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\n\nvadd\n,\nvsub\nInteger byte/half-word/word addition/subtraction.\nvabsdiff\nInteger byte/half-word/word absolute value of difference.\nvmin\n,\nvmax\nInteger byte/half-word/word minimum/maximum.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}       d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vadd, vsub, vabsdiff, vmin, vmax };\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nPerform scalar arithmetic operation with optional saturate, and optional secondary arithmetic operation or subword data merge.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\n\nswitch ( vop ) {\n    case vadd:     tmp = ta + tb;\n    case vsub:     tmp = ta - tb;\n    case vabsdiff: tmp = | ta - tb |;\n    case vmin:     tmp = MIN( ta, tb );\n    case vmax:     tmp = MAX( ta, tb );\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\nrequire\nsm_20\nor higher.\nExamples\nvadd.s32.u32.s32.sat      r1, r2.b0, r3.h0;\nvsub.s32.s32.u32.sat      r1, r2.h1, r3.h1;\nvabsdiff.s32.s32.s32.sat  r1.h0, r2.b0, r3.b2, c;\nvmin.s32.s32.s32.sat.add  r1, r2, r3, c;\n9.7.18.1.2.\nScalar Video Instructions:\nvshl\n,\nvshr\n\nvshl\n,\nvshr\nInteger byte/half-word/word left/right shift.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.u32{.sat}.mode       d, a{.asel}, b{.bsel};\nvop.dtype.atype.u32{.sat}.mode.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.u32{.sat}.mode  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vshl, vshr };\n.dtype = .atype = { .u32, .s32 };\n.mode  = { .clamp, .wrap };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nvshl\nShift\na\nleft by unsigned amount in\nb\nwith optional saturate, and optional secondary\narithmetic operation or subword data merge. Left shift fills with zero.\nvshr\nShift\na\nright by unsigned amount in\nb\nwith optional saturate, and optional secondary\narithmetic operation or subword data merge. Signed shift fills with the sign bit, unsigned shift\nfills with zero.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a,atype, asel );\ntb = partSelectSignExtend( b, .u32, bsel );\nif ( mode == .clamp  && tb > 32 )  tb = 32;\nif ( mode == .wrap )                       tb = tb & 0x1f;\nswitch ( vop ){\n   case vshl:  tmp = ta << tb;\n   case vshr:  tmp = ta >> tb;\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvshl\n,\nvshr\nrequire\nsm_20\nor higher.\nExamples\nvshl.s32.u32.u32.clamp  r1, r2, r3;\nvshr.u32.u32.u32.wrap   r1, r2, r3.h1;\n9.7.18.1.3.\nScalar Video Instructions:\nvmad\n\nvmad\nInteger byte/half-word/word multiply-accumulate.\nSyntax\n// 32-bit scalar operation\nvmad.dtype.atype.btype{.sat}{.scale}     d, {-}a{.asel}, {-}b{.bsel},\n                                         {-}c;\nvmad.dtype.atype.btype.po{.sat}{.scale}  d, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.scale = { .shr7, .shr15 };\nDescription\nCalculate\n(a*b)\n+\nc\n, with optional operand negates,\nplus one\nmode, and scaling.\nThe source operands support optional negation with some restrictions. Although PTX syntax allows\nseparate negation of the\na\nand\nb\noperands, internally this is represented as negation of the\nproduct\n(a*b)\n. That is,\n(a*b)\nis negated if and only if exactly one of\na\nor\nb\nis\nnegated. PTX allows negation of either\n(a*b)\nor\nc\n.\nThe plus one mode (\n.po\n) computes\n(a*b)\n+\nc\n+\n1\n, which is used in computing averages. Source\noperands may not be negated in\n.po\nmode.\nThe intermediate result of\n(a*b)\nis unsigned if atype and btype are unsigned and the product\n(a*b)\nis not negated; otherwise, the intermediate result is signed. Input\nc\nhas the same\nsign as the intermediate result.\nThe final result is unsigned if the intermediate result is unsigned and\nc\nis not negated.\nDepending on the sign of the\na\nand\nb\noperands, and the operand negates, the following\ncombinations of operands are supported for VMAD:\n(u32 * u32) + u32  // intermediate unsigned; final unsigned\n-(u32 * u32) + s32  // intermediate   signed; final   signed\n (u32 * u32) - u32  // intermediate unsigned; final   signed\n (u32 * s32) + s32  // intermediate   signed; final   signed\n-(u32 * s32) + s32  // intermediate   signed; final   signed\n (u32 * s32) - s32  // intermediate   signed; final   signed\n (s32 * u32) + s32  // intermediate   signed; final   signed\n-(s32 * u32) + s32  // intermediate   signed; final   signed\n (s32 * u32) - s32  // intermediate   signed; final   signed\n (s32 * s32) + s32  // intermediate   signed; final   signed\n-(s32 * s32) + s32  // intermediate   signed; final   signed\n (s32 * s32) - s32  // intermediate   signed; final   signed\nThe intermediate result is optionally scaled via right-shift; this result is sign-extended if the\nfinal result is signed, and zero-extended otherwise.\nThe final result is optionally saturated to the appropriate 32-bit range based on the type (signed\nor unsigned) of the final result.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\nsignedFinal = isSigned(atype) || isSigned(btype) ||\n                                 (a.negate ^ b.negate) || c.negate;\ntmp[127:0] = ta * tb;\n\nlsb = 0;\nif ( .po )                  {              lsb = 1; } else\nif ( a.negate ^ b.negate )  { tmp = ~tmp;  lsb = 1; } else\nif ( c.negate )             { c   = ~c;    lsb = 1; }\n\nc128[127:0] = (signedFinal) sext32( c ) : zext ( c );\ntmp = tmp + c128 + lsb;\nswitch( scale ) {\n   case .shr7:   result = (tmp >>  7) & 0xffffffffffffffff;\n   case .shr15:  result = (tmp >> 15) & 0xffffffffffffffff;\n}\nif ( .sat ) {\n     if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN);\n     else             result = CLAMP(result, U32_MAX, U32_MIN);\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvmad\nrequires\nsm_20\nor higher.\nExamples\nvmad.s32.s32.u32.sat    r0, r1, r2, -r3;\nvmad.u32.u32.u32.shr15  r0, r1.h0, r2.h0, r3;\n9.7.18.1.4.\nScalar Video Instructions:\nvset\n\nvset\nInteger byte/half-word/word comparison.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvset.atype.btype.cmp       d, a{.asel}, b{.bsel};\nvset.atype.btype.cmp.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvset.atype.btype.cmp  d.dsel, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nCompare input values using specified comparison, with optional secondary arithmetic operation or\nsubword data merge.\nThe intermediate result of the comparison is always unsigned, and therefore destination\nd\nand\noperand\nc\nare also unsigned.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\ntmp = compare( ta, tb, cmp ) ? 1 : 0;\nd = optSecondaryOp( op2, tmp, c );    // optional secondary operation\nd = optMerge( dsel, tmp, c );         // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvset\nrequires\nsm_20\nor higher.\nExamples\nvset.s32.u32.lt    r1, r2, r3;\nvset.u32.u32.ne    r1, r2, r3.h1;\n9.7.18.2.\nSIMD Video Instructions\n\nThe SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values.\nThe SIMD video instructions are:\nvadd2\n,\nvadd4\nvsub2\n,\nvsub4\nvavrg2\n,\nvavrg4\nvabsdiff2\n,\nvabsdiff4\nvmin2\n,\nvmin4\nvmax2\n,\nvmax4\nvset2\n,\nvset4\nPTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit\nvalues. The SIMD video instructions execute the following stages:\nForm input vectors by extracting and sign- or zero-extending byte or half-word values from the\nsource operands, to form pairs of signed 17-bit values.\nPerform a SIMD arithmetic operation on the input pairs.\nOptionally clamp the result to the appropriate signed or unsigned range, as determinted by the\ndestination type.\nOptionally perform one of the following:\nperform a second SIMD merge operation, or\napply a scalar accumulate operation to reduce the intermediate SIMD results to a single\nscalar.\nThe general format of dual half-word SIMD video instructions is as follows:\n// 2-way SIMD operation, with second SIMD merge or accumulate\nvop2.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };\n.asel  = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } };\nThe general format of quad byte SIMD video instructions is as follows:\n// 4-way SIMD operation, with second SIMD merge or accumulate\nvop4.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\nThe source and destination operands are all 32-bit registers. The type of each operand (\n.u32\nor\n.s32\n) is specified in the instruction type; all combinations of\ndtype\n,\natype\n, and\nbtype\nare valid. Using the\natype/btype\nand\nasel/bsel\nspecifiers, the input values are\nextracted and sign- or zero-extended internally to\n.s33\nvalues. The primary operation is then\nperformed to produce an\n.s34\nintermediate result. The sign of the intermediate result depends on\ndtype\n.\nThe intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging.\n9.7.18.2.1.\nSIMD Video Instructions:\nvadd2\n,\nvsub2\n,\nvavrg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\n\nvadd2\n,\nvsub2\nInteger dual half-word SIMD addition/subtraction.\nvavrg2\nInteger dual half-word SIMD average.\nvabsdiff2\nInteger dual half-word SIMD absolute value of difference.\nvmin2\n,\nvmax2\nInteger dual half-word SIMD minimum/maximum.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvop2.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop2.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n vop2  = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 };\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32\nDescription\nTwo-way SIMD parallel arithmetic operation with secondary operation.\nElements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected half-words are then operated on in parallel.\nThe results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.\nFor instructions with a secondary SIMD merge operation:\nFor half-word positions indicated in mask, the selected half-word results are copied into\ndestination\nd\n. For all other positions, the corresponding half-word from source operand\nc\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor half-word positions indicated in mask, the selected half-word results are added to operand\nc\n, producing a result in\nd\n.\nSemantics\n// extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\n\nfor (i=0; i<2; i++) {\n    switch ( vop2 ) {\n       case vadd2:             t[i] = Va[i] + Vb[i];\n       case vsub2:             t[i] = Va[i] - Vb[i];\n       case vavrg2:            if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n       case vabsdiff2:         t[i] = | Va[i] - Vb[i] |;\n       case vmin2:             t[i] = MIN( Va[i], Vb[i] );\n       case vmax2:             t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S16_MAX, S16_MIN );\n        else                   t[i] = CLAMP( t[i], U16_MAX, U16_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvadd2\n,\nvsub2\n,\nvarvg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\nrequire\nsm_30\nor higher.\nExamples\nvadd2.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub2.s32.s32.s32.sat  r1.h0, r2.h10, r3.h32, r1;\nvmin2.s32.u32.u32.add  r1.h10, r2.h00, r3.h22, r1;\n9.7.18.2.2.\nSIMD Video Instructions:\nvset2\n\nvset2\nInteger dual half-word SIMD comparison.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvset2.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset2.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32\nDescription\nTwo-way SIMD parallel comparison with secondary operation.\nElements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected half-words are then compared in parallel.\nThe intermediate result of the comparison is always unsigned, and therefore the half-words of\ndestination\nd\nand operand\nc\nare also unsigned.\nFor instructions with a secondary SIMD merge operation:\nFor half-word positions indicated in mask, the selected half-word results are copied into\ndestination\nd\n. For all other positions, the corresponding half-word from source operand\nb\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor half-word positions indicated in mask, the selected half-word results are added to operand\nc\n, producing\na\nresult in\nd\n.\nSemantics\n// extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\nfor (i=0; i<2; i++) {\n    t[i] = compare( Va[i], Vb[i], .cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvset2\nrequires\nsm_30\nor higher.\nExamples\nvset2.s32.u32.lt      r1, r2, r3, r0;\nvset2.u32.u32.ne.add  r1, r2, r3, r0;\n9.7.18.2.3.\nSIMD Video Instructions:\nvadd4\n,\nvsub4\n,\nvavrg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\n\nvadd4\n,\nvsub4\nInteger quad byte SIMD addition/subtraction.\nvavrg4\nInteger quad byte SIMD average.\nvabsdiff4\nInteger quad byte SIMD absolute value of difference.\nvmin4\n,\nvmax4\nInteger quad byte SIMD minimum/maximum.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvop4.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop4.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\nvop4  = { vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 };\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654\nDescription\nFour-way SIMD parallel arithmetic operation with secondary operation.\nElements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected bytes are then operated on in parallel.\nThe results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.\nFor instructions with a secondary SIMD merge operation:\nFor byte positions indicated in mask, the selected byte results are copied into destination\nd\n. For all other positions, the corresponding byte from source operand\nc\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor byte positions indicated in mask, the selected byte results are added to operand\nc\n,\nproducing a result in\nd\n.\nSemantics\n// extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    switch ( vop4 ) {\n        case vadd4:            t[i] = Va[i] + Vb[i];\n        case vsub4:            t[i] = Va[i] - Vb[i];\n        case vavrg4:           if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n        case vabsdiff4:        t[i] = | Va[i] - Vb[i] |;\n        case vmin4:            t[i] = MIN( Va[i], Vb[i] );\n        case vmax4:            t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S8_MAX, S8_MIN );\n        else                   t[i] = CLAMP( t[i], U8_MAX, U8_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvadd4\n,\nvsub4\n,\nvarvg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\nrequire\nsm_30\nor higher.\nExamples\nvadd4.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub4.s32.s32.s32.sat  r1.b0, r2.b3210, r3.b7654, r1;\nvmin4.s32.u32.u32.add  r1.b00, r2.b0000, r3.b2222, r1;\n9.7.18.2.4.\nSIMD Video Instructions:\nvset4\n\nvset4\nInteger quad byte SIMD comparison.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvset4.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset4.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654\nDescription\nFour-way SIMD parallel comparison with secondary operation.\nElements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected bytes are then compared in parallel.\nThe intermediate result of the comparison is always unsigned, and therefore the bytes of destination\nd\nand operand\nc\nare also unsigned.\nFor instructions with a secondary SIMD merge operation:\nFor byte positions indicated in mask, the selected byte results are copied into destination\nd\n. For all other positions, the corresponding byte from source operand\nb\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor byte positions indicated in mask, the selected byte results are added to operand\nc\n,\nproducing a result in\nd\n.\nSemantics\n// extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    t[i] = compare( Va[i], Vb[i], cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvset4\nrequires\nsm_30\nor higher.\nExamples\nvset4.s32.u32.lt      r1, r2, r3, r0;\nvset4.u32.u32.ne.max  r1, r2, r3, r0;\n9.7.19.\nMiscellaneous Instructions\n\nThe Miscellaneous instructions are:\nbrkpt\nnanosleep\npmevent\ntrap\nsetmaxnreg\n9.7.19.1.\nMiscellaneous Instructions:\nbrkpt\n\nbrkpt\nBreakpoint.\nSyntax\nbrkpt;\nDescription\nSuspends execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nbrkpt\nrequires\nsm_11\nor higher.\nExamples\nbrkpt;\n@p  brkpt;\n9.7.19.2.\nMiscellaneous Instructions:\nnanosleep\n\nnanosleep\nSuspend the thread for an approximate delay given in nanoseconds.\nSyntax\nnanosleep.u32 t;\nDescription\nSuspends the thread for a sleep duration approximately close to the delay\nt\n, specified in\nnanoseconds.\nt\nmay be a register or an immediate value.\nThe sleep duration is approximated, but guaranteed to be in the interval\n[0,\n2*t]\n. The maximum\nsleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual\nthreads within a warp such that all sleeping threads in the warp wake up together.\nPTX ISA Notes\nnanosleep\nintroduced in PTX ISA 6.3.\nTarget ISA Notes\nnanosleep\nrequires\nsm_70\nor higher.\nExamples\n.reg .b32 r;\n.reg .pred p;\n\nnanosleep.u32 r;\nnanosleep.u32 42;\n@p nanosleep.u32 r;\n9.7.19.3.\nMiscellaneous Instructions:\npmevent\n\npmevent\nTrigger one or more Performance Monitor events.\nSyntax\npmevent       a;    // trigger a single performance monitor event\npmevent.mask  a;    // trigger one or more performance monitor events\nDescription\nTriggers one or more of a fixed number of performance monitor events, with event index or mask\nspecified by immediate operand\na\n.\npmevent\n(without modifier\n.mask\n) triggers a single performance monitor event indexed by\nimmediate operand\na\n, in the range\n0..15\n.\npmevent.mask\ntriggers one or more of the performance monitor events. Each bit in the 16-bit\nimmediate operand\na\ncontrols an event.\nProgrammatic performance moniter events may be combined with other hardware events using Boolean\nfunctions to increment one of the four performance counters. The relationship between events and\ncounters is programmed via API calls from the host.\nNotes\nCurrently, there are sixteen performance monitor events, numbered 0 through 15.\nPTX ISA Notes\npmevent\nintroduced in PTX ISA version 1.4.\npmevent.mask\nintroduced in PTX ISA version 3.0.\nTarget ISA Notes\npmevent supported on all target architectures.\npmevent.mask\nrequires\nsm_20\nor higher.\nExamples\npmevent      1;\n@p  pmevent      7;\n@q  pmevent.mask 0xff;\n9.7.19.4.\nMiscellaneous Instructions:\ntrap\n\ntrap\nPerform trap operation.\nSyntax\ntrap;\nDescription\nAbort execution and generate an interrupt to the host CPU.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ntrap;\n@p  trap;\n9.7.19.5.\nMiscellaneous Instructions:\nsetmaxnreg\n\nsetmaxnreg\nHint to change the number of registers owned by the warp.\nSyntax\nsetmaxnreg.action.sync.aligned.u32 imm-reg-count;\n\n.action = { .inc, .dec };\nDescription\nsetmaxnreg\nprovides a hint to the system to update the maximum number of per-thread registers\nowned by the executing warp to the value specified by the\nimm-reg-count\noperand.\nQualifier\n.dec\nis used to release extra registers such that the absolute per-thread maximum\nregister count is reduced from its current value to\nimm-reg-count\n. Qualifier\n.inc\nis used to\nrequest additional registers such that the absolute per-thread maximum register count is increased\nfrom its current value to\nimm-reg-count\n.\nA pool of available registers is maintained per-CTA. Register adjustments requested by the\nsetmaxnreg\ninstructions are handled by supplying extra registers from this pool to the\nrequesting warp or by releasing extra registers from the requesting warp to this pool, depending\nupon the value of the\n.action\nqualifier.\nThe\nsetmaxnreg.inc\ninstruction blocks the execution until enough registers are available in the\nCTA’s register pool. After the instruction\nsetmaxnreg.inc\nobtains new registers from the CTA\npool, the initial contents of the new registers are undefined. The new registers must be initialized\nbefore they are used.\nThe same\nsetmaxnreg\ninstruction must be executed by all warps in a\nwarpgroup\n. After executing a\nsetmaxnreg\ninstruction, all warps in the\nwarpgroup\nmust synchronize explicitly before\nexecuting subsequent setmaxnreg instructions. If a\nsetmaxnreg\ninstruction is not executed by all\nwarps in the\nwarpgroup\n, then the behavior is undefined.\nOperand\nimm-reg-count\nis an integer constant. The value of\nimm-reg-count\nmust be in the\nrange 24 to 256 (both inclusive) and must be a multiple of 8.\nChanges to the register file of the warp always happen at the tail-end of the register file.\nThe\nsetmaxnreg\ninstruction requires that the kernel has been launched with a valid value of\nmaximum number of per-thread registers specified via the appropriate compilation via the appropriate\ncompile-time option or the appropriate performance tuning directive. Otherwise, the\nsetmaxnreg\ninstruction may have no effect.\nWhen qualifier\n.dec\nis specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of\nsetmaxnreg\ninstruction should be greater than or equal to the\nimm-reg-count\n. Otherwise, the behaviour is undefined.\nWhen qualifier\n.inc\nis specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of\nsetmaxnreg\ninstruction should be less than or equal to the\nimm-reg-count\n. Otherwise, the behaviour is undefined.\nThe mandatory\n.sync\nqualifier indicates that\nsetmaxnreg\ninstruction causes the executing\nthread to wait until all threads in the warp execute the same\nsetmaxnreg\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nsetmaxnreg\ninstruction. In conditionally executed code,\nsetmaxnreg\ninstruction should\nonly be used if it is known that all threads in warpgroup evaluate the condition identically,\notherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on following architectures:\nsm_90a\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\nsetmaxnreg.dec.sync.aligned.u32 64;\nsetmaxnreg.inc.sync.aligned.u32 192;\n10.\nSpecial Registers\n\nPTX includes a number of predefined, read-only variables, which are\nvisible as special registers and accessed through\nmov\nor\ncvt\ninstructions.\nThe special registers are:\n%tid\n%ntid\n%laneid\n%warpid\n%nwarpid\n%ctaid\n%nctaid\n%smid\n%nsmid\n%gridid\n%is_explicit_cluster\n%clusterid\n%nclusterid\n%cluster_ctaid\n%cluster_nctaid\n%cluster_ctarank\n%cluster_nctarank\n%lanemask_eq\n,\n%lanemask_le\n,\n%lanemask_lt\n,\n%lanemask_ge\n,\n%lanemask_gt\n%clock\n,\n%clock_hi\n,\n%clock64\n%pm0,\n...,\n%pm7\n%pm0_64,\n...,\n%pm7_64\n%envreg0,\n...,\n%envreg31\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset<2>\n%total_smem_size\n%aggr_smem_size\n%dynamic_smem_size\n%current_graph_exec\n10.1.\nSpecial Registers:\n%tid\n\n%tid\nThread identifier within a CTA.\nSyntax (predefined)\n.sreg .v4 .u32 %tid;                  // thread id vector\n.sreg .u32 %tid.x, %tid.y, %tid.z;    // thread id components\nDescription\nA predefined, read-only, per-thread special register initialized with the thread identifier within\nthe CTA. The\n%tid\nspecial register contains a 1D, 2D, or 3D vector to match the CTA shape; the\n%tid\nvalue in unused dimensions is\n0\n. The fourth element is unused and always returns\nzero. The number of threads in each dimension are specified by the predefined special register\n%ntid\n.\nEvery thread in the CTA has a unique\n%tid\n.\n%tid\ncomponent values range from\n0\nthrough\n%ntid-1\nin each CTA dimension.\n%tid.y\n==\n%tid.z\n==\n0\nin 1D CTAs.\n%tid.z\n==\n0\nin 2D CTAs.\nIt is guaranteed that:\n0  <=  %tid.x <  %ntid.x\n0  <=  %tid.y <  %ntid.y\n0  <=  %tid.z <  %ntid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%tid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32      %r1,%tid.x;  // move tid.x to %rh\n\n// legacy code accessing 16-bit components of %tid\nmov.u16      %rh,%tid.x;\ncvt.u32.u16  %r2,%tid.z;  // zero-extend tid.z to %r2\n10.2.\nSpecial Registers:\n%ntid\n\n%ntid\nNumber of thread IDs per CTA.\nSyntax (predefined)\n.sreg .v4 .u32 %ntid;                   // CTA shape vector\n.sreg .u32 %ntid.x, %ntid.y, %ntid.z;   // CTA dimensions\nDescription\nA predefined, read-only special register initialized with the number of thread ids in each CTA\ndimension. The\n%ntid\nspecial register contains a 3D CTA shape vector that holds the CTA\ndimensions. CTA dimensions are non-zero; the fourth element is unused and always returns zero. The\ntotal number of threads in a CTA is\n(%ntid.x\n*\n%ntid.y\n*\n%ntid.z)\n.\n%ntid.y == %ntid.z == 1 in 1D CTAs.\n%ntid.z ==1 in 2D CTAs.\nMaximum values of %ntid.{x,y,z} are as follows:\n.target architecture\n%ntid.x\n%ntid.y\n%ntid.z\nsm_1x\n512\n512\n64\nsm_20\n,\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n,\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n1024\n1024\n64\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%ntid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n// compute unified thread id for 2D CTA\nmov.u32  %r0,%tid.x;\nmov.u32  %h1,%tid.y;\nmov.u32  %h2,%ntid.x;\nmad.u32  %r0,%h1,%h2,%r0;\n\nmov.u16  %rh,%ntid.x;      // legacy code\n10.3.\nSpecial Registers:\n%laneid\n\n%laneid\nLane Identifier.\nSyntax (predefined)\n.sreg .u32 %laneid;\nDescription\nA predefined, read-only special register that returns the thread’s lane within the warp. The lane\nidentifier ranges from zero to\nWARP_SZ-1\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %laneid;\n10.4.\nSpecial Registers:\n%warpid\n\n%warpid\nWarp identifier.\nSyntax (predefined)\n.sreg .u32 %warpid;\nDescription\nA predefined, read-only special register that returns the thread’s warp identifier. The warp\nidentifier provides a unique warp number within a CTA but not across CTAs within a grid. The warp\nidentifier will be the same for all threads within a single warp.\nNote that\n%warpid\nreturns the location of a thread at the moment when read, but\nits value may change during execution, e.g., due to rescheduling of threads following\npreemption. For this reason,\n%ctaid\nand\n%tid\nshould be used to compute a virtual warp index\nif such a value is needed in kernel code;\n%warpid\nis intended mainly to enable profiling and\ndiagnostic code to sample and log information such as work place mapping and load distribution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %warpid;\n10.5.\nSpecial Registers:\n%nwarpid\n\n%nwarpid\nNumber of warp identifiers.\nSyntax (predefined)\n.sreg .u32 %nwarpid;\nDescription\nA predefined, read-only special register that returns the maximum number of warp identifiers.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%nwarpid\nrequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %nwarpid;\n10.6.\nSpecial Registers:\n%ctaid\n\n%ctaid\nCTA identifier within a grid.\nSyntax (predefined)\n.sreg .v4 .u32 %ctaid;                      // CTA id vector\n.sreg .u32 %ctaid.x, %ctaid.y, %ctaid.z;    // CTA id components\nDescription\nA predefined, read-only special register initialized with the CTA identifier within the CTA\ngrid. The\n%ctaid\nspecial register contains a 1D, 2D, or 3D vector, depending on the shape and\nrank of the CTA grid. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %ctaid.x <  %nctaid.x\n0  <=  %ctaid.y <  %nctaid.y\n0  <=  %ctaid.z <  %nctaid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%ctaid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r0,%ctaid.x;\nmov.u16  %rh,%ctaid.y;   // legacy code\n10.7.\nSpecial Registers:\n%nctaid\n\n%nctaid\nNumber of CTA ids per grid.\nSyntax (predefined)\n.sreg .v4 .u32 %nctaid                      // Grid shape vector\n.sreg .u32 %nctaid.x,%nctaid.y,%nctaid.z;   // Grid dimensions\nDescription\nA predefined, read-only special register initialized with the number of CTAs in each grid\ndimension. The\n%nctaid\nspecial register contains a 3D grid shape vector, with each element\nhaving a value of at least\n1\n. The fourth element is unused and always returns zero.\nMaximum values of %nctaid.{x,y,z} are as follows:\n.target architecture\n%nctaid.x\n%nctaid.y\n%nctaid.z\nsm_1x\n,\nsm_20\n65535\n65535\n65535\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n,\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n2\n31\n-1\n65535\n65535\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%nctaid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r0,%nctaid.x;\nmov.u16  %rh,%nctaid.x;     // legacy code\n10.8.\nSpecial Registers:\n%smid\n\n%smid\nSM identifier.\nSyntax (predefined)\n.sreg .u32 %smid;\nDescription\nA predefined, read-only special register that returns the processor (SM) identifier on which a\nparticular thread is executing. The SM identifier ranges from\n0\nto\n%nsmid-1\n. The SM\nidentifier numbering is not guaranteed to be contiguous.\nNotes\nNote that\n%smid\nreturns the location of a thread at the moment when read, but\nits value may change during execution, e.g. due to rescheduling of threads following\npreemption.\n%smid\nis intended mainly to enable profiling and diagnostic code to sample and log\ninformation such as work place mapping and load distribution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %smid;\n10.9.\nSpecial Registers:\n%nsmid\n\n%nsmid\nNumber of SM identifiers.\nSyntax (predefined)\n.sreg .u32 %nsmid;\nDescription\nA predefined, read-only special register that returns the maximum number of SM identifiers. The SM\nidentifier numbering is not guaranteed to be contiguous, so\n%nsmid\nmay be larger than the\nphysical number of SMs in the device.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%nsmid\nrequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %nsmid;\n10.10.\nSpecial Registers:\n%gridid\n\n%gridid\nGrid identifier.\nSyntax (predefined)\n.sreg .u64 %gridid;\nDescription\nA predefined, read-only special register initialized with the per-grid temporal grid identifier. The\n%gridid\nis used by debuggers to distinguish CTAs and clusters within concurrent (small) grids.\nDuring execution, repeated launches of programs may occur, where each launch starts a\ngrid-of-CTAs. This variable provides the temporal grid launch number for this context.\nFor\nsm_1x\ntargets,\n%gridid\nis limited to the range [0..2\n16\n-1]. For\nsm_20\n,\n%gridid\nis limited to the range [0..2\n32\n-1].\nsm_30\nsupports the entire 64-bit range.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 as type\n.u16\n.\nRedefined as type\n.u32\nin PTX ISA version 1.3.\nRedefined as type\n.u64\nin PTX ISA version 3.0.\nFor compatibility with legacy PTX code, 16-bit and 32-bit\nmov\nand\ncvt\ninstructions may be\nused to read the lower 16-bits or 32-bits of each component of\n%gridid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u64  %s, %gridid;  // 64-bit read of %gridid\nmov.u32  %r, %gridid;  // legacy code with 32-bit %gridid\n10.11.\nSpecial Registers:\n%is_explicit_cluster\n\n%is_explicit_cluster\nChecks if user has explicitly specified cluster launch.\nSyntax (predefined)\n.sreg .pred %is_explicit_cluster;\nDescription\nA predefined, read-only special register initialized with the predicate value of whether the cluster\nlaunch is explicitly specified by user.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .pred p;\n\nmov.pred  p, %is_explicit_cluster;\n10.12.\nSpecial Registers:\n%clusterid\n\n%clusterid\nCluster identifier within a grid.\nSyntax (predefined)\n.sreg .v4 .u32 %clusterid;\n.sreg .u32 %clusterid.x, %clusterid.y, %clusterid.z;\nDescription\nA predefined, read-only special register initialized with the cluster identifier in a grid in each\ndimension. Each cluster in a grid has a unique identifier.\nThe\n%clusterid\nspecial register contains a 1D, 2D, or 3D vector, depending upon the shape and\nrank of the cluster. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %clusterid.x <  %nclusterid.x\n0  <=  %clusterid.y <  %nclusterid.y\n0  <=  %clusterid.z <  %nclusterid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %clusterid.x;\nmov.u32     %r1, %clusterid.z;\nmov.v4.u32  %rx, %clusterid;\n10.13.\nSpecial Registers:\n%nclusterid\n\n%nclusterid\nNumber of cluster identifiers per grid.\nSyntax (predefined)\n.sreg .v4 .u32 %nclusterid;\n.sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z;\nDescription\nA predefined, read-only special register initialized with the number of clusters in each grid\ndimension.\nThe\n%nclusterid\nspecial register contains a 3D grid shape vector that holds the grid dimensions\nin terms of clusters. The fourth element is unused and always returns zero.\nRefer to the\nCuda Programming Guide\nfor details on the maximum values of\n%nclusterid.{x,y,z}\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %nclusterid.x;\nmov.u32     %r1, %nclusterid.z;\nmov.v4.u32  %rx, %nclusterid;\n10.14.\nSpecial Registers:\n%cluster_ctaid\n\n%cluster_ctaid\nCTA identifier within a cluster.\nSyntax (predefined)\n.sreg .v4 .u32 %cluster_ctaid;\n.sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z;\nDescription\nA predefined, read-only special register initialized with the CTA identifier in a cluster in each\ndimension. Each CTA in a cluster has a unique CTA identifier.\nThe\n%cluster_ctaid\nspecial register contains a 1D, 2D, or 3D vector, depending upon the shape of\nthe cluster. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %cluster_ctaid.x <  %cluster_nctaid.x\n0  <=  %cluster_ctaid.y <  %cluster_nctaid.y\n0  <=  %cluster_ctaid.z <  %cluster_nctaid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_ctaid.x;\nmov.u32     %r1, %cluster_ctaid.z;\nmov.v4.u32  %rx, %cluster_ctaid;\n10.15.\nSpecial Registers:\n%cluster_nctaid\n\n%cluster_nctaid\nNumber of CTA identifiers per cluster.\nSyntax (predefined)\n.sreg .v4 .u32 %cluster_nctaid;\n.sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z;\nDescription\nA predefined, read-only special register initialized with the number of CTAs in a cluster in each\ndimension.\nThe\n%cluster_nctaid\nspecial register contains a 3D grid shape vector that holds the cluster\ndimensions in terms of CTAs. The fourth element is unused and always returns zero.\nRefer to the\nCuda Programming Guide\nfor details on the maximum values of\n%cluster_nctaid.{x,y,z}\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_nctaid.x;\nmov.u32     %r1, %cluster_nctaid.z;\nmov.v4.u32  %rx, %cluster_nctaid;\n10.16.\nSpecial Registers:\n%cluster_ctarank\n\n%cluster_ctarank\nCTA identifier in a cluster across all dimensions.\nSyntax (predefined)\n.sreg .u32 %cluster_ctarank;\nDescription\nA predefined, read-only special register initialized with the CTA rank within a cluster across all\ndimensions.\nIt is guaranteed that:\n0  <=  %cluster_ctarank <  %cluster_nctarank\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r;\n\nmov.u32  %r, %cluster_ctarank;\n10.17.\nSpecial Registers:\n%cluster_nctarank\n\n%cluster_nctarank\nNumber of CTA identifiers in a cluster across all dimensions.\nSyntax (predefined)\n.sreg .u32 %cluster_nctarank;\nDescription\nA predefined, read-only special register initialized with the nunber of CTAs within a cluster across\nall dimensions.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r;\n\nmov.u32  %r, %cluster_nctarank;\n10.18.\nSpecial Registers:\n%lanemask_eq\n\n%lanemask_eq\n32-bit mask with bit set in position equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_eq;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with a bit set in the\nposition equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_eq\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_eq;\n10.19.\nSpecial Registers:\n%lanemask_le\n\n%lanemask_le\n32-bit mask with bits set in positions less than or equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_le;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than or equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_le\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_le\n10.20.\nSpecial Registers:\n%lanemask_lt\n\n%lanemask_lt\n32-bit mask with bits set in positions less than the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_lt;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_lt\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_lt;\n10.21.\nSpecial Registers:\n%lanemask_ge\n\n%lanemask_ge\n32-bit mask with bits set in positions greater than or equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_ge;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than or equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_ge\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_ge;\n10.22.\nSpecial Registers:\n%lanemask_gt\n\n%lanemask_gt\n32-bit mask with bits set in positions greater than the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_gt;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_gt\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_gt;\n10.23.\nSpecial Registers:\n%clock\n,\n%clock_hi\n\n%clock\n,\n%clock_hi\n%clock\nA predefined, read-only 32-bit unsigned cycle counter.\n%clock_hi\nThe upper 32-bits of\n%clock64\nspecial register.\nSyntax (predefined)\n.sreg .u32 %clock;\n.sreg .u32 %clock_hi;\nDescription\nSpecial register\n%clock\nand\n%clock_hi\nare unsigned 32-bit read-only cycle counters that wrap\nsilently.\nPTX ISA Notes\n%clock\nintroduced in PTX ISA version 1.0.\n%clock_hi\nintroduced in PTX ISA version 5.0.\nTarget ISA Notes\n%clock\nsupported on all target architectures.\n%clock_hi\nrequires\nsm_20\nor higher.\nExamples\nmov.u32 r1,%clock;\nmov.u32 r2, %clock_hi;\n10.24.\nSpecial Registers:\n%clock64\n\n%clock64\nA predefined, read-only 64-bit unsigned cycle counter.\nSyntax (predefined)\n.sreg .u64 %clock64;\nDescription\nSpecial register\n%clock64\nis an unsigned 64-bit read-only cycle counter that wraps silently.\nNotes\nThe lower 32-bits of\n%clock64\nare identical to\n%clock\n.\nThe upper 32-bits of\n%clock64\nare identical to\n%clock_hi\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%clock64\nrequires\nsm_20\nor higher.\nExamples\nmov.u64  r1,%clock64;\n10.25.\nSpecial Registers:\n%pm0\n…\n%pm7\n\n%pm0\n…\n%pm7\nPerformance monitoring counters.\nSyntax (predefined)\n.sreg .u32 %pm<8>;\nDescription\nSpecial registers\n%pm0\n…\n%pm7\nare unsigned 32-bit read-only performance monitor counters. Their\nbehavior is currently undefined.\nPTX ISA Notes\n%pm0\n…\n%pm3\nintroduced in PTX ISA version 1.3.\n%pm4\n…\n%pm7\nintroduced in PTX ISA version 3.0.\nTarget ISA Notes\n%pm0\n…\n%pm3\nsupported on all target architectures.\n%pm4\n…\n%pm7\nrequire\nsm_20\nor higher.\nExamples\nmov.u32  r1,%pm0;\nmov.u32  r1,%pm7;\n10.26.\nSpecial Registers:\n%pm0_64\n…\n%pm7_64\n\n%pm0_64\n…\n%pm7_64\n64 bit Performance monitoring counters.\nSyntax (predefined)\n.sreg .u64 %pm0_64;\n.sreg .u64 %pm1_64;\n.sreg .u64 %pm2_64;\n.sreg .u64 %pm3_64;\n.sreg .u64 %pm4_64;\n.sreg .u64 %pm5_64;\n.sreg .u64 %pm6_64;\n.sreg .u64 %pm7_64;\nDescription\nSpecial registers\n%pm0_64\n…\n%pm7_64\nare unsigned 64-bit read-only performance monitor\ncounters. Their behavior is currently undefined.\nNotes\nThe lower 32bits of\n%pm0_64\n…\n%pm7_64\nare identical to\n%pm0\n…\n%pm7\n.\nPTX ISA Notes\n%pm0_64\n…\n%pm7_64\nintroduced in PTX ISA version 4.0.\nTarget ISA Notes\n%pm0_64\n…\n%pm7_64\nrequire\nsm_50\nor higher.\nExamples\nmov.u32  r1,%pm0_64;\nmov.u32  r1,%pm7_64;\n10.27.\nSpecial Registers:\n%envreg<32>\n\n%envreg<32>\nDriver-defined read-only registers.\nSyntax (predefined)\n.sreg .b32 %envreg<32>;\nDescription\nA set of 32 pre-defined read-only registers used to capture execution environment of PTX program\noutside of PTX virtual machine. These registers are initialized by the driver prior to kernel launch\nand can contain cta-wide or grid-wide values.\nPrecise semantics of these registers is defined in the driver documentation.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.b32      %r1,%envreg0;  // move envreg0 to %r1\n10.28.\nSpecial Registers:\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n%globaltimer\nA predefined, 64-bit global nanosecond timer.\n%globaltimer_lo\nThe lower 32-bits of %globaltimer.\n%globaltimer_hi\nThe upper 32-bits of %globaltimer.\nSyntax (predefined)\n.sreg .u64 %globaltimer;\n.sreg .u32 %globaltimer_lo, %globaltimer_hi;\nDescription\nSpecial registers intended for use by NVIDIA tools. The behavior is target-specific and may change\nor be removed in future GPUs. When JIT-compiled to other targets, the value of these registers is\nunspecified.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nRequires target\nsm_30\nor higher.\nExamples\nmov.u64  r1,%globaltimer;\n10.29.\nSpecial Registers:\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n%reserved_smem_offset_begin\nStart of the reserved shared memory region.\n%reserved_smem_offset_end\nEnd of the reserved shared memory region.\n%reserved_smem_offset_cap\nTotal size of the reserved shared memory region.\n%reserved_smem_offset_<2>\nOffsets in the reserved shared memory region.\nSyntax (predefined)\n.sreg .b32 %reserved_smem_offset_begin;\n.sreg .b32 %reserved_smem_offset_end;\n.sreg .b32 %reserved_smem_offset_cap;\n.sreg .b32 %reserved_smem_offset_<2>;\nDescription\nThese are predefined, read-only special registers containing information about the shared memory\nregion which is reserved for the NVIDIA system software use. This region of shared memory is not\navailable to users, and accessing this region from user code results in undefined behavior. Refer to\nCUDA Programming Guide\nfor details.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nRequire\nsm_80\nor higher.\nExamples\n.reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1;\n\nmov.b32 %reg_begin,   %reserved_smem_offset_begin;\nmov.b32 %reg_end,     %reserved_smem_offset_end;\nmov.b32 %reg_cap,     %reserved_smem_offset_cap;\nmov.b32 %reg_offset0, %reserved_smem_offset_0;\nmov.b32 %reg_offset1, %reserved_smem_offset_1;\n10.30.\nSpecial Registers:\n%total_smem_size\n\n%total_smem_size\nTotal size of shared memory used by a CTA of a kernel.\nSyntax (predefined)\n.sreg .u32 %total_smem_size;\nDescription\nA predefined, read-only special register initialized with total size of shared memory allocated\n(statically and dynamically, excluding the shared memory reserved for the NVIDIA system software\nuse) for the CTA of a kernel at launch time.\nSize is returned in multiples of shared memory allocation unit size supported by target\narchitecture.\nAllocation unit values are as follows:\nTarget architecture\nShared memory allocation unit size\nsm_2x\n128 bytes\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n256 bytes\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n128 bytes\nPTX ISA Notes\nIntroduced in PTX ISA version 4.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %total_smem_size;\n10.31.\nSpecial Registers:\n%aggr_smem_size\n\n%aggr_smem_size\nTotal size of shared memory used by a CTA of a kernel.\nSyntax (predefined)\n.sreg .u32 %aggr_smem_size;\nDescription\nA predefined, read-only special register initialized with total aggregated size of shared memory\nconsisting of the size of user shared memory allocated (statically and dynamically) at launch time\nand the size of shared memory region which is reserved for the NVIDIA system software use.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmov.u32  %r, %aggr_smem_size;\n10.32.\nSpecial Registers:\n%dynamic_smem_size\n\n%dynamic_smem_size\nSize of shared memory allocated dynamically at kernel launch.\nSyntax (predefined)\n.sreg .u32 %dynamic_smem_size;\nDescription\nSize of shared memory allocated dynamically at kernel launch.\nA predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %dynamic_smem_size;\n10.33.\nSpecial Registers:\n%current_graph_exec\n\n%current_graph_exec\nAn Identifier for currently executing CUDA device graph.\nSyntax (predefined)\n.sreg .u64 %current_graph_exec;\nDescription\nA predefined, read-only special register initialized with the identifier referring to the CUDA\ndevice graph being currently executed. This register is 0 if the executing kernel is not part of a\nCUDA device graph.\nRefer to the\nCUDA Programming Guide\nfor more details on CUDA device graphs.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nExamples\nmov.u64  r1, %current_graph_exec;\n11.\nDirectives\n\n11.1.\nPTX Module Directives\n\nThe following directives declare the PTX ISA version of the code in the module, the target\narchitecture for which the code was generated, and the size of addresses within the PTX module.\n.version\n.target\n.address_size\n11.1.1.\nPTX Module Directives:\n.version\n\n.version\nPTX ISA version number.\nSyntax\n.version  major.minor    // major, minor are integers\nDescription\nSpecifies the PTX language version number.\nThe\nmajor\nnumber is incremented when there are incompatible changes to the PTX language, such as\nchanges to the syntax or semantics. The version major number is used by the PTX compiler to ensure\ncorrect execution of legacy PTX code.\nThe\nminor\nnumber is incremented when new features are added to PTX.\nSemantics\nIndicates that this module must be compiled with tools that support an equal or greater version\nnumber.\nEach PTX module must begin with a\n.version\ndirective, and no other\n.version\ndirective is\nallowed anywhere else within the module.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.version 3.1\n.version 3.0\n.version 2.3\n11.1.2.\nPTX Module Directives:\n.target\n\n.target\nArchitecture and Platform target.\nSyntax\n.target stringlist         // comma separated list of target specifiers\nstring = { sm_120a, sm_120f, sm_120,          // sm_12x target architectures\n           sm_121a, sm_121f, sm_121,          // sm_12x target architectures\n           sm_110a, sm_110f, sm_110,          // sm_11x target architectures\n           sm_100a, sm_100f, sm_100,          // sm_10x target architectures\n           sm_101a, sm_101f, sm_101,          // sm_10x target architectures\n           sm_103a, sm_103f, sm_103           // sm_10x target architectures\n           sm_90a, sm_90,                     // sm_9x target architectures\n           sm_80, sm_86, sm_87, sm_88, sm_89, // sm_8x target architectures\n           sm_70, sm_72, sm_75,               // sm_7x target architectures\n           sm_60, sm_61, sm_62,               // sm_6x target architectures\n           sm_50, sm_52, sm_53,               // sm_5x target architectures\n           sm_30, sm_32, sm_35, sm_37,        // sm_3x target architectures\n           sm_20,                             // sm_2x target architectures\n           sm_10, sm_11, sm_12, sm_13,        // sm_1x target architectures\n           texmode_unified, texmode_independent,   // texturing mode\n           debug,                                  // platform option\n           map_f64_to_f32 };                       // platform option\nDescription\nSpecifies the set of features in the target architecture for which the current PTX code was\ngenerated. In general, generations of SM architectures follow an\nonion layer\nmodel, where each\ngeneration adds new features and retains all features of previous generations. The onion layer model\nallows the PTX code generated for a given target to be run on later generation devices.\nTarget architectures with suffix “\na\n”, such as\nsm_90a\n, include architecture-specific\nfeatures that are supported on the specified architecture only, hence such targets do not follow the\nonion layer model. Therefore, PTX code generated for such targets cannot be run on later generation\ndevices. Architecture-specific features can only be used with targets that support these\nfeatures.\nTarget architectures with suffix “\nf\n”, such as\nsm_100f\n, include family-specific features that\nare supported only within the same architecture family. Therefore, PTX code generated for such\ntargets can run only on later generation devices in the same family. Family-specific features can be\nused with f-targets as well as a-targets of later generation devices in the same family.\nTable 58\ndefines the architecture families.\nTable 58\nArchitecture Families\n\nFamily\nTarget SM architectures included\nsm_10x family\nsm_100f, sm_103f, future targets\nin sm_10x family\nsm_11x family\nsm_110f, sm_101f, future targets\nin sm_11x family\nsm_12x family\nsm_120f, sm_121f, future targets\nin sm_12x family\nSemantics\nEach PTX module must begin with a\n.version\ndirective, immediately followed by a\n.target\ndirective containing a target architecture and optional platform options. A\n.target\ndirective\nspecifies a single target architecture, but subsequent\n.target\ndirectives can be used to change\nthe set of target features allowed during parsing. A program with multiple\n.target\ndirectives\nwill compile and run only on devices that support all features of the highest-numbered architecture\nlisted in the program.\nPTX features are checked against the specified target architecture, and an error is generated if an\nunsupported feature is used. The following table summarizes the features in PTX that vary according\nto target architecture.\nTarget\nDescription\nsm_120\nBaseline feature set for\nsm_120\narchitecture.\nsm_120f\nAdds support for\nsm_120f\nfamily specific features.\nsm_120a\nAdds support for\nsm_120a\narchitecture-specific features.\nsm_121\nBaseline feature set for\nsm_121\narchitecture.\nsm_121f\nAdds support for\nsm_121f\nfamily specific features.\nsm_121a\nAdds support for\nsm_121a\narchitecture-specific features.\nTarget\nDescription\nsm_110\nBaseline feature set for\nsm_110\narchitecture.\nsm_110f\nAdds support for\nsm_110f\nfamily specific features.\nsm_110a\nAdds support for\nsm_110a\narchitecture-specific features.\nTarget\nDescription\nsm_100\nBaseline feature set for\nsm_100\narchitecture.\nsm_100f\nAdds support for\nsm_100f\nfamily specific features.\nsm_100a\nAdds support for\nsm_100a\narchitecture-specific features.\nsm_101\nBaseline feature set for\nsm_101\narchitecture. (Renamed to\nsm_110\n)\nsm_101f\nAdds support for\nsm_101f\nfamily specific features. (Renamed to\nsm_110f\n)\nsm_101a\nAdds support for\nsm_101a\narchitecture-specific features. (Renamed to\nsm_110a\n)\nsm_103\nBaseline feature set for\nsm_103\narchitecture.\nsm_103f\nAdds support for\nsm_103f\nfamily specific features.\nsm_103a\nAdds support for\nsm_103a\narchitecture-specific features.\nTarget\nDescription\nsm_90\nBaseline feature set for\nsm_90\narchitecture.\nsm_90a\nAdds support for\nsm_90a\narchitecture-specific features.\nTarget\nDescription\nsm_80\nBaseline feature set for\nsm_80\narchitecture.\nsm_86\nAdds support for\n.xorsign\nmodifier on\nmin\nand\nmax\ninstructions.\nsm_87\nBaseline feature set for\nsm_87\narchitecture.\nsm_88\nBaseline feature set for\nsm_88\narchitecture.\nsm_89\nBaseline feature set for\nsm_89\narchitecture.\nTarget\nDescription\nsm_70\nBaseline feature set for\nsm_70\narchitecture.\nsm_72\nAdds support for integer multiplicand and accumulator matrices in\nwmma\ninstructions.\nAdds support for\ncvt.pack\ninstruction.\nsm_75\nAdds support for sub-byte integer and single-bit multiplicant matrices in\nwmma\ninstructions.\nAdds support for\nldmatrix\ninstruction.\nAdds support for\nmovmatrix\ninstruction.\nAdds support for\ntanh\ninstruction.\nTarget\nDescription\nsm_60\nBaseline feature set for\nsm_60\narchitecture.\nsm_61\nAdds support for\ndp2a\nand\ndp4a\ninstructions.\nsm_62\nBaseline feature set for\nsm_61\narchitecture.\nTarget\nDescription\nsm_50\nBaseline feature set for\nsm_50\narchitecture.\nsm_52\nBaseline feature set for\nsm_50\narchitecture.\nsm_53\nAdds support for arithmetic, comparsion and texture instructions for\n.f16\nand\n.f16x2\ntypes.\nTarget\nDescription\nsm_30\nBaseline feature set for\nsm_30\narchitecture.\nsm_32\nAdds 64-bit\n{atom,red}.{and,or,xor,min,max}\ninstructions.\nAdds\nshf\ninstruction.\nAdds\nld.global.nc\ninstruction.\nsm_35\nAdds support for CUDA Dynamic Parallelism.\nsm_37\nBaseline feature set for\nsm_35\narchitecture.\nTarget\nDescription\nsm_20\nBaseline feature set for\nsm_20\narchitecture.\nTarget\nDescription\nsm_10\nBaseline feature set for\nsm_10\narchitecture.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_11\nAdds 64-bit\n{atom,red}.{and,or,xor,min,max}\ninstructions.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_12\nAdds\n{atom,red}.shared\n, 64-bit\n{atom,red}.global\n,\nvote\ninstructions.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_13\nAdds double-precision support, including expanded rounding modifiers.\nDisallows use of\nmap_f64_to_f32\n.\nThe texturing mode is specified for an entire module and cannot be changed within the module.\nThe\n.target\ndebug option declares that the PTX file contains DWARF debug information, and\nsubsequent compilation of PTX will retain information needed for source-level debugging. If the\ndebug option is declared, an error message is generated if no DWARF information is found in the\nfile. The debug option requires PTX ISA version 3.0 or later.\nmap_f64_to_f32\nindicates that all double-precision instructions map to single-precision\nregardless of the target architecture. This enables high-level language compilers to compile\nprograms containing type double to target device that do not support double-precision\noperations. Note that\n.f64\nstorage remains as 64-bits, with only half being used by instructions\nconverted from\n.f64\nto\n.f32\n.\nNotes\nTargets of the form\ncompute_xx\nare also accepted as synonyms for\nsm_xx\ntargets.\nTargets\nsm_{101,101f,101a}\nare renamed to targets\nsm_{110,110f,110a}\nfrom PTX ISA version 9.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget strings\nsm_10\nand\nsm_11\nintroduced in PTX ISA version 1.0.\nTarget strings\nsm_12\nand\nsm_13\nintroduced in PTX ISA version 1.2.\nTexturing mode introduced in PTX ISA version 1.5.\nTarget string\nsm_20\nintroduced in PTX ISA version 2.0.\nTarget string\nsm_30\nintroduced in PTX ISA version 3.0.\nPlatform option\ndebug\nintroduced in PTX ISA version 3.0.\nTarget string\nsm_35\nintroduced in PTX ISA version 3.1.\nTarget strings\nsm_32\nand\nsm_50\nintroduced in PTX ISA version 4.0.\nTarget strings\nsm_37\nand\nsm_52\nintroduced in PTX ISA version 4.1.\nTarget string\nsm_53\nintroduced in PTX ISA version 4.2.\nTarget string\nsm_60\n,\nsm_61\n,\nsm_62\nintroduced in PTX ISA version 5.0.\nTarget string\nsm_70\nintroduced in PTX ISA version 6.0.\nTarget string\nsm_72\nintroduced in PTX ISA version 6.1.\nTarget string\nsm_75\nintroduced in PTX ISA version 6.3.\nTarget string\nsm_80\nintroduced in PTX ISA version 7.0.\nTarget string\nsm_86\nintroduced in PTX ISA version 7.1.\nTarget string\nsm_87\nintroduced in PTX ISA version 7.4.\nTarget string\nsm_88\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_89\nintroduced in PTX ISA version 7.8.\nTarget string\nsm_90\nintroduced in PTX ISA version 7.8.\nTarget string\nsm_90a\nintroduced in PTX ISA version 8.0.\nTarget string\nsm_100\nintroduced in PTX ISA version 8.6.\nTarget string\nsm_100f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_100a\nintroduced in PTX ISA version 8.6.\nTarget string\nsm_101\nintroduced in PTX ISA version 8.6. (Renamed to\nsm_110\n)\nTarget string\nsm_101f\nintroduced in PTX ISA version 8.8. (Renamed to\nsm_110f\n)\nTarget string\nsm_101a\nintroduced in PTX ISA version 8.6. (Renamed to\nsm_110a\n)\nTarget string\nsm_103\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_103f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_103a\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_110\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_110f\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_110a\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_120\nintroduced in PTX ISA version 8.7.\nTarget string\nsm_120f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_120a\nintroduced in PTX ISA version 8.7.\nTarget string\nsm_121\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_121f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_121a\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nThe\n.target\ndirective is supported on all target architectures.\nExamples\n.target sm_10       // baseline target architecture\n.target sm_13       // supports double-precision\n.target sm_20, texmode_independent\n.target sm_90       // baseline target architecture\n.target sm_90a      // PTX using architecture-specific features\n.target sm_100f     // PTX using family-specific features\n11.1.3.\nPTX Module Directives:\n.address_size\n\n.address_size\nAddress size used throughout PTX module.\nSyntax\n.address_size  address-size\naddress-size = { 32, 64 };\nDescription\nSpecifies the address size assumed throughout the module by the PTX code and the binary DWARF\ninformation in PTX.\nRedefinition of this directive within a module is not allowed. In the presence of separate\ncompilation all modules must specify (or default to) the same address size.\nThe\n.address_size\ndirective is optional, but it must immediately follow the\n.target\ndirective if present within a module.\nSemantics\nIf the\n.address_size\ndirective is omitted, the address size defaults to 32.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n// example directives\n   .address_size 32       // addresses are 32 bit\n   .address_size 64       // addresses are 64 bit\n\n// example of directive placement within a module\n   .version 2.3\n   .target sm_20\n   .address_size 64\n...\n.entry foo () {\n...\n}\n11.2.\nSpecifying Kernel Entry Points and Functions\n\nThe following directives specify kernel entry points and functions.\n.entry\n.func\n11.2.1.\nKernel and Function Directives:\n.entry\n\n.entry\nKernel entry point and body, with optional parameters.\nSyntax\n.entry kernel-name ( param-list )  kernel-body\n.entry kernel-name  kernel-body\nDescription\nDefines a kernel entry point name, parameters, and body for the kernel function.\nParameters are passed via\n.param\nspace memory and are listed within an optional parenthesized\nparameter list. Parameters may be referenced by name within the kernel body and loaded into\nregisters using\nld.param{::entry}\ninstructions.\nIn addition to normal parameters, opaque\n.texref\n,\n.samplerref\n, and\n.surfref\nvariables\nmay be passed as parameters. These parameters can only be referenced by name within texture and\nsurface load, store, and query instructions and cannot be accessed via\nld.param\ninstructions.\nThe shape and size of the CTA executing the kernel are available in special registers.\nSemantics\nSpecify the entry point for a kernel program.\nAt kernel launch, the kernel dimensions and properties are established and made available via\nspecial registers, e.g.,\n%ntid\n,\n%nctaid\n, etc.\nPTX ISA Notes\nFor PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter\nlist. For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body.\nThe maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764\nbytes. Depending upon the PTX ISA version, the parameter size limit varies. The following table\nshows the allowed parameter size for a PTX ISA version:\nPTX ISA Version\nMaximum parameter size (In bytes)\nPTX ISA version 8.1 and above\n32764\nPTX ISA version 1.5 and above\n4352\nPTX ISA version 1.4 and above\n256\nThe CUDA and OpenCL drivers support the following limits for parameter memory:\nDriver\nParameter memory size\nCUDA\n256 bytes for\nsm_1x\n, 4096 bytes for\nsm_2x\nand\nhigher\n,\n32764 bytes fo\nsm_70\nand higher\nOpenCL\n32764 bytes for\nsm_70\nand higher, 4352 bytes on\nsm_6x\nand lower\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry cta_fft\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r<99>;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n\n.entry prefix_sum ( .param .align 4 .s32 pitch[8000] )\n{\n    .reg .s32 %t;\n    ld.param::entry.s32  %t, [pitch];\n    ...\n}\n11.2.2.\nKernel and Function Directives:\n.func\n\n.func\nFunction definition.\nSyntax\n.func {.attribute(attr-list)} fname {.noreturn} {.abi_preserve N} {.abi_preserve_control N} function-body\n.func {.attribute(attr-list)} fname (param-list) {.noreturn} {.abi_preserve N} {.abi_preserve_control N} function-body\n.func {.attribute(attr-list)} (ret-param) fname (param-list) {.abi_preserve N} {.abi_preserve_control N} function-body\nDescription\nDefines a function, including input and return parameters and optional function body.\nAn optional\n.noreturn\ndirective indicates that the function does not return to the caller\nfunction.\n.noreturn\ndirective cannot be specified on functions which have return parameters. See\nthe description of\n.noreturn\ndirective in\nPerformance-Tuning Directives: .noreturn\n.\nAn optional\n.attribute\ndirective specifies additional information associated with the\nfunction. See the description of\nVariable and Function Attribute Directive: .attribute\nfor allowed attributes.\nOptional\n.abi_preserve\nand\n.abi_preserve_control\ndirectives are used to specify the number\nof general purpose registers and control registers. See description of\nPerformance-Tuning Directives: .abi_preserve\nand\nPerformance-Tuning Directives: .abi_preserve_control\nfor more details.\nDirectives, if any specified, for a function, e.g.\n.noreturn\n, must be specified consistently\nbetween function declaration and definition.\nA\n.func\ndefinition with no body provides a function prototype.\nThe parameter lists define locally-scoped variables in the function body. Parameters must be base\ntypes in either the register or parameter state space. Parameters in register state space may be\nreferenced directly within instructions in the function body. Parameters in\n.param\nspace are\naccessed using\nld.param{::func}\nand\nst.param{::func}\ninstructions in the body. Parameter\npassing is call-by-value.\nThe last parameter in the parameter list may be a\n.param\narray of type\n.b8\nwith no size\nspecified. It is used to pass an arbitrary number of parameters to the function packed into a single\narray object.\nWhen calling a function with such an unsized last argument, the last argument may be omitted from\nthe\ncall\ninstruction if no parameter is passed through it. Accesses to this array parameter must\nbe within the bounds of the array. The result of an access is undefined if no array was passed, or\nif the access was outside the bounds of the actual array being passed.\nSemantics\nThe PTX syntax hides all details of the underlying calling convention and ABI.\nThe implementation of parameter passing is left to the optimizing translator, which may use a\ncombination of registers and stack locations to pass parameters.\nRelease Notes\nFor PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and\nrecursion is illegal.\nPTX ISA versions 2.0 and later with target\nsm_20\nor higher allow parameters in the\n.param\nstate space, implements an ABI with stack, and supports recursion.\nPTX ISA versions 2.0 and later with target\nsm_20\nor higher support at most one return value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nSupport for unsized array parameter introduced in PTX ISA version 6.0.\nSupport for\n.noreturn\ndirective introduced in PTX ISA version 6.4.\nSupport for\n.attribute\ndirective introduced in PTX ISA version 8.0.\nSupport for\n.abi_preserve\nand\n.abi_preserve_control\ndirectives introduced in PTX ISA version 9.0.\nTarget ISA Notes\nFunctions without unsized array parameter supported on all target architectures.\nUnsized array parameter requires\nsm_30\nor higher.\n.noreturn\ndirective requires\nsm_30\nor higher.\n.attribute\ndirective requires\nsm_90\nor higher.\n.abi_preserve\nand\n.abi_preserve_control\ndirectives require\nsm_80\nor higher.\nExamples\n.func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl)\n{\n.reg .b32 localVar;\n\n... use N, dbl;\nother code;\n\nmov.b32 rval,result;\nret;\n}\n\n...\ncall (fooval), foo, (val0, val1);  // return value in fooval\n...\n\n.func foo (.reg .b32 N, .reg .f64 dbl) .noreturn\n{\n.reg .b32 localVar;\n... use N, dbl;\nother code;\nmov.b32 rval, result;\nret;\n}\n...\ncall foo, (val0, val1);\n...\n\n.func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[])\n{\n    .reg .b32 input0, input1;\n    ld.param.b32   input0, [numbers + 0];\n    ld.param.b32   input1, [numbers + 4];\n    ...\n    other code;\n    ret;\n}\n...\n\n.param .u32 N;\n.param .align 4 .b8 numbers[8];\nst.param.u32    [N], 2;\nst.param.b32    [numbers + 0], 5;\nst.param.b32    [numbers + 4], 10;\ncall (rval), bar, (N, numbers);\n...\n11.2.3.\nKernel and Function Directives:\n.alias\n\n.alias\nDefine an alias to existing function symbol.\nSyntax\n.alias fAlias, fAliasee;\nDescription\n.alias\nis a module scope directive that defines identifier\nfAlias\nto be an alias to function\nspecified by\nfAliasee\n.\nBoth\nfAlias\nand\nfAliasee\nare non-entry function symbols.\nIdentifier\nfAlias\nis a function declaration without body.\nIdentifier\nfAliasee\nis a function symbol which must be defined in the same module as\n.alias\ndeclaration. Function\nfAliasee\ncannot have\n.weak\nlinkage.\nPrototype of\nfAlias\nand\nfAliasee\nmust match.\nProgram can use either\nfAlias\nor\nfAlisee\nidentifiers to reference function defined with\nfAliasee\n.\nPTX ISA Notes\n.alias\ndirective introduced in PTX ISA 6.3.\nTarget ISA Notes\n.alias\ndirective requires\nsm_30\nor higher.\nExamples\n.visible .func foo(.param .u32 p) {\n   ...\n}\n.visible .func bar(.param .u32 p);\n.alias bar, foo;\n.entry test()\n{\n      .param .u32 p;\n      ...\n      call foo, (p);       // call foo directly\n       ...\n       .param .u32 p;\n       call bar, (p);        // call foo through alias\n}\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r1, %r2, %r3;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n11.3.\nControl Flow Directives\n\nPTX provides directives for specifying potential targets for\nbrx.idx\nand\ncall\ninstructions. See the descriptions of\nbrx.idx\nand\ncall\nfor more information.\n.branchtargets\n.calltargets\n.callprototype\n11.3.1.\nControl Flow Directives:\n.branchtargets\n\n.branchtargets\nDeclare a list of potential branch targets.\nSyntax\nLabel:   .branchtargets  list-of-labels ;\nDescription\nDeclares a list of potential branch targets for a subsequent\nbrx.idx\n, and associates the list\nwith the label at the start of the line.\nAll control flow labels in the list must occur within the same function as the declaration.\nThe list of labels may use the compact, shorthand syntax for enumerating a range of labels having a\ncommon prefix, similar to the syntax described in\nParameterized Variable Names\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\n.function foo () {\n      .reg .u32 %r0;\n      ...\n      L1:\n      ...\n      L2:\n      ...\n      L3:\n      ...\n      ts: .branchtargets L1, L2, L3;\n      @p brx.idx %r0, ts;\n      ...\n\n.function bar() {\n      .reg .u32 %r0;\n      ...\n      N0:\n      ...\n      N1:\n      ...\n      N2:\n      ...\n      N3:\n      ...\n      N4:\n      ...\n      ts: .branchtargets N<5>;\n      @p brx.idx %r0, ts;\n      ...\n11.3.2.\nControl Flow Directives:\n.calltargets\n\n.calltargets\nDeclare a list of potential call targets.\nSyntax\nLabel:   .calltargets  list-of-functions ;\nDescription\nDeclares a list of potential call targets for a subsequent indirect call, and associates the list\nwith the label at the start of the line.\nAll functions named in the list must be declared prior to the\n.calltargets\ndirective, and all\nfunctions must have the same type signature.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ncalltgt:  .calltargets  fastsin, fastcos;\n...\n@p   call  (%f1), %r0, (%x), calltgt;\n...\n11.3.3.\nControl Flow Directives:\n.callprototype\n\n.callprototype\nDeclare a prototype for use in an indirect call.\nSyntax\n// no input or return parameters\nlabel: .callprototype _ .noreturn {.abi_preserve N} {.abi_preserve_control N};\n// input params, no return params\nlabel: .callprototype _ (param-list) .noreturn {.abi_preserve N} {.abi_preserve_control N};\n// no input params, // return params\nlabel: .callprototype (ret-param) _ {.abi_preserve N} {.abi_preserve_control N};\n// input, return parameters\nlabel: .callprototype (ret-param) _ (param-list) {.abi_preserve N} {.abi_preserve_control N};\nDescription\nDefines a prototype with no specific function name, and associates the prototype with a label. The\nprototype may then be used in indirect call instructions where there is incomplete knowledge of the\npossible call targets.\nParameters may have either base types in the register or parameter state spaces, or array types in\nparameter state space. The sink symbol\n'_'\nmay be used to avoid dummy parameter names.\nAn optional\n.noreturn\ndirective indicates that the function does not return to the caller\nfunction.\n.noreturn\ndirective cannot be specified on functions which have return parameters. See\nthe description of .noreturn directive in\nPerformance-Tuning Directives: .noreturn\n.\nOptional\n.abi_preserve\nand\n.abi_preserve_control\ndirectives are used to specify the number\nof general purpose registers and control registers. See description of\nPerformance-Tuning Directives: .abi_preserve\nand\nPerformance-Tuning Directives: .abi_preserve_control\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nSupport for\n.noreturn\ndirective introduced in PTX ISA version 6.4.\nSupport for\n.abi_preserve\nand\n.abi_preserve_control\ndirectives introduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\n.noreturn\ndirective requires\nsm_30\nor higher.\n.abi_preserve\nand\n.abi_preserve_control\ndirectives require\nsm_80\nor higher.\nExamples\nFproto1: .callprototype  _ ;\nFproto2: .callprototype  _ (.param .f32 _);\nFproto3: .callprototype  (.param .u32 _) _ ;\nFproto4: .callprototype  (.param .u32 _) _ (.param .f32 _);\n...\n@p   call  (%val), %r0, (%f1), Fproto4;\n...\n\n// example of array parameter\nFproto5: .callprototype _ (.param .b8 _[12]);\n\nFproto6: .callprototype  _ (.param .f32 _) .noreturn;\n...\n@p   call  %r0, (%f1), Fproto6;\n...\n\n// example of .abi_preserve\nFproto7: .callprototype _ (.param .b32 _) .abi_preserve 10;\n...\n@p   call %r0, (%r1), Fproto7;\n...\n11.4.\nPerformance-Tuning Directives\n\nTo provide a mechanism for low-level performance tuning, PTX supports the following directives,\nwhich pass information to the optimizing backend compiler.\n.maxnreg\n.maxntid\n.reqntid\n.minnctapersm\n.maxnctapersm\n(deprecated)\n.pragma\n.abi_preserve\n.abi_preserve_control\nThe\n.maxnreg\ndirective specifies the maximum number of registers to be allocated to a single\nthread;\nthe\n.maxntid\ndirective specifies the maximum number of threads\nin a thread block (CTA); the\n.reqntid\ndirective specifies the required number of threads in a\nthread block (CTA); and the\n.minnctapersm\ndirective specifies a minimum number of thread blocks\nto be scheduled on a single multiprocessor (SM). These can be used, for example, to throttle the\nresource requirements (e.g., registers) to increase total thread count and provide a greater\nopportunity to hide memory latency. The\n.minnctapersm\ndirective can be used together with either\nthe\n.maxntid\nor\n.reqntid\ndirective to trade-off registers-per-thread against multiprocessor\nutilization without needed to directly specify a maximum number of registers. This may achieve better\nperformance when compiling PTX for multiple devices having different numbers of registers per SM.\nDevice function directives\n.abi_preserve\nand\n.abi_preserve_control\nspecify number of data\nand control registers from callee save registers that a function must preserve for its caller. This\ncan be considered to be the number of general purpose and control registers live in the caller when function\nis called. Control registers refer to the number of divergent program points that happen in the calltree\nleading to current function call.\nCurrently, the\n.maxnreg\n,\n.maxntid\n,\n.reqntid\n, and\n.minnctapersm\ndirectives may be applied per-entry and must appear between an\n.entry\ndirective and its body.\nThe directives take precedence over any module-level constraints passed to the optimizing backend.\nA warning message is generated if the directives’ constraints are inconsistent or cannot be met for\nthe specified target device.\nA general\n.pragma\ndirective is supported for passing information to the PTX backend. The\ndirective passes a list of strings to the backend, and the strings have no semantics within the PTX\nvirtual machine model. The interpretation of\n.pragma\nvalues is determined by the backend\nimplementation and is beyond the scope of the PTX ISA. Note that\n.pragma\ndirectives may appear\nat module (file) scope, at entry-scope, or as statements within a kernel or device function body.\n11.4.1.\nPerformance-Tuning Directives:\n.maxnreg\n\n.maxnreg\nMaximum number of registers that can be allocated per thread.\nSyntax\n.maxnreg n\nDescription\nDeclare the maximum number of registers per thread in a CTA.\nSemantics\nThe compiler guarantees that this limit will not be exceeded. The actual number of registers used\nmay be less; for example, the backend may be able to compile to fewer registers, or the maximum\nnumber of registers may be further constrained by\n.maxntid\nand\n.maxctapersm\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxnreg 16 { ... }  // max regs per thread = 16\n11.4.2.\nPerformance-Tuning Directives:\n.maxntid\n\n.maxntid\nMaximum number of threads in the thread block (CTA).\nSyntax\n.maxntid nx\n.maxntid nx, ny\n.maxntid nx, ny, nz\nDescription\nDeclare the maximum number of threads in the thread block (CTA). This maximum is specified by giving\nthe maximum extent of each dimension of the 1D, 2D, or 3D CTA. The maximum number of threads is the\nproduct of the maximum extent in each dimension.\nSemantics\nThe maximum number of threads in the thread block, computed as the product of the maximum extent\nspecified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in\nwhich this directive appears. Exceeding the maximum number of threads results in a runtime error or\nkernel launch failure.\nNote that this directive guarantees that the\ntotal\nnumber of threads does not exceed the maximum,\nbut does not guarantee that the limit in any particular dimension is not exceeded.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256       { ... }  // max threads = 256\n.entry bar .maxntid 16,16,4   { ... }  // max threads = 1024\n11.4.3.\nPerformance-Tuning Directives:\n.reqntid\n\n.reqntid\nNumber of threads in the thread block (CTA).\nSyntax\n.reqntid nx\n.reqntid nx, ny\n.reqntid nx, ny, nz\nDescription\nDeclare the number of threads in the thread block (CTA) by specifying the extent of each dimension\nof the 1D, 2D, or 3D CTA. The total number of threads is the product of the number of threads in\neach dimension.\nSemantics\nThe size of each CTA dimension specified in any invocation of the kernel is required to be equal to\nthat specified in this directive. Specifying a different CTA dimension at launch will result in a\nruntime error or kernel launch failure.\nNotes\nThe\n.reqntid\ndirective cannot be used in conjunction with the\n.maxntid\ndirective.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .reqntid 256       { ... }  // num threads = 256\n.entry bar .reqntid 16,16,4   { ... }  // num threads = 1024\n11.4.4.\nPerformance-Tuning Directives:\n.minnctapersm\n\n.minnctapersm\nMinimum number of CTAs per SM.\nSyntax\n.minnctapersm ncta\nDescription\nDeclare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor\n(SM).\nNotes\nOptimizations based on\n.minnctapersm\nneed either\n.maxntid\nor\n.reqntid\nto be specified as\nwell.\nIf the total number of threads on a single SM resulting from\n.minnctapersm\nand\n.maxntid\n/\n.reqntid\nexceed maximum number of threads supported by an SM then directive\n.minnctapersm\nwill be ignored.\nIn PTX ISA version 2.1 or higher, a warning is generated if\n.minnctapersm\nis specified without\nspecifying either\n.maxntid\nor\n.reqntid\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0 as a replacement for\n.maxnctapersm\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256 .minnctapersm 4 { ... }\n11.4.5.\nPerformance-Tuning Directives:\n.maxnctapersm\n(deprecated)\n\n.maxnctapersm\nMaximum number of CTAs per SM.\nSyntax\n.maxnctapersm ncta\nDescription\nDeclare the maximum number of CTAs from the kernel’s grid that may be mapped to a single\nmultiprocessor (SM).\nNotes\nOptimizations based on .maxnctapersm generally need\n.maxntid\nto be specified as well. The\noptimizing backend compiler uses\n.maxntid\nand\n.maxnctapersm\nto compute an upper-bound on\nper-thread register usage so that the specified number of CTAs can be mapped to a single\nmultiprocessor. However, if the number of registers used by the backend is sufficiently lower than\nthis bound, additional CTAs may be mapped to a single multiprocessor. For this reason,\n.maxnctapersm\nhas been renamed to .minnctapersm in PTX ISA version 2.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3. Deprecated in PTX ISA version 2.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256 .maxnctapersm 4 { ... }\n11.4.6.\nPerformance-Tuning Directives:\n.noreturn\n\n.noreturn\nIndicate that the function does not return to its caller function.\nSyntax\n.noreturn\nDescription\nIndicate that the function does not return to its caller function.\nSemantics\nAn optional\n.noreturn\ndirective indicates that the function does not return to caller\nfunction.\n.noreturn\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nThe directive cannot be specified on functions which have return parameters.\nIf a function with\n.noreturn\ndirective returns to the caller function at runtime, then the\nbehavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.4.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\n.func foo .noreturn { ... }\n11.4.7.\nPerformance-Tuning Directives:\n.pragma\n\n.pragma\nPass directives to PTX backend compiler.\nSyntax\n.pragma list-of-strings ;\nDescription\nPass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler.\nThe\n.pragma\ndirective may occur at module-scope, at entry-scope, or at statement-level.\nSemantics\nThe interpretation of\n.pragma\ndirective strings is implementation-specific and has no impact on\nPTX semantics. See\nDescriptions of .pragma Strings\nfor\ndescriptions of the pragma strings defined in\nptxas\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.pragma \"nounroll\";    // disable unrolling in backend\n\n// disable unrolling for current kernel\n.entry foo .pragma \"nounroll\"; { ... }\n11.4.8.\nPerformance-Tuning Directives:\n.abi_preserve\n\n.abi_preserve\nSpecify number of general purpose registers that should be preserved by the callers of this function.\nSyntax\n.abi_preserve N\nDescription\nIt is an architecture agnostic value specifying actual number of general purpose registers.\nInternally ABI defines some general purpose registers as preserved (callee save) registers.\nInteger N specifies the actual number of general purpose registers that should be preserved by\nthe function.\n.abi_preserve\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nSemantics\nWhen this directive is specified compiler backend modifies low level ABI components to ensure that\nnumber of live data variables in the callers of this function that are stored in the callee save\nregisters are less than specified value.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.func bar() .abi_preserve 8\n\n// Indirect call via call prototype\n.func (.param .b32 out[30]) foo (.param .b32 in[30]) .abi_preserve 10 { ... }\n...\nmov.b64 lpfoo, foo;\nprot: .callprototype (.param .b32 out[30]) _ (.param .b32 in[30]) .abi_preserve 10;\ncall (out), lpfoo, (in), prot;\n11.4.9.\nPerformance-Tuning Directives:\n.abi_preserve_control\n\n.abi_preserve_control\nSpecify number of control registers that should be preserved by the callers of this function.\nSyntax\n.abi_preserve_control N\nDescription\nIt is an architecture agnostic value specifying the number of divergent program points that happen\nin the calltree leading to current function call.\nInternally ABI defines some control registers as preserved (callee save) registers.\nInteger N specifies the actual number of control registers that should be preserved by the function.\n.abi_preserve_control\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nSemantics\nWhen this directive is specified compiler backend modifies low level ABI components to ensure that\nnumber of live control variables in the callers of this function that are stored in the callee save\ncontrol registers are less than specified value.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.func foo() .abi_preserve_control 14\n\n// Indirect call via call prototype\n.func (.param .b32 out[30]) bar (.param .b32 in[30]) .abi_preserve_control 10 { ... }\n...\nmov.b64 lpbar, bar;\nprot: .callprototype (.param .b32 out[30]) _ (.param .b32 in[30]) .abi_preserve_control 10;\ncall (out), lpbar, (in), prot;\n11.5.\nDebugging Directives\n\nDWARF-format debug information is passed through PTX modules using the following directives:\n@@DWARF\n.section\n.file\n.loc\nThe\n.section\ndirective was introduced in PTX ISA version 2.0 and replaces the\n@@DWARF\nsyntax. The\n@@DWARF\nsyntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX\nISA version 1.x code.\nBeginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the\n.target\ndebug\nplatform option. This forward declaration directs PTX compilation to retain\nmappings for source-level debugging.\n11.5.1.\nDebugging Directives:\n@@dwarf\n\n@@dwarf\nDWARF-format information.\nSyntax\n@@DWARF dwarf-string\n\ndwarf-string may have one of the\n.byte   byte-list   // comma-separated hexadecimal byte values\n.4byte  int32-list  // comma-separated hexadecimal integers in range [0..2^32-1]\n.quad   int64-list  // comma-separated hexadecimal integers in range [0..2^64-1]\n.4byte  label\n.quad   label\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2. Deprecated as of PTX ISA version 2.0, replaced by\n.section\ndirective.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n@@DWARF .section .debug_pubnames, \"\", @progbits\n@@DWARF .byte   0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n@@DWARF .4byte  .debug_info\n@@DWARF .4byte  0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n@@DWARF .4byte  0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n@@DWARF .byte   0x00, 0x00, 0x00, 0x00, 0x00\n11.5.2.\nDebugging Directives:\n.section\n\n.section\nPTX section definition.\nSyntax\n.section section_name { dwarf-lines }\n\ndwarf-lines have the following formats:\n  .b8    byte-list       // comma-separated list of integers\n                         // in range [-128..255]\n  .b16   int16-list      // comma-separated list of integers\n                         // in range [-2^15..2^16-1]\n  .b32   int32-list      // comma-separated list of integers\n                         // in range [-2^31..2^32-1]\n  label:                 // Define label inside the debug section\n  .b64   int64-list      // comma-separated list of integers\n                         // in range [-2^63..2^64-1]\n  .b32   label\n  .b64   label\n  .b32   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 32bit)\n  .b64   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 64bit)\n  .b32   label1-label2   // a difference in label addresses between labels in\n                         // the same dwarf section (32bit)\n  .b64   label3-label4   // a difference in label addresses between labels in\n                         // the same dwarf section (64bit)\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0, replaces\n@@DWARF\nsyntax.\nlabel+imm expression introduced in PTX ISA version 3.2.\nSupport for\n.b16\nintegers in dwarf-lines introduced in PTX ISA version 6.0.\nSupport for defining\nlabel\ninside the DWARF section is introduced in PTX ISA version 7.2.\nlabel1-label2\nexpression introduced in PTX ISA version 7.5.\nNegative numbers in dwarf lines introduced in PTX ISA version 7.5.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.section .debug_pubnames\n{\n    .b32    LpubNames_end0-LpubNames_begin0\n  LpubNames_begin0:\n    .b8     0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n    .b32    .debug_info\n  info_label1:\n    .b32    0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n    .b32    0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n    .b8     0x00, 0x00, 0x00, 0x00, 0x00\n  LpubNames_end0:\n}\n\n.section .debug_info\n{\n    .b32 11430\n    .b8 2, 0\n    .b32 .debug_abbrev\n    .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49\n    .b8 0\n    .b32 3, 37, 176, -99\n    .b32 info_label1\n    .b32 .debug_loc+0x4\n    .b8 -11, 11, 112, 97\n    .b32 info_label1+12\n    .b64 -1\n    .b16 -5, -65535\n}\n11.5.3.\nDebugging Directives:\n.file\n\n.file\nSource file name.\nSyntax\n.file file_index \"filename\" {, timestamp, file_size}\nDescription\nAssociates a source filename with an integer index.\n.loc\ndirectives reference source files by\nindex.\n.file\ndirective allows optionally specifying an unsigned number representing time of last\nmodification and an unsigned integer representing size in bytes of source file.\ntimestamp\nand\nfile_size\nvalue can be 0 to indicate this information is not available.\ntimestamp\nvalue is in format of C and C++ data type\ntime_t\n.\nfile_size\nis an unsigned 64-bit integer.\nThe\n.file\ndirective is allowed only in the outermost scope, i.e., at the same level as kernel\nand device function declarations.\nSemantics\nIf timestamp and file size are not specified, they default to 0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTimestamp and file size introduced in PTX ISA version 3.2.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.file 1 \"example.cu\"\n.file 2 \"kernel.cu\"\n.file 1 \"kernel.cu\", 1339013327, 64118\n11.5.4.\nDebugging Directives:\n.loc\n\n.loc\nSource file location.\nSyntax\n.loc file_index line_number column_position\n.loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2\nDescription\nDeclares the source file location (source file, line number, and column position) to be associated\nwith lexically subsequent PTX instructions.\n.loc\nrefers to\nfile_index\nwhich is defined by a\n.file\ndirective.\nTo indicate PTX instructions that are generated from a function that got inlined, additional\nattribute\n.inlined_at\ncan be specified as part of the\n.loc\ndirective.\n.inlined_at\nattribute specifies source location at which the specified function is inlined.\nfile_index2\n,\nline_number2\n, and\ncolumn_position2\nspecify the location at which function is inlined. Source\nlocation specified as part of\n.inlined_at\ndirective must lexically precede as source location in\n.loc\ndirective.\nThe\nfunction_name\nattribute specifies an offset in the DWARF section named\n.debug_str\n. Offset is specified as\nlabel\nexpression or\nlabel\n+\nimmediate\nexpression\nwhere\nlabel\nis defined in\n.debug_str\nsection. DWARF section\n.debug_str\ncontains ASCII\nnull-terminated strings that specify the name of the function that is inlined.\nNote that a PTX instruction may have a single associated source location, determined by the nearest\nlexically preceding .loc directive, or no associated source location if there is no preceding .loc\ndirective. Labels in PTX inherit the location of the closest lexically following instruction. A\nlabel with no following PTX instruction has no associated source location.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nfunction_name\nand\ninlined_at\nattributes are introduced in PTX ISA version 7.2.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.loc 2 4237 0\nL1:                        // line 4237, col 0 of file #2,\n                           // inherited from mov\n    mov.u32  %r1,%r2;      // line 4237, col 0 of file #2\n    add.u32  %r2,%r1,%r3;  // line 4237, col 0 of file #2\n...\nL2:                        // line 4239, col 5 of file #2,\n                           // inherited from sub\n    .loc 2 4239 5\n    sub.u32  %r2,%r1,%r3;  // line 4239, col 5 of file #2\n    .loc 1 21 3\n    .loc 1 9 3, function_name info_string0, inlined_at 1 21 3\n    ld.global.u32   %r1, [gg]; // Function at line 9\n    setp.lt.s32 %p1, %r1, 8;   // inlined at line 21\n    .loc 1 27 3\n    .loc 1 10 5, function_name info_string1, inlined_at 1 27 3\n    .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5\n    setp.ne.s32 %p2, %r1, 18;\n    @%p2 bra    BB2_3;\n\n    .section .debug_str {\n    info_string0:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 102 // f\n     .b8 111 // o\n     .b8 111 // o\n     .b8 118 // v\n     .b8 0\n\n    info_string1:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 98  // b\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 99  // c\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n    }\n11.6.\nLinking Directives\n\n.extern\n.visible\n.weak\n11.6.1.\nLinking Directives:\n.extern\n\n.extern\nExternal symbol declaration.\nSyntax\n.extern identifier\nDescription\nDeclares identifier to be defined external to the current module. The module defining such\nidentifier must define it as\n.weak\nor\n.visible\nonly once in a single object file. Extern\ndeclaration of symbol may appear multiple times and references to that get resolved against the\nsingle definition of that symbol.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.extern .global .b32 foo;  // foo is defined in another module\n11.6.2.\nLinking Directives:\n.visible\n\n.visible\nVisible (externally) symbol declaration.\nSyntax\n.visible identifier\nDescription\nDeclares identifier to be globally visible. Unlike C, where identifiers are globally visible unless\ndeclared static, PTX identifiers are visible only within the current module unless declared\n.visible\noutside the current.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.visible .global .b32 foo;  // foo will be externally visible\n11.6.3.\nLinking Directives:\n.weak\n\n.weak\nVisible (externally) symbol declaration.\nSyntax\n.weak identifier\nDescription\nDeclares identifier to be globally visible but\nweak\n. Weak symbols are similar to globally visible\nsymbols, except during linking, weak symbols are only chosen after globally visible symbols during\nsymbol resolution. Unlike globally visible symbols, multiple object files may declare the same weak\nsymbol, and references to a symbol get resolved against a weak symbol only if no global symbols have\nthe same name.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.weak .func (.reg .b32 val) foo;  // foo will be externally visible\n11.6.4.\nLinking Directives:\n.common\n\n.common\nVisible (externally) symbol declaration.\nSyntax\n.common identifier\nDescription\nDeclares identifier to be globally visible but “common”.\nCommon symbols are similar to globally visible symbols. However multiple object files may declare\nthe same common symbol and they may have different types and sizes and references to a symbol get\nresolved against a common symbol with the largest size.\nOnly one object file can initialize a common symbol and that must have the largest size among all\nother definitions of that common symbol from different object files.\n.common\nlinking directive can be used only on variables with\n.global\nstorage. It cannot be\nused on function symbols or on symbols with opaque type.\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\n.common\ndirective requires\nsm_20\nor higher.\nExamples\n.common .global .u32 gbl;\n11.7.\nCluster Dimension Directives\n\nThe following directives specify information about clusters:\n.reqnctapercluster\n.explicitcluster\n.maxclusterrank\nThe\n.reqnctapercluster\ndirective specifies the number of CTAs in the cluster. The\n.explicitcluster\ndirective specifies that the kernel should be launched with explicit cluster\ndetails. The\n.maxclusterrank\ndirective specifies the maximum number of CTAs in the cluster.\nThe cluster dimension directives can be applied only on kernel functions.\n11.7.1.\nCluster Dimension Directives:\n.reqnctapercluster\n\n.reqnctapercluster\nDeclare the number of CTAs in the cluster.\nSyntax\n.reqnctapercluster nx\n.reqnctapercluster nx, ny\n.reqnctapercluster nx, ny, nz\nDescription\nSet the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of\nthe 1D, 2D, or 3D cluster. The total number of CTAs is the product of the number of CTAs in each\ndimension. For kernels with\n.reqnctapercluster\ndirective specified, runtime will use the\nspecified values for configuring the launch if the same are not specified at launch time.\nSemantics\nIf cluster dimension is explicitly specified at launch time, it should be equal to the values\nspecified in this directive. Specifying a different cluster dimension at launch will result in a\nruntime error or kernel launch failure.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .reqnctapercluster 2         { . . . }\n.entry bar .reqnctapercluster 2, 2, 1   { . . . }\n.entry ker .reqnctapercluster 3, 2      { . . . }\n11.7.2.\nCluster Dimension Directives:\n.explicitcluster\n\n.explicitcluster\nDeclare that Kernel must be launched with cluster dimensions explicitly specified.\nSyntax\n.explicitcluster\nDescription\nDeclares that this Kernel should be launched with cluster dimension explicitly specified.\nSemantics\nKernels with\n.explicitcluster\ndirective must be launched with cluster dimension explicitly\nspecified (either at launch time or via\n.reqnctapercluster\n), otherwise program will fail with\nruntime error or kernel launch failure.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .explicitcluster         { . . . }\n11.7.3.\nCluster Dimension Directives:\n.maxclusterrank\n\n.maxclusterrank\nDeclare the maximum number of CTAs that can be part of the cluster.\nSyntax\n.maxclusterrank n\nDescription\nDeclare the maximum number of thread blocks (CTAs) allowed to be part of the cluster.\nSemantics\nProduct of the number of CTAs in each cluster dimension specified in any invocation of the kernel is\nrequired to be less or equal to that specified in this directive. Otherwise invocation will result\nin a runtime error or kernel launch failure.\nThe\n.maxclusterrank\ndirective cannot be used in conjunction with the\n.reqnctapercluster\ndirective.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo ..maxclusterrank 8         { . . . }\n11.8.\nMiscellaneous Directives\n\nPTX provides the following miscellaneous directives:\n.blocksareclusters\n11.8.1.\nMiscellaneous Directives:\n.blocksareclusters\n\n.blocksareclusters\nSpecify that CUDA thread blocks are mapped to clusters.\nSyntax\n.blocksareclusters\nDescription\nDefault behavior of CUDA API is to specify the grid launch configuration by specifying the number of\nthread blocks and the number of threads per block.\nWhen\n.blocksareclusters\ndirective is specified, it implies that the grid launch configuration\nfor the corresponding\n.entry\nfunction is specifying the number of clusters, i.e. the launch\nconfiguration is specifying number of clusters instead of the number of thread blocks. In this case,\nthe number of thread blocks per cluster is specified by\n.reqnctapercluster\ndirective and the\nthread block size is specified with the\n.reqntid\ndirective.\n.blocksareclusters\ndirective is only allowed for\n.entry\nfunctions and also needs\n.reqntid\nand\n.reqnctapercluster\ndirectives to be specified.\nRefer to\nCUDA Programming Guide\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .reqntid 32, 32, 1 .reqnctapercluster 32, 32, 1 .blocksareclusters { ... }\n12.\nDescriptions of\n.pragma\nStrings\n\nThis section describes the\n.pragma\nstrings defined by ptxas.\n12.1.\nPragma Strings:\n\"nounroll\"\n\n\"nounroll\"\nDisable loop unrolling in optimizing the backend compiler.\nSyntax\n.pragma \"nounroll\";\nDescription\nThe\n\"nounroll\"\npragma\nis a directive to disable loop unrolling in the optimizing backend\ncompiler.\nThe\n\"nounroll\"\npragma\nis allowed at module, entry-function, and statement levels, with the\nfollowing meanings:\nmodule scope\ndisables unrolling for all loops in module, including loops preceding the\n.pragma\n.\nentry-function scope\ndisables unrolling for all loops in the entry function body.\nstatement-level pragma\ndisables unrolling of the loop for which the current block is the loop header.\nNote that in order to have the desired effect at statement level, the\n\"nounroll\"\ndirective must\nappear before any instruction statements in the loop header basic block for the desired loop. The\nloop header block is defined as the block that dominates all blocks in the loop body and is the\ntarget of the loop backedge. Statement-level\n\"nounroll\"\ndirectives appearing outside of loop\nheader blocks are silently ignored.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher. Ignored for\nsm_1x\ntargets.\nExamples\n.entry foo (...)\n.pragma \"nounroll\";  // do not unroll any loop in this function\n{\n...\n}\n\n.func bar (...)\n{\n...\nL1_head:\n     .pragma \"nounroll\";  // do not unroll this loop\n     ...\n@p   bra L1_end;\nL1_body:\n     ...\nL1_continue:\n     bra L1_head;\nL1_end:\n     ...\n}\n12.2.\nPragma Strings:\n\"used_bytes_mask\"\n\n\"used_bytes_mask\"\nMask for indicating used bytes in data of ld operation.\nSyntax\n.pragma \"used_bytes_mask mask\";\nDescription\nThe\n\"used_bytes_mask\"\npragma\nis a directive that specifies used bytes in a load\noperation based on the mask provided.\n\"used_bytes_mask\"\npragma\nneeds to be specified prior to a load instruction for which\ninformation about bytes used from the load operation is needed.\nPragma is ignored if instruction following it is not a load instruction.\nFor a load instruction without this pragma, all bytes from the load operation are assumed\nto be used.\nOperand\nmask\nis a 32-bit integer with set bits indicating the used bytes in data of\nload operation.\nSemantics\nEach bit in mask operand corresponds to a byte data where each set bit represents the used byte.\nMost-significant bit corresponds to most-significant byte of data.\n\n// For 4 bytes load with only lower 3 bytes used\n.pragma \"used_bytes_mask 0x7\";\nld.global.u32 %r0, [gbl];     // Higher 1 byte from %r0 is unused\n\n// For vector load of 16 bytes with lower 12 bytes used\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];  // %r3 unused\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nExamples\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // Only lower 12 bytes used\n12.3.\nPragma Strings:\n\"enable_smem_spilling\"\n\n\"enable_smem_spilling\"\nEnable shared memory spilling for CUDA kernels.\nSyntax\n.pragma \"enable_smem_spilling\";\nDescription\nThe\n\"enable_smem_spilling\"\npragma\nis a directive that enables register spilling into shared memory.\nDuring the spilling process, registers are first spilled into shared memory, and once the allocated\nshared memory is full, any additional spills are redirected to local memory. This can enhance\nperformance by reducing memory access latency since shared memory accesses are faster than local memory.\nThe\n\"enable_smem_spilling\"\npragma\nis only allowed within the function scope. When applied, it enables\nshared memory spilling for the specified function.\nThe usage of pragma is valid only in certain scenarios and specific compilation modes. The usage of\npragma is disallowed under following cases and may result in an error:\nPer-function compilation mode: e.g., Separate Compilation, Device-debug, Whole program with recursive\nfunction calls, Extensible-whole-program\nKernels utilizing dynamically allocated shared memory\nKernels using\nsetmaxnreg\ninstruction\nNote\nIf launch bounds are not explicitly specified, the compiler assumes the maximum possible number of\nthreads per CTA to estimate shared memory allocated per CTA and corresponding spill size. However,\nif the kernel is launched with fewer threads per CTA than estimated, the shared memory allocated\nper CTA may exceed the compiler estimated size, thereby potentially limiting the number of CTAs\nthat can be launched on an SM. Due to this, using the pragma without launch bounds may lead to\nperformance regressions. Hence it is recommended to use this pragma only when launch bounds are\nexplicitly specified.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\n.entry foo (...)\n{\n    ...\n    .pragma \"enable_smem_spilling\";   // Enable shared memory spilling for this function\n    ...\n}\n12.4.\nPragma Strings:\n\"frequency\"\n\n\"frequency\"\nSpecify frequency for basic block execution.\nSyntax\n.pragma \"frequency n\";\nDescription\nThe\n\"frequency\"\npragma\nis a directive that specifies the number of times a basic block is\nexecuted by an executing thread. The optimizing compiler backend treats this pragma as a hint\nwhich will be used for optimizations.\nOperand\nn\nis a 64-bit non-negative integer constant that specifies the execution frequency.\nNote that in order to have the desired effect of this pragma, it should be specified at the start of\nthe basic block. Basic block is defined as a straight-line sequence of instructions with only one\nentry point and one exit point.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo (...)\n{\n    .pragma \"frequency 32\";\n    ...\n}\n13.\nRelease Notes\n\nThis section describes the history of change in the PTX ISA and implementation. The first section\ndescribes ISA and implementation changes in the current release of PTX ISA version 9.1, and the\nremaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX\nISA version 2.0.\nTable 59\nshows the PTX release history.\nTable 59\nPTX Release History\n\nPTX ISA Version\nCUDA Release\nSupported Targets\nPTX ISA 1.0\nCUDA 1.0\nsm_{10,11}\nPTX ISA 1.1\nCUDA 1.1\nsm_{10,11}\nPTX ISA 1.2\nCUDA 2.0\nsm_{10,11,12,13}\nPTX ISA 1.3\nCUDA 2.1\nsm_{10,11,12,13}\nPTX ISA 1.4\nCUDA 2.2\nsm_{10,11,12,13}\nPTX ISA 1.5\ndriver r190\nsm_{10,11,12,13}\nPTX ISA 2.0\nCUDA 3.0, driver r195\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.1\nCUDA 3.1, driver r256\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.2\nCUDA 3.2, driver r260\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.3\nCUDA 4.0, driver r270\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 3.0\nCUDA 4.1, driver r285\nsm_{10,11,12,13}\n,\nsm_20\nCUDA 4.2, driver r295\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_30\nPTX ISA 3.1\nCUDA 5.0, driver r302\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,35}\nPTX ISA 3.2\nCUDA 5.5, driver r319\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,35}\nPTX ISA 4.0\nCUDA 6.0, driver r331\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35}\n,\nsm_50\nPTX ISA 4.1\nCUDA 6.5, driver r340\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52}\nPTX ISA 4.2\nCUDA 7.0, driver r346\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\nPTX ISA 4.3\nCUDA 7.5, driver r352\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\nPTX ISA 5.0\nCUDA 8.0, driver r361\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\nPTX ISA 6.0\nCUDA 9.0, driver r384\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\nPTX ISA 6.1\nCUDA 9.1, driver r387\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\nPTX ISA 6.2\nCUDA 9.2, driver r396\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\nPTX ISA 6.3\nCUDA 10.0, driver r400\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 6.4\nCUDA 10.1, driver r418\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 6.5\nCUDA 10.2, driver r440\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 7.0\nCUDA 11.0, driver r445\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_80\nPTX ISA 7.1\nCUDA 11.1, driver r455\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.2\nCUDA 11.2, driver r460\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.3\nCUDA 11.3, driver r465\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.4\nCUDA 11.4, driver r470\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.5\nCUDA 11.5, driver r495\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.6\nCUDA 11.6, driver r510\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.7\nCUDA 11.7, driver r515\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.8\nCUDA 11.8, driver r520\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_90\nPTX ISA 8.0\nCUDA 12.0, driver r525\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.1\nCUDA 12.1, driver r530\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.2\nCUDA 12.2, driver r535\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.3\nCUDA 12.3, driver r545\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.4\nCUDA 12.4, driver r550\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.5\nCUDA 12.5, driver r555\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nCUDA 12.6, driver r560\nPTX ISA 8.6\nCUDA 12.7, driver r565\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100a,101,101a}\nPTX ISA 8.7\nCUDA 12.8, driver r570\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100,101,101a}\n,\nsm_{120,120a}\nPTX ISA 8.8\nCUDA 12.9, driver r575\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,101,101f,101a,103,103f,103a}\n,\nsm_{120,120f,120a,121,121f,121a}\nPTX ISA 9.0\nCUDA 13.0, driver r580\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,88,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,103,103f,103a}\n,\nsm_{110,110f,110a}\n,\nsm_{120,120f,120a,121,121f,121a}\nPTX ISA 9.1\nCUDA 13.1, driver r590\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,88,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,103,103f,103a}\n,\nsm_{110,110f,110a}\n,\nsm_{120,120f,120a,121,121f,121a}\nTable 60\nArch-specific/ Family-specific PTX Features Release History\n\nInstruction\nVariant\nPTX ISA Version\nSupported Targets\ntensormap.replace\nBase variant\n8.3\nsm_90a\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ntensormap.replace.swizzle_atomicity\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\n.elemtype\nfor\n.field3\nwith values\n13\n,\n14\n,\n15\nfor\nnew_val\n8.7\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\n.swizzle_mode\nfor\n.field3\nwith value\n4\nfor\nnew_val\n8.8\nsm_103a\nwgmma.mma_async\n,\nwgmma.mma_async.sp\n,\nwgmma.fence\n,\nwgmma.commit_group\n,\nwgmma.wait_group\nBase variant\n8.0\nsm_90a\nsetmaxnreg\nBase variant\n8.0\nsm_90a\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nTypes\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\n,\n.e4m3x4\n,\n.e5m2x4\n8.6\nsm_100a\n,\nsm_120a\n,\nsm_121a\n8.8\nsm_100f\n9.0\nsm_110f\n.acc::f16\nqualifier\n8.6\nsm_100a\n,\nsm_120a\n,\nsm_121a\n8.8\nsm_100f\n9.0\nsm_110f\ncvt\ncvt\nfor\n.f32\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n/\n.ue8m0x2\nAND\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\nto\n.f16x2\nAND\n.ue8m0x2\nto\n.bf16x2\nAND\n.bf16x2\nto\n.ue8m0x2\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ncvt\nwith\n.rs\nrounding mode\n8.7\nsm_100a\n8.8\nsm_103a\ncvt\nwith\n.s2f6x2\ntype\n9.1\nsm_100a\n,\nsm_103a\n,\nsm_110a\n,\nsm_120a\n,\nsm_121a\ncvt\nfor\n.f16x2\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n9.1\nsm_100f\n,\nsm_110f\n,\nsm_120f\ncvt\nfor\n.bf16x2\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n/\n.e4m3x2\n/\n.e5m2x2\n9.1\nsm_100f\n,\nsm_110f\n,\nsm_120f\ncp.async.bulk.tensor\n.tile::gather4\nand\n.im2col::w\nwith\n.shared::cluster\nas destination state\nspace\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\n.tile::scatter4\nand\n.im2col::w::128\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\n.cta_group\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ncp.async.bulk.prefetch.tensor\n.tile::gather4\n,\n.im2col::w\n,\n.im2col::w::128\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nredux.sync\nType\n.f32\nand\n.abs\n,\n.NaN\nqualifiers\n8.6\nsm_100a\n8.8\nsm_100f\nclusterlaunchcontrol.try_cancel\n.multicast::cluster::all\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nldmatrix\nShapes\n.m16n16\n,\n.m8n16\nAND\nType\n.b8\nAND\nQualifiers\n.src_fmt\n,\n.dst_fmt\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nstmatrix\nShapes\n.m16n8\nAND\nType\n.b8\nAND\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ntcgen05.ld\n,\ntcgen05.st\n,\ntcgen05.wait\n,\ntcgen05.cp\n,\ntcgen05.fence\n,\ntcgen05.commit\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ntcgen05.ld.red\nBase variant\n8.8\nsm_103f\n9.0\nsm_110f\ntcgen05.shift\nBase variant\n8.6\nsm_100a\n8.8\nsm_103a\n9.0\nsm_110a\ntcgen05.mma\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nArgument\nscale-input-d\n8.6\nsm_100a\n8.8\nsm_100f\nQualifiers\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\n8.6\nsm_100a\nQualifiers\n.block16\n,\n.block32\n8.8\nsm_100f\n,\nsm_110f\nK shape value\n96\n8.8\nsm_103a\ntcgen05.mma.sp\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nKind\n.kind::mxf4nvf4\nand\n.kind::mxf4\n8.6\nsm_100a\n8.8\nsm_103a\n9.0\nsm_110a\nArgument\nscale-input-d\n8.6\nsm_100a\n8.8\nsm_100f\nQualifiers\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\n8.6\nsm_100a\nQualifiers\n.block16\n,\n.block32\n8.8\nsm_100f\n,\nsm_110f\ntcgen05.mma.ws\n,\ntcgen05.mma.ws.sp\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nmma\nTypes\n.e3m2\n,\n.e2m3\n,\n.e2m1\nAND Qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n8.7\nsm_120a\n8.8\nsm_120f\nmma.sp\nTypes\n.e3m2\n,\n.e2m3\n,\n.e2m1\nAND Qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n8.7\nsm_120a\n8.8\nsm_120f\nKind\n.kind::mxf4nvf4\nand\n.kind::mxf4\n8.7\nsm_120a\n,\nsm_121a\n13.1.\nChanges in PTX ISA Version 9.1\n\nNew Features\nPTX ISA version 9.1 introduces the following new features:\nAdds support for\n.volatile\nqualifier with\n.local\nstate space for\nld\nand\nst\ninstructions.\nAdds support for\n.f16x2\nand\n.bf16x2\nsource types for\ncvt\ninstruction\nwith destination types\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x2\n,\n.e5m2x2\n.\nAdds support for\n.scale_vec::4X\nwith\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nfor\nmma\n/\nmma.sp\ninstructions.\nAdds support for\n.s2f6x2\ninstruction type for\ncvt\ninstruction.\nAdds support for\nmultimem.cp.async.bulk\nand\nmultimem.cp.reduce.async.bulk\ninstructions.\nSemantic Changes and Clarifications\nNone.\n13.2.\nChanges in PTX ISA Version 9.0\n\nNew Features\nPTX ISA version 9.0 introduces the following new features:\nAdds support for\nsm_88\ntarget architecture.\nAdds support for\nsm_110\ntarget architecture.\nAdds support for target\nsm_110f\nthat supports family-specific features.\nAdds support for target\nsm_110a\nthat supports architecture-specific features.\nAdds support for pragma\nenable_smem_spilling\nthat is used to enable shared\nmemory spilling for a function.\nAdds support for pragma\nfrequency\nthat is used to specify the execution frequency of a basic\nblock.\nAdds support for directive\n.blocksareclusters\nthat is used to specify that CUDA thread blocks\nare mapped to clusters.\nExtends\nsize\noperand of\nst.bulk\ninstruction to support 32-bit length.\nAdds support for performance-tuning directives\n.abi_preserve\nand\n.abi_preserve_control\nthat are used to specify the number of data and control registers that should be preserved by the\ncallers of a function.\nNotes\nTargets\nsm_{101,101f,101a}\nare renamed to targets\nsm_{110,110f,110a}\nfrom PTX ISA version 9.0.\nSemantic Changes and Clarifications\nAll\ntcgen05\ninstructions(\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n,\ntcgen05.cp\n,\ntcgen05.shift\n,\ntcgen05.mma\n,\ntcgen05.mma.sp\n,\ntcgen05.mma.ws,\ntcgen05.mma.ws.sp\n,\ntcgen05.commit\n) within a kernel must specify the same value for the\n.cta_group\nqualifier.\n13.3.\nChanges in PTX ISA Version 8.8\n\nNew Features\nPTX ISA version 8.8 introduces the following new features:\nAdds support for\nsm_103\ntarget architecture.\nAdds support for target\nsm_103a\nthat supports architecture-specific features.\nAdds support for\nsm_121\ntarget architecture.\nAdds support for target\nsm_121a\nthat supports architecture-specific features.\nIntroduces family-specific target architectures that are represented with “f” suffix.\nPTX for family-specific targets is compatible with all subsequent targets in same family.\nAdds support for\nsm_100f\n,\nsm_101f\n,\nsm_103f\n,\nsm_120f\n,\nsm_121f\n.\nExtends\nmin\nand\nmax\ninstructions to support three input arguments.\nExtends\ntcgen05.mma\ninstruction to add support for new\nscale_vectorsize\nqualifiers\n.block16\nand\n.block32\nand K dimension 96.\nExtends\n.field3\nof\ntensormap.replace\ninstruction to support 96B swizzle mode.\nAdds support for\ntcgen05.ld.red\ninstruction.\nExtends\nld\n,\nld.global.nc\nand\nst\ninstructions to support 256b load/store operations.\nTable 61\nshows the list of features that are\nsupported on family-specific targets:\nTable 61\nList of features promoted to family-specific architecture\n\nFeature\nSupported targets\n.m16n8\n,\n.m16n16\n,\n.m8n16\nshapes and\n.b8\ntype for\nldmatrix\n/\nstmatrix\nsm_100f\n,\nsm_101f\n,\nsm_120f\nShapes for\ntcgen05\n.16x64b\n.16x128b\n,\n.16x256b\n,\n.16x32bx2\n,\n.32x32b\n,\n.4x256b\n,\n.32x128b\n,\n.64x128b\n,\n.128x256b\n,\n.128x128b\n,\n.31x256b\nsm_100f\n,\nsm_101f\nsetmaxnreg\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.cta_group\nmodifier\nsm_100f\n,\nsm_101f\ncvt\nwith\n.e2m1x2\n,\n.e3m2x2\n,\n.e2m3x2\n,\n.ue8m0x2\nsm_100f\n,\nsm_101f\n,\nsm_120f\nmultimem\nwith\n.acc::f16\nand\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\ntypes\nsm_100f\n,\nsm_101f\ntensormap.replace\nsm_100f\n,\nsm_101f\n,\nsm_120f\ntcgen05.ld.red\nsm_101f\n,\nsm_103f\ntcgen05.ld\n/\nst\n/\nfence\n/\nwait\n/\ncommit\n/\ncp\n/\nalloc\n/\ndealloc\n/\nrelinquish_alloc_permit\nsm_100f\n,\nsm_101f\ntcgen05.mma{.ws}{.sp}\n(except\nkind::mxf4\n/\nkind::mxf4nvf4\nfor\n.sp\n)\nsm_100f\n,\nsm_101f\ntcgen05\n.kind::mxf4nvf4\n,\n.kind::mxf4\n,\n.kind::mxf8f6f4\n,\n.kind::f16\n,\n.kind::tf32\n,\n.kind::f8f6f4\nsm_100f\n,\nsm_101f\n.ashift\n,\n.collector_usage\nmodifiers for\ntcgen05\nsm_100f\n,\nsm_101f\nModifiers\n.b8x16\n,\n.b6x16_p32\n,\n.b4x16_p64\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.block_scale\nmodifier\nsm_100f\n,\nsm_101f\n,\nsm_120f\nmma{.sp}\nwith\n.e3m2\n,\n.e2m3\n,\n.e2m1\ntypes and\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nmodifiers\n(except\n.sp\nwith\nmxf4\n/\nmxf4nvf4\n)\nsm_120f\n.scale_vec::1X\n/\n2X\n/\n4X\nmodifiers\nsm_120f\n.block16\n/\n.block32\nmodifiers (alias to\nscale_vec\n)\nsm_100f\n,\nsm_101f\n.warpx2::02_13\n,\n.warpx2::01_23\n,\n.warpx4\n,\n.pack::16b\n,\n.unpack::16b\nmodifiers for\ntcgen05\nsm_100f\n,\nsm_101f\nclusterlaunchcontrol.try_cancel\nmulticast::cluster::all\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.tile::scatter4\n,\n.tile::gather4\n,\n.im2col::w\n,\n.im2col::w::128\nsm_100f\n,\nsm_101f\nredux.f32\nsm_100f\nscale-input-d\nfor\ntcgen05\nsm_100f\nSemantic Changes and Clarifications\nClarified the behavior of float-to-integer conversions for\nNaN\ninput.\n13.4.\nChanges in PTX ISA Version 8.7\n\nNew Features\nPTX ISA version 8.7 introduces the following new features:\nAdds support for\nsm_120\ntarget architecture.\nAdds support for target\nsm_120a\nthat supports architecture-specific features.\nExtends\ntcgen05.mma\ninstruction to add support for\n.kind::mxf4nvf4\nand\n.scale_vec::4X\nqualifiers.\nExtends\nmma\ninstructions to support\n.f16\ntype accumulator and shape\n.m16n8k16\nwith\nFP8 types\n.e4m3\nand\n.e5m2\n.\nExtends\ncvt\ninstruction to add support for\n.rs\nrounding mode and destination types\n.e2m1x4\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e3m2x4\n,\n.e2m3x4\n.\nExtends support for\nst.async\nand\nred.async\ninstructions to add support for\n.mmio\n,\n.release\n,\n.global\nand\n.scope\nqualifiers.\nExtends\ntensormap.replace\ninstruction to add support for values\n13\nto\n15\nfor\n.elemtype\nqualifier.\nExtends\nmma\nand\nmma.sp::ordered_metadata\ninstructions to add support for types\n.e3m2\n/\n.e2m3\n/\n.e2m1\nand qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n.\nSemantic Changes and Clarifications\nClarified that in\n.tile::gather4\n,\n.tile::scatter4\nmodes, tensor coordinates need to be\nspecified as {col_idx, row_idx0, row_idx1, row_idx2, row_idx3} i.e. {x, y0, y1, y2, y3} instead\nof {x0, x1, x2, x3, y}.\nUpdated\nInstruction descriptor\nof\ntcgen05.mma\ninstruction\nto clarify the bits that are reserved for future use.\n13.5.\nChanges in PTX ISA Version 8.6\n\nNew Features\nPTX ISA version 8.6 introduces the following new features:\nAdds support for\nsm_100\ntarget architecture.\nAdds support for target\nsm_100a\nthat supports architecture-specific features.\nAdds support for\nsm_101\ntarget architecture.\nAdds support for target\nsm_101a\nthat supports architecture-specific features.\nExtends\ncp.async.bulk\nand\ncp.async.bulk.tensor\ninstructions to add\n.shared::cta\nas destination state space.\nExtends\nfence\ninstruction to add support for\n.acquire\nand\n.release\nqualifiers.\nExtends\nfence\nand\nfence.proxy\ninstructions to add support for\n.sync_restrict\nqualifier.\nExtends\nldmatrix\ninstruction to support\n.m16n16\n,\n.m8n16\nshapes and\n.b8\ntype.\nExtends\nldmatrix\ninstruction to support\n.src_fmt\n,\n.dst_fmt\nqualifiers.\nExtends\nstmatrix\ninstruction to support\n.m16n8\nshape and\n.b8\ntype.\nAdds support for\nclusterlaunchcontrol\ninstruction.\nExtends\nadd\n,\nsub\nand\nfma\ninstructions to support mixed precision floating point\noperations with\n.f32\nas destaination operand type and\n.f16\n/\n.bf16\nas source operand\ntypes.\nExtends\nadd\n,\nsub\n,\nmul\nand\nfma\ninstructions to support\n.f32x2\ntype.\nExtends\ncvt\ninstruction with\n.tf32\ntype to support\n.satfinite\nqualifier\nfor\n.rn\n/\n.rz\nrounding modes.\nExtends\ncp.async.bulk\ninstruction to support\n.cp_mask\nqualifier and\nbyteMask\noperand.\nExtends\nmultimem.ld_reduce\nand\nmultimem.st\ninstructions to support\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\nand\n.e4m3x4\ntypes.\nExtends\ncvt\ninstruction to support conversions to/from\n.e2m1x2\n,\n.e3m2x2\n,\n.e2m3x2\nand\n.ue8m0x2\ntypes.\nExtends\ncp.async.bulk.tensor\nand\ncp.async.bulk.prefetch.tensor\ninstructions to\nsupport new load_mode qualifiers\n.tile::scatter4\nand\n.tile::gather4\n.\nExtends\ntensormap.replace\ninstruction to add support for new qualifier\n.swizzle_atomicity\nfor supporting new swizzle modes.\nExtends\nmbarrier.arrive\n,\nmbarrier.arrive_drop\n,\n.mbarrier.test_wait\nand\n.mbarrier.try_wait\ninstructions to support\n.relaxed\nqualifier.\nExtends\ncp.async.bulk.tensor\nand\ncp.async.bulk.prefetch.tensor\ninstructions to\nsupport new load_mode qualifiers\n.im2col::w\nand\n.im2col::w::128\n.\nExtends\ncp.async.bulk.tensor\ninstruction to support new qualifier\n.cta_group\n.\nAdd support for\nst.bulk\ninstruction.\nAdds support for tcgen05 features and related instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n,\ntcgen05.ld\n,\ntcgen05.st\n,\ntcgen05.wait\n,\ntcgen05.cp\n,\ntcgen05.shift\n,\ntcgen05.mma\n,\ntcgen05.mma.sp\n,\ntcgen05.mma.ws\n,\ntcgen05.mma.ws.sp\n,\ntcgen05.fence\nand\ntcgen05.commit\n.\nExtends\nredux.sync\ninstruction to add support for\n.f32\ntype with qualifiers\n.abs\nand\n.NaN\n.\nSemantic Changes and Clarifications\nNone.\n13.6.\nChanges in PTX ISA Version 8.5\n\nNew Features\nPTX ISA version 8.5 introduces the following new features:\nAdds support for\nmma.sp::ordered_metadata\ninstruction.\nSemantic Changes and Clarifications\nValues\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nfor sparsity metadata (operand\ne\n)\nof instruction\nmma.sp\nare invalid and their usage results in undefined behavior.\n13.7.\nChanges in PTX ISA Version 8.4\n\nNew Features\nPTX ISA version 8.4 introduces the following new features:\nExtends\nld\n,\nst\nand\natom\ninstructions with\n.b128\ntype to support\n.sys\nscope.\nExtends integer\nwgmma.mma_async\ninstruction to support\n.u8.s8\nand\n.s8.u8\nas\n.atype\nand\n.btype\nrespectively.\nExtends\nmma\n,\nmma.sp\ninstructions to support FP8 types\n.e4m3\nand\n.e5m2\n.\nSemantic Changes and Clarifications\nNone.\n13.8.\nChanges in PTX ISA Version 8.3\n\nNew Features\nPTX ISA version 8.3 introduces the following new features:\nAdds support for pragma\nused_bytes_mask\nthat is used to specify mask for used bytes for a load operation.\nExtends\nisspacep\n,\ncvta.to\n,\nld\nand\nst\ninstructions to accept\n::entry\nand\n::func\nsub-qualifiers with\n.param\nstate space qualifier.\nAdds support for\n.b128\ntype on instructions\nld\n,\nld.global.nc\n,\nldu\n,\nst\n,\nmov\nand\natom\n.\nAdd support for instructions\ntensormap.replace\n,\ntensormap.cp_fenceproxy\nand support for qualifier\n.to_proxykind::from_proxykind\non instruction\nfence.proxy\nto support modifying\ntensor-map\n.\nSemantic Changes and Clarifications\nNone.\n13.9.\nChanges in PTX ISA Version 8.2\n\nNew Features\nPTX ISA version 8.2 introduces the following new features:\nAdds support for\n.mmio\nqualifier on\nld\nand\nst\ninstructions.\nExtends\nlop3\ninstruction to allow predicate destination.\nExtends\nmultimem.ld_reduce\ninstruction to support\n.acc::f32\nqualifer to allow\n.f32\nprecision of the intermediate accumulation.\nExtends the asynchronous warpgroup-level matrix multiply-and-accumulate operation\nwgmma.mma_async\nto support\n.sp\nmodifier that allows matrix multiply-accumulate operation\nwhen input matrix A is sparse.\nSemantic Changes and Clarifications\nThe\n.multicast::cluster\nqualifier on\ncp.async.bulk\nand\ncp.async.bulk.tensor\ninstructions\nis optimized for target architecture\nsm_90a\nand may have substantially reduced performance on\nother targets and hence\n.multicast::cluster\nis advised to be used with\nsm_90a\n.\n13.10.\nChanges in PTX ISA Version 8.1\n\nNew Features\nPTX ISA version 8.1 introduces the following new features:\nAdds support for\nst.async\nand\nred.async\ninstructions for asynchronous store and\nasynchronous reduction operations respectively on shared memory.\nAdds support for\n.oob\nmodifier on half-precision\nfma\ninstruction.\nAdds support for\n.satfinite\nsaturation modifer on\ncvt\ninstruction for\n.f16\n,\n.bf16\nand\n.tf32\nformats.\nExtends support for\ncvt\nwith\n.e4m3\n/\n.e5m2\nto\nsm_89\n.\nExtends\natom\nand\nred\ninstructions to support vector types.\nAdds support for special register\n%aggr_smem_size\n.\nExtends\nsured\ninstruction with 64-bit\nmin\n/\nmax\noperations.\nAdds support for increased kernel parameter size of 32764 bytes.\nAdds support for multimem addresses in memory consistency model.\nAdds support for\nmultimem.ld_reduce\n,\nmultimem.st\nand\nmultimem.red\ninstructions to\nperform memory operations on multimem addresses.\nSemantic Changes and Clarifications\nNone.\n13.11.\nChanges in PTX ISA Version 8.0\n\nNew Features\nPTX ISA version 8.0 introduces the following new features:\nAdds support for target\nsm_90a\nthat supports architecture-specific features.\nAdds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation\nwgmma\n.\nExtends the asynchronous copy operations with bulk operations that operate on large data,\nincluding tensor data.\nIntroduces packed integer types\n.u16x2\nand\n.s16x2\n.\nExtends integer arithmetic instruction\nadd\nto allow packed integer types\n.u16x2\nand\n.s16x2\n.\nExtends integer arithmetic instructions\nmin\nand\nmax\nto allow packed integer types\n.u16x2\nand\n.s16x2\n, as well as saturation modifier\n.relu\non\n.s16x2\nand\n.s32\ntypes.\nAdds support for special register\n%current_graph_exec\nthat identifies the currently executing\nCUDA device graph.\nAdds support for\nelect.sync\ninstruction.\nAdds support for\n.unified\nattribute on functions and variables.\nAdds support for\nsetmaxnreg\ninstruction.\nAdds support for\n.sem\nqualifier on\nbarrier.cluster\ninstruction.\nExtends the\nfence\ninstruction to allow opcode-specific synchronizaion using\nop_restrict\nqualifier.\nAdds support for\n.cluster\nscope on\nmbarrier.arrive\n,\nmbarrier.arrive_drop\n,\nmbarrier.test_wait\nand\nmbarrier.try_wait\noperations.\nAdds support for transaction count operations on\nmbarrier\nobjects, specified with\n.expect_tx\nand\n.complete_tx\nqualifiers.\nSemantic Changes and Clarifications\nNone.\n13.12.\nChanges in PTX ISA Version 7.8\n\nNew Features\nPTX ISA version 7.8 introduces the following new features:\nAdds support for\nsm_89\ntarget architecture.\nAdds support for\nsm_90\ntarget architecture.\nExtends\nbar\nand\nbarrier\ninstructions to accept optional scope qualifier\n.cta\n.\nExtends\n.shared\nstate space qualifier with optional sub-qualifier\n::cta\n.\nAdds support for\nmovmatrix\ninstruction which transposes a matrix in registers across a warp.\nAdds support for\nstmatrix\ninstruction which stores one or more matrices to shared memory.\nExtends the\n.f64\nfloating point type\nmma\noperation with shapes\n.m16n8k4\n,\n.m16n8k8\n,\nand\n.m16n8k16\n.\nExtends\nadd\n,\nsub\n,\nmul\n,\nset\n,\nsetp\n,\ncvt\n,\ntanh\n,\nex2\n,\natom\nand\nred\ninstructions with\nbf16\nalternate floating point data format.\nAdds support for new alternate floating-point data formats\n.e4m3\nand\n.e5m2\n.\nExtends\ncvt\ninstruction to convert\n.e4m3\nand\n.e5m2\nalternate floating point data formats.\nAdds support for\ngriddepcontrol\ninstruction as a communication mechanism to control the\nexecution of dependent grids.\nExtends\nmbarrier\ninstruction to allow a new phase completion check operation\ntry_wait\n.\nAdds support for new thread scope\n.cluster\nwhich is a set of Cooperative Thread Arrays (CTAs).\nExtends\nfence\n/\nmembar\n,\nld\n,\nst\n,\natom\n, and\nred\ninstructions to accept\n.cluster\nscope.\nAdds support for extended visibility of shared state space to all threads within a cluster.\nExtends\n.shared\nstate space qualifier with\n::cluster\nsub-qualifier for cluster-level\nvisibility of shared memory.\nExtends\nisspacep\n,\ncvta\n,\nld\n,\nst\n,\natom\n, and\nred\ninstructions to accept\n::cluster\nsub-qualifier with\n.shared\nstate space qualifier.\nAdds support for\nmapa\ninstruction to map a shared memory address to the corresponding address\nin a different CTA within the cluster.\nAdds support for\ngetctarank\ninstruction to query the rank of the CTA that contains a given\naddress.\nAdds support for new barrier synchronization instruction\nbarrier.cluster\n.\nExtends the memory consistency model to include the new cluster scope.\nAdds support for special registers related to cluster information:\n%is_explicit_cluster\n,\n%clusterid\n,\n%nclusterid\n,\n%cluster_ctaid\n,\n%cluster_nctaid\n,\n%cluster_ctarank\n,\n%cluster_nctarank\n.\nAdds support for cluster dimension directives\n.reqnctapercluster\n,\n.explicitcluster\n, and\n.maxclusterrank\n.\nSemantic Changes and Clarifications\nNone.\n13.13.\nChanges in PTX ISA Version 7.7\n\nNew Features\nPTX ISA version 7.7 introduces the following new features:\nExtends\nisspacep\nand\ncvta\ninstructions to include the\n.param\nstate space for kernel\nfunction parameters.\nSemantic Changes and Clarifications\nNone.\n13.14.\nChanges in PTX ISA Version 7.6\n\nNew Features\nPTX ISA version 7.6 introduces the following new features:\nSupport for\nszext\ninstruction which performs sign-extension or zero-extension on a specified\nvalue.\nSupport for\nbmsk\ninstruction which creates a bitmask of the specified width starting at the\nspecified bit position.\nSupport for special registers\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset<2>\n.\nSemantic Changes and Clarifications\nNone.\n13.15.\nChanges in PTX ISA Version 7.5\n\nNew Features\nPTX ISA version 7.5 introduces the following new features:\nDebug information enhancements to support label difference and negative values in the\n.section\ndebugging directive.\nSupport for\nignore-src\noperand on\ncp.async\ninstruction.\nExtensions to the memory consistency model to introduce the following new concepts:\nA\nmemory proxy\nas an abstract label for different methods of memory access.\nVirtual aliases as distinct memory addresses accessing the same physical memory location.\nSupport for new\nfence.proxy\nand\nmembar.proxy\ninstructions to allow synchronization of\nmemory accesses performed via virtual aliases.\nSemantic Changes and Clarifications\nNone.\n13.16.\nChanges in PTX ISA Version 7.4\n\nNew Features\nPTX ISA version 7.4 introduces the following new features:\nSupport for\nsm_87\ntarget architecture.\nSupport for\n.level::eviction_priority\nqualifier which allows specifying cache eviction\npriority hints on\nld\n,\nld.global.nc\n,\nst\n, and\nprefetch\ninstructions.\nSupport for\n.level::prefetch_size\nqualifier which allows specifying data prefetch hints on\nld\nand\ncp.async\ninstructions.\nSupport for\ncreatepolicy\ninstruction which allows construction of different types of cache\neviction policies.\nSupport for\n.level::cache_hint\nqualifier which allows the use of cache eviction policies with\nld\n,\nld.global.nc\n,\nst\n,\natom\n,\nred\nand\ncp.async\ninstructions.\nSupport for\napplypriority\nand\ndiscard\noperations on cached data.\nSemantic Changes and Clarifications\nNone.\n13.17.\nChanges in PTX ISA Version 7.3\n\nNew Features\nPTX ISA version 7.3 introduces the following new features:\nExtends\nmask()\noperator used in initializers to also support integer constant expression.\nAdds support for stack manpulation instructions that allow manipulating stack using\nstacksave\nand\nstackrestore\ninstructions and allocation of per-thread stack using\nalloca\ninstruction.\nSemantic Changes and Clarifications\nThe unimplemented version of\nalloca\nfrom the older PTX ISA specification has been replaced with\nnew stack manipulation instructions in PTX ISA version 7.3.\n13.18.\nChanges in PTX ISA Version 7.2\n\nNew Features\nPTX ISA version 7.2 introduces the following new features:\nEnhances\n.loc\ndirective to represent inline function information.\nAdds support to define labels inside the debug sections.\nExtends\nmin\nand\nmax\ninstructions to support\n.xorsign\nand\n.abs\nmodifiers.\nSemantic Changes and Clarifications\nNone.\n13.19.\nChanges in PTX ISA Version 7.1\n\nNew Features\nPTX ISA version 7.1 introduces the following new features:\nSupport for\nsm_86\ntarget architecture.\nAdds a new operator,\nmask()\n, to extract a specific byte from variable’s address used in\ninitializers.\nExtends\ntex\nand\ntld4\ninstructions to return an optional predicate that indicates if data\nat specified coordinates is resident in memory.\nExtends single-bit\nwmma\nand\nmma\ninstructions to support\n.and\noperation.\nExtends\nmma\ninstruction to support\n.sp\nmodifier that allows matrix multiply-accumulate\noperation when input matrix A is sparse.\nExtends\nmbarrier.test_wait\ninstruction to test the completion of specific phase parity.\nSemantic Changes and Clarifications\nNone.\n13.20.\nChanges in PTX ISA Version 7.0\n\nNew Features\nPTX ISA version 7.0 introduces the following new features:\nSupport for\nsm_80\ntarget architecture.\nAdds support for asynchronous copy instructions that allow copying of data asynchronously from one\nstate space to another.\nAdds support for\nmbarrier\ninstructions that allow creation of\nmbarrier objects\nin memory and\nuse of these objects to synchronize threads and asynchronous copy operations initiated by threads.\nAdds support for\nredux.sync\ninstruction which allows reduction operation across threads in a\nwarp.\nAdds support for new alternate floating-point data formats\n.bf16\nand\n.tf32\n.\nExtends\nwmma\ninstruction to support\n.f64\ntype with shape\n.m8n8k4\n.\nExtends\nwmma\ninstruction to support\n.bf16\ndata format.\nExtends\nwmma\ninstruction to support\n.tf32\ndata format with shape\n.m16n16k8\n.\nExtends\nmma\ninstruction to support\n.f64\ntype with shape\n.m8n8k4\n.\nExtends\nmma\ninstruction to support\n.bf16\nand\n.tf32\ndata formats with shape\n.m16n8k8\n.\nExtends\nmma\ninstruction to support new shapes\n.m8n8k128\n,\n.m16n8k4\n,\n.m16n8k16\n,\n.m16n8k32\n,\n.m16n8k64\n,\n.m16n8k128\nand\n.m16n8k256\n.\nExtends\nabs\nand\nneg\ninstructions to support\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\nmin\nand\nmax\ninstructions to support\n.NaN\nmodifier and\n.f16\n,\n.f16x2\n,\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\nfma\ninstruction to support\n.relu\nsaturation mode and\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\ncvt\ninstruction to support\n.relu\nsaturation mode and\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\nand\n.tf32\ndestination formats.\nAdds support for\ntanh\ninstruction that computes hyperbolic-tangent.\nExtends\nex2\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nSemantic Changes and Clarifications\nNone.\n13.21.\nChanges in PTX ISA Version 6.5\n\nNew Features\nPTX ISA version 6.5 introduces the following new features:\nAdds support for integer destination types for half precision comparison instruction\nset\n.\nExtends\nabs\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nAdds support for\ncvt.pack\ninstruction which allows converting two integer values and packing\nthe results together.\nAdds new shapes\n.m16n8k8\n,\n.m8n8k16\nand\n.m8n8k32\non the\nmma\ninstruction.\nAdds support for\nldmatrix\ninstruction which loads one or more matrices from shared memory for\nmma\ninstruction.\nRemoved Features\nPTX ISA version 6.5 removes the following features:\nSupport for\n.satfinite\nqualifier on floating point\nwmma.mma\ninstruction has been\nremoved. This support was deprecated since PTX ISA version 6.4.\nSemantic Changes and Clarifications\nNone.\n13.22.\nChanges in PTX ISA Version 6.4\n\nNew Features\nPTX ISA version 6.4 introduces the following new features:\nAdds support for\n.noreturn\ndirective which can be used to indicate a function does not return\nto it’s caller function.\nAdds support for\nmma\ninstruction which allows performing matrix multiply-and-accumulate\noperation.\nDeprecated Features\nPTX ISA version 6.4 deprecates the following features:\nSupport for\n.satfinite\nqualifier on floating point\nwmma.mma\ninstruction.\nRemoved Features\nPTX ISA version 6.4 removes the following features:\nSupport for\nshfl\nand\nvote\ninstructions without the\n.sync\nqualifier has been removed\nfor\n.target\nsm_70\nand higher. This support was deprecated since PTX ISA version 6.0 as\ndocumented in PTX ISA version 6.2.\nSemantic Changes and Clarifications\nClarified that resolving references of a\n.weak\nsymbol considers only\n.weak\nor\n.visible\nsymbols with the same name and does not consider local symbols with the same name.\nClarified that in\ncvt\ninstruction, modifier\n.ftz\ncan only be specified when either\n.atype\nor\n.dtype\nis\n.f32\n.\n13.23.\nChanges in PTX ISA Version 6.3\n\nNew Features\nPTX ISA version 6.3 introduces the following new features:\nSupport for\nsm_75\ntarget architecture.\nAdds support for a new instruction\nnanosleep\nthat suspends a thread for a specified duration.\nAdds support for\n.alias\ndirective which allows definining alias to function symbol.\nExtends\natom\ninstruction to perform\n.f16\naddition operation and\n.cas.b16\noperation.\nExtends\nred\ninstruction to perform\n.f16\naddition operation.\nThe\nwmma\ninstructions are extended to support multiplicand matrices of type\n.s8\n,\n.u8\n,\n.s4\n,\n.u4\n,\n.b1\nand accumulator matrices of type\n.s32\n.\nSemantic Changes and Clarifications\nIntroduced the mandatory\n.aligned\nqualifier for all\nwmma\ninstructions.\nSpecified the alignment required for the base address and stride parameters passed to\nwmma.load\nand\nwmma.store\n.\nClarified that layout of fragment returned by\nwmma\noperation is architecture dependent and\npassing\nwmma\nfragments around functions compiled for different link compatible SM\narchitectures may not work as expected.\nClarified that atomicity for\n{atom/red}.f16x2}\noperations is guranteed separately for each of\nthe two\n.f16\nelements but not guranteed to be atomic as single 32-bit access.\n13.24.\nChanges in PTX ISA Version 6.2\n\nNew Features\nPTX ISA version 6.2 introduces the following new features:\nA new instruction\nactivemask\nfor querying active threads in a warp.\nExtends atomic and reduction instructions to perform\n.f16x2\naddition operation with mandatory\n.noftz\nqualifier.\nDeprecated Features\nPTX ISA version 6.2 deprecates the following features:\nThe use of\nshfl\nand\nvote\ninstructions without the\n.sync\nis deprecated retrospectively\nfrom PTX ISA version 6.0, which introduced the\nsm_70\narchitecture that implements\nIndependent Thread Scheduling\n.\nSemantic Changes and Clarifications\nClarified that\nwmma\ninstructions can be used in conditionally executed code only if it is\nknown that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined.\nIn the memory consistency model, the definition of\nmorally strong operations\nwas updated to\nexclude fences from the requirement of\ncomplete overlap\nsince fences do not access memory.\n13.25.\nChanges in PTX ISA Version 6.1\n\nNew Features\nPTX ISA version 6.1 introduces the following new features:\nSupport for\nsm_72\ntarget architecture.\nSupport for new matrix shapes\n32x8x16\nand\n8x32x16\nin\nwmma\ninstruction.\nSemantic Changes and Clarifications\nNone.\n13.26.\nChanges in PTX ISA Version 6.0\n\nNew Features\nPTX ISA version 6.0 introduces the following new features:\nSupport for\nsm_70\ntarget architecture.\nSpecifies the memory consistency model for programs running on\nsm_70\nand later architectures.\nVarious extensions to memory instructions to specify memory synchronization semantics and scopes\nat which such synchronization can be observed.\nNew instruction\nwmma\nfor matrix operations which allows loading matrices from memory,\nperforming multiply-and-accumulate on them and storing result in memory.\nSupport for new\nbarrier\ninstruction.\nExtends\nneg\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nA new instruction\nfns\nwhich allows finding n-th set bit in integer.\nA new instruction\nbar.warp.sync\nwhich allows synchronizing threads in warp.\nExtends\nvote\nand\nshfl\ninstructions with\n.sync\nmodifier which waits for specified\nthreads before executing the\nvote\nand\nshfl\noperation respectively.\nA new instruction\nmatch.sync\nwhich allows broadcasting and comparing a value across threads in\nwarp.\nA new instruction\nbrx.idx\nwhich allows branching to a label indexed from list of potential\ntargets.\nSupport for unsized array parameter for\n.func\nwhich can be used to implement variadic\nfunctions.\nSupport for\n.b16\ninteger type in dwarf-lines.\nSupport for taking address of device function return parameters using\nmov\ninstruction.\nSemantic Changes and Clarifications\nSemantics of\nbar\ninstruction were updated to indicate that executing thread waits for other\nnon-exited threads from it’s warp.\nSupport for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from\nthe spec.\nSupport for taking address of labels, using labels in initializers which was unimplemented has\nbeen removed from the spec.\nSupport for variadic functions which was unimplemented has been removed from the spec.\n13.27.\nChanges in PTX ISA Version 5.0\n\nNew Features\nPTX ISA version 5.0 introduces the following new features:\nSupport for\nsm_60\n,\nsm_61\n,\nsm_62\ntarget architecture.\nExtends atomic and reduction instructions to perform double-precision add operation.\nExtends atomic and reduction instructions to specify\nscope\nmodifier.\nA new\n.common\ndirective to permit linking multiple object files containing declarations of the\nsame symbol with different size.\nA new\ndp4a\ninstruction which allows 4-way dot product with accumulate operation.\nA new\ndp2a\ninstruction which allows 2-way dot product with accumulate operation.\nSupport for special register\n%clock_hi\n.\nSemantic Changes and Clarifications\nSemantics of cache modifiers on\nld\nand\nst\ninstructions were clarified to reflect cache\noperations are treated as performance hint only and do not change memory consistency behavior of the\nprogram.\nSemantics of\nvolatile\noperations on\nld\nand\nst\ninstructions were clarified to reflect how\nvolatile\noperations are handled by optimizing compiler.\n13.28.\nChanges in PTX ISA Version 4.3\n\nNew Features\nPTX ISA version 4.3 introduces the following new features:\nA new\nlop3\ninstruction which allows arbitrary logical operation on 3 inputs.\nAdds support for 64-bit computations in extended precision arithmetic instructions.\nExtends\ntex.grad\ninstruction to support\ncube\nand\nacube\ngeometries.\nExtends\ntld4\ninstruction to support\na2d\n,\ncube\nand\nacube\ngeometries.\nExtends\ntex\nand\ntld4\ninstructions to support optional operands for offset vector and depth\ncompare.\nExtends\ntxq\ninstruction to support querying texture fields from specific LOD.\nSemantic Changes and Clarifications\nNone.\n13.29.\nChanges in PTX ISA Version 4.2\n\nNew Features\nPTX ISA version 4.2 introduces the following new features:\nSupport for\nsm_53\ntarget architecture.\nSupport for arithmetic, comparsion and texture instructions for\n.f16\nand\n.f16x2\ntypes.\nSupport for\nmemory_layout\nfield for surfaces and\nsuq\ninstruction support for querying this\nfield.\nSemantic Changes and Clarifications\nSemantics for parameter passing under ABI were updated to indicate\nld.param\nand\nst.param\ninstructions used for argument passing cannot be predicated.\nSemantics of\n{atom/red}.add.f32\nwere updated to indicate subnormal inputs and results are\nflushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on\nshared memory preserve subnormal inputs and results and don’t flush them to zero.\n13.30.\nChanges in PTX ISA Version 4.1\n\nNew Features\nPTX ISA version 4.1 introduces the following new features:\nSupport for\nsm_37\nand\nsm_52\ntarget architectures.\nSupport for new fields\narray_size\n,\nnum_mipmap_levels\nand\nnum_samples\nfor Textures, and\nthe\ntxq\ninstruction support for querying these fields.\nSupport for new field\narray_size\nfor Surfaces, and the\nsuq\ninstruction support for\nquerying this field.\nSupport for special registers\n%total_smem_size\nand\n%dynamic_smem_size\n.\nSemantic Changes and Clarifications\nNone.\n13.31.\nChanges in PTX ISA Version 4.0\n\nNew Features\nPTX ISA version 4.0 introduces the following new features:\nSupport for\nsm_32\nand\nsm_50\ntarget architectures.\nSupport for 64bit performance counter special registers\n%pm0_64,..,%pm7_64\n.\nA new\nistypep\ninstruction.\nA new instruction,\nrsqrt.approx.ftz.f64\nhas been added to compute a fast approximation of the\nsquare root reciprocal of a value.\nSupport for a new directive\n.attribute\nfor specifying special attributes of a variable.\nSupport for\n.managed\nvariable attribute.\nSemantic Changes and Clarifications\nThe\nvote\ninstruction semantics were updated to clearly indicate that an inactive thread in a\nwarp contributes a 0 for its entry when participating in\nvote.ballot.b32\n.\n13.32.\nChanges in PTX ISA Version 3.2\n\nNew Features\nPTX ISA version 3.2 introduces the following new features:\nThe texture instruction supports reads from multi-sample and multisample array textures.\nExtends\n.section\ndebugging directive to include label + immediate expressions.\nExtends\n.file\ndirective to include timestamp and file size information.\nSemantic Changes and Clarifications\nThe\nvavrg2\nand\nvavrg4\ninstruction semantics were updated to indicate that instruction adds 1\nonly if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than\nbeing divided by 2).\n13.33.\nChanges in PTX ISA Version 3.1\n\nNew Features\nPTX ISA version 3.1 introduces the following new features:\nSupport for\nsm_35\ntarget architecture.\nSupport for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work.\nld.global.nc\nfor loading read-only global data though the non-coherent texture cache.\nA new funnel shift instruction,\nshf\n.\nExtends atomic and reduction instructions to perform 64-bit\n{and,\nor,\nxor}\noperations, and\n64-bit integer\n{min,\nmax}\noperations.\nAdds support for\nmipmaps\n.\nAdds support for indirect access to textures and surfaces.\nExtends support for generic addressing to include the\n.const\nstate space, and adds a new\noperator,\ngeneric()\n, to form a generic address for\n.global\nor\n.const\nvariables used in\ninitializers.\nA new\n.weak\ndirective to permit linking multiple object files containing declarations of the\nsame symbol.\nSemantic Changes and Clarifications\nPTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit\ngeneric()\noperator for each global variable used in an initializer. PTX 3.1 code should either\ninclude explicit\ngeneric()\noperators in initializers, use\ncvta.global\nto form generic\naddresses at runtime, or load from the non-generic address using\nld.global\n.\nInstruction\nmad.f32\nrequires a rounding modifier for\nsm_20\nand higher targets. However for\nPTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and\nmad.f32\nsilently\ndefaults to\nmad.rn.f32\n. For PTX ISA version 3.1, ptxas generates a warning and defaults to\nmad.rn.f32\n, and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later.\n13.34.\nChanges in PTX ISA Version 3.0\n\nNew Features\nPTX ISA version 3.0 introduces the following new features:\nSupport for\nsm_30\ntarget architectures.\nSIMD video instructions.\nA new warp shuffle instruction.\nInstructions\nmad.cc\nand\nmadc\nfor efficient, extended-precision integer multiplication.\nSurface instructions with 3D and array geometries.\nThe texture instruction supports reads from cubemap and cubemap array textures.\nPlatform option\n.target\ndebug to declare that a PTX module contains\nDWARF\ndebug information.\npmevent.mask\n, for triggering multiple performance monitor events.\nPerformance monitor counter special registers\n%pm4..%pm7\n.\nSemantic Changes and Clarifications\nSpecial register\n%gridid\nhas been extended from 32-bits to 64-bits.\nPTX ISA version 3.0 deprecates module-scoped\n.reg\nand\n.local\nvariables when compiling to the\nApplication Binary Interface (ABI). When compiling without use of the ABI, module-scoped\n.reg\nand\n.local\nvariables are supported as before. When compiling legacy PTX code (ISA versions prior\nto 3.0) containing module-scoped\n.reg\nor\n.local\nvariables, the compiler silently disables\nuse of the ABI.\nThe\nshfl\ninstruction semantics were updated to clearly indicate that value of source operand\na\nis unpredictable for inactive and predicated-off threads within the warp.\nPTX modules no longer allow duplicate\n.version\ndirectives. This feature was unimplemented, so\nthere is no semantic change.\nUnimplemented instructions\nsuld.p\nand\nsust.p.{u32,s32,f32}\nhave been removed.\n13.35.\nChanges in PTX ISA Version 2.3\n\nNew Features\nPTX 2.3 adds support for texture arrays. The texture array feature supports access to an array of 1D\nor 2D textures, where an integer indexes into the array of textures, and then one or two\nsingle-precision floating point coordinates are used to address within the selected 1D or 2D\ntexture.\nPTX 2.3 adds a new directive,\n.address_size\n, for specifying the size of addresses.\nVariables in\n.const\nand\n.global\nstate spaces are initialized to zero by default.\nSemantic Changes and Clarifications\nThe semantics of the\n.maxntid\ndirective have been updated to match the current\nimplementation. Specifically,\n.maxntid\nonly guarantees that the total number of threads in a\nthread block does not exceed the maximum. Previously, the semantics indicated that the maximum was\nenforced separately in each dimension, which is not the case.\nBit field extract and insert instructions BFE and BFI now indicate that the\nlen\nand\npos\noperands are restricted to the value range\n0..255\n.\nUnimplemented instructions\n{atom,red}.{min,max}.f32\nhave been removed.\n13.36.\nChanges in PTX ISA Version 2.2\n\nNew Features\nPTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a\nnew directives for specifying that a kernel parameter is a pointer, for specifying to which state\nspace the parameter points, and for optionally specifying the alignment of the memory to which the\nparameter points.\nPTX 2.2 adds a new field named\nforce_unnormalized_coords\nto the\n.samplerref\nopaque\ntype. This field is used in the independent texturing mode to override the\nnormalized_coords\nfield in the texture header. This field is needed to support languages such as OpenCL, which\nrepresent the property of normalized/unnormalized coordinates in the sampler header rather than in\nthe texture header.\nPTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the\n.const\nstate space. Legacy PTX that uses explicit constant banks is still supported.\nPTX 2.2 adds a new\ntld4\ninstruction for loading a component (\nr\n,\ng\n,\nb\n, or\na\n) from\nthe four texels compising the bilinear interpolation footprint of a given texture location. This\ninstruction may be used to compute higher-precision bilerp results in software, or for performing\nhigher-bandwidth texture loads.\nSemantic Changes and Clarifications\nNone.\n13.37.\nChanges in PTX ISA Version 2.1\n\nNew Features\nThe underlying, stack-based ABI is supported in PTX ISA version 2.1 for\nsm_2x\ntargets.\nSupport for indirect calls has been implemented for\nsm_2x\ntargets.\nNew directives,\n.branchtargets\nand\n.calltargets\n, have been added for specifying potential\ntargets for indirect branches and indirect function calls. A\n.callprototype\ndirective has been\nadded for declaring the type signatures for indirect function calls.\nThe names of\n.global\nand\n.const\nvariables can now be specified in variable initializers to\nrepresent their addresses.\nA set of thirty-two driver-specific execution environment special registers has been added. These\nare named\n%envreg0..%envreg31\n.\nTextures and surfaces have new fields for channel data type and channel order, and the\ntxq\nand\nsuq\ninstructions support queries for these fields.\nDirective\n.minnctapersm\nhas replaced the\n.maxnctapersm\ndirective.\nDirective\n.reqntid\nhas been added to allow specification of exact CTA dimensions.\nA new instruction,\nrcp.approx.ftz.f64\n, has been added to compute a fast, gross approximate\nreciprocal.\nSemantic Changes and Clarifications\nA warning is emitted if\n.minnctapersm\nis specified without also specifying\n.maxntid\n.\n13.38.\nChanges in PTX ISA Version 2.0\n\nNew Features\nFloating Point Extensions\nThis section describes the floating-point changes in PTX ISA version 2.0 for\nsm_20\ntargets. The\ngoal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility\nwith legacy PTX ISA version 1.x code and\nsm_1x\ntargets.\nThe changes from PTX ISA version 1.x are as follows:\nSingle-precision instructions support subnormal numbers by default for\nsm_20\ntargets. The\n.ftz\nmodifier may be used to enforce backward compatibility with\nsm_1x\n.\nSingle-precision\nadd\n,\nsub\n, and\nmul\nnow support\n.rm\nand\n.rp\nrounding modifiers\nfor\nsm_20\ntargets.\nA single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754\ncompliant rounding modifiers and support for subnormal numbers. The\nfma.f32\ninstruction also\nsupports\n.ftz\nand\n.sat\nmodifiers.\nfma.f32\nrequires\nsm_20\n. The\nmad.f32\ninstruction has been extended with rounding modifiers so that it’s synonymous with\nfma.f32\nfor\nsm_20\ntargets. Both\nfma.f32\nand\nmad.f32\nrequire a rounding modifier for\nsm_20\ntargets.\nThe\nmad.f32\ninstruction\nwithout rounding\nis retained so that compilers can generate code for\nsm_1x\ntargets. When code compiled for\nsm_1x\nis executed on\nsm_20\ndevices,\nmad.f32\nmaps to\nfma.rn.f32\n.\nSingle- and double-precision\ndiv\n,\nrcp\n, and\nsqrt\nwith IEEE 754 compliant rounding have\nbeen added. These are indicated by the use of a rounding modifier and require\nsm_20\n.\nInstructions\ntestp\nand\ncopysign\nhave been added.\nNew Instructions\nA\nload uniform\ninstruction,\nldu\n, has been added.\nSurface instructions support additional\n.clamp\nmodifiers,\n.clamp\nand\n.zero\n.\nInstruction\nsust\nnow supports formatted surface stores.\nA\ncount leading zeros\ninstruction,\nclz\n, has been added.\nA\nfind leading non-sign bit instruction\n,\nbfind\n, has been added.\nA\nbit reversal\ninstruction,\nbrev\n, has been added.\nBit field extract and insert instructions,\nbfe\nand\nbfi\n, have been added.\nA\npopulation count\ninstruction,\npopc\n, has been added.\nA\nvote ballot\ninstruction,\nvote.ballot.b32\n, has been added.\nInstructions\n{atom,red}.add.f32\nhave been implemented.\nInstructions\n{atom,red}\n.shared have been extended to handle 64-bit data types for\nsm_20\ntargets.\nA system-level membar instruction,\nmembar.sys\n, has been added.\nThe\nbar\ninstruction has been extended as follows:\nA\nbar.arrive\ninstruction has been added.\nInstructions\nbar.red.popc.u32\nand\nbar.red.{and,or}.pred\nhave been added.\nbar\nnow supports optional thread count and register operands.\nScalar video instructions (includes\nprmt\n) have been added.\nInstruction\nisspacep\nfor querying whether a generic address falls within a specified state space\nwindow has been added.\nInstruction\ncvta\nfor converting global, local, and shared addresses to generic address and\nvice-versa has been added.\nOther New Features\nInstructions\nld\n,\nldu\n,\nst\n,\nprefetch\n,\nprefetchu\n,\nisspacep\n,\ncvta\n,\natom\n,\nand\nred\nnow support generic addressing.\nNew special registers\n%nwarpid\n,\n%nsmid\n,\n%clock64\n,\n%lanemask_{eq,le,lt,ge,gt}\nhave\nbeen added.\nCache operations have been added to instructions\nld\n,\nst\n,\nsuld\n, and\nsust\n, e.g., for\nprefetching\nto specified level of memory hierarchy. Instructions\nprefetch\nand\nprefetchu\nhave also been added.\nThe\n.maxnctapersm\ndirective was deprecated and replaced with\n.minnctapersm\nto better match\nits behavior and usage.\nA new directive,\n.section\n, has been added to replace the\n@@DWARF\nsyntax for passing\nDWARF-format debugging information through PTX.\nA new directive,\n.pragma\nnounroll\n, has been added to allow users to disable loop unrolling.\nSemantic Changes and Clarifications\nThe errata in\ncvt.ftz\nfor PTX ISA versions 1.4 and earlier, where single-precision subnormal\ninputs and results were not flushed to zero if either source or destination type size was 64-bits,\nhas been fixed. In PTX ISA version 1.5 and later,\ncvt.ftz\n(and\ncvt\nfor\n.target\nsm_1x\n,\nwhere\n.ftz\nis implied) instructions flush single-precision subnormal inputs and results to\nsign-preserving zero for all combinations of floating-point instruction types. To maintain\ncompatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs\nand results are flushed to sign-preserving zero only when neither source nor destination type size\nis 64-bits.\nComponents of special registers\n%tid\n,\n%ntid\n,\n%ctaid\n, and\n%nctaid\nhave been extended\nfrom 16-bits to 32-bits. These registers now have type\n.v4.u32\n.\nThe number of samplers available in independent texturing mode was incorrectly listed as thirty-two\nin PTX ISA version 1.5; the correct number is sixteen.\n14.\nNotices\n\n14.1.\nNotice\n\nThis document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\n14.2.\nOpenCL\n\nOpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.\n14.3.\nTrademarks\n\nNVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.\nPrivacy Policy\n|\nManage My Privacy\n|\nDo Not Sell or Share My Data\n|\nTerms of Service\n|\nAccessibility\n|\nCorporate Policies\n|\nProduct Security\n|\nContact\nCopyright © 2007-2026, NVIDIA Corporation & affiliates. All rights reserved.\nLast updated on Jan 08, 2026."}}
{"ts": 1770916551.985794, "event": "final_llm_input", "payload": {"messages": [{"role": "system", "content": "You are a synthesis agent. Produce the final deliverable using the mandate, execution history, final notes, accumulated deliverables, and retrieved context. Be accurate and complete, but avoid unnecessary verbosity. Return JSON with keys: 'deliverable' and 'summary'."}, {"role": "user", "content": "MANDATE\n=======\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nEXECUTION HISTORY\n=================\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n4. [Tick 4] Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.\n\nNOTES\n=====\n1. Next: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).\n2. Next: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.\n3. Search PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.\n4. Next: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.\n\nRETRIEVED CONTEXT\n=================\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] CVE-2022-40982 public disclosure was August 10, 2023 per NVD. Research by Daniel Moghimi (downfall.page) with PoC code linked from the project site to GitHub. Intel advisory is INTEL-SA-00828 naming it Gather Data Sampling (GDS).\n[3] Downfall (CVE-2022-40982) exploits Intel's vector 'Gather' instruction (vgather* variants) allowing transient/execution leakage of the internal vector register file. Intel advisory INTEL-SA-00828 names this Gather Data Sampling (GDS) issue and credits researcher Daniel Moghimi.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] Linux kernel added documentation and mitigation support for Gather Data Sampling under Documentation/admin-guide/hw-vuln/gather_data_sampling.rst and related x86 mitigation code; distributions offer kernel/microcode updates and a gather_data_sampling= kernel parameter to control mitigation behavior.\n[6] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[9] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] OpenSSH release-9.8 / 9.8p1 advisory notes the regreSSHion (CVE-2024-6387) fix: Portable OpenSSH 8.5p1–9.7p1 were affected and the 9.8/9.8p1 release contains the vendor fix; advisory lists affected versions and mitigation details.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution."}]}}
{"ts": 1770916841.560254, "event": "final_output", "payload": {"correlation_id": null, "final_deliverable": "We could not complete the full mandate due to unavailable LLM service. ", "action_summary": "", "success": false, "metrics": {"ticks": 5, "searches": 2, "search_results": 20, "visits": 1, "visited_urls": ["https://docs.nvidia.com/cuda/parallel-thread-execution/index.html"], "observations_chars": 1204016, "observations_bytes": 1207147, "prompt_chars": 1236357, "prompt_bytes": 1239548, "llm_output_chars": 3981, "llm_output_bytes": 3983, "llm_prompt_tokens": 8460, "llm_completion_tokens": 7650, "llm_total_tokens": 16110, "llm_calls": 4, "llm_inputs": [[{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n1 / 100"}], [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n2 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n\nNOTES:\nNext: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[2] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[3] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[4] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[5] NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides official description, references to Intel advisory INTEL-SA-00828 and the researcher site downfall.page, and shows the public publication date as 2023-08-11.\n[6] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[7] Researcher site (downfall.page) by Daniel Moghimi explains that Downfall exploits Intel’s vector Gather instruction (AVX2/AVX-512) during speculative execution to leak stale data from internal vector register files across privilege boundaries. It includes paper links, timelines, and PoC references.\n[8] Public proof-of-concept code for Downfall is hosted on GitHub in the flowyroll/downfall repository; the POC directory contains demos that exploit the Gather instruction to demonstrate transient leakage from vector registers.\n[9] Downfall (CVE-2022-40982) exploits Intel’s Gather instruction during speculative execution to leak data from the internal vector register file, enabling cross-privilege and cross-process data exfiltration (e.g., AES keys, kernel data). The researcher provides PoC code and demos, and reports initial disclosure to Intel on 2022-08-24; public presentations in Aug 2023.\n[10] NVD entry for CVE-2022-40982 (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state in Intel vector execution units. NVD published date is 2023-08-10. References include Intel advisory INTEL-SA-00828 and the researcher site downfall.page, plus various vendor advisories. CVSS v3.1 (CNA) base score 6.5. Use this as primary CVE citation.\n[11] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[12] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n\nOBSERVATIONS:\nWeb search for 'PTX wmma.mma.sync syntax site:nvidia.com PDF':\n- https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html (https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html)\n9.7.14.3. Block Scaling for mma.sync · 9.7.14.4. Matrix multiply-accumulate operation using wmma instructions · 9.7.14.4.1. Matrix Fragments for WMMA · 9.7.14.4.2. Matrix Storage for WMMA · 9.7.14.4.3. Warp-level Matrix Load Instruction: wmma.load · 9.7.14.4.4. Warp-level Matrix Store Instruction: wmma.store ·\n- https://docs.nvidia.com/cuda/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://forums.developer.nvidia.com/t/understand-the-mma-instruction-in-ptx/294471 (https://forums.developer.nvidia.com/t/understand-the-mma-instruction-in-ptx/294471)\nHowever I can not understand why. ... {%4,%5,%6,%7}, {%8,%9}, {\u0010,\u0011,\u0012,\u0013};\\n&quot; : <strong>&quot;=f&quot;(C[cd[0]]), // D[0] 32bit &quot;=f&quot;(C[cd[1]]), // D[1] &quot;=f&quot;(C[cd[2]]), // D[2] &quot;=f&quot;(C[cd[3]])</strong> ......\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf)\n9.7.12.15 Parallel Synchronization and Communication Instructions: mbarrier ... Warp Level Matrix Multiply-Accumulate Instructions . . . . . . . . . . . . . . . . . . . . 315 ... Matrix Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 ... Matrix Data-types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318 ... Matrix multiply-accumulate operation using wmma instructions .\n- https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf (https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf)\nVOLTA MMA.SYNC · <strong>D = A * B + C</strong> · PTX Syntax · mma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype · d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .ctype · = {.f16, .f32}; .dtype · = {.f16, .f32}; d: 8 x .dtype · a: 4 x .f16 · b: 4 x .f16 ·\n- https://forums.developer.nvidia.com/t/complete-minimal-ptx-example-for-mma-sync-aligned-m16n8k16-row-col-f32-f16-f16-f32/311164 (https://forums.developer.nvidia.com/t/complete-minimal-ptx-example-for-mma-sync-aligned-m16n8k16-row-col-f32-f16-f16-f32/311164)\nHere is my minimal example: (doesn’t work) // tensor_mma_kernel.ptx .version 8.4 .target sm_89 .address_size 64 // Entry point for the kernel .entry tensor_mma_kernel( .param.u64.ptr.global.align 16 A_ptr, // Pointer to matrix A in global memory .param.u64.ptr.global.align 16 B_ptr, // Pointer to matrix B in global memory .param.u64.ptr.global.align 16 C_ptr // Pointer to matrix C in global memory ) { // Declare registers for pointers .reg .u64 %ra, %rb, %rc; ...\n- https://forums.developer.nvidia.com/t/why-does-wmma-and-mma-support-different-matrix-tile-size/271067 (https://forums.developer.nvidia.com/t/why-does-wmma-and-mma-support-different-matrix-tile-size/271067)\nI want to do 8-bit matrix multiplications with tensor cores on A6000. According to the PTX guide, <strong>the WMMA instructions support m16n16k16, m8n32k16, and m32n8k16 for 8-bit matrix multiplication</strong>.\n- https://forums.developer.nvidia.com/t/inline-ptx-and-wmma-instructions/285763 (https://forums.developer.nvidia.com/t/inline-ptx-and-wmma-instructions/285763)\nSome WMMA instructions are supposed to take a sequence of 8 values together, like so: <strong>wmma.load.a.sync.aligned.row.m16n16k16.global.f16 {%r26, %r27, %r28, %r29, %r30, %r31, %r32, %r33}, [%rd4], %r25;</strong> Now, here is…\n- https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://forums.developer.nvidia.com/t/confused-about-a-b-layout-on-mma-sync-instructions/286776 (https://forums.developer.nvidia.com/t/confused-about-a-b-layout-on-mma-sync-instructions/286776)\nHello, I was wondering about the .alayout and .blayout modifiers on mma instructions. The layout seems to be very strict and described in sections 9.7.13.4.[1-13] of the PTX ISA specification. Could I get some informati…"}], [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n3 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n\nNOTES:\nNext: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[2] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[3] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[4] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) shows description, references (Intel SA-00828, downfall.page), and the NVD Published Date 2023-08-10. It anchors official metadata and links for building the citation chain.\n[5] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state after transient execution in Intel vector execution units. It lists an NVD published date of 2023-08-10 and references Intel’s INTEL-SA-00828 advisory and the researcher site (downfall.page), providing authoritative citations and a starting point for source tracing.\n[6] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[7] Public proof-of-concept code for Downfall is hosted on GitHub in the flowyroll/downfall repository; the POC directory contains demos that exploit the Gather instruction to demonstrate transient leakage from vector registers.\n[8] Downfall (Gather Data Sampling, CVE-2022-40982) exploits Intel's AVX gather instructions (VGATHER family in AVX2/AVX-512). The transient-execution primitive leaks vector-register or microarchitectural state populated by gather operations, enabling cross-boundary data exposure during speculative execution.\n[9] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n\nOBSERVATIONS:\n[Action error: ActionType.VISIT] HTTP visit failed: https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf status=None"}], [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n4 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n\nNOTES:\nSearch PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] Authoritative sources required: NVD CVE entry for dates/refs, Intel INTEL-SA-00828 advisory for mechanism/mitigation, researcher site (downfall.page) with paper and PoC, and a Linux kernel commit that implements GDS mitigation (docs + gather_data_sampling= parameter).\n[3] NVD entry for CVE-2022-40982 (Downfall/Gather Data Sampling) describes information exposure through microarchitectural state in Intel vector execution units. NVD published date is 2023-08-10. References include Intel advisory INTEL-SA-00828 and the researcher site downfall.page, plus various vendor advisories. CVSS v3.1 (CNA) base score 6.5. Use this as primary CVE citation.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] NVD CVE-2022-40982 (Downfall/Gather Data Sampling) provides the official CVE description, publish date (2023-08-08), and references to Intel SA-00828 and the researcher site. It describes a transient execution side-channel allowing data leakage via gather operations on affected Intel CPUs.\n[6] Downfall (CVE-2022-40982), aka Gather Data Sampling (GDS), exploits transient execution in Intel vector 'gather' instructions, enabling leakage from vector registers and microarchitectural buffers. Researcher Daniel Moghimi published technical writeups and a PoC at the flowyroll/downfall GitHub repository; Intel published INTEL-SA-00828 with mitigation guidance.\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] Researcher site (downfall.page) by Daniel Moghimi explains that Downfall exploits Intel’s vector Gather instruction (AVX2/AVX-512) during speculative execution to leak stale data from internal vector register files across privilege boundaries. It includes paper links, timelines, and PoC references.\n[9] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] Downfall (Gather Data Sampling, CVE-2022-40982) exploits Intel's AVX gather instructions (VGATHER family in AVX2/AVX-512). The transient-execution primitive leaks vector-register or microarchitectural state populated by gather operations, enabling cross-boundary data exposure during speculative execution.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n[13] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[14] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[15] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n\nOBSERVATIONS:\nWeb search for 'NVIDIA PTX ISA wmma.mma.sync.aligned site:docs.nvidia.com':\n- https://docs.nvidia.com/cuda/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html (https://docs.nvidia.com/cuda/parallel-thread-execution/contents.html)\n9.7.13.18. Parallel Synchronization and Communication Instructions: clusterlaunchcontrol.query_cancel · 9.7.14. Warp Level Matrix Multiply-Accumulate Instructions ... 9.7.15.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf)\n9.7.12.15 Parallel Synchronization and Communication Instructions: mbarrier ... Warp Level Matrix Multiply-Accumulate Instructions . . . . . . . . . . . . . . . . . . . . 315 ... Matrix Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 ... Matrix Data-types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318 ... Matrix multiply-accumulate operation using wmma instructions .\n- https://docs.nvidia.com/cuda/pdf/ptx_isa_8.7.pdf (https://docs.nvidia.com/cuda/pdf/ptx_isa_8.7.pdf)\nThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).\n- https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/12.9.0/hopper-tuning-guide/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.9.0/hopper-tuning-guide/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/10.1/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/10.1/parallel-thread-execution/index.html)\nWhen compiling to use the Application Binary Interface (ABI), register variables are restricted to function scope and may not be declared at module scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped .reg variables, the compiler silently disables use of the ABI. Registers may have alignment boundaries required by multi-word loads and stores.\n- https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html (https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html)\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: ... Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async instructions.\n- https://docs.nvidia.com/cuda/archive/10.2/pdf/ptx_isa_6.5.pdf (https://docs.nvidia.com/cuda/archive/10.2/pdf/ptx_isa_6.5.pdf)\nThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA)."}], [{"role": "system", "content": "You are a cost-optimized autonomous agent operating in discrete ticks. Each tick is a small step: decide, act, store, retrieve. Use tick count to plan and update notes with a brief plan. Minimize generated text each tick; reading is cheaper than writing. Use the vector database aggressively as primary memory. It is acceptable to spend multiple ticks only retrieving or only storing facts. Final deliverables can be deferred; small partials are ok when useful.\n\nINPUTS:\n- MANDATE: immutable goal.\n- TICK STATUS: current tick and max ticks.\n- SHORT TERM MEMORY: brief recent summaries.\n- NOTES: tiny scratchpad, keep minimal.\n- RETRIEVED CONTEXT: vector DB hits from prior cache_retrieve.\n- OBSERVATIONS: new external input (may be long, full pages are ok).\n\nMEMORY STRATEGY:\n- Always check vector memory before searching the internet.\n- Use NOTES as pointers to memory (what to retrieve next); keep them short.\n- Anything not written to NOTES or stored in vector DB will be lost next tick.\n- Rely on cache_update/cache_retrieve every tick to reduce context growth.\n- Store small, atomic facts with strong titles and tags in metadata.\n- You can query memory like a search engine; use precise queries.\n- If a visit returns 403/blocked, do not retry that site; pivot to other sources.\n- When blocked, log the block in notes and shift strategy or revisit planning.\n- If you determine you are stuck (blocked with no alternatives), use exit and return what you have.\n- If unclear, do more searches/visits rather than long reasoning.\n\nOUTPUT FORMAT (JSON only, exact keys):\n* history_update: 1-2 concise sentences about what happened and why.\n* note_update: 1-2 short sentences; keep minimal, prefer cache_update.\n* cache_update: list of dicts for vector DB storage.\n  Format: [{'document': '...', 'metadata': {'KEY': 'VALUE'}}]\n  - document: ~40-80 words focused on a single fact/idea.\n  - metadata values must be scalars; use comma-separated strings for multi-values.\n  - include metadata.title and metadata.topics for better retrieval.\n* next_action: 'ACTION, PARAM'\n* cache_retrieve: list of short query sentences.\n* deliverable: empty unless you have a usable partial or final output.\nNo extra keys or commentary.\n\nACTIONS (case sensitive):\n* think : internal reasoning, no param.\n* search : web search. Example: 'search, query'.\n* visit : visit URL. Example: 'visit, https://...'.\n* exit : end program, only when finished."}, {"role": "user", "content": "MANDATE:\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nTICK STATUS:\n5 / 100\n\nSHORT TERM MEMORY (recent history):\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n4. [Tick 4] Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.\n\nNOTES:\nNext: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.\n\nRETRIEVED LONG-TERM CONTEXT:\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] CVE-2022-40982 public disclosure was August 10, 2023 per NVD. Research by Daniel Moghimi (downfall.page) with PoC code linked from the project site to GitHub. Intel advisory is INTEL-SA-00828 naming it Gather Data Sampling (GDS).\n[3] Downfall (CVE-2022-40982) exploits Intel's vector 'Gather' instruction (vgather* variants) allowing transient/execution leakage of the internal vector register file. Intel advisory INTEL-SA-00828 names this Gather Data Sampling (GDS) issue and credits researcher Daniel Moghimi.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] Linux kernel added documentation and mitigation support for Gather Data Sampling under Documentation/admin-guide/hw-vuln/gather_data_sampling.rst and related x86 mitigation code; distributions offer kernel/microcode updates and a gather_data_sampling= kernel parameter to control mitigation behavior.\n[6] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[9] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] OpenSSH release-9.8 / 9.8p1 advisory notes the regreSSHion (CVE-2024-6387) fix: Portable OpenSSH 8.5p1–9.7p1 were affected and the 9.8/9.8p1 release contains the vendor fix; advisory lists affected versions and mitigation details.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution.\n\nOBSERVATIONS:\nVisited https://docs.nvidia.com/cuda/parallel-thread-execution/index.html:\n1. Introduction — PTX ISA 9.1 documentation\n1. Introduction\n1.1. Scalable Data-Parallel Computing using GPUs\n1.2. Goals of PTX\n1.3. PTX ISA Version 9.1\n1.4. Document Structure\n2. Programming Model\n2.1. A Highly Multithreaded Coprocessor\n2.2. Thread Hierarchy\n2.2.1. Cooperative Thread Arrays\n2.2.2. Cluster of Cooperative Thread Arrays\n2.2.3. Grid of Clusters\n2.3. Memory Hierarchy\n3. PTX Machine Model\n3.1. A Set of SIMT Multiprocessors\n3.2. Independent Thread Scheduling\n3.3. On-chip Shared Memory\n4. Syntax\n4.1. Source Format\n4.2. Comments\n4.3. Statements\n4.3.1. Directive Statements\n4.3.2. Instruction Statements\n4.4. Identifiers\n4.5. Constants\n4.5.1. Integer Constants\n4.5.2. Floating-Point Constants\n4.5.3. Predicate Constants\n4.5.4. Constant Expressions\n4.5.5. Integer Constant Expression Evaluation\n4.5.6. Summary of Constant Expression Evaluation Rules\n5. State Spaces, Types, and Variables\n5.1. State Spaces\n5.1.1. Register State Space\n5.1.2. Special Register State Space\n5.1.3. Constant State Space\n5.1.3.1. Banked Constant State Space (deprecated)\n5.1.4. Global State Space\n5.1.5. Local State Space\n5.1.6. Parameter State Space\n5.1.6.1. Kernel Function Parameters\n5.1.6.2. Kernel Function Parameter Attributes\n5.1.6.3. Kernel Parameter Attribute:\n.ptr\n5.1.6.4. Device Function Parameters\n5.1.7. Shared State Space\n5.1.8. Texture State Space (deprecated)\n5.2. Types\n5.2.1. Fundamental Types\n5.2.2. Restricted Use of Sub-Word Sizes\n5.2.3. Alternate Floating-Point Data Formats\n5.2.4. Fixed-point Data format\n5.2.5. Packed Data Types\n5.2.5.1. Packed Floating Point Data Types\n5.2.5.2. Packed Integer Data Types\n5.2.5.3. Packed Fixed-Point Data Types\n5.3. Texture Sampler and Surface Types\n5.3.1. Texture and Surface Properties\n5.3.2. Sampler Properties\n5.3.3. Channel Data Type and Channel Order Fields\n5.4. Variables\n5.4.1. Variable Declarations\n5.4.2. Vectors\n5.4.3. Array Declarations\n5.4.4. Initializers\n5.4.5. Alignment\n5.4.6. Parameterized Variable Names\n5.4.7. Variable Attributes\n5.4.8. Variable and Function Attribute Directive:\n.attribute\n5.5. Tensors\n5.5.1. Tensor Dimension, size and format\n5.5.1.1. Sub-byte Types\n5.5.1.1.1. Padding and alignment of the sub-byte types\n5.5.2. Tensor Access Modes\n5.5.3. Tiled Mode\n5.5.3.1. Bounding Box\n5.5.3.2. Traversal-Stride\n5.5.3.3. Out of Boundary Access\n5.5.3.4.\n.tile::scatter4\nand\n.tile::gather4\nmodes\n5.5.3.4.1. Bounding Box\n5.5.4.\nim2col\nmode\n5.5.4.1. Bounding Box\n5.5.4.2. Traversal Stride\n5.5.4.3. Out of Boundary Access\n5.5.5.\nim2col::w\nand\nim2col::w::128\nmodes\n5.5.5.1. Bounding Box\n5.5.5.2. Traversal Stride\n5.5.5.3.\nwHalo\n5.5.5.4.\nwOffset\n5.5.6. Interleave layout\n5.5.7. Swizzling Modes\n5.5.8. Tensor-map\n6. Instruction Operands\n6.1. Operand Type Information\n6.2. Source Operands\n6.3. Destination Operands\n6.4. Using Addresses, Arrays, and Vectors\n6.4.1. Addresses as Operands\n6.4.1.1. Generic Addressing\n6.4.2. Arrays as Operands\n6.4.3. Vectors as Operands\n6.4.4. Labels and Function Names as Operands\n6.5. Type Conversion\n6.5.1. Scalar Conversions\n6.5.2. Rounding Modifiers\n6.6. Operand Costs\n7. Abstracting the ABI\n7.1. Function Declarations and Definitions\n7.1.1. Changes from PTX ISA Version 1.x\n7.2. Variadic Functions\n7.3. Alloca\n8. Memory Consistency Model\n8.1. Scope and applicability of the model\n8.1.1. Limitations on atomicity at system scope\n8.2. Memory operations\n8.2.1. Overlap\n8.2.2. Aliases\n8.2.3. Multimem Addresses\n8.2.4. Memory Operations on Vector Data Types\n8.2.5. Memory Operations on Packed Data Types\n8.2.6. Initialization\n8.3. State spaces\n8.4. Operation types\n8.4.1. mmio Operation\n8.4.2. volatile Operation\n8.5. Scope\n8.6. Proxies\n8.7. Morally strong operations\n8.7.1. Conflict and Data-races\n8.7.2. Limitations on Mixed-size Data-races\n8.8. Release and Acquire Patterns\n8.9. Ordering of memory operations\n8.9.1. Program Order\n8.9.1.1. Asynchronous Operations\n8.9.2. Observation Order\n8.9.3. Fence-SC Order\n8.9.4. Memory synchronization\n8.9.5. Causality Order\n8.9.6. Coherence Order\n8.9.7. Communication Order\n8.10. Axioms\n8.10.1. Coherence\n8.10.2. Fence-SC\n8.10.3. Atomicity\n8.10.4. No Thin Air\n8.10.5. Sequential Consistency Per Location\n8.10.6. Causality\n8.11. Special Cases\n8.11.1. Reductions do not form Acquire Patterns\n9. Instruction Set\n9.1. Format and Semantics of Instruction Descriptions\n9.2. PTX Instructions\n9.3. Predicated Execution\n9.3.1. Comparisons\n9.3.1.1. Integer and Bit-Size Comparisons\n9.3.1.2. Floating Point Comparisons\n9.3.2. Manipulating Predicates\n9.4. Type Information for Instructions and Operands\n9.4.1. Operand Size Exceeding Instruction-Type Size\n9.5. Divergence of Threads in Control Constructs\n9.6. Semantics\n9.6.1. Machine-Specific Semantics of 16-bit Code\n9.7. Instructions\n9.7.1. Integer Arithmetic Instructions\n9.7.1.1. Integer Arithmetic Instructions:\nadd\n9.7.1.2. Integer Arithmetic Instructions:\nsub\n9.7.1.3. Integer Arithmetic Instructions:\nmul\n9.7.1.4. Integer Arithmetic Instructions:\nmad\n9.7.1.5. Integer Arithmetic Instructions:\nmul24\n9.7.1.6. Integer Arithmetic Instructions:\nmad24\n9.7.1.7. Integer Arithmetic Instructions:\nsad\n9.7.1.8. Integer Arithmetic Instructions:\ndiv\n9.7.1.9. Integer Arithmetic Instructions:\nrem\n9.7.1.10. Integer Arithmetic Instructions:\nabs\n9.7.1.11. Integer Arithmetic Instructions:\nneg\n9.7.1.12. Integer Arithmetic Instructions:\nmin\n9.7.1.13. Integer Arithmetic Instructions:\nmax\n9.7.1.14. Integer Arithmetic Instructions:\npopc\n9.7.1.15. Integer Arithmetic Instructions:\nclz\n9.7.1.16. Integer Arithmetic Instructions:\nbfind\n9.7.1.17. Integer Arithmetic Instructions:\nfns\n9.7.1.18. Integer Arithmetic Instructions:\nbrev\n9.7.1.19. Integer Arithmetic Instructions:\nbfe\n9.7.1.20. Integer Arithmetic Instructions:\nbfi\n9.7.1.21. Integer Arithmetic Instructions:\nszext\n9.7.1.22. Integer Arithmetic Instructions:\nbmsk\n9.7.1.23. Integer Arithmetic Instructions:\ndp4a\n9.7.1.24. Integer Arithmetic Instructions:\ndp2a\n9.7.2. Extended-Precision Integer Arithmetic Instructions\n9.7.2.1. Extended-Precision Arithmetic Instructions:\nadd.cc\n9.7.2.2. Extended-Precision Arithmetic Instructions:\naddc\n9.7.2.3. Extended-Precision Arithmetic Instructions:\nsub.cc\n9.7.2.4. Extended-Precision Arithmetic Instructions:\nsubc\n9.7.2.5. Extended-Precision Arithmetic Instructions:\nmad.cc\n9.7.2.6. Extended-Precision Arithmetic Instructions:\nmadc\n9.7.3. Floating-Point Instructions\n9.7.3.1. Floating Point Instructions:\ntestp\n9.7.3.2. Floating Point Instructions:\ncopysign\n9.7.3.3. Floating Point Instructions:\nadd\n9.7.3.4. Floating Point Instructions:\nsub\n9.7.3.5. Floating Point Instructions:\nmul\n9.7.3.6. Floating Point Instructions:\nfma\n9.7.3.7. Floating Point Instructions:\nmad\n9.7.3.8. Floating Point Instructions:\ndiv\n9.7.3.9. Floating Point Instructions:\nabs\n9.7.3.10. Floating Point Instructions:\nneg\n9.7.3.11. Floating Point Instructions:\nmin\n9.7.3.12. Floating Point Instructions:\nmax\n9.7.3.13. Floating Point Instructions:\nrcp\n9.7.3.14. Floating Point Instructions:\nrcp.approx.ftz.f64\n9.7.3.15. Floating Point Instructions:\nsqrt\n9.7.3.16. Floating Point Instructions:\nrsqrt\n9.7.3.17. Floating Point Instructions:\nrsqrt.approx.ftz.f64\n9.7.3.18. Floating Point Instructions:\nsin\n9.7.3.19. Floating Point Instructions:\ncos\n9.7.3.20. Floating Point Instructions:\nlg2\n9.7.3.21. Floating Point Instructions:\nex2\n9.7.3.22. Floating Point Instructions:\ntanh\n9.7.4. Half Precision Floating-Point Instructions\n9.7.4.1. Half Precision Floating Point Instructions:\nadd\n9.7.4.2. Half Precision Floating Point Instructions:\nsub\n9.7.4.3. Half Precision Floating Point Instructions:\nmul\n9.7.4.4. Half Precision Floating Point Instructions:\nfma\n9.7.4.5. Half Precision Floating Point Instructions:\nneg\n9.7.4.6. Half Precision Floating Point Instructions:\nabs\n9.7.4.7. Half Precision Floating Point Instructions:\nmin\n9.7.4.8. Half Precision Floating Point Instructions:\nmax\n9.7.4.9. Half Precision Floating Point Instructions:\ntanh\n9.7.4.10. Half Precision Floating Point Instructions:\nex2\n9.7.5. Mixed Precision Floating-Point Instructions\n9.7.5.1. Mixed Precision Floating Point Instructions:\nadd\n9.7.5.2. Mixed Precision Floating Point Instructions:\nsub\n9.7.5.3. Mixed Precision Floating Point Instructions:\nfma\n9.7.6. Comparison and Selection Instructions\n9.7.6.1. Comparison and Selection Instructions:\nset\n9.7.6.2. Comparison and Selection Instructions:\nsetp\n9.7.6.3. Comparison and Selection Instructions:\nselp\n9.7.6.4. Comparison and Selection Instructions:\nslct\n9.7.7. Half Precision Comparison Instructions\n9.7.7.1. Half Precision Comparison Instructions:\nset\n9.7.7.2. Half Precision Comparison Instructions:\nsetp\n9.7.8. Logic and Shift Instructions\n9.7.8.1. Logic and Shift Instructions:\nand\n9.7.8.2. Logic and Shift Instructions:\nor\n9.7.8.3. Logic and Shift Instructions:\nxor\n9.7.8.4. Logic and Shift Instructions:\nnot\n9.7.8.5. Logic and Shift Instructions:\ncnot\n9.7.8.6. Logic and Shift Instructions:\nlop3\n9.7.8.7. Logic and Shift Instructions:\nshf\n9.7.8.8. Logic and Shift Instructions:\nshl\n9.7.8.9. Logic and Shift Instructions:\nshr\n9.7.9. Data Movement and Conversion Instructions\n9.7.9.1. Cache Operators\n9.7.9.2. Cache Eviction Priority Hints\n9.7.9.3. Data Movement and Conversion Instructions:\nmov\n9.7.9.4. Data Movement and Conversion Instructions:\nmov\n9.7.9.5. Data Movement and Conversion Instructions:\nshfl\n(deprecated)\n9.7.9.6. Data Movement and Conversion Instructions:\nshfl.sync\n9.7.9.7. Data Movement and Conversion Instructions:\nprmt\n9.7.9.8. Data Movement and Conversion Instructions:\nld\n9.7.9.9. Data Movement and Conversion Instructions:\nld.global.nc\n9.7.9.10. Data Movement and Conversion Instructions:\nldu\n9.7.9.11. Data Movement and Conversion Instructions:\nst\n9.7.9.12. Data Movement and Conversion Instructions:\nst.async\n9.7.9.13. Data Movement and Conversion Instructions:\nst.bulk\n9.7.9.14. Data Movement and Conversion Instructions:\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\n9.7.9.15. Data Movement and Conversion Instructions:\nprefetch\n,\nprefetchu\n9.7.9.16. Data Movement and Conversion Instructions:\napplypriority\n9.7.9.17. Data Movement and Conversion Instructions:\ndiscard\n9.7.9.18. Data Movement and Conversion Instructions:\ncreatepolicy\n9.7.9.19. Data Movement and Conversion Instructions:\nisspacep\n9.7.9.20. Data Movement and Conversion Instructions:\ncvta\n9.7.9.21. Data Movement and Conversion Instructions:\ncvt\n9.7.9.22. Data Movement and Conversion Instructions:\ncvt.pack\n9.7.9.23. Data Movement and Conversion Instructions:\nmapa\n9.7.9.24. Data Movement and Conversion Instructions:\ngetctarank\n9.7.9.25. Data Movement and Conversion Instructions: Asynchronous copy\n9.7.9.25.1. Completion Mechanisms for Asynchronous Copy Operations\n9.7.9.25.1.1. Async-group mechanism\n9.7.9.25.1.2. Mbarrier-based mechanism\n9.7.9.25.2. Async Proxy\n9.7.9.25.3. Data Movement and Conversion Instructions: Non-bulk copy\n9.7.9.25.3.1. Data Movement and Conversion Instructions:\ncp.async\n9.7.9.25.3.2. Data Movement and Conversion Instructions:\ncp.async.commit_group\n9.7.9.25.3.3. Data Movement and Conversion Instructions:\ncp.async.wait_group\n/\ncp.async.wait_all\n9.7.9.25.4. Data Movement and Conversion Instructions: Bulk copy\n9.7.9.25.4.1. Data Movement and Conversion Instructions:\ncp.async.bulk\n9.7.9.25.4.2. Data Movement and Conversion Instructions:\ncp.reduce.async.bulk\n9.7.9.25.4.3. Data Movement and Conversion Instructions:\ncp.async.bulk.prefetch\n9.7.9.26. Data Movement and Conversion Instructions:\nmultimem.cp.async.bulk\n9.7.9.27. Data Movement and Conversion Instructions:\nmultimem.cp.reduce.async.bulk\n9.7.9.27.1. Data Movement and Conversion Instructions: Tensor copy\n9.7.9.27.1.1. Restriction on Tensor Copy instructions\n9.7.9.27.1.2. Data Movement and Conversion Instructions:\ncp.async.bulk.tensor\n9.7.9.27.1.3. Data Movement and Conversion Instructions:\ncp.reduce.async.bulk.tensor\n9.7.9.27.1.4. Data Movement and Conversion Instructions:\ncp.async.bulk.prefetch.tensor\n9.7.9.27.2. Data Movement and Conversion Instructions: Bulk and Tensor copy completion instructions\n9.7.9.27.2.1. Data Movement and Conversion Instructions:\ncp.async.bulk.commit_group\n9.7.9.27.2.2. Data Movement and Conversion Instructions:\ncp.async.bulk.wait_group\n9.7.9.28. Data Movement and Conversion Instructions:\ntensormap.replace\n9.7.10. Texture Instructions\n9.7.10.1. Texturing Modes\n9.7.10.2. Mipmaps\n9.7.10.3. Texture Instructions:\ntex\n9.7.10.4. Texture Instructions:\ntld4\n9.7.10.5. Texture Instructions:\ntxq\n9.7.10.6. Texture Instructions:\nistypep\n9.7.11. Surface Instructions\n9.7.11.1. Surface Instructions:\nsuld\n9.7.11.2. Surface Instructions:\nsust\n9.7.11.3. Surface Instructions:\nsured\n9.7.11.4. Surface Instructions:\nsuq\n9.7.12. Control Flow Instructions\n9.7.12.1. Control Flow Instructions:\n{}\n9.7.12.2. Control Flow Instructions:\n@\n9.7.12.3. Control Flow Instructions:\nbra\n9.7.12.4. Control Flow Instructions:\nbrx.idx\n9.7.12.5. Control Flow Instructions:\ncall\n9.7.12.6. Control Flow Instructions:\nret\n9.7.12.7. Control Flow Instructions:\nexit\n9.7.13. Parallel Synchronization and Communication Instructions\n9.7.13.1. Parallel Synchronization and Communication Instructions:\nbar\n,\nbarrier\n9.7.13.2. Parallel Synchronization and Communication Instructions:\nbar.warp.sync\n9.7.13.3. Parallel Synchronization and Communication Instructions:\nbarrier.cluster\n9.7.13.4. Parallel Synchronization and Communication Instructions:\nmembar\n/\nfence\n9.7.13.5. Parallel Synchronization and Communication Instructions:\natom\n9.7.13.6. Parallel Synchronization and Communication Instructions:\nred\n9.7.13.7. Parallel Synchronization and Communication Instructions:\nred.async\n9.7.13.8. Parallel Synchronization and Communication Instructions:\nvote\n(deprecated)\n9.7.13.9. Parallel Synchronization and Communication Instructions:\nvote.sync\n9.7.13.10. Parallel Synchronization and Communication Instructions:\nmatch.sync\n9.7.13.11. Parallel Synchronization and Communication Instructions:\nactivemask\n9.7.13.12. Parallel Synchronization and Communication Instructions:\nredux.sync\n9.7.13.13. Parallel Synchronization and Communication Instructions:\ngriddepcontrol\n9.7.13.14. Parallel Synchronization and Communication Instructions:\nelect.sync\n9.7.13.15. Parallel Synchronization and Communication Instructions:\nmbarrier\n9.7.13.15.1. Size and alignment of mbarrier object\n9.7.13.15.2. Contents of the mbarrier object\n9.7.13.15.3. Lifecycle of the mbarrier object\n9.7.13.15.4. Phase of the mbarrier object\n9.7.13.15.5. Tracking asynchronous operations by the mbarrier object\n9.7.13.15.5.1. expect-tx operation\n9.7.13.15.5.2. complete-tx operation\n9.7.13.15.6. Phase Completion of the mbarrier object\n9.7.13.15.7. Arrive-on operation on mbarrier object\n9.7.13.15.8. mbarrier support with shared memory\n9.7.13.15.9. Parallel Synchronization and Communication Instructions:\nmbarrier.init\n9.7.13.15.10. Parallel Synchronization and Communication Instructions:\nmbarrier.inval\n9.7.13.15.11. Parallel Synchronization and Communication Instructions:\nmbarrier.expect_tx\n9.7.13.15.12. Parallel Synchronization and Communication Instructions:\nmbarrier.complete_tx\n9.7.13.15.13. Parallel Synchronization and Communication Instructions:\nmbarrier.arrive\n9.7.13.15.14. Parallel Synchronization and Communication Instructions:\nmbarrier.arrive_drop\n9.7.13.15.15. Parallel Synchronization and Communication Instructions:\ncp.async.mbarrier.arrive\n9.7.13.15.16. Parallel Synchronization and Communication Instructions:\nmbarrier.test_wait\n/\nmbarrier.try_wait\n9.7.13.15.17. Parallel Synchronization and Communication Instructions:\nmbarrier.pending_count\n9.7.13.16. Parallel Synchronization and Communication Instructions:\ntensormap.cp_fenceproxy\n9.7.13.17. Parallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.try_cancel\n9.7.13.18. Parallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.query_cancel\n9.7.14. Warp Level Matrix Multiply-Accumulate Instructions\n9.7.14.1. Matrix Shape\n9.7.14.2. Matrix Data-types\n9.7.14.3. Block Scaling for\nmma.sync\n9.7.14.4. Matrix multiply-accumulate operation using\nwmma\ninstructions\n9.7.14.4.1. Matrix Fragments for WMMA\n9.7.14.4.2. Matrix Storage for WMMA\n9.7.14.4.3. Warp-level Matrix Load Instruction:\nwmma.load\n9.7.14.4.4. Warp-level Matrix Store Instruction:\nwmma.store\n9.7.14.4.5. Warp-level Matrix Multiply-and-Accumulate Instruction:\nwmma.mma\n9.7.14.5. Matrix multiply-accumulate operation using\nmma\ninstruction\n9.7.14.5.1. Matrix Fragments for\nmma.m8n8k4\nwith\n.f16\nfloating point type\n9.7.14.5.2. Matrix Fragments for\nmma.m8n8k4\nwith\n.f64\nfloating point type\n9.7.14.5.3. Matrix Fragments for\nmma.m8n8k16\n9.7.14.5.4. Matrix Fragments for\nmma.m8n8k32\n9.7.14.5.5. Matrix Fragments for\nmma.m8n8k128\n9.7.14.5.6. Matrix Fragments for\nmma.m16n8k4\n9.7.14.5.7. Matrix Fragments for\nmma.m16n8k8\n9.7.14.5.8. Matrix Fragments for\nmma.m16n8k16\nwith floating point type\n9.7.14.5.9. Matrix Fragments for\nmma.m16n8k16\nwith integer type\n9.7.14.5.10. Matrix Fragments for\nmma.m16n8k32\n9.7.14.5.11. Matrix Fragments for\nmma.m16n8k64\n9.7.14.5.12. Matrix Fragments for\nmma.m16n8k128\n9.7.14.5.13. Matrix Fragments for\nmma.m16n8k256\n9.7.14.5.14. Multiply-and-Accumulate Instruction:\nmma\n9.7.14.5.15. Warp-level matrix load instruction:\nldmatrix\n9.7.14.5.16. Warp-level matrix store instruction:\nstmatrix\n9.7.14.5.17. Warp-level matrix transpose instruction:\nmovmatrix\n9.7.14.6. Matrix multiply-accumulate operation using\nmma.sp\ninstruction with sparse matrix A\n9.7.14.6.1. Sparse matrix storage\n9.7.14.6.2. Matrix fragments for multiply-accumulate operation with sparse matrix A\n9.7.14.6.2.1. Matrix Fragments for sparse\nmma.m16n8k16\nwith\n.f16\nand\n.bf16\ntypes\n9.7.14.6.2.2. Matrix Fragments for sparse\nmma.m16n8k32\nwith\n.f16\nand\n.bf16\ntypes\n9.7.14.6.2.3. Matrix Fragments for sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type\n9.7.14.6.2.4. Matrix Fragments for sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type\n9.7.14.6.2.5. Matrix Fragments for sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type\n9.7.14.6.2.6. Matrix Fragments for sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\ntype\n9.7.14.6.2.7. Matrix Fragments for sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type\n9.7.14.6.2.8. Matrix Fragments for sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\ninteger type\n9.7.14.6.3. Multiply-and-Accumulate Instruction:\nmma.sp\n/\nmma.sp::ordered_metadata\n9.7.15. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions\n9.7.15.1. Warpgroup\n9.7.15.2. Matrix Shape\n9.7.15.3. Matrix Data-types\n9.7.15.4. Async Proxy\n9.7.15.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using\nwgmma.mma_async\ninstruction\n9.7.15.5.1. Register Fragments and Shared Memory Matrix Layouts\n9.7.15.5.1.1. Register Fragments\n9.7.15.5.1.1.1. Matrix Fragments for\nwgmma.mma_async.m64nNk16\n9.7.15.5.1.1.2. Matrix Fragments for\nwgmma.mma_async.m64nNk8\n9.7.15.5.1.1.3. Matrix Fragments for\nwgmma.mma_async.m64nNk32\n9.7.15.5.1.1.4. Matrix Fragments for\nwgmma.mma_async.m64nNk256\n9.7.15.5.1.2. Shared Memory Matrix Layout\n9.7.15.5.1.2.1. Major-ness supported by Strides\n9.7.15.5.1.2.1.1. Leading Dimension Byte Offset\n9.7.15.5.1.2.1.2. Stride Dimension Byte Offset\n9.7.15.5.1.2.1.3. Canonical Layouts\n9.7.15.5.1.2.2. Matrix Descriptor Format\n9.7.15.5.2. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async\n9.7.15.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using\nwgmma.mma_async.sp\ninstruction\n9.7.15.6.1. Sparse matrix storage\n9.7.15.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A\n9.7.15.6.2.1. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk32\n9.7.15.6.2.2. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk16\n9.7.15.6.2.3. Matrix Fragments for sparse\nwgmma.mma_async.m64nNk64\n9.7.15.6.3. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async.sp\n9.7.15.7. Asynchronous\nwgmma\nProxy Operations\n9.7.15.7.1. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.fence\n9.7.15.7.2. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.commit_group\n9.7.15.7.3. Asynchronous Multiply-and-Accumulate Instruction:\nwgmma.wait_group\n9.7.16. TensorCore 5th Generation Family Instructions\n9.7.16.1. Tensor Memory\n9.7.16.1.1. Tensor Memory Addressing\n9.7.16.1.2. Tensor Memory Allocation\n9.7.16.2. Matrix and Data Movement Shape\n9.7.16.2.1. Matrix Shape\n9.7.16.2.1.1. Target ISA Note\n9.7.16.2.2. Specifying Matrix Shape\n9.7.16.2.3. Data Movement Shape\n9.7.16.2.3.1. Memory Layout\n9.7.16.2.3.1.1. Matrix fragments for shape .32x32b\n9.7.16.2.3.1.2. Matrix fragments for shape .16x64b\n9.7.16.2.3.1.3. Matrix fragments for shape .16x128b\n9.7.16.2.3.1.4. Matrix fragments for shape .16x256b\n9.7.16.2.3.1.5. Matrix fragments for shape .16x32bx2\n9.7.16.3. Major-ness supported by Strides\n9.7.16.3.1. Leading Dimension Stride: relative offset or absolute address\n9.7.16.3.1.1. Relative offset mode\n9.7.16.3.1.2. Absolute address mode for K dimension being 48B\n9.7.16.3.1.2.1. Restrictions on the Leading Dimension Absolute Address Stride\n9.7.16.3.2. Stride Dimension Byte Offset\n9.7.16.3.3. Canonical Layouts\n9.7.16.4. Matrix Descriptors\n9.7.16.4.1. Shared memory descriptor\n9.7.16.4.1.1. Target ISA Note\n9.7.16.4.2. Instruction descriptor\n9.7.16.4.3. Zero-Column Mask Descriptor\n9.7.16.5. Issue Granularity\n9.7.16.5.1. CTA Pair\n9.7.16.5.2. Peer CTA\n9.7.16.6. Memory Consistency Model for 5th generation of TensorCore operations\n9.7.16.6.1. Asynchronous Operations\n9.7.16.6.2. Pipelined tcgen05 Instructions\n9.7.16.6.2.1. Implicitly pipelined tcgen05 Instructions\n9.7.16.6.2.1.1. mbarrier based completion mechanism\n9.7.16.6.2.1.2.\ntcgen05.wait\ninstruction based completion mechanism\n9.7.16.6.3. Specialized Inter-thread Synchronization for tcgen05 instructions\n9.7.16.6.4. Canonical synchronization patterns\n9.7.16.6.4.1. Pipelined instructions, same thread\n9.7.16.6.4.2. Non-pipelined instructions, same thread\n9.7.16.6.4.3. Pipelined instructions, different thread\n9.7.16.6.4.4. Non-pipelined instructions, different thread\n9.7.16.6.4.5. Register dependencies, same thread\n9.7.16.6.5. Shared Memory Accesses\n9.7.16.7. Tensor Memory Allocation and Management Instructions\n9.7.16.7.1. Tensorcore 5th Generation Instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n9.7.16.8. Tensor Memory and Register Load/Store Instructions\n9.7.16.8.1. Access restrictions\n9.7.16.8.2. Packing and Unpacking\n9.7.16.8.3. Tensorcore 5th Generation Instructions:\ntcgen05.ld\n9.7.16.8.4. Tensorcore 5th Generation Instructions:\ntcgen05.st\n9.7.16.8.5. Tensorcore 5th Generation Instructions:\ntcgen05.wait\n9.7.16.9. Tensor Memory Data Movement Instructions\n9.7.16.9.1. Optional Decompression\n9.7.16.9.1.1. Decompression of 4-bit floating point to 8-bit type\n9.7.16.9.1.2. Decompression of 6-bit floating point to 8-bit type\n9.7.16.9.2. Tensorcore 5th Generation Instructions:\ntcgen05.cp\n9.7.16.9.3. Tensorcore 5th Generation Instructions:\ntcgen05.shift\n9.7.16.10. TensorCore 5th Generation Matrix Multiply and accumulate Operations\n9.7.16.10.1. Transpose and Negate operations\n9.7.16.10.2. Matrix Layout Organization\n9.7.16.10.3. Valid Combinations of Type-Size, Major-ness and Swizzling\n9.7.16.10.4. Packing formats of elements in Tensor and Shared memory\n9.7.16.10.4.1. Packing format for matrix D in Tensor Memory\n9.7.16.10.4.2. Packing format for matrix A and B\n9.7.16.10.4.3. Packing format used for matrix A by\n.kind::mxf8f6f4\nin Tensor Memory\n9.7.16.10.4.4. Packing format used for matrix A and B by\n.kind::mxf8f6f4\nin Shared Memory\n9.7.16.10.4.5. Packing format used for matrix A by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Tensor Memory\n9.7.16.10.4.6. Packing format used for matrix A and B by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Shared Memory\n9.7.16.10.5. Data Path Layout Organization\n9.7.16.10.5.1. Layout A (M = 256)\n9.7.16.10.5.2. Layout B (M = 128 + cta-group::2 + Dense A matrix)\n9.7.16.10.5.3. Layout C (M = 128 + cta-group::2 + Sparse A matrix)\n9.7.16.10.5.4. Layout D (M = 128 + cta-group::1)\n9.7.16.10.5.5. Layout E (M = 64 + .ws mode)\n9.7.16.10.5.6. Layout F (M = 64 + non .ws mode)\n9.7.16.10.5.7. Layout G (M = 32)\n9.7.16.10.6. Shared Memory Layout and Swizzling\n9.7.16.10.7. Block Scaling for\ntcgen05.mma.sync\n9.7.16.10.7.1. Valid combinations of scale_vectorsize with types and MMA-Kind\n9.7.16.10.7.2. Scale Factor A ID\n9.7.16.10.7.2.1. Layout of the Scale Factor A Matrix for scale_vec::1X/block32 with K=32/K=64\n9.7.16.10.7.2.2. Layout of the Scale Factor A Matrix for scale_vec::2X/block32 with K=64/K=128\n9.7.16.10.7.2.3. Layout of the Scale Factor A Matrix for scale_vec::4X/block16 with K=64/K=128\n9.7.16.10.7.2.4. Layout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n9.7.16.10.7.2.5. Layout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n9.7.16.10.7.3. Scale Factor B ID\n9.7.16.10.7.3.1. Layout of the Scale Factor B Matrix for scale_vec::1X/block32 with K=32/K=64\n9.7.16.10.7.3.2. Layout of the Scale Factor B Matrix for scale_vec::2X/block32 with K=64/K=128\n9.7.16.10.7.3.3. Layout of the Scale Factor B Matrix for scale_vec::4X/block16 with K=64/K=128\n9.7.16.10.7.3.4. Layout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n9.7.16.10.7.3.5. Layout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n9.7.16.10.8. Sparse Matrices\n9.7.16.10.8.1. Sparse\ntcgen05.mma.sp\nwith\n.kind::tf32\n9.7.16.10.8.2. Sparse\ntcgen05.mma.sp\nwith\n.kind::f16\n,\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n9.7.16.10.8.3. Sparse\ntcgen05.mma.sp\nwith\n.kind::mxf4\nand\n.kind::mxf4nvf4\n9.7.16.10.8.4. Sparsity selector\n9.7.16.10.8.4.1. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f16\n9.7.16.10.8.4.2. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f16\n9.7.16.10.8.4.3. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::tf32\n9.7.16.10.8.4.4. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::tf32\n9.7.16.10.8.4.5. Layout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n9.7.16.10.8.4.6. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n9.7.16.10.8.5. Alignment restriction\n9.7.16.10.9. TensorCore 5th Generation of MMA Instructions\n9.7.16.10.9.1. TensorCore 5th Generation Instructions:\ntcgen05.mma\n9.7.16.10.9.2. TensorCore 5th Generation Instructions:\ntcgen05.mma.sp\n9.7.16.10.9.3. TensorCore 5th Generation Instructions:\ntcgen05.mma.ws\n9.7.16.10.9.4. TensorCore 5th Generation Instructions:\ntcgen05.mma.ws.sp\n9.7.16.11. TensorCore 5th Generation Specialized Synchronization Operations\n9.7.16.11.1. TensorCore 5th Generation Instructions:\ntcgen05.fence\n9.7.16.12. TensorCore 5th Generation Async Synchronization Operations\n9.7.16.12.1. TensorCore 5th Generation Instructions:\ntcgen05.commit\n9.7.17. Stack Manipulation Instructions\n9.7.17.1. Stack Manipulation Instructions:\nstacksave\n9.7.17.2. Stack Manipulation Instructions:\nstackrestore\n9.7.17.3. Stack Manipulation Instructions:\nalloca\n9.7.18. Video Instructions\n9.7.18.1. Scalar Video Instructions\n9.7.18.1.1. Scalar Video Instructions:\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\n9.7.18.1.2. Scalar Video Instructions:\nvshl\n,\nvshr\n9.7.18.1.3. Scalar Video Instructions:\nvmad\n9.7.18.1.4. Scalar Video Instructions:\nvset\n9.7.18.2. SIMD Video Instructions\n9.7.18.2.1. SIMD Video Instructions:\nvadd2\n,\nvsub2\n,\nvavrg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\n9.7.18.2.2. SIMD Video Instructions:\nvset2\n9.7.18.2.3. SIMD Video Instructions:\nvadd4\n,\nvsub4\n,\nvavrg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\n9.7.18.2.4. SIMD Video Instructions:\nvset4\n9.7.19. Miscellaneous Instructions\n9.7.19.1. Miscellaneous Instructions:\nbrkpt\n9.7.19.2. Miscellaneous Instructions:\nnanosleep\n9.7.19.3. Miscellaneous Instructions:\npmevent\n9.7.19.4. Miscellaneous Instructions:\ntrap\n9.7.19.5. Miscellaneous Instructions:\nsetmaxnreg\n10. Special Registers\n10.1. Special Registers:\n%tid\n10.2. Special Registers:\n%ntid\n10.3. Special Registers:\n%laneid\n10.4. Special Registers:\n%warpid\n10.5. Special Registers:\n%nwarpid\n10.6. Special Registers:\n%ctaid\n10.7. Special Registers:\n%nctaid\n10.8. Special Registers:\n%smid\n10.9. Special Registers:\n%nsmid\n10.10. Special Registers:\n%gridid\n10.11. Special Registers:\n%is_explicit_cluster\n10.12. Special Registers:\n%clusterid\n10.13. Special Registers:\n%nclusterid\n10.14. Special Registers:\n%cluster_ctaid\n10.15. Special Registers:\n%cluster_nctaid\n10.16. Special Registers:\n%cluster_ctarank\n10.17. Special Registers:\n%cluster_nctarank\n10.18. Special Registers:\n%lanemask_eq\n10.19. Special Registers:\n%lanemask_le\n10.20. Special Registers:\n%lanemask_lt\n10.21. Special Registers:\n%lanemask_ge\n10.22. Special Registers:\n%lanemask_gt\n10.23. Special Registers:\n%clock\n,\n%clock_hi\n10.24. Special Registers:\n%clock64\n10.25. Special Registers:\n%pm0\n…\n%pm7\n10.26. Special Registers:\n%pm0_64\n…\n%pm7_64\n10.27. Special Registers:\n%envreg<32>\n10.28. Special Registers:\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n10.29. Special Registers:\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n10.30. Special Registers:\n%total_smem_size\n10.31. Special Registers:\n%aggr_smem_size\n10.32. Special Registers:\n%dynamic_smem_size\n10.33. Special Registers:\n%current_graph_exec\n11. Directives\n11.1. PTX Module Directives\n11.1.1. PTX Module Directives:\n.version\n11.1.2. PTX Module Directives:\n.target\n11.1.3. PTX Module Directives:\n.address_size\n11.2. Specifying Kernel Entry Points and Functions\n11.2.1. Kernel and Function Directives:\n.entry\n11.2.2. Kernel and Function Directives:\n.func\n11.2.3. Kernel and Function Directives:\n.alias\n11.3. Control Flow Directives\n11.3.1. Control Flow Directives:\n.branchtargets\n11.3.2. Control Flow Directives:\n.calltargets\n11.3.3. Control Flow Directives:\n.callprototype\n11.4. Performance-Tuning Directives\n11.4.1. Performance-Tuning Directives:\n.maxnreg\n11.4.2. Performance-Tuning Directives:\n.maxntid\n11.4.3. Performance-Tuning Directives:\n.reqntid\n11.4.4. Performance-Tuning Directives:\n.minnctapersm\n11.4.5. Performance-Tuning Directives:\n.maxnctapersm\n(deprecated)\n11.4.6. Performance-Tuning Directives:\n.noreturn\n11.4.7. Performance-Tuning Directives:\n.pragma\n11.4.8. Performance-Tuning Directives:\n.abi_preserve\n11.4.9. Performance-Tuning Directives:\n.abi_preserve_control\n11.5. Debugging Directives\n11.5.1. Debugging Directives:\n@@dwarf\n11.5.2. Debugging Directives:\n.section\n11.5.3. Debugging Directives:\n.file\n11.5.4. Debugging Directives:\n.loc\n11.6. Linking Directives\n11.6.1. Linking Directives:\n.extern\n11.6.2. Linking Directives:\n.visible\n11.6.3. Linking Directives:\n.weak\n11.6.4. Linking Directives:\n.common\n11.7. Cluster Dimension Directives\n11.7.1. Cluster Dimension Directives:\n.reqnctapercluster\n11.7.2. Cluster Dimension Directives:\n.explicitcluster\n11.7.3. Cluster Dimension Directives:\n.maxclusterrank\n11.8. Miscellaneous Directives\n11.8.1. Miscellaneous Directives:\n.blocksareclusters\n12. Descriptions of\n.pragma\nStrings\n12.1. Pragma Strings:\n\"nounroll\"\n12.2. Pragma Strings:\n\"used_bytes_mask\"\n12.3. Pragma Strings:\n\"enable_smem_spilling\"\n12.4. Pragma Strings:\n\"frequency\"\n13. Release Notes\n13.1. Changes in PTX ISA Version 9.1\n13.2. Changes in PTX ISA Version 9.0\n13.3. Changes in PTX ISA Version 8.8\n13.4. Changes in PTX ISA Version 8.7\n13.5. Changes in PTX ISA Version 8.6\n13.6. Changes in PTX ISA Version 8.5\n13.7. Changes in PTX ISA Version 8.4\n13.8. Changes in PTX ISA Version 8.3\n13.9. Changes in PTX ISA Version 8.2\n13.10. Changes in PTX ISA Version 8.1\n13.11. Changes in PTX ISA Version 8.0\n13.12. Changes in PTX ISA Version 7.8\n13.13. Changes in PTX ISA Version 7.7\n13.14. Changes in PTX ISA Version 7.6\n13.15. Changes in PTX ISA Version 7.5\n13.16. Changes in PTX ISA Version 7.4\n13.17. Changes in PTX ISA Version 7.3\n13.18. Changes in PTX ISA Version 7.2\n13.19. Changes in PTX ISA Version 7.1\n13.20. Changes in PTX ISA Version 7.0\n13.21. Changes in PTX ISA Version 6.5\n13.22. Changes in PTX ISA Version 6.4\n13.23. Changes in PTX ISA Version 6.3\n13.24. Changes in PTX ISA Version 6.2\n13.25. Changes in PTX ISA Version 6.1\n13.26. Changes in PTX ISA Version 6.0\n13.27. Changes in PTX ISA Version 5.0\n13.28. Changes in PTX ISA Version 4.3\n13.29. Changes in PTX ISA Version 4.2\n13.30. Changes in PTX ISA Version 4.1\n13.31. Changes in PTX ISA Version 4.0\n13.32. Changes in PTX ISA Version 3.2\n13.33. Changes in PTX ISA Version 3.1\n13.34. Changes in PTX ISA Version 3.0\n13.35. Changes in PTX ISA Version 2.3\n13.36. Changes in PTX ISA Version 2.2\n13.37. Changes in PTX ISA Version 2.1\n13.38. Changes in PTX ISA Version 2.0\n14. Notices\n14.1. Notice\n14.2. OpenCL\n14.3. Trademarks\nPTX ISA\n»\n1.\nIntroduction\nv9.1 |\nPDF\n|\nArchive\nParallel Thread Execution ISA Version 9.1\nThe programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture).\n1.\nIntroduction\n\nThis document describes PTX, a low-level\nparallel thread execution\nvirtual machine and instruction\nset architecture (ISA). PTX exposes the GPU as a data-parallel computing\ndevice\n.\n1.1.\nScalable Data-Parallel Computing using GPUs\n\nDriven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable\nGPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous\ncomputational horsepower and very high memory bandwidth. The GPU is especially well-suited to\naddress problems that can be expressed as data-parallel computations - the same program is executed\non many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic\noperations to memory operations. Because the same program is executed for each data element, there\nis a lower requirement for sophisticated flow control; and because it is executed on many data\nelements and has high arithmetic intensity, the memory access latency can be hidden with\ncalculations instead of big data caches.\nData-parallel processing maps data elements to parallel processing threads. Many applications that\nprocess large data sets can use a data-parallel programming model to speed up the computations. In\n3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and\nmedia processing applications such as post-processing of rendered images, video encoding and\ndecoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to\nparallel processing threads. In fact, many algorithms outside the field of image rendering and\nprocessing are accelerated by data-parallel processing, from general signal processing or physics\nsimulation to computational finance or computational biology.\nPTX\ndefines a virtual machine and ISA for general purpose parallel thread execution. PTX programs\nare translated at install time to the target hardware instruction set. The PTX-to-GPU translator\nand driver enable NVIDIA GPUs to be used as programmable parallel computers.\n1.2.\nGoals of PTX\n\nPTX\nprovides a stable programming model and instruction set for general purpose parallel\nprogramming. It is designed to be efficient on NVIDIA GPUs supporting the computation features\ndefined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA\nand C/C++ generate PTX instructions, which are optimized for and translated to native\ntarget-architecture instructions.\nThe goals for PTX include the following:\nProvide a stable ISA that spans multiple GPU generations.\nAchieve performance in compiled applications comparable to native GPU performance.\nProvide a machine-independent ISA for C/C++ and other compilers to target.\nProvide a code distribution ISA for application and middleware developers.\nProvide a common source-level ISA for optimizing code generators and translators, which map PTX to\nspecific target machines.\nFacilitate hand-coding of libraries, performance kernels, and architecture tests.\nProvide a scalable programming model that spans GPU sizes from a single unit to many parallel units.\n1.3.\nPTX ISA Version 9.1\n\nPTX ISA version 9.1 introduces the following new features:\nAdds support for\n.volatile\nqualifier with\n.local\nstate space for\nld\nand\nst\ninstructions.\nAdds support for\n.f16x2\nand\n.bf16x2\nsource types for\ncvt\ninstruction\nwith destination types\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x2\n,\n.e5m2x2\n.\nAdds support for\n.scale_vec::4X\nwith\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nfor\nmma\n/\nmma.sp\ninstructions.\nAdds support for\n.s2f6x2\ninstruction type for\ncvt\ninstruction.\nAdds support for\nmultimem.cp.async.bulk\nand\nmultimem.cp.reduce.async.bulk\ninstructions.\n1.4.\nDocument Structure\n\nThe information in this document is organized into the following Chapters:\nProgramming Model\noutlines the programming model.\nPTX Machine Model\ngives an overview of the PTX virtual machine model.\nSyntax\ndescribes the basic syntax of the PTX language.\nState Spaces, Types, and Variables\ndescribes\nstate spaces, types, and variable declarations.\nInstruction Operands\ndescribes instruction operands.\nAbstracting the ABI\ndescribes the function and call syntax,\ncalling convention, and PTX support for abstracting the\nApplication Binary Interface (ABI)\n.\nInstruction Set\ndescribes the instruction set.\nSpecial Registers\nlists special registers.\nDirectives\nlists the assembly directives supported in PTX.\nRelease Notes\nprovides release notes for PTX ISA versions 2.x and\nbeyond.\nReferences\n754-2008 IEEE Standard for Floating-Point Arithmetic. ISBN 978-0-7381-5752-8, 2008.\nhttp://ieeexplore.ieee.org/servlet/opac?punumber=4610933\nThe OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011.\nhttp://www.khronos.org/registry/cl/specs/opencl-1.1.pdf\nCUDA Programming Guide.\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\nCUDA Dynamic Parallelism Programming Guide.\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism\nCUDA Atomicity Requirements.\nhttps://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity\nPTX Writers Guide to Interoperability.\nhttps://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html\n2.\nProgramming Model\n\n2.1.\nA Highly Multithreaded Coprocessor\n\nThe GPU is a compute device capable of executing a very large number of threads in parallel. It\noperates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive\nportions of applications running on the host are off-loaded onto the device.\nMore precisely, a portion of an application that is executed many times, but independently on\ndifferent data, can be isolated into a kernel function that is executed on the GPU as many different\nthreads. To that effect, such a function is compiled to the PTX instruction set and the resulting\nkernel is translated at install time to the target GPU instruction set.\n2.2.\nThread Hierarchy\n\nThe batch of threads that executes a kernel is organized as a grid. A grid consists of either\ncooperative thread arrays or clusters of cooperative thread arrays as described in this section and\nillustrated in\nFigure 1\nand\nFigure 2\n.\nCooperative thread arrays (CTAs)\nimplement CUDA\nthread blocks and clusters implement CUDA thread block clusters.\n2.2.1.\nCooperative Thread Arrays\n\nThe\nParallel Thread Execution (PTX)\nprogramming model is explicitly parallel: a PTX program\nspecifies the execution of a given thread of a parallel thread array. A\ncooperative thread array\n,\nor CTA, is an array of threads that execute a kernel concurrently or in parallel.\nThreads within a CTA can communicate with each other. To coordinate the communication of the threads\nwithin the CTA, one can specify synchronization points where threads wait until all threads in the\nCTA have arrived.\nEach thread has a unique thread identifier within the CTA. Programs use a data parallel\ndecomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread\nuses its thread identifier to determine its assigned role, assign specific input and output\npositions, compute addresses, and select work to perform. The thread identifier is a three-element\nvector\ntid\n, (with elements\ntid.x\n,\ntid.y\n, and\ntid.z\n) that specifies the thread’s\nposition within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the\nnumber of thread ids in that CTA dimension.\nEach CTA has a 1D, 2D, or 3D shape specified by a three-element vector\nntid\n(with elements\nntid.x\n,\nntid.y\n, and\nntid.z\n). The vector\nntid\nspecifies the number of threads in each\nCTA dimension.\nThreads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called\nwarps\n. A\nwarp\nis a maximal subset of threads from a single CTA, such that the threads execute\nthe same instructions at the same time. Threads within a warp are sequentially numbered. The warp\nsize is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be\nable to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate\nconstant,\nWARP_SZ\n, which may be used in any instruction where an immediate operand is allowed.\n2.2.2.\nCluster of Cooperative Thread Arrays\n\nCluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate\nwith each other via shared memory. The executing CTA has to make sure that the shared memory of the\npeer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before\ncompleting the shared memory operation.\nThreads within the different CTAs in a cluster can synchronize and communicate with each other via\nshared memory. Cluster-wide barriers can be used to synchronize all the threads within the\ncluster. Each CTA in a cluster has a unique CTA identifier within its cluster\n(\ncluster_ctaid\n). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter\ncluster_nctaid\n. Each CTA in the cluster also has a unique CTA identifier (\ncluster_ctarank\n)\nacross all dimensions. The total number of CTAs across all the dimensions in the cluster is\nspecified by\ncluster_nctarank\n. Threads may read and use these values through predefined, read-only\nspecial registers\n%cluster_ctaid\n,\n%cluster_nctaid\n,\n%cluster_ctarank\n,\n%cluster_nctarank\n.\nCluster level is applicable only on target architecture\nsm_90\nor higher. Specifying cluster\nlevel during launch time is optional. If the user specifies the cluster dimensions at launch time\nthen it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster\nlaunch with default dimension 1x1x1. PTX provides read-only special register\n%is_explicit_cluster\nto differentiate between explicit and implicit cluster launch.\n2.2.3.\nGrid of Clusters\n\nThere is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a\ncluster can contain. However, clusters with CTAs that execute the same kernel can be batched\ntogether into a grid of clusters, so that the total number of threads that can be launched in a\nsingle kernel invocation is very large. This comes at the expense of reduced thread communication\nand synchronization, because threads in different clusters cannot communicate and synchronize with\neach other.\nEach cluster has a unique cluster identifier (\nclusterid\n) within a grid of clusters. Each grid of\nclusters has a 1D, 2D , or 3D shape specified by the parameter\nnclusterid\n. Each grid also has a\nunique temporal grid identifier (\ngridid\n). Threads may read and use these values through\npredefined, read-only special registers\n%tid\n,\n%ntid\n,\n%clusterid\n,\n%nclusterid\n, and\n%gridid\n.\nEach CTA has a unique identifier (\nctaid\n) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape\nspecified by the parameter\nnctaid\n. Thread may use and read these values through predefined,\nread-only special registers\n%ctaid\nand\n%nctaid\n.\nEach kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs\nwhere cluster is optional level and is applicable only for target architectures\nsm_90\nand\nhigher.\nFigure 1\nshows a grid consisting of CTAs and\nFigure 2\nshows a grid consisting of clusters.\nGrids may be launched with dependencies between one another - a grid may be a dependent grid and/or\na prerequisite grid. To understand how grid dependencies may be defined, refer to the section on\nCUDA Graphs\nin the\nCuda Programming Guide\n.\nFigure 1\nGrid with CTAs\n\nFigure 2\nGrid with clusters\n\nA cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads\nthat execute the same kernel program. A grid is a set of clusters consisting of CTAs that\nexecute independently.\n2.3.\nMemory Hierarchy\n\nPTX threads may access data from multiple state spaces during their execution as illustrated by\nFigure 3\nwhere cluster level is introduced from\ntarget architecture\nsm_90\nonwards. Each thread has a private local memory. Each thread block\n(CTA) has a shared memory visible to all threads of the block and to all active blocks in the\ncluster and with the same lifetime as the block. Finally, all threads have access to the same global\nmemory.\nThere are additional state spaces accessible by all threads: the constant, param, texture, and\nsurface state spaces.  Constant and texture memory are read-only; surface memory is readable and\nwritable. The global, constant, param, texture, and surface state spaces are optimized for different\nmemory usages. For example, texture memory offers different addressing modes as well as data\nfiltering for specific data formats. Note that texture and surface memory is cached, and within the\nsame kernel call, the cache is not kept coherent with respect to global memory writes and surface\nmemory writes, so any texture fetch or surface read to an address that has been written to via a\nglobal or a surface write in the same kernel call returns undefined data. In other words, a thread\ncan safely read some texture or surface memory location only if this memory location has been\nupdated by a previous kernel call or memory copy, but not if it has been previously updated by the\nsame thread or another thread from the same kernel call.\nThe global, constant, and texture state spaces are persistent across kernel launches by the same\napplication.\nBoth the host and the device maintain their own local memory, referred to as\nhost memory\nand\ndevice memory\n, respectively. The device memory may be mapped and read or written by the host, or,\nfor more efficient transfer, copied from the host memory through optimized API calls that utilize\nthe device’s high-performance\nDirect Memory Access (DMA)\nengine.\nFigure 3\nMemory Hierarchy\n\n3.\nPTX Machine Model\n\n3.1.\nA Set of SIMT Multiprocessors\n\nThe NVIDIA GPU architecture is built around a scalable array of multithreaded\nStreaming\nMultiprocessors (SMs)\n. When a host program invokes a kernel grid, the blocks of the grid are\nenumerated and distributed to multiprocessors with available execution capacity. The threads of a\nthread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are\nlaunched on the vacated multiprocessors.\nA multiprocessor consists of multiple\nScalar Processor (SP)\ncores, a multithreaded instruction\nunit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent\nthreads in hardware with zero scheduling overhead. It implements a single-instruction barrier\nsynchronization. Fast barrier synchronization together with lightweight thread creation and\nzero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for\nexample, a low granularity decomposition of problems by assigning one thread to each data element\n(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation).\nTo manage hundreds of threads running several different programs, the multiprocessor employs an\narchitecture we call\nSIMT (single-instruction, multiple-thread)\n. The multiprocessor maps each\nthread to one scalar processor core, and each scalar thread executes independently with its own\ninstruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,\nand executes threads in groups of parallel threads called\nwarps\n. (This term originates from\nweaving, the first parallel thread technology.) Individual threads composing a SIMT warp start\ntogether at the same program address but are otherwise free to branch and execute independently.\nWhen a multiprocessor is given one or more thread blocks to execute, it splits them into warps that\nget scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp\ncontains threads of consecutive, increasing thread IDs with the first warp containing thread 0.\nAt every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues\nthe next instruction to the active threads of the warp. A warp executes one common instruction at a\ntime, so full efficiency is realized when all threads of a warp agree on their execution path. If\nthreads of a warp diverge via a data-dependent conditional branch, the warp serially executes each\nbranch path taken, disabling threads that are not on that path, and when all paths complete, the\nthreads converge back to the same execution path. Branch divergence occurs only within a warp;\ndifferent warps execute independently regardless of whether they are executing common or disjointed\ncode paths.\nSIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a\nsingle instruction controls multiple processing elements. A key difference is that SIMD vector\norganizations expose the SIMD width to the software, whereas SIMT instructions specify the execution\nand branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables\nprogrammers to write thread-level parallel code for independent, scalar threads, as well as\ndata-parallel code for coordinated threads. For the purposes of correctness, the programmer can\nessentially ignore the SIMT behavior; however, substantial performance improvements can be realized\nby taking care that the code seldom requires threads in a warp to diverge. In practice, this is\nanalogous to the role of cache lines in traditional code: Cache line size can be safely ignored when\ndesigning for correctness but must be considered in the code structure when designing for peak\nperformance. Vector architectures, on the other hand, require the software to coalesce loads into\nvectors and manage divergence manually.\nHow many blocks a multiprocessor can process at once depends on how many registers per thread and\nhow much shared memory per block are required for a given kernel since the multiprocessor’s\nregisters and shared memory are split among all the threads of the batch of blocks. If there are not\nenough registers or shared memory available per multiprocessor to process at least one block, the\nkernel will fail to launch.\nFigure 4\nHardware Model\n\nA set of SIMT multiprocessors with on-chip shared memory.\n3.2.\nIndependent Thread Scheduling\n\nOn architectures prior to Volta, warps used a single program counter shared amongst all 32 threads\nin the warp together with an active mask specifying the active threads of the warp. As a result,\nthreads from the same warp in divergent regions or different states of execution cannot signal each\nother or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or\nmutexes can easily lead to deadlock, depending on which warp the contending threads come from.\nStarting with the Volta architecture,\nIndependent Thread Scheduling\nallows full concurrency\nbetween threads, regardless of warp. With\nIndependent Thread Scheduling\n, the GPU maintains\nexecution state per thread, including a program counter and call stack, and can yield execution at a\nper-thread granularity, either to make better use of execution resources or to allow one thread to\nwait for data to be produced by another. A schedule optimizer determines how to group active threads\nfrom the same warp together into SIMT units. This retains the high throughput of SIMT execution as\nin prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at\nsub-warp granularity.\nIndependent Thread Scheduling\ncan lead to a rather different set of threads participating in the\nexecuted code than intended if the developer made assumptions about warp-synchronicity of previous\nhardware architectures. In particular, any warp-synchronous code (such as synchronization-free,\nintra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the\nsection on Compute Capability 7.x in the\nCuda Programming Guide\nfor further details.\n3.3.\nOn-chip Shared Memory\n\nAs illustrated by\nFigure 4\n, each multiprocessor has\non-chip memory of the four following types:\nOne set of local 32-bit\nregisters\nper processor,\nA parallel data cache or\nshared memory\nthat is shared by all scalar processor cores and is where\nthe shared memory space resides,\nA read-only\nconstant cache\nthat is shared by all scalar processor cores and speeds up reads from\nthe constant memory space, which is a read-only region of device memory,\nA read-only\ntexture cache\nthat is shared by all scalar processor cores and speeds up reads from\nthe texture memory space, which is a read-only region of device memory; each multiprocessor\naccesses the texture cache via a\ntexture unit\nthat implements the various addressing modes and\ndata filtering.\nThe local and global memory spaces are read-write regions of device memory.\n4.\nSyntax\n\nPTX programs are a collection of text source modules (files). PTX source modules have an\nassembly-language style syntax with instruction operation codes and operands. Pseudo-operations\nspecify symbol and addressing management. The ptxas optimizing backend compiler optimizes and\nassembles PTX source modules to produce corresponding binary object files.\n4.1.\nSource Format\n\nSource modules are ASCII text. Lines are separated by the newline character (\n\\n\n).\nAll whitespace characters are equivalent; whitespace is ignored except for its use in separating\ntokens in the language.\nThe C preprocessor cpp may be used to process PTX source modules. Lines beginning with\n#\nare\npreprocessor directives. The following are common preprocessor directives:\n#include\n,\n#define\n,\n#if\n,\n#ifdef\n,\n#else\n,\n#endif\n,\n#line\n,\n#file\nC: A Reference Manual\nby Harbison and Steele provides a good description of the C preprocessor.\nPTX is case sensitive and uses lowercase for keywords.\nEach PTX module must begin with a\n.version\ndirective specifying the PTX language version,\nfollowed by a\n.target\ndirective specifying the target architecture assumed. See\nPTX Module Directives\nfor a more information on these directives.\n4.2.\nComments\n\nComments in PTX follow C/C++ syntax, using non-nested\n/*\nand\n*/\nfor comments that may span\nmultiple lines, and using\n//\nto begin a comment that extends up to the next newline character,\nwhich terminates the current line. Comments cannot occur within character constants, string\nliterals, or within other comments.\nComments in PTX are treated as whitespace.\n4.3.\nStatements\n\nA PTX statement is either a directive or an instruction. Statements begin with an optional label and\nend with a semicolon.\nExamples\n.reg     .b32 r1, r2;\n        .global  .f32  array[N];\n\nstart:  mov.b32   r1, %tid.x;\n        shl.b32   r1, r1, 2;          // shift thread id by 2 bits\n        ld.global.b32 r2, array[r1];  // thread[tid] gets array[tid]\n        add.f32   r2, r2, 0.5;        // add 1/2\n4.3.1.\nDirective Statements\n\nDirective keywords begin with a dot, so no conflict is possible with user-defined identifiers. The\ndirectives in PTX are listed in\nTable 1\nand\ndescribed in\nState Spaces, Types, and Variables\nand\nDirectives\n.\nTable 1\nPTX Directives\n\n.address_size\n.explicitcluster\n.maxnreg\n.section\n.alias\n.extern\n.maxntid\n.shared\n.align\n.file\n.minnctapersm\n.sreg\n.branchtargets\n.func\n.noreturn\n.target\n.callprototype\n.global\n.param\n.tex\n.calltargets\n.loc\n.pragma\n.version\n.common\n.local\n.reg\n.visible\n.const\n.maxclusterrank\n.reqnctapercluster\n.weak\n.entry\n.maxnctapersm\n.reqntid\n4.3.2.\nInstruction Statements\n\nInstructions are formed from an instruction opcode followed by a comma-separated list of zero or\nmore operands, and terminated with a semicolon. Operands may be register variables, constant\nexpressions, address expressions, or label names. Instructions have an optional guard predicate\nwhich controls conditional execution. The guard predicate follows the optional label and precedes\nthe opcode, and is written as\n@p\n, where\np\nis a predicate register. The guard predicate may\nbe optionally negated, written as\n@!p\n.\nThe destination operand is first, followed by source operands.\nInstruction keywords are listed in\nTable 2\n. All instruction keywords are\nreserved tokens in PTX.\nTable 2\nReserved Instruction Keywords\n\nabs\ncvta\nmembar\nsetp\nvabsdiff\nactivemask\ndiscard\nmin\nshf\nvabsdiff2\nadd\ndiv\nmma\nshfl\nvabsdiff4\naddc\ndp2a\nmov\nshl\nvadd\nalloca\ndp4a\nmovmatrix\nshr\nvadd2\nand\nelect\nmul\nsin\nvadd4\napplypriority\nex2\nmul24\nslct\nvavrg2\natom\nexit\nmultimem\nsqrt\nvavrg4\nbar\nfence\nnanosleep\nst\nvmad\nbarrier\nfma\nneg\nstackrestore\nvmax\nbfe\nfns\nnot\nstacksave\nvmax2\nbfi\ngetctarank\nor\nstmatrix\nvmax4\nbfind\ngriddepcontrol\npmevent\nsub\nvmin\nbmsk\nisspacep\npopc\nsubc\nvmin2\nbra\nistypep\nprefetch\nsuld\nvmin4\nbrev\nld\nprefetchu\nsuq\nvote\nbrkpt\nldmatrix\nprmt\nsured\nvset\nbrx\nldu\nrcp\nsust\nvset2\ncall\nlg2\nred\nszext\nvset4\nclz\nlop3\nredux\ntanh\nvshl\ncnot\nmad\nrem\ntcgen05\nvshr\ncopysign\nmad24\nret\ntensormap\nvsub\ncos\nmadc\nrsqrt\ntestp\nvsub2\nclusterlaunchcontrol\nmapa\nsad\ntex\nvsub4\ncp\nmatch\nselp\ntld4\nwgmma\ncreatepolicy\nmax\nset\ntrap\nwmma\ncvt\nmbarrier\nsetmaxnreg\ntxq\nxor\n4.4.\nIdentifiers\n\nUser-defined identifiers follow extended C++ rules: they either start with a letter followed by zero\nor more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar,\nor percentage character followed by one or more letters, digits, underscore, or dollar characters:\nfollowsym:   [a-zA-Z0-9_$]\nidentifier:  [a-zA-Z]{followsym}* | {[_$%]{followsym}+\nPTX does not specify a maximum length for identifiers and suggests that all implementations support\na minimum length of at least 1024 characters.\nMany high-level languages such as C and C++ follow similar rules for identifier names, except that\nthe percentage sign is not allowed. PTX allows the percentage sign as the first character of an\nidentifier. The percentage sign can be used to avoid name conflicts, e.g., between user-defined\nvariable names and compiler-generated names.\nPTX predefines one constant and a small number of special registers that begin with the percentage\nsign, listed in\nTable 3\n.\nTable 3\nPredefined Identifiers\n\n%aggr_smem_size\n%dynamic_smem_size\n%lanemask_gt\n%reserved_smem_offset_begin\n%clock\n%envreg<32>\n%lanemask_le\n%reserved_smem_offset_cap\n%clock64\n%globaltimer\n%lanemask_lt\n%reserved_smem_offset_end\n%cluster_ctaid\n%globaltimer_hi\n%nclusterid\n%smid\n%cluster_ctarank\n%globaltimer_lo\n%nctaid\n%tid\n%cluster_nctaid\n%gridid\n%nsmid\n%total_smem_size\n%cluster_nctarank\n%is_explicit_cluster\n%ntid\n%warpid\n%clusterid\n%laneid\n%nwarpid\nWARP_SZ\n%ctaid\n%lanemask_eq\n%pm0,\n...,\n%pm7\n%current_graph_exec\n%lanemask_ge\n%reserved_smem_offset_<2>\n4.5.\nConstants\n\nPTX supports integer and floating-point constants and constant expressions. These constants may be\nused in data initialization and as operands to instructions. Type checking rules remain the same for\ninteger, floating-point, and bit-size types. For predicate-type data and instructions, integer\nconstants are allowed and are interpreted as in C, i.e., zero values are\nFalse\nand non-zero\nvalues are\nTrue\n.\n4.5.1.\nInteger Constants\n\nInteger constants are 64-bits in size and are either signed or unsigned, i.e., every integer\nconstant has type\n.s64\nor\n.u64\n. The signed/unsigned nature of an integer constant is needed\nto correctly evaluate constant expressions containing operations such as division and ordered\ncomparisons, where the behavior of the operation depends on the operand types. When used in an\ninstruction or data initialization, each integer constant is converted to the appropriate size based\non the data or instruction type at its use.\nInteger literals may be written in decimal, hexadecimal, octal, or binary notation. The syntax\nfollows that of C. Integer literals may be followed immediately by the letter\nU\nto indicate that\nthe literal is unsigned.\nhexadecimal literal:  0[xX]{hexdigit}+U?\noctal literal:        0{octal digit}+U?\nbinary literal:       0[bB]{bit}+U?\ndecimal literal       {nonzero-digit}{digit}*U?\nInteger literals are non-negative and have a type determined by their magnitude and optional type\nsuffix as follows: literals are signed (\n.s64\n) unless the value cannot be fully represented in\n.s64\nor the unsigned suffix is specified, in which case the literal is unsigned (\n.u64\n).\nThe predefined integer constant\nWARP_SZ\nspecifies the number of threads per warp for the target\nplatform; to date, all target architectures have a\nWARP_SZ\nvalue of 32.\n4.5.2.\nFloating-Point Constants\n\nFloating-point constants are represented as 64-bit double-precision values, and all floating-point\nconstant expressions are evaluated using 64-bit double precision arithmetic. The only exception is\nthe 32-bit hex notation for expressing an exact single-precision floating-point value; such values\nretain their exact 32-bit single-precision value and may not be used in constant expressions. Each\n64-bit floating-point constant is converted to the appropriate floating-point size based on the data\nor instruction type at its use.\nFloating-point literals may be written with an optional decimal point and an optional signed\nexponent. Unlike C and C++, there is no suffix letter to specify size; literals are always\nrepresented in 64-bit double-precision format.\nPTX includes a second representation of floating-point constants for specifying the exact machine\nrepresentation using a hexadecimal constant. To specify IEEE 754 double-precision floating point\nvalues, the constant begins with\n0d\nor\n0D\nfollowed by 16 hex digits. To specify IEEE 754\nsingle-precision floating point values, the constant begins with\n0f\nor\n0F\nfollowed by 8 hex\ndigits.\n0[fF]{hexdigit}{8}      // single-precision floating point\n0[dD]{hexdigit}{16}     // double-precision floating point\nExample\nmov.f32  $f3, 0F3f800000;       //  1.0\n4.5.3.\nPredicate Constants\n\nIn PTX, integer constants may be used as predicates. For predicate-type data initializers and\ninstruction operands, integer constants are interpreted as in C, i.e., zero values are\nFalse\nand\nnon-zero values are\nTrue\n.\n4.5.4.\nConstant Expressions\n\nIn PTX, constant expressions are formed using operators as in C and are evaluated using rules\nsimilar to those in C, but simplified by restricting types and sizes, removing most casts, and\ndefining full semantics to eliminate cases where expression evaluation in C is implementation\ndependent.\nConstant expressions are formed from constant literals, unary plus and minus, basic arithmetic\noperators (addition, subtraction, multiplication, division), comparison operators, the conditional\nternary operator (\n?:\n), and parentheses. Integer constant expressions also allow unary logical\nnegation (\n!\n), bitwise complement (\n~\n), remainder (\n%\n), shift operators (\n<<\nand\n>>\n), bit-type operators (\n&\n,\n|\n, and\n^\n), and logical operators (\n&&\n,\n||\n).\nConstant expressions in PTX do not support casts between integer and floating-point.\nConstant expressions are evaluated using the same operator precedence as\nin C.\nTable 4\ngives operator precedence and\nassociativity. Operator precedence is highest for unary operators and decreases with each line in\nthe chart. Operators on the same line have the same precedence and are evaluated right-to-left for\nunary operators and left-to-right for binary operators.\nTable 4\nOperator Precedence\n\nKind\nOperator Symbols\nOperator Names\nAssociates\nPrimary\n()\nparenthesis\nn/a\nUnary\n+-\n!\n~\nplus, minus, negation, complement\nright\n(.s64)\n(.u64)\ncasts\nright\nBinary\n*/\n%\nmultiplication, division, remainder\nleft\n+-\naddition, subtraction\n>>\n<<\nshifts\n<\n>\n<=\n>=\nordered comparisons\n==\n!=\nequal, not equal\n&\nbitwise AND\n^\nbitwise XOR\n|\nbitwise OR\n&&\nlogical AND\n||\nlogical OR\nTernary\n?:\nconditional\nright\n4.5.5.\nInteger Constant Expression Evaluation\n\nInteger constant expressions are evaluated at compile time according to a set of rules that\ndetermine the type (signed\n.s64\nversus unsigned\n.u64\n) of each sub-expression. These rules\nare based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and\nbehavior is fully defined in all cases (specifically, for remainder and shift operators).\nLiterals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a\nU\nsuffix. For example:\n42\n,\n0x1234\n,\n0123\nare signed.\n0xfabc123400000000\n,\n42U\n,\n0x1234U\nare unsigned.\nUnary plus and minus preserve the type of the input operand. For example:\n+123\n,\n-1\n,\n-(-42)\nare signed.\n-1U\n,\n-0xfabc123400000000\nare unsigned.\nUnary logical negation (\n!\n) produces a signed result with value\n0\nor\n1\n.\nUnary bitwise complement (\n~\n) interprets the source operand as unsigned and produces an\nunsigned result.\nSome binary operators require normalization of source operands. This normalization is known as\nthe usual arithmetic conversions\nand simply converts both operands to unsigned type if either\noperand is unsigned.\nAddition, subtraction, multiplication, and division perform the usual arithmetic conversions and\nproduce a result with the same type as the converted operands. That is, the operands and result\nare unsigned if either source operand is unsigned, and is otherwise signed.\nRemainder (\n%\n) interprets the operands as unsigned. Note that this differs from C, which allows\na negative divisor but defines the behavior to be implementation dependent.\nLeft and right shift interpret the second operand as unsigned and produce a result with the same\ntype as the first operand. Note that the behavior of right-shift is determined by the type of the\nfirst operand: right shift of a signed value is arithmetic and preserves the sign, and right shift\nof an unsigned value is logical and shifts in a zero bit.\nAND (\n&\n), OR (\n|\n), and XOR (\n^\n) perform the usual arithmetic conversions and produce a\nresult with the same type as the converted operands.\nAND_OP (\n&&\n), OR_OP (\n||\n), Equal (\n==\n), and Not_Equal (\n!=\n) produce a signed\nresult. The result value is 0 or 1.\nOrdered comparisons (\n<\n,\n<=\n,\n>\n,\n>=\n) perform the usual arithmetic conversions on\nsource operands and produce a signed result. The result value is\n0\nor\n1\n.\nCasting of expressions to signed or unsigned is supported using (\n.s64\n) and (\n.u64\n) casts.\nFor the conditional operator (\n?\n:\n) , the first operand must be an integer, and the second\nand third operands are either both integers or both floating-point. The usual arithmetic\nconversions are performed on the second and third operands, and the result type is the same as the\nconverted type.\n4.5.6.\nSummary of Constant Expression Evaluation Rules\n\nTable 5\ncontains a summary of the constant expression evaluation rules.\nTable 5\nConstant Expression Evaluation Rules\n\nKind\nOperator\nOperand Types\nOperand Interpretation\nResult Type\nPrimary\n()\nany type\nsame as source\nsame as source\nconstant literal\nn/a\nn/a\n.u64\n,\n.s64\n, or\n.f64\nUnary\n+-\nany type\nsame as source\nsame as source\n!\ninteger\nzero or non-zero\n.s64\n~\ninteger\n.u64\n.u64\nCast\n(.u64)\ninteger\n.u64\n.u64\n(.s64)\ninteger\n.s64\n.s64\nBinary\n+-\n*\n/\n.f64\n.f64\n.f64\ninteger\nuse usual conversions\nconverted type\n<\n>\n<=\n>=\n.f64\n.f64\n.s64\ninteger\nuse usual conversions\n.s64\n==\n!=\n.f64\n.f64\n.s64\ninteger\nuse usual conversions\n.s64\n%\ninteger\n.u64\n.s64\n>>\n<<\ninteger\n1st unchanged, 2nd is\n.u64\nsame as 1st operand\n&\n|\n^\ninteger\n.u64\n.u64\n&&\n||\ninteger\nzero or non-zero\n.s64\nTernary\n?:\nint\n?\n.f64\n:\n.f64\nsame as sources\n.f64\nint\n?\nint\n:\nint\nuse usual conversions\nconverted type\n5.\nState Spaces, Types, and Variables\n\nWhile the specific resources available in a given target GPU will vary, the kinds of resources will\nbe common across platforms, and these resources are abstracted in PTX through state spaces and data\ntypes.\n5.1.\nState Spaces\n\nA state space is a storage area with particular characteristics. All variables reside in some state\nspace. The characteristics of a state space include its size, addressability, access speed, access\nrights, and level of sharing between threads.\nThe state spaces defined in PTX are a byproduct of parallel programming and graphics\nprogramming. The list of state spaces is shown in\nTable 6\n,and\nproperties of state spaces are shown in\nTable 7\n.\nTable 6\nState Spaces\n\nName\nDescription\n.reg\nRegisters, fast.\n.sreg\nSpecial registers. Read-only; pre-defined; platform-specific.\n.const\nShared, read-only memory.\n.global\nGlobal memory, shared by all threads.\n.local\nLocal memory, private to each thread.\n.param\nKernel parameters, defined per-grid; or\nFunction or local parameters, defined per-thread.\n.shared\nAddressable memory, defined per CTA, accessible to all threads in the cluster\nthroughout the lifetime of the CTA that defines it.\n.tex\nGlobal texture memory (deprecated).\nTable 7\nProperties of State Spaces\n\nName\nAddressable\nInitializable\nAccess\nSharing\n.reg\nNo\nNo\nR/W\nper-thread\n.sreg\nNo\nNo\nRO\nper-CTA\n.const\nYes\nYes\n1\nRO\nper-grid\n.global\nYes\nYes\n1\nR/W\nContext\n.local\nYes\nNo\nR/W\nper-thread\n.param\n(as input to kernel)\nYes\n2\nNo\nRO\nper-grid\n.param\n(used in functions)\nRestricted\n3\nNo\nR/W\nper-thread\n.shared\nYes\nNo\nR/W\nper-cluster\n5\n.tex\nNo\n4\nYes, via driver\nRO\nContext\nNotes:\n1\nVariables in\n.const\nand\n.global\nstate spaces are initialized to zero by default.\n2\nAccessible only via the\nld.param{::entry}\ninstruction. Address may be taken via\nmov\ninstruction.\n3\nAccessible via\nld.param{::func}\nand\nst.param{::func}\ninstructions. Device function\ninput and return parameters may have their address taken via\nmov\n; the parameter is then located\non the stack frame and its address is in the\n.local\nstate space.\n4\nAccessible only via the\ntex\ninstruction.\n5\nVisible to the owning CTA and other active CTAs in the cluster.\n5.1.1.\nRegister State Space\n\nRegisters (\n.reg\nstate space) are fast storage locations. The number of registers is limited, and\nwill vary from platform to platform. When the limit is exceeded, register variables will be spilled\nto memory, causing changes in performance. For each architecture, there is a recommended maximum\nnumber of registers to use (see the\nCUDA Programming Guide\nfor details).\nRegisters may be typed (signed integer, unsigned integer, floating point, predicate) or\nuntyped. Register size is restricted; aside from predicate registers which are 1-bit, scalar\nregisters have a width of 8-, 16-, 32-, 64-, or 128-bits, and vector registers have a width of\n16-, 32-, 64-, or 128-bits. The most common use of 8-bit registers is with\nld\n,\nst\n, and\ncvt\ninstructions, or as elements of vector tuples.\nRegisters differ from the other state spaces in that they are not fully addressable, i.e., it is not\npossible to refer to the address of a register. When compiling to use the Application Binary\nInterface (ABI), register variables are restricted to function scope and may not be declared at\nmodule scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped\n.reg\nvariables, the compiler silently disables use of the ABI. Registers may have alignment\nboundaries required by multi-word loads and stores.\n5.1.2.\nSpecial Register State Space\n\nThe special register (\n.sreg\n) state space holds predefined, platform-specific registers, such as\ngrid, cluster, CTA, and thread parameters, clock counters, and performance monitoring registers. All\nspecial registers are predefined.\n5.1.3.\nConstant State Space\n\nThe constant (\n.const\n) state space is a read-only memory initialized by the host. Constant memory\nis accessed with a\nld.const\ninstruction. Constant memory is restricted in size, currently\nlimited to 64 KB which can be used to hold statically-sized constant variables. There is an\nadditional 640 KB of constant memory, organized as ten independent 64 KB regions. The driver may\nallocate and initialize constant buffers in these regions and pass pointers to the buffers as kernel\nfunction parameters. Since the ten regions are not contiguous, the driver must ensure that constant\nbuffers are allocated so that each buffer fits entirely within a 64 KB region and does not span a\nregion boundary.\nStatically-sized constant variables have an optional variable initializer; constant variables with\nno explicit initializer are initialized to zero by default. Constant buffers allocated by the driver\nare initialized by the host, and pointers to such buffers are passed to the kernel as\nparameters. See the description of kernel parameter attributes in\nKernel Function Parameter Attributes\nfor more details on passing pointers\nto constant buffers as kernel parameters.\n5.1.3.1.\nBanked Constant State Space (deprecated)\n\nPrevious versions of PTX exposed constant memory as a set of eleven 64 KB banks, with explicit bank\nnumbers required for variable declaration and during access.\nPrior to PTX ISA version 2.2, the constant memory was organized into fixed size banks. There were\neleven 64 KB banks, and banks were specified using the\n.const[bank]\nmodifier, where\nbank\nranged from 0 to 10. If no bank number was given, bank zero was assumed.\nBy convention, bank zero was used for all statically-sized constant variables. The remaining banks\nwere used to declare\nincomplete\nconstant arrays (as in C, for example), where the size is not\nknown at compile time. For example, the declaration\n.extern .const[2] .b32 const_buffer[];\nresulted in\nconst_buffer\npointing to the start of constant bank two. This pointer could then be\nused to access the entire 64 KB constant bank. Multiple incomplete array variables declared in the\nsame bank were aliased, with each pointing to the start address of the specified constant bank.\nTo access data in contant banks 1 through 10, the bank number was required in the state space of the\nload instruction. For example, an incomplete array in bank 2 was accessed as follows:\n.extern .const[2] .b32 const_buffer[];\nld.const[2].b32  %r1, [const_buffer+4]; // load second word\nIn PTX ISA version 2.2, we eliminated explicit banks and replaced the incomplete array\nrepresentation of driver-allocated constant buffers with kernel parameter attributes that allow\npointers to constant buffers to be passed as kernel parameters.\n5.1.4.\nGlobal State Space\n\nThe global (\n.global\n) state space is memory that is accessible by all threads in a context. It is\nthe mechanism by which threads in different CTAs, clusters, and grids can communicate. Use\nld.global\n,\nst.global\n, and\natom.global\nto access global variables.\nGlobal variables have an optional variable initializer; global variables with no explicit\ninitializer are initialized to zero by default.\n5.1.5.\nLocal State Space\n\nThe local state space (\n.local\n) is private memory for each thread to keep its own data. It is\ntypically standard memory with cache. The size is limited, as it must be allocated on a per-thread\nbasis. Use\nld.local\nand\nst.local\nto access local variables.\nWhen compiling to use the\nApplication Binary Interface (ABI)\n,\n.local\nstate-space variables\nmust be declared within function scope and are allocated on the stack. In implementations that do\nnot support a stack, all local memory variables are stored at fixed addresses, recursive function\ncalls are not supported, and\n.local\nvariables may be declared at module scope. When compiling\nlegacy PTX code (ISA versions prior to 3.0) containing module-scoped\n.local\nvariables, the\ncompiler silently disables use of the ABI.\n5.1.6.\nParameter State Space\n\nThe parameter (\n.param\n) state space is used (1) to pass input arguments from the host to the\nkernel, (2a) to declare formal input and return parameters for device functions called from within\nkernel execution, and (2b) to declare locally-scoped byte array variables that serve as function\ncall arguments, typically for passing large structures by value to a function. Kernel function\nparameters differ from device function parameters in terms of access and sharing (read-only versus\nread-write, per-kernel versus per-thread). Note that PTX ISA versions 1.x supports only kernel\nfunction parameters in .param space; device function parameters were previously restricted to the\nregister state space. The use of parameter state space for device function parameters was introduced\nin PTX ISA version 2.0 and requires target architecture\nsm_20\nor higher. Additional sub-qualifiers\n::entry\nor\n::func\ncan be specified on instructions with\n.param\nstate space to indicate\nwhether the address refers to kernel function parameter or device function parameter. If no\nsub-qualifier is specified with the\n.param\nstate space, then the default sub-qualifier is specific\nto and dependent on the exact instruction. For example,\nst.param\nis equivalent to\nst.param::func\nwhereas\nisspacep.param\nis equivalent to\nisspacep.param::entry\n. Refer to the instruction\ndescription for more details on default sub-qualifier assumption.\nNote\nThe location of parameter space is implementation specific. For example, in some implementations\nkernel parameters reside in global memory. No access protection is provided between parameter and\nglobal space in this case. Though the exact location of the kernel parameter space is\nimplementation specific, the kernel parameter space window is always contained within the global\nspace window. Similarly, function parameters are mapped to parameter passing registers and/or\nstack locations based on the function calling conventions of the\nApplication Binary Interface\n(ABI)\n. Therefore, PTX code should make no assumptions about the relative locations or ordering\nof\n.param\nspace variables.\n5.1.6.1.\nKernel Function Parameters\n\nEach kernel function definition includes an optional list of parameters. These parameters are\naddressable, read-only variables declared in the\n.param\nstate space. Values passed from the host\nto the kernel are accessed through these parameter variables using\nld.param\ninstructions. The\nkernel parameter variables are shared across all CTAs from all clusters within a grid.\nThe address of a kernel parameter may be moved into a register using the\nmov\ninstruction. The\nresulting address is in the\n.param\nstate space and is accessed using\nld.param\ninstructions.\nExample\n.entry foo ( .param .b32 N, .param .align 8 .b8 buffer[64] )\n{\n    .reg .u32 %n;\n    .reg .f64 %d;\n\n    ld.param.u32 %n, [N];\n    ld.param.f64 %d, [buffer];\n    ...\nExample\n.entry bar ( .param .b32 len )\n{\n    .reg .u32 %ptr, %n;\n\n    mov.u32      %ptr, len;\n    ld.param.u32 %n, [%ptr];\n    ...\nKernel function parameters may represent normal data values, or they may hold addresses to objects\nin constant, global, local, or shared state spaces. In the case of pointers, the compiler and\nruntime system need information about which parameters are pointers, and to which state space they\npoint. Kernel parameter attribute directives are used to provide this information at the PTX\nlevel. See\nKernel Function Parameter Attributes\nfor a description of kernel parameter attribute\ndirectives.\nNote\nThe current implementation does not allow creation of generic pointers to constant variables\n(\ncvta.const\n) in programs that have pointers to constant buffers passed as kernel parameters.\n5.1.6.2.\nKernel Function Parameter Attributes\n\nKernel function parameters may be declared with an optional .ptr attribute to indicate that a\nparameter is a pointer to memory, and also indicate the state space and alignment of the memory\nbeing pointed to.\nKernel Parameter Attribute: .ptr\ndescribes the\n.ptr\nkernel parameter attribute.\n5.1.6.3.\nKernel Parameter Attribute:\n.ptr\n\n.ptr\nKernel parameter alignment attribute.\nSyntax\n.param .type .ptr .space .align N  varname\n.param .type .ptr        .align N  varname\n\n.space = { .const, .global, .local, .shared };\nDescription\nUsed to specify the state space and, optionally, the alignment of memory pointed to by a pointer\ntype kernel parameter. The alignment value\nN\n, if present, must be a power of two. If no state\nspace is specified, the pointer is assumed to be a generic address pointing to one of const, global,\nlocal, or shared memory. If no alignment is specified, the memory pointed to is assumed to be\naligned to a 4 byte boundary.\nSpaces between\n.ptr\n,\n.space\n, and\n.align\nmay be eliminated to improve readability.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.2.\nSupport for generic addressing of .const space added in PTX ISA version 3.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo ( .param .u32 param1,\n             .param .u32 .ptr.global.align 16 param2,\n             .param .u32 .ptr.const.align 8 param3,\n             .param .u32 .ptr.align 16 param4  // generic address\n                                               // pointer\n) { .. }\n5.1.6.4.\nDevice Function Parameters\n\nPTX ISA version 2.0 extended the use of parameter space to device function parameters. The most\ncommon use is for passing objects by value that do not fit within a PTX register, such as C\nstructures larger than 8 bytes. In this case, a byte array in parameter space is used. Typically,\nthe caller will declare a locally-scoped\n.param\nbyte array variable that represents a flattened\nC structure or union. This will be passed by value to a callee, which declares a\n.param\nformal\nparameter having the same size and alignment as the passed argument.\nExample\n// pass object of type struct { double d; int y; };\n.func foo ( .reg .b32 N, .param .align 8 .b8 buffer[12] )\n{\n    .reg .f64 %d;\n    .reg .s32 %y;\n\n    ld.param.f64 %d, [buffer];\n    ld.param.s32 %y, [buffer+8];\n    ...\n}\n\n// code snippet from the caller\n// struct { double d; int y; } mystruct; is flattened, passed to foo\n    ...\n    .reg .f64 dbl;\n    .reg .s32 x;\n    .param .align 8 .b8 mystruct;\n    ...\n    st.param.f64 [mystruct+0], dbl;\n    st.param.s32 [mystruct+8], x;\n    call foo, (4, mystruct);\n    ...\nSee the section on function call syntax for more details.\nFunction input parameters may be read via\nld.param\nand function return parameters may be written\nusing\nst.param\n; it is illegal to write to an input parameter or read from a return parameter.\nAside from passing structures by value,\n.param\nspace is also required whenever a formal\nparameter has its address taken within the called function. In PTX, the address of a function input\nparameter may be moved into a register using the\nmov\ninstruction. Note that the parameter will\nbe copied to the stack if necessary, and so the address will be in the\n.local\nstate space and is\naccessed via\nld.local\nand\nst.local\ninstructions. It is not possible to use\nmov\nto get\nthe address of or a locally-scoped\n.param\nspace variable. Starting PTX ISA version 6.0, it is\npossible to use\nmov\ninstruction to get address of return parameter of device function.\nExample\n// pass array of up to eight floating-point values in buffer\n.func foo ( .param .b32 N, .param .b32 buffer[32] )\n{\n    .reg .u32  %n, %r;\n    .reg .f32  %f;\n    .reg .pred %p;\n\n    ld.param.u32 %n, [N];\n    mov.u32      %r, buffer;  // forces buffer to .local state space\nLoop:\n    setp.eq.u32  %p, %n, 0;\n@%p bra         Done;\n    ld.local.f32 %f, [%r];\n    ...\n    add.u32      %r, %r, 4;\n    sub.u32      %n, %n, 1;\n    bra          Loop;\nDone:\n    ...\n}\n5.1.7.\nShared State Space\n\nThe shared (\n.shared\n) state space is a memory that is owned by an executing CTA and is accessible\nto the threads of all the CTAs within a cluster. An address in shared memory can be read and written\nby any thread in a CTA cluster.\nAdditional sub-qualifiers\n::cta\nor\n::cluster\ncan be specified on instructions with\n.shared\nstate space to indicate whether the address belongs to the shared memory window of the\nexecuting CTA or of any CTA in the cluster respectively. The addresses in the\n.shared::cta\nwindow also fall within the\n.shared::cluster\nwindow. If no sub-qualifier is specified with the\n.shared\nstate space, then it defaults to\n::cta\n. For example,\nld.shared\nis equivalent to\nld.shared::cta\n.\nVariables declared in\n.shared\nstate space refer to the memory addresses in the current\nCTA. Instruction\nmapa\ngives the\n.shared::cluster\naddress of the corresponding variable in\nanother CTA in the cluster.\nShared memory typically has some optimizations to support the sharing. One example is broadcast;\nwhere all threads read from the same address. Another is sequential access from sequential threads.\n5.1.8.\nTexture State Space (deprecated)\n\nThe texture (\n.tex\n) state space is global memory accessed via the texture instruction. It is\nshared by all threads in a context. Texture memory is read-only and cached, so accesses to texture\nmemory are not coherent with global memory stores to the texture image.\nThe GPU hardware has a fixed number of texture bindings that can be accessed within a single kernel\n(typically 128). The .tex directive will bind the named texture memory variable to a hardware\ntexture identifier, where texture identifiers are allocated sequentially beginning with\nzero. Multiple names may be bound to the same physical texture identifier. An error is generated if\nthe maximum number of physical resources is exceeded. The texture name must be of type\n.u32\nor\n.u64\n.\nPhysical texture resources are allocated on a per-kernel granularity, and\n.tex\nvariables are\nrequired to be defined in the global scope.\nTexture memory is read-only. A texture’s base address is assumed to be aligned to a 16 byte\nboundary.\nExample\n.tex .u32 tex_a;         // bound to physical texture 0\n.tex .u32 tex_c, tex_d;  // both bound to physical texture 1\n.tex .u32 tex_d;         // bound to physical texture 2\n.tex .u32 tex_f;         // bound to physical texture 3\nNote\nExplicit declarations of variables in the texture state space is deprecated, and programs should\ninstead reference texture memory through variables of type\n.texref\n. The\n.tex\ndirective is\nretained for backward compatibility, and variables declared in the\n.tex\nstate space are\nequivalent to module-scoped\n.texref\nvariables in the\n.global\nstate space.\nFor example, a legacy PTX definitions such as\n.tex .u32 tex_a;\nis equivalent to:\n.global .texref tex_a;\nSee\nTexture Sampler and Surface Types\nfor the\ndescription of the\n.texref\ntype and\nTexture Instructions\nfor its use in texture instructions.\n5.2.\nTypes\n\n5.2.1.\nFundamental Types\n\nIn PTX, the fundamental types reflect the native data types supported by the target architectures. A\nfundamental type specifies both a basic type and a size. Register variables are always of a\nfundamental type, and instructions operate on these types. The same type-size specifiers are used\nfor both variable definitions and for typing instructions, so their names are intentionally short.\nTable 8\nlists the fundamental type specifiers for\neach basic type:\nTable 8\nFundamental Type Specifiers\n\nBasic Type\nFundamental Type Specifiers\nSigned integer\n.s8\n,\n.s16\n,\n.s32\n,\n.s64\nUnsigned integer\n.u8\n,\n.u16\n,\n.u32\n,\n.u64\nFloating-point\n.f16\n,\n.f16x2\n,\n.f32\n,\n.f64\nBits (untyped)\n.b8\n,\n.b16\n,\n.b32\n,\n.b64\n,\n.b128\nPredicate\n.pred\nMost instructions have one or more type specifiers, needed to fully specify instruction\nbehavior. Operand types and sizes are checked against instruction types for compatibility.\nTwo fundamental types are compatible if they have the same basic type and are the same size. Signed\nand unsigned integer types are compatible if they have the same size. The bit-size type is\ncompatible with any fundamental type having the same size.\nIn principle, all variables (aside from predicates) could be declared using only bit-size types, but\ntyped variables enhance program readability and allow for better operand type checking.\n5.2.2.\nRestricted Use of Sub-Word Sizes\n\nThe\n.u8\n,\n.s8\n, and\n.b8\ninstruction types are restricted to\nld\n,\nst\n, and\ncvt\ninstructions. The\n.f16\nfloating-point type is allowed only in conversions to and from\n.f32\n,\n.f64\ntypes, in half precision floating point instructions and texture fetch instructions. The\n.f16x2\nfloating point type is allowed only in half precision floating point arithmetic\ninstructions and texture fetch instructions.\nFor convenience,\nld\n,\nst\n, and\ncvt\ninstructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes.\n5.2.3.\nAlternate Floating-Point Data Formats\n\nThe fundamental floating-point types supported in PTX have implicit bit representations that\nindicate the number of bits used to store exponent and mantissa. For example, the\n.f16\ntype\nindicates 5 bits reserved for exponent and 10 bits reserved for mantissa. In addition to the\nfloating-point representations assumed by the fundamental types, PTX allows the following alternate\nfloating-point data formats:\nbf16\ndata format:\nThis data format is a 16-bit floating point format with 8 bits for exponent and 7 bits for\nmantissa. A register variable containing\nbf16\ndata must be declared with\n.b16\ntype.\ne4m3\ndata format:\nThis data format is an 8-bit floating point format with 4 bits for exponent and 3 bits for\nmantissa. The\ne4m3\nencoding does not support infinity and\nNaN\nvalues are limited to\n0x7f\nand\n0xff\n. A register variable containing\ne4m3\nvalue must be declared using\nbit-size type.\ne5m2\ndata format:\nThis data format is an 8-bit floating point format with 5 bits for exponent and 2 bits for\nmantissa. A register variable containing\ne5m2\nvalue must be declared using bit-size type.\ntf32\ndata format:\nThis data format is a special 32-bit floating point format supported by the matrix\nmultiply-and-accumulate instructions, with the same range as\n.f32\nand reduced precision (>=10\nbits). The internal layout of\ntf32\nformat is implementation defined. PTX facilitates\nconversion from single precision\n.f32\ntype to\ntf32\nformat. A register variable containing\ntf32\ndata must be declared with\n.b32\ntype.\ne2m1\ndata format:\nThis data format is a 4-bit floating point format with 2 bits for exponent and 1 bit for mantissa.\nThe\ne2m1\nencoding does not support infinity and\nNaN\n.\ne2m1\nvalues must be used in a\npacked format specified as\ne2m1x2\n. A register variable containing two\ne2m1\nvalues must be\ndeclared with\n.b8\ntype.\ne2m3\ndata format:\nThis data format is a 6-bit floating point format with 2 bits for exponent and 3 bits for mantissa.\nThe\ne2m3\nencoding does not support infinity and\nNaN\n.\ne2m3\nvalues must be used in a\npacked format specified as\ne2m3x2\n. A register variable containing two\ne2m3\nvalues must be\ndeclared with\n.b16\ntype where each\n.b8\nelement has 6-bit floating point value and 2 MSB\nbits padded with zeros.\ne3m2\ndata format:\nThis data format is a 6-bit floating point format with 3 bits for exponent and 2 bits for mantissa.\nThe\ne3m2\nencoding does not support infinity and\nNaN\n.\ne3m2\nvalues must be used in a\npacked format specified as\ne3m2x2\n. A register variable containing two\ne3m2\nvalues must be\ndeclared with\n.b16\ntype where each\n.b8\nelement has 6-bit floating point value and 2 MSB\nbits padded with zeros.\nue8m0\ndata format:\nThis data format is an 8-bit unsigned floating-point format with 8 bits for exponent and 0 bits for\nmantissa. The\nue8m0\nencoding does not support infinity.\nNaN\nvalue is limited to\n0xff\n.\nue8m0\nvalues must be used in a packed format specified as\nue8m0x2\n. A register variable\ncontaining two\nue8m0\nvalues must be declared with\n.b16\ntype.\nue4m3\ndata format:\nThis data format is a 7-bit unsigned floating-point format with 4 bits for exponent and 3 bits for\nmantissa. The\nue4m3\nencoding does not support infinity.\nNaN\nvalue is limited to\n0x7f\n.\nA register variable containing single\nue4m3\nvalue must be declared with\n.b8\ntype having\nMSB bit padded with zero.\nAlternate data formats cannot be used as fundamental types. They are supported as source or\ndestination formats by certain instructions.\n5.2.4.\nFixed-point Data format\n\nPTX supports following fixed-point data formats:\ns2f6\ndata format:\nThis data format is 8-bit signed 2’s complement integer with 2 sign-integer bits and\n6 fractional bits with form\nxx.xxxxxx\n. The\ns2f6\nencoding does not support infinity\nand\nNaN\n.\ns2f6\nvalue = s8 value * 2^(-6)\nPositive max representation = 01.111111 = 127 * 2^(-6) = 1.984375\nNegative max representation = 10.000000 = -128 * 2^(-6) = -2.0\n5.2.5.\nPacked Data Types\n\nCertain PTX instructions operate on two or more sets of inputs in parallel, and produce two or more\noutputs. Such instructions can use the data stored in a packed format. PTX supports packing two or\nfour values of the same scalar data type into a single, larger value. The packed value is considered\nas a value of a\npacked data type\n. In this section we describe the packed data types supported in PTX.\n5.2.5.1.\nPacked Floating Point Data Types\n\nPTX supports various variants of packed floating point data types. Out of them, only\n.f16x2\nis\nsupported as a fundamental type, while others cannot be used as fundamental types - they are\nsupported as instruction types on certain instructions. When using an instruction with such\nnon-fundamental types, the operand data variables must be of bit type of appropriate size.\nFor example, all of the operand variables must be of type\n.b32\nfor an instruction with\ninstruction type as\n.bf16x2\n.\nTable 9\ndescribed various variants\nof packed floating point data types in PTX.\nTable 9\nOperand types for packed floating point instruction type.\n\nPacked floating\npoint type\nNumber of elements\ncontained in a\npacked format\nType of each\nelement\nRegister variable type\nto be used in the\ndeclaration\n.f16x2\nTwo\n.f16\n.f16x2\nor\n.b32\n.f32x2\n.f32\n.b64\n.bf16x2\n.bf16\n.b32\n.e4m3x2\n.e4m3\n.b16\n.e5m2x2\n.e5m2\n.e2m3x2\n.e2m3\n.e3m2x2\n.e3m2\n.ue8m0x2\n.ue8m0\n.s2f6x2\n.s2f6\n.e2m1x2\n.e2m1\n.b8\n.e4m3x4\nFour\n.e4m3\n.b32\n.e5m2x4\n.e5m2\n.e2m3x4\n.e2m3\n.e3m2x4\n.e3m2\n.e2m1x4\n.e2m1\n.b16\n5.2.5.2.\nPacked Integer Data Types\n\nPTX supports two variants of packed integer data types:\n.u16x2\nand\n.s16x2\n. The packed data\ntype consists of two\n.u16\nor\n.s16\nvalues. A register variable containing\n.u16x2\nor\n.s16x2\ndata must be declared with\n.b32\ntype. Packed integer data types cannot be used as\nfundamental types. They are supported as instruction types on certain instructions.\n5.2.5.3.\nPacked Fixed-Point Data Types\n\nPTX supports\n.s2f6x2\npacked fixed-point data type consisting of two\n.s2f6\npacked\nfixed-point values. A register variable containing\n.s2f6x2\nvalue must be declared with\n.b16\ntype. Packed fixed-point data type cannot be used as fundamental type and is only\nsupported as instruction type.\n5.3.\nTexture Sampler and Surface Types\n\nPTX includes built-in\nopaque\ntypes for defining texture, sampler, and surface descriptor\nvariables. These types have named fields similar to structures, but all information about layout,\nfield ordering, base address, and overall size is hidden to a PTX program, hence the term\nopaque\n. The use of these opaque types is limited to:\nVariable definition within global (module) scope and in kernel entry parameter lists.\nStatic initialization of module-scope variables using comma-delimited static assignment\nexpressions for the named members of the type.\nReferencing textures, samplers, or surfaces via texture and surface load/store instructions\n(\ntex\n,\nsuld\n,\nsust\n,\nsured\n).\nRetrieving the value of a named member via query instructions (\ntxq\n,\nsuq\n).\nCreating pointers to opaque variables using\nmov\n, e.g.,\nmov.u64\nreg,\nopaque_var;\n. The\nresulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and\nde-referenced by texture and surface load, store, and query instructions, but the pointer cannot\notherwise be treated as an address, i.e., accessing the pointer with\nld\nand\nst\ninstructions, or performing pointer arithmetic will result in undefined results.\nOpaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque\nvariable.\nNote\nIndirect access to textures and surfaces using pointers to opaque variables is supported\nbeginning with PTX ISA version 3.1 and requires target\nsm_20\nor later.\nIndirect access to textures is supported only in unified texture mode (see below).\nThe three built-in types are\n.texref\n,\n.samplerref\n, and\n.surfref\n. For working with\ntextures and samplers, PTX has two modes of operation. In the\nunified mode,\ntexture and sampler\ninformation is accessed through a single\n.texref\nhandle. In the\nindependent mode\n, texture and\nsampler information each have their own handle, allowing them to be defined separately and combined\nat the site of usage in the program. In independent mode, the fields of the\n.texref\ntype that\ndescribe sampler properties are ignored, since these properties are defined by\n.samplerref\nvariables.\nTable 10\nand\nTable 11\nlist the named members\nof each type for unified and independent texture modes. These members and their values have\nprecise mappings to methods and values defined in the texture\nHW\nclass as well as\nexposed values via the API.\nTable 10\nOpaque Type Fields in Unified Texture Mode\n\nMember\n.texref values\n.surfref values\nwidth\nin elements\nheight\nin elements\ndepth\nin elements\nchannel_data_type\nenum\ntype corresponding to source language API\nchannel_order\nenum\ntype corresponding to source language API\nnormalized_coords\n0\n,\n1\nN/A\nfilter_mode\nnearest\n,\nlinear\nN/A\naddr_mode_0\n,\naddr_mode_1\n,\naddr_mode_2\nwrap\n,\nmirror\n,\nclamp_ogl\n,\nclamp_to_edge\n,\nclamp_to_border\nN/A\narray_size\nas number of textures in a texture\narray\nas number of surfaces in a surface array\nnum_mipmap_levels\nas number of levels in a mipmapped\ntexture\nN/A\nnum_samples\nas number of samples in a multi-sample\ntexture\nN/A\nmemory_layout\nN/A\n1\nfor linear memory layout;\n0\notherwise\n5.3.1.\nTexture and Surface Properties\n\nFields\nwidth\n,\nheight\n, and\ndepth\nspecify the size of the texture or surface in number of\nelements in each dimension.\nThe\nchannel_data_type\nand\nchannel_order\nfields specify these properties of the texture or\nsurface using enumeration types corresponding to the source language API. For example, see\nChannel Data Type and Channel Order Fields\nfor\nthe OpenCL enumeration types currently supported in PTX.\n5.3.2.\nSampler Properties\n\nThe\nnormalized_coords\nfield indicates whether the texture or surface uses normalized coordinates\nin the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N). If no value is\nspecified, the default is set by the runtime system based on the source language.\nThe\nfilter_mode\nfield specifies how the values returned by texture reads are computed based on\nthe input texture coordinates.\nThe\naddr_mode_{0,1,2}\nfields define the addressing mode in each dimension, which determine how\nout-of-range coordinates are handled.\nSee the\nCUDA C++ Programming Guide\nfor more details of these properties.\nTable 11\nOpaque Type Fields in Independent Texture Mode\n\nMember\n.samplerref values\n.texref values\n.surfref values\nwidth\nN/A\nin elements\nheight\nN/A\nin elements\ndepth\nN/A\nin elements\nchannel_data_type\nN/A\nenum\ntype corresponding to source\nlanguage API\nchannel_order\nN/A\nenum\ntype corresponding to source\nlanguage AP\nnormalized_coords\nN/A\n0\n,\n1\nN/A\nforce_unnormalized_coords\n0\n,\n1\nN/A\nN/A\nfilter_mode\nnearest\n,\nlinear\nignored\nN/A\naddr_mode_0\n,\naddr_mode_1\n,\naddr_mode_2\nwrap\n,\nmirror\n,\nclamp_ogl\n,\nclamp_to_edge\n,\nclamp_to_border\nN/A\nN/A\narray_size\nN/A\nas number of textures\nin a texture array\nas number of surfaces in\na surface array\nnum_mipmap_levels\nN/A\nas number of levels\nin a mipmapped\ntexture\nN/A\nnum_samples\nN/A\nas number of samples\nin a multi-sample\ntexture\nN/A\nmemory_layout\nN/A\nN/A\n1\nfor linear memory\nlayout;\n0\notherwise\nIn independent texture mode, the sampler properties are carried in an independent\n.samplerref\nvariable, and these fields are disabled in the\n.texref\nvariables. One additional sampler\nproperty,\nforce_unnormalized_coords\n, is available in independent texture mode.\nThe\nforce_unnormalized_coords\nfield is a property of\n.samplerref\nvariables that allows the\nsampler to override the texture header\nnormalized_coords\nproperty. This field is defined only in\nindependent texture mode. When\nTrue\n, the texture header setting is overridden and unnormalized\ncoordinates are used; when\nFalse\n, the texture header setting is used.\nThe\nforce_unnormalized_coords\nproperty is used in compiling OpenCL; in OpenCL, the property of\nnormalized coordinates is carried in sampler headers. To compile OpenCL to PTX, texture headers are\nalways initialized with\nnormalized_coords\nset to True, and the OpenCL sampler-based\nnormalized_coords\nflag maps (negated) to the PTX-level\nforce_unnormalized_coords\nflag.\nVariables using these types may be declared at module scope or within kernel entry parameter\nlists. At module scope, these variables must be in the\n.global\nstate space. As kernel\nparameters, these variables are declared in the\n.param\nstate space.\nExample\n.global .texref     my_texture_name;\n.global .samplerref my_sampler_name;\n.global .surfref    my_surface_name;\nWhen declared at module scope, the types may be initialized using a list of static expressions\nassigning values to the named members.\nExample\n.global .texref tex1;\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n5.3.3.\nChannel Data Type and Channel Order Fields\n\nThe\nchannel_data_type\nand\nchannel_order\nfields have enumeration types corresponding to the\nsource language API. Currently, OpenCL is the only source language that defines these\nfields.\nTable 13\nand\nTable 12\nshow the\nenumeration values defined in OpenCL version 1.0 for channel data type and channel order.\nTable 12\nOpenCL 1.0 Channel Data Type Definition\n\nCL_SNORM_INT8\n0x10D0\nCL_SNORM_INT16\n0x10D1\nCL_UNORM_INT8\n0x10D2\nCL_UNORM_INT16\n0x10D3\nCL_UNORM_SHORT_565\n0x10D4\nCL_UNORM_SHORT_555\n0x10D5\nCL_UNORM_INT_101010\n0x10D6\nCL_SIGNED_INT8\n0x10D7\nCL_SIGNED_INT16\n0x10D8\nCL_SIGNED_INT32\n0x10D9\nCL_UNSIGNED_INT8\n0x10DA\nCL_UNSIGNED_INT16\n0x10DB\nCL_UNSIGNED_INT32\n0x10DC\nCL_HALF_FLOAT\n0x10DD\nCL_FLOAT\n0x10DE\nTable 13\nOpenCL 1.0 Channel Order Definition\n\nCL_R\n0x10B0\nCL_A\n0x10B1\nCL_RG\n0x10B2\nCL_RA\n0x10B3\nCL_RGB\n0x10B4\nCL_RGBA\n0x10B5\nCL_BGRA\n0x10B6\nCL_ARGB\n0x10B7\nCL_INTENSITY\n0x10B8\nCL_LUMINANCE\n0x10B9\n5.4.\nVariables\n\nIn PTX, a variable declaration describes both the variable’s type and its state space. In addition\nto fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays.\n5.4.1.\nVariable Declarations\n\nAll storage for data is specified with variable declarations. Every variable must reside in one of\nthe state spaces enumerated in the previous section.\nA variable declaration names the space in which the variable resides, its type and size, its name,\nan optional array size, an optional initializer, and an optional fixed address for the variable.\nPredicate variables may only be declared in the register state space.\nExamples\n.global .u32 loc;\n.reg    .s32 i;\n.const  .f32 bias[] = {-1.0, 1.0};\n.global .u8  bg[4] = {0, 0, 0, 0};\n.reg    .v4 .f32 accel;\n.reg    .pred p, q, r;\n5.4.2.\nVectors\n\nLimited-length vector types are supported. Vectors of length 2 and 4 of any non-predicate\nfundamental type can be declared by prefixing the type with\n.v2\nor\n.v4\n. Vectors must be\nbased on a fundamental type, and they may reside in the register space. Vectors cannot exceed\n128-bits in length; for example,\n.v4\n.f64\nis not allowed. Three-element vectors may be\nhandled by using a\n.v4\nvector, where the fourth element provides padding. This is a common case\nfor three-dimensional grids, textures, etc.\nExamples\n.global .v4 .f32 V;   // a length-4 vector of floats\n.shared .v2 .u16 uv;  // a length-2 vector of unsigned ints\n.global .v4 .b8  v;   // a length-4 vector of bytes\nBy default, vector variables are aligned to a multiple of their overall size (vector length times\nbase-type size), to enable vector load and store instructions which require addresses aligned to a\nmultiple of the access size.\n5.4.3.\nArray Declarations\n\nArray declarations are provided to allow the programmer to reserve space. To declare an array, the\nvariable name is followed with dimensional declarations similar to fixed-size array declarations\nin C. The size of each dimension is a constant expression.\nExamples\n.local  .u16 kernel[19][19];\n.shared .u8  mailbox[128];\nThe size of the array specifies how many elements should be reserved. For the declaration of array\nkernel\nabove, 19*19 = 361 halfwords are reserved, for a total of 722 bytes.\nWhen declared with an initializer, the first dimension of the array may be omitted. The size of the\nfirst array dimension is determined by the number of elements in the array initializer.\nExamples\n.global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n.global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} };\nArray\nindex\nhas eight elements, and array\noffset\nis a 4x2 array.\n5.4.4.\nInitializers\n\nDeclared variables may specify an initial value using a syntax similar to C/C++, where the variable\nname is followed by an equals sign and the initial value or values for the variable. A scalar takes\na single value, while vectors and arrays take nested lists of values inside of curly braces (the\nnesting matches the dimensionality of the declaration).\nAs in C, array initializers may be incomplete, i.e., the number of initializer elements may be less\nthan the extent of the corresponding array dimension, with remaining array locations initialized to\nthe default value for the specified array type.\nExamples\n.const  .f32 vals[8] = { 0.33, 0.25, 0.125 };\n.global .s32 x[3][2] = { {1,2}, {3} };\nis equivalent to\n.const  .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 };\n.global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} };\nCurrently, variable initialization is supported only for constant and global state spaces. Variables\nin constant and global state spaces with no explicit initializer are initialized to zero by\ndefault. Initializers are not allowed in external variable declarations.\nVariable names appearing in initializers represent the address of the variable; this can be used to\nstatically initialize a pointer to a variable. Initializers may also contain\nvar+offset\nexpressions, where\noffset\nis a byte offset added to the address of\nvar\n. Only variables in\n.global\nor\n.const\nstate spaces may be used in initializers. By default, the resulting\naddress is the offset in the variable’s state space (as is the case when taking the address of a\nvariable with a\nmov\ninstruction). An operator,\ngeneric()\n, is provided to create a generic\naddress for variables used in initializers.\nStarting PTX ISA version 7.1, an operator\nmask()\nis provided, where\nmask\nis an integer\nimmediate. The only allowed expressions in the\nmask()\noperator are integer constant expression\nand symbol expression representing address of variable. The\nmask()\noperator extracts\nn\nconsecutive bits from the expression used in initializers and inserts these bits at the lowest\nposition of the initialized variable. The number\nn\nand the starting position of the bits to be\nextracted is specified by the integer immediate\nmask\n. PTX ISA version 7.1 only supports\nextracting a single byte starting at byte boundary from the address of the variable. PTX ISA version\n7.3 supports Integer constant expression as an operand in the\nmask()\noperator.\nSupported values for\nmask\nare: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000,\n0xFF000000000000, 0xFF00000000000000.\nExamples\n.const  .u32 foo = 42;\n.global .u32 bar[] = { 2, 3, 5 };\n.global .u32 p1 = foo;          // offset of foo in .const space\n.global .u32 p2 = generic(foo); // generic address of foo\n\n// array of generic-address pointers to elements of bar\n.global .u32 parr[] = { generic(bar), generic(bar)+4,\ngeneric(bar)+8 };\n\n// examples using mask() operator are pruned for brevity\n.global .u8 addr[] = {0xff(foo), 0xff00(foo), 0xff0000(foo), ...};\n\n.global .u8 addr2[] = {0xff(foo+4), 0xff00(foo+4), 0xff0000(foo+4),...}\n\n.global .u8 addr3[] = {0xff(generic(foo)), 0xff00(generic(foo)),...}\n\n.global .u8 addr4[] = {0xff(generic(foo)+4), 0xff00(generic(foo)+4),...}\n\n// mask() operator with integer const expression\n.global .u8 addr5[] = { 0xFF(1000 + 546), 0xFF00(131187), ...};\nNote\nPTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit\ngeneric()\noperator for each global variable used in an initializer. PTX 3.1 code should\neither include explicit\ngeneric()\noperators in initializers, use\ncvta.global\nto form\ngeneric addresses at runtime, or load from the non-generic address using\nld.global\n.\nDevice function names appearing in initializers represent the address of the first instruction in\nthe function; this can be used to initialize a table of function pointers to be used with indirect\ncalls. Beginning in PTX ISA version 3.1, kernel function names can be used as initializers e.g. to\ninitialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch\nkernels from GPU. See the\nCUDA Dynamic Parallelism Programming Guide\nfor details.\nLabels cannot be used in initializers.\nVariables that hold addresses of variables or functions should be of type\n.u8\nor\n.u32\nor\n.u64\n.\nType\n.u8\nis allowed only if the\nmask()\noperator is used.\nInitializers are allowed for all types except\n.f16\n,\n.f16x2\nand\n.pred\n.\nExamples\n.global .s32 n = 10;\n.global .f32 blur_kernel[][3]\n               = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}};\n\n.global .u32 foo[] = { 2, 3, 5, 7, 9, 11 };\n.global .u64 ptr = generic(foo);   // generic address of foo[0]\n.global .u64 ptr = generic(foo)+8; // generic address of foo[2]\n5.4.5.\nAlignment\n\nByte alignment of storage for all addressable variables can be specified in the variable\ndeclaration. Alignment is specified using an optional\n.align\nbyte-count\nspecifier immediately\nfollowing the state-space specifier. The variable will be aligned to an address which is an integer\nmultiple of byte-count. The alignment value byte-count must be a power of two. For arrays, alignment\nspecifies the address alignment for the starting address of the entire array, not for individual\nelements.\nThe default alignment for scalar and array variables is to a multiple of the base-type size. The\ndefault alignment for vector variables is to a multiple of the overall vector size.\nExamples\n// allocate array at 4-byte aligned address.  Elements are bytes.\n.const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0};\nNote that all PTX instructions that access memory require that the address be aligned to a multiple\nof the access size. The access size of a memory instruction is the total number of bytes accessed in\nmemory. For example, the access size of\nld.v4.b32\nis 16 bytes, while the access size of\natom.f16x2\nis 4 bytes.\n5.4.6.\nParameterized Variable Names\n\nSince PTX supports virtual registers, it is quite common for a compiler frontend to generate a large\nnumber of register names. Rather than require explicit declaration of every name, PTX supports a\nsyntax for creating a set of variables having a common prefix string appended with integer suffixes.\nFor example, suppose a program uses a large number, say one hundred, of\n.b32\nvariables, named\n%r0\n,\n%r1\n, …,\n%r99\n. These 100 register variables can be declared as follows:\n.reg .b32 %r<100>;    // declare %r0, %r1, ..., %r99\nThis shorthand syntax may be used with any of the fundamental types and with any state space, and\nmay be preceded by an alignment specifier. Array variables cannot be declared this way, nor are\ninitializers permitted.\n5.4.7.\nVariable Attributes\n\nVariables may be declared with an optional\n.attribute\ndirective which allows specifying special\nattributes of variables. Keyword\n.attribute\nis followed by attribute specification inside\nparenthesis. Multiple attributes are separated by comma.\nVariable and Function Attribute Directive: .attribute\ndescribes the\n.attribute\ndirective.\n5.4.8.\nVariable and Function Attribute Directive:\n.attribute\n\n.attribute\nVariable and function attributes\nDescription\nUsed to specify special attributes of a variable or a function.\nThe following attributes are supported.\n.managed\n.managed\nattribute specifies that variable will be allocated at a location in unified virtual\nmemory environment where host and other devices in the system can reference the variable\ndirectly. This attribute can only be used with variables in .global state space. See the\nCUDA\nUVM-Lite Programming Guide\nfor details.\n.unified\n.unified\nattribute specifies that function has the same memory address on the host and on\nother devices in the system. Integer constants\nuuid1\nand\nuuid2\nrespectively specify upper\nand lower 64 bits of the unique identifier associated with the function or the variable. This\nattribute can only be used on device functions or on variables in the\n.global\nstate\nspace. Variables with\n.unified\nattribute are read-only and must be loaded by specifying\n.unified\nqualifier on the address operand of\nld\ninstruction, otherwise the behavior is\nundefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.0.\nSupport for function attributes introduced in PTX ISA version 8.0.\nTarget ISA Notes\n.managed\nattribute requires\nsm_30\nor higher.\n.unified\nattribute requires\nsm_90\nor higher.\nExamples\n.global .attribute(.managed) .s32 g;\n.global .attribute(.managed) .u64 x;\n\n.global .attribute(.unified(19,95)) .f32 f;\n\n.func .attribute(.unified(0xAB, 0xCD)) bar() { ... }\n5.5.\nTensors\n\nA tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following\nproperties:\nDimensionality\nDimension sizes across each dimension\nIndividual element types\nTensor stride across each dimension\nPTX supports instructions which can operate on the tensor data. PTX Tensor instructions include:\nCopying data between global and shared memories\nReducing the destination tensor data with the source.\nThe Tensor data can be operated on by various\nwmma.mma\n,\nmma\nand\nwgmma.mma_async\ninstructions.\nPTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure\nand treat the data in the shared memory as a linear data.\n5.5.1.\nTensor Dimension, size and format\n\nTensors can have dimensions: 1D, 2D, 3D, 4D or 5D.\nEach dimension has a size which represents the number of elements along the dimension. The elements\ncan have one the following types:\nBit-sized type:\n.b32\n,\n.b64\nSub-byte types:\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\n,\n.b6p2x16\nInteger:\n.u8\n,\n.u16\n,\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\nFloating point and alternate floating point:\n.f16\n,\n.bf16\n,\n.tf32\n,\n.f32\n,\n.f64\n(rounded to nearest even).\nTensor can have padding at the end in each of the dimensions to provide alignment for the data in\nthe subsequent dimensions. Tensor stride can be used to specify the amount of padding in each\ndimension.\n5.5.1.1.\nSub-byte Types\n\n5.5.1.1.1.\nPadding and alignment of the sub-byte types\n\nThe sub-byte types are expected to packed contiguously in the global memory and\nthe Tensor copy instruction will expand them by appending empty spaces as shown below:\nType\n.b4x16\n:\nWith this type, there is no padding involved and the packed sixteen\n.b4\nelements\nin a 64-bits container is copied as is between the shared memory and the global memory.\nType\n.b4x16_p64\n:\nWith this type, sixteen contiguous 4-bits of data is copied from global memory to the\nshared memory with the append of 64-bits of padding as shown in\nFigure 5\nFigure 5\nLayout for .b4x16_p64\n\nThe padded region that gets added is un-initialized.\nType\n.b6x16_p32\n:\nWith this type, sixteen 6-bits of data is copied from global memory to the shared memory\nwith an append of 32-bits of padding as shown in\nFigure 6\nFigure 6\nLayout for .b6x16_p32\n\nThe padded region that gets added is un-initialized.\nType\n.b6p2x16\n:\nWith this type, sixteen elements, each containing 6-bits of data at the LSB and 2-bits\nof padding at the MSB, are copied from shared memory into the global memory by discarding\nthe 2-bits of padding data and packing the 6-bits data contiguously as shown in\nFigure 7\nFigure 7\nLayout for .b6p2x16\n\nIn case of\n.b6x16_p32\nand\n.b4x16_p64\n, the padded region that gets added is\nun-initialized.\nThe types\n.b6x16_p32\nand\n.b6p2x16\nshare the same encoding value in the\ndescriptor (value 15) as the two types are applicable for different types of\ntensor copy operations:\nType\nValid Tensor Copy Direction\n.b6x16_p32\n.shared::cluster.global\n,\n.shared::cta.global\n.b6p2x16\n.global.shared::cta\n5.5.2.\nTensor Access Modes\n\nTensor data can be accessed in two modes:\nTiled mode:\nIn tiled mode, the source multi-dimensional tensor layout is preserved at the destination.\nIm2col mode:\nIn im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns\nat the destination. Refer\nhere\nfor more details.\n5.5.3.\nTiled Mode\n\nThis section talks about how Tensor and Tensor access work in tiled mode.\n5.5.3.1.\nBounding Box\n\nA tensor can be accessed in chunks known as\nBounding Box\n. The Bounding Box has the same\ndimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple\nof 16 bytes. The address of the bounding Box must also be aligned to 16 bytes.\nBounding Box has the following access properties:\nBounding Box dimension sizes\nOut of boundary access mode\nTraversal strides\nThe tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the\nbounding box. Starting offset of the bounding box along with the rest of the bounding box\ninformation together are used to determine the elements which are to be accessed.\n5.5.3.2.\nTraversal-Stride\n\nWhile the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies\nthe exact number of elements to be skipped. If no jump over is required, default value of 1 must be\nspecified.\nThe traversal stride in dimension 0 can be used for the\nInterleave layout\n.\nFor non-interleaved layout, the traversal stride in\ndimension 0 must always be 1.\nFigure 8\nillustrates tensor, tensor size, tensor stride,\nBounding Box size and traversal stride.\nFigure 8\nTiled mode bounding box, tensor size and traversal stride\n\n5.5.3.3.\nOut of Boundary Access\n\nPTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor\nboundary in any dimension. There are 2 modes:\nZero fill mode:\nElements in the Bounding Box which fall outside of the tensor boundary are set to 0.\nOOB-NaN\nfill mode:\nElements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN\ncalled\nOOB-NaN\n.\nFigure 9\nshows an example of the out of boundary access.\nFigure 9\nOut of boundary access\n\n5.5.3.4.\n.tile::scatter4\nand\n.tile::gather4\nmodes\n\nThese modes are similar to the tiled mode with restriction that these modes work only on 2D tensor data.\nTile::scatter4\nand\nTile::gather4\nmodes are used to access multiple non-contiguous rows of tensor data.\nIn\nTile::scatter4\nmode single 2D source tensor is divided into four rows in the 2D destination tensor.\nIn\nTile::gather4\nmode four rows in the source 2D tensor are combined to form single 2D destination tensor.\nThese modes work on four rows and hence the instruction will take:\nfour tensor coordinates across the dimension 0\none tensor coordinate across the dimension 1\nThe interleave layout is not supported for\n.tile::scatter4\nand\n.tile::gather4\nmodes.\nAll other constraints and rules of the tile mode apply to these modes as well.\n5.5.3.4.1.\nBounding Box\n\nFor\nTile::scatter4\nand\nTile::gather4\nmodes, four request coordinates will form four Bounding\nBoxes in the tensor space.\nFigure 10\nshows an example of the same with start\ncoordinates (1, 2), (1, 5), (1, 0) and (1, 9).\nThe size of the bounding box in the dimension 0 represents the length of the rows.\nThe size of the bounding box in the dimension 1 must be one.\nFigure 10\ntiled::scatter4/tiled::gather4 mode bounding box example\n\n5.5.4.\nim2col\nmode\n\nIm2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data\nis treated as a batch of images with the following properties:\nN : number of images in the batch\nD, H, W : size of a 3D image (depth, height and width)\nC: channels per image element\nThe above properties are associated with 3D, 4D and 5D tensors as follows:\nDimension\nN/D/H/W/C applicability\n3D\nNWC\n4D\nNHWC\n5D\nNDHWC\n5.5.4.1.\nBounding Box\n\nIn im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are\nspecified by Pixels-per-Column and Channels-per-Pixel parameters as described below.\nThe dimensionality of the Bounding Box is two less than the tensor dimensionality.\nThe following properties describe how to access of the elements in im2col mode:\nBounding-Box Lower-Corner\nBounding-Box Upper-Corner\nPixels-per-Column\nChannels-per-Pixel\nBounding-box Lower-Corner\nand\nBounding-box Upper-Corner\nspecify the two opposite corners of the\nBounding Box in the DHW space.\nBounding-box Lower-Corner\nspecifies the corner with the smallest\ncoordinate and\nBounding-box Upper-Corner\nspecifies the corner with the largest coordinate.\nBounding-box Upper-\nand\nLower-Corners\nare 16-bit signed values whose limits varies across the\ndimensions and are as shown below:\n3D\n4D\n5D\nUpper- / Lower- Corner sizes\n[-2\n15\n, 2\n15\n-1]\n[-2\n7\n, 2\n7\n-1]\n[-2\n4\n, 2\n4\n-1]\nFigure 11\nand\nFigure 12\nshow the Upper-Corners and Lower-Corners.\nFigure 11\nim2col mode bounding box example 1\n\nFigure 12\nim2col mode bounding box example 2\n\nThe\nBounding-box Upper-\nand\nLower- Corners\nspecify only the boundaries and not the number of\nelements to be accessed.\nPixels-per-Column\nspecifies the number of elements to be accessed in the\nNDHW space.\nChannels-per-Pixel\nspecifies the number of elements to access across the C dimension.\nThe tensor coordinates, specified in the PTX tensor instructions, behaves differently in different\ndimensions:\nAcross N and C dimensions: specify the starting offsets along the dimension, similar to the tiled\nmode.\nAcross DHW dimensions: specify the location of the convolution filter base in the tensor\nspace. The filter corner location must be within the bounding box.\nThe im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter\nbase coordinates to determine the starting location in the tensor space from where the elements are\naccessed.\nThe size of the im2col offsets varies across the dimensions and their valid ranges are as shown\nbelow:\n3D\n4D\n5D\nim2col offsets range\n[0, 2\n16\n-1]\n[0, 2\n8\n-1]\n[0, 2\n5\n-1]\nFollowing are some examples of the im2col mode accesses:\nExample 1 (\nFigure 13\n):\nTensor\nSize\n[\n0\n]\n=\n64\nTensor\nSize\n[\n1\n]\n=\n9\nTensor\nSize\n[\n2\n]\n=\n14\nTensor\nSize\n[\n3\n]\n=\n64\nPixels\n-\nper\n-\nColumn\n=\n64\nchannels\n-\nper\n-\npixel\n=\n8\nBounding\n-\nBox\nLower\n-\nCorner\nW\n=\n-1\nBounding\n-\nBox\nLower\n-\nCorner\nH\n=\n-1\nBounding\n-\nBox\nUpper\n-\nCorner\nW\n=\n-1\nBounding\n-\nBox\nUpper\n-\nCorner\nH\n=\n-1.\ntensor\ncoordinates\n=\n(\n7\n,\n7\n,\n4\n,\n0\n)\nim2col\noffsets\n:\n(\n0\n,\n0\n)\nFigure 13\nim2col mode example 1\n\nExample 2 (\nFigure 14\n):\nTensor\nSize\n[\n0\n]\n=\n64\nTensor\nSize\n[\n1\n]\n=\n9\nTensor\nSize\n[\n2\n]\n=\n14\nTensor\nSize\n[\n3\n]\n=\n64\nPixels\n-\nper\n-\nColumn\n=\n64\nchannels\n-\nper\n-\npixel\n=\n8\nBounding\n-\nBox\nLower\n-\nCorner\nW\n=\n0\nBounding\n-\nBox\nLower\n-\nCorner\nH\n=\n0\nBounding\n-\nBox\nUpper\n-\nCorner\nW\n=\n-2\nBounding\n-\nBox\nUpper\n-\nCorner\nH\n=\n-2\ntensor\ncoordinates\n=\n(\n7\n,\n7\n,\n4\n,\n0\n)\nim2col\noffsets\n:\n(\n2\n,\n2\n)\nFigure 14\nim2col mode example 2\n\n5.5.4.2.\nTraversal Stride\n\nThe traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being\naccessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being\naccessed, in im2col mode.\nThe number of elements traversed along the D, H and W dimensions is strided by the traversal stride\nfor that dimension.\nThe following example with\nFigure 15\nillustrates accesse with traversal-strides:\nTensor Size[0] = 64\nTensor Size[1] = 8\nTensor Size[2] = 14\nTensor Size[3] = 64\nTraversal Stride = 2\nPixels-per-Column = 32\nchannels-per-pixel = 16\nBounding-Box Lower-Corner W = -1\nBounding-Box Lower-Corner H = -1\nBounding-Box Upper-Corner W = -1\nBounding-Box Upper-Corner H = -1.\nTensor coordinates in the instruction = (7, 7, 5, 0)\nIm2col offsets in the instruction : (1, 1)\nFigure 15\nim2col mode traversal stride example\n\n5.5.4.3.\nOut of Boundary Access\n\nIn im2col mode, when the number of requested pixels in NDHW space specified by\nPixels-per-Column\nexceeds the number of available pixels in the image batch then out-of-bounds access is performed.\nSimilar to tiled mode, zero fill or\nOOB-NaN\nfill can be performed based on the Fill-Mode\nspecified.\n5.5.5.\nim2col::w\nand\nim2col::w::128\nmodes\n\nThese modes are similar to the im2col mode with the restriction that elements are accessed across\nthe\nW\ndimension only while keeping the\nH\nand\nD\ndimension constant.\nAll the constraints and rules of the im2col mode apply to these modes as well.\nThe number of elements accessed in the\nim2col::w::128\nmode is fixed and is equal to 128.\nThe number of elements accessed in the\nim2col::w\nmode depends on the field Pixels-per-Column\nfield in the TensorMap.\n5.5.5.1.\nBounding Box\n\nIn these modes, the size of the bounding box in\nD\nand\nH\ndimensions are 1.\nThe\nD\nand\nH\ndimensions in the tensor coordinates argument in the PTX instruction specify\nthe position of the bounding box in the tensor space.\nThe Bounding-Box\nLower-Corner-W\nand Bounding-Box\nUpper-Corner-W\nspecify the two opposite\ncorners of the Bounding Box in the\nW\ndimension.\nThe\nW\ndimension in the tensor coordinates argument in the PTX instruction specify the location\nof the first element that is to be accessed in the bounding box.\nNumber of pixels loaded in\nim2col::w\nmode is as specified by Pixels-per-Column in the TensorMap.\nNumber of pixels loaded in\nim2col::w::128\nmode is always 128. So, Pixels-per-Column is ignored\nin\nim2col::w::128\nmode.\nFigure 16\nshows an example of the\nim2col::w\nand\nim2col::w:128\nmodes.\nFigure 16\nim2col::w and im2col::w::128 modes example\n\nThe first element can lie outside of the Bounding Box in the W-dimension only and only on the left\nside of the Bounding Box.\nFigure 17\nshows of an example of this.\nFigure 17\nim2col::w and im2col::w::128 modes first element outside Bounding Box example\n\n5.5.5.2.\nTraversal Stride\n\nThis is similar to im2col mode with the exception of that the number of elements traversed\nalong only the\nW\ndimension is strided by the traversal stride as specified in the TensorMap.\n5.5.5.3.\nwHalo\n\nIn\nim2col::w\nmode, the\nwHalo\nargument in the PTX instruction specifies how many filter\nhalo elements must be loaded at the end of the image.\nIn\nim2col::w::128\nmode, the halo elements are loaded after every 32 elements in the bounding\nbox along the\nW\ndimension. The\nwHalo\nargument in the PTX instruction specifies how many\nhalo elements must be loaded after every 32 elements.\nFollowing is an example of\n.im2col::w\nmode access:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 7\nTensor Size [3] = 64\nPixels-per-column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = 0\nBounding Box Upper Corner W = 0\n\nTensor Coordinates in the instruction = (7, 2, 3, 0)\nwHalo in the instruction = 2 (as 3x3 convolution filter is used)\nA tensor copy operation with the above parameters loads 128 pixels and the two halo pixels as shown in\nFigure 18\n.\nFigure 18\ntensor copy operation with im2col::w mode example\n\nThe halo pixels are always loaded in the shared memory next to the main row pixels as shown in\nFigure 18\n.\nFollowing is an example of\n.im2col::w::128\nmode access:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 7\nTensor Size [3] = 64\nChannels-per-pixel = 64\nBounding Box Lower Corner W = 0\nBounding Box Upper Corner W = 0\n\nTensor Coordinates in the instruction = (7, 2, 3, 0)\nwHalo in the instruction = 2 (as 3x3 convolution filter is used)\nA tensor copy operation with the above parameters loads 128 elements such that after every 32 elements,\nwHalo number of elements are loaded as shown in\nFigure 19\n.\nFigure 19\ntensor copy operation with im2col::w::128 mode example\n\n5.5.5.4.\nwOffset\n\nIn the convolution calculations, the same elements along the\nW\ndimension are reused for different\nlocations within the convolution filter footprint. Based on the number of times a pixel is used, the\npixels may be loaded into different shared memory buffers. Each buffer can be loaded by a separate\ntensor copy operation.\nThe\nwOffset\nargument in the tensor copy and prefetch instruction adjusts the source pixel location\nfor each buffer. The exact position of the buffer is adjusted along the\nW\ndimension using the\nfollowing formula:\nBounding Box Lower Corner W += wOffset\nBounding Box Upper Corner W += wOffset\nW += wOffset\nFollowing are examples of tensor copy to multiple buffers with various\nwHalo\nand\nwOffset\nvalues:\nExample 1:\nTensor Size [0] = 128\nTensor Size [1] = 9\nTensor Size [2] = 67\nTensor Size [3] = 64\nPixels-per-Column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = -1\nBounding Box Upper Corner W = 0\nTraversal Stride = 2\n\nTensor Coordinates in the instruction = (7, 2, -1, 0)\n\nShared memory buffer 1:\n   wHalo = 1\n   wOffset = 0\n\nShared memory buffer 2:\n   wHalo = 0\n   wOffset = 1\nFigure 20\ntensor copy operation to buffer 1 of Example 1\n\nFigure 21\ntensor copy operation to buffer 2 of Example 1\n\nExample 2:\nTensor Size [0] = 128\nTensor Size [1] = 7\nTensor Size [2] = 7\nTensor Size [3] = 64\nPixels-per-Column = 128\nChannels-per-pixel = 64\nBounding Box Lower Corner W = -1\nBounding Box Upper Corner W = -1\nTraversal Stride = 3\n\nTensor Coordinates in the instruction = (7, 2, -1, 0)\n\nShared memory buffer 1:\n   wHalo = 0\n   wOffset = 0\n\nShared memory buffer 2:\n   wHalo = 0\n   wOffset = 1\n\nShared memory buffer 3:\n   wHalo = 0\n   wOffset = 2\nFigure 22\ntensor copy operation to buffer 1 of Example 2\n\nFigure 23\ntensor copy operation to buffer 2 of Example 2\n\nFigure 24\ntensor copy operation to buffer 3 of Example 2\n\n5.5.6.\nInterleave layout\n\nTensor can be interleaved and the following interleave layouts are supported:\nNo interleave (NDHWC)\n8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel.\n16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel.\nThe\nC\ninformation is organized in slices where sequential C elements are grouped in 16 byte or 32\nbyte quantities.\nIf the total number of channels is not a multiple of the number of channels per slice, then the last\nslice must be padded with zeros to make it complete 16B or 32B slice.\nInterleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D.\nThe interleave layout is not supported for\n.im2col::w\nand\n.im2col::w::128\nmodes.\n5.5.7.\nSwizzling Modes\n\nThe layout of the data in the shared memory can be different to that of global memory, for access\nperformance reasons. The following describes various swizzling modes:\nNo swizzle mode:\nThere is no swizzling in this mode and the destination data layout is exactly similar to the\nsource data layout.\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n… Pattern repeats …\n32 byte swizzle mode:\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n256 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n… Pattern repeats …\nAn example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension,\nwith the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in\nFigure 25\n.\nFigure 25\n32-byte swizzle mode example\n\nFigure 26\nshows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1.\nFigure 26\n32-byte swizzle mode fragments\n\nFigure 27\nshows the destination data layout with 32 byte swizzling.\nFigure 27\n32-byte swizzle mode destination data layout\n\n64 byte swizzle mode:\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n512 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n2\n3\n0\n1\n6\n7\n4\n5\n3\n2\n1\n0\n7\n6\n5\n4\n… Pattern repeats …\nAn example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 32 channels, is shown in\nFigure 28\n.\nFigure 28\n64-byte swizzle mode example\n\nEach colored cell represents 8 channels.\nFigure 29\nshows the source data layout.\nFigure 29\n64-byte swizzle mode source data layout\n\nFigure 30\nshows the destination data layout with 64 byte swizzling.\nFigure 30\n64-byte swizzle mode destination data layout\n\n96 byte swizzle mode:\nThe following table where each element (numbered cell) is 16 byte shows the swizzling pattern at the destination\ndata layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n… Pattern repeats …\nAn example of the data layout in global memory and its swizzled data layout in shared memory where each element\n(colored cell) is 16 bytes and the starting address is 256 bytes aligned is shown in\nFigure 31\n.\nFigure 31\n96-byte swizzle mode example\n\n128 byte swizzle mode:\nThe 128-byte swizzling mode supports the following sub-modes:\n16-byte atomicity sub-mode:\nIn this sub-mode, the 16-byte of data is kept intact while swizzling.\nThe following table, where each elements (numbered cell) is 16 byte and the starting address is\n1024 bytes aligned, shows the pattern of the destination data layout:\n0\n1\n2\n3\n4\n5\n6\n7\n1\n0\n3\n2\n5\n4\n7\n6\n2\n3\n0\n1\n6\n7\n4\n5\n3\n2\n1\n0\n7\n6\n5\n4\n4\n5\n6\n7\n0\n1\n2\n3\n5\n4\n7\n6\n1\n0\n3\n2\n6\n7\n4\n5\n2\n3\n0\n1\n7\n6\n5\n4\n3\n2\n1\n0\n… Pattern repeats …\nAn example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /\nchannel and 64 channels, is shown in\nFigure 32\n.\nFigure 32\n128-byte swizzle mode example\n\nEach colored cell represents 8 channels.\nFigure 33\nshows the source data layout.\nFigure 33\n128-byte swizzle mode source data layout\n\nFigure 34\nshows the destination data layout with 128 byte swizzling.\nFigure 34\n128-byte swizzle mode destination data layout\n\n32-byte atomicity sub-mode:\nIn this sub-mode, the 32-byte of data is kept intact while swizzling.\nThe following table where each element (numbered cell) is 16 byte shows the\nswizzling pattern at the destination data layout:\n0   1\n2   3\n4   5\n6   7\n2   3\n0   1\n6   7\n4   5\n4   5\n6   7\n0   1\n2   3\n6   7\n4   5\n2   3\n0   1\n… Pattern repeats …\nThis sub-mode requires 32 byte alignment at shared memory.\nAn example of the data layout in global memory and its swizzled data layout in shared memory\nwhere each element (colored cell) is 16 bytes is shown in\nFigure 35\nFigure 35\n128-byte swizzle mode example with 32-byte atomicity\n\n32-byte atomicity with 8-byte flip sub-mode:\nThe swizzling pattern for this sub-mode is similar to the 32-byte atomicity sub-mode except that\nthere is a flip of adjacent 8-bytes within the 16-byte data at every alternate shared memory line.\nAn example of the data layout in global memory and its swizzled data layout in shared memory where\neach element (colored cell) is 16 bytes (two 8-byte sub-elements for each 16-byte colored cell are\nshown to show the flip) is shown in\nFigure 36\nFigure 36\n128-byte swizzle mode example with 32-byte atomicity with 8-byte flip\n\n64-byte atomicity sub-mode:\nIn this sub-mode, the 64-byte of data is kept intact while swizzling.\nThe following table where each element (numbered cell) is 16 byte shows the swizzling\npattern at the destination data layout:\n0   1   2   3\n4   5   6   7\n4   5   6   7\n0   1   2   3\n… Pattern repeats …\nThis sub-mode requires 64-byte alignment at shared memory.\nAn example of the data layout in global memory and its swizzled data layout\nin shared memory where each element (colored cell) is 16 bytes is shown\nin\nFigure 37\nFigure 37\n128-byte swizzle mode example with 64-byte atomicity\n\nTable 14\nlists the valid combination of swizzle-atomicity with the swizzling-mode.\nTable 14\nValid combination of swizzle-atomicity with swizzling-mode\n\nSwizzling Mode\nSwizzle-Atomicity\nNo Swizzling\n–\n32B Swizzling Mode\n16B\n64B Swizzling Mode\n16B\n96B Swizzling Mode\n16B\n128B Swizzling Mode\n16B\n32B\n32B + 8B-flip\n64B\nThe value of swizzle base offset is 0 when the\ndstMem\nshared memory address is located\nat the following boundary:\nSwizzling Mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n96-Byte swizzle\n256-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the swizzle base offset is a non-zero value, computed using following formula:\nSwizzling Mode\nFormula\n128-Byte swizzle\nbase offset = (dstMem / 128) % 8\n96-Byte swizzle\nbase offset = (dstMem / 128) % 2\n64-Byte swizzle\nbase offset = (dstMem / 128) % 4\n32-Byte swizzle\nbase offset = (dstMem / 128) % 2\n5.5.8.\nTensor-map\n\nThe tensor-map is a 128-byte opaque object either in\n.const\nspace or\n.param\n(kernel function\nparameter) space or\n.global\nspace which describes the tensor properties and the access properties\nof the tensor data described in previous sections.\nTensor-Map can be created using CUDA APIs. Refer to\nCUDA programming guide\nfor more details.\n6.\nInstruction Operands\n\n6.1.\nOperand Type Information\n\nAll operands in instructions have a known type from their declarations. Each operand type must be\ncompatible with the type determined by the instruction template and instruction type. There is no\nautomatic conversion between types.\nThe bit-size type is compatible with every type having the same size. Integer types of a common size\nare compatible with each other. Operands having type different from but compatible with the\ninstruction type are silently cast to the instruction type.\n6.2.\nSource Operands\n\nThe source operands are denoted in the instruction descriptions by the names\na\n,\nb\n, and\nc\n. PTX describes a load-store machine, so operands for ALU instructions must all be in variables\ndeclared in the\n.reg\nregister state space. For most operations, the sizes of the operands must\nbe consistent.\nThe\ncvt\n(convert) instruction takes a variety of operand types and sizes, as its job is to\nconvert from nearly any data type to any other data type (and size).\nThe\nld\n,\nst\n,\nmov\n, and\ncvt\ninstructions copy data from one location to\nanother. Instructions\nld\nand\nst\nmove data from/to addressable state spaces to/from\nregisters. The\nmov\ninstruction copies data between registers.\nMost instructions have an optional predicate guard that controls conditional execution, and a few\ninstructions have additional predicate source operands. Predicate operands are denoted by the names\np\n,\nq\n,\nr\n,\ns\n.\n6.3.\nDestination Operands\n\nPTX instructions that produce a single result store the result in the field denoted by\nd\n(for\ndestination) in the instruction descriptions. The result operand is a scalar or vector variable in\nthe register state space.\n6.4.\nUsing Addresses, Arrays, and Vectors\n\nUsing scalar variables as operands is straightforward. The interesting capabilities begin with\naddresses, arrays, and vectors.\n6.4.1.\nAddresses as Operands\n\nAll the memory instructions take an address operand that specifies the memory location being\naccessed. This addressable operand is one of:\n[var]\nthe name of an addressable variable\nvar\n.\n[reg]\nan integer or bit-size type register\nreg\ncontaining a byte address.\n[reg+immOff]\na sum of register\nreg\ncontaining a byte address plus a constant integer byte offset (signed, 32-bit).\n[var+immOff]\na sum of address of addressable variable\nvar\ncontaining a byte address plus a constant integer\nbyte offset (signed, 32-bit).\n[immAddr]\nan immediate absolute byte address (unsigned, 32-bit).\nvar[immOff]\nan array element as described in\nArrays as Operands\n.\nThe register containing an address may be declared as a bit-size type or integer type.\nThe access size of a memory instruction is the total number of bytes accessed in memory. For\nexample, the access size of\nld.v4.b32\nis 16 bytes, while the access size of\natom.f16x2\nis 4\nbytes.\nThe address must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined. For example, among other things, the access\nmay proceed by silently masking off low-order address bits to achieve proper rounding, or the\ninstruction may fault.\nThe address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. Addresses are\nzero-extended to the specified width as needed, and truncated if the register width exceeds the\nstate space address width for the target architecture.\nAddress arithmetic is performed using integer arithmetic and logical instructions. Examples include\npointer arithmetic and pointer comparisons. All addresses and address computations are byte-based;\nthere is no support for C-style pointer arithmetic.\nThe\nmov\ninstruction can be used to move the address of a variable into a pointer. The address is\nan offset in the state space in which the variable is declared. Load and store operations move data\nbetween registers and locations in addressable state spaces. The syntax is similar to that used in\nmany assembly languages, where scalar variables are simply named and addresses are de-referenced by\nenclosing the address expression in square brackets. Address expressions include variable names,\naddress registers, address register plus byte offset, and immediate address expressions which\nevaluate at compile-time to a constant address.\nHere are a few examples:\n.shared .u16 x;\n.reg    .u16 r0;\n.global .v4 .f32 V;\n.reg    .v4 .f32 W;\n.const  .s32 tbl[256];\n.reg    .b32 p;\n.reg    .s32 q;\n\nld.shared.u16   r0,[x];\nld.global.v4.f32 W, [V];\nld.const.s32    q, [tbl+12];\nmov.u32         p, tbl;\n6.4.1.1.\nGeneric Addressing\n\nIf a memory instruction does not specify a state space, the operation is performed using generic\naddressing. The state spaces\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.local\nand\n.shared\nare modeled as\nwindows within the generic address space. Each window is defined by a window base and a window size\nthat is equal to the size of the corresponding state space. A generic address maps to\nglobal\nmemory unless it falls within the window for\nconst\n,\nlocal\n, or\nshared\nmemory. The\nKernel Function Parameters\n(\n.param\n) window is contained\nwithin the\n.global\nwindow. Within each window, a generic address maps to an address in the\nunderlying state space by subtracting the window base from the generic address.\n6.4.2.\nArrays as Operands\n\nArrays of all types can be declared, and the identifier becomes an address constant in the space\nwhere the array is declared. The size of the array is a constant in the program.\nArray elements can be accessed using an explicitly calculated byte address, or by indexing into the\narray using square-bracket notation. The expression within square brackets is either a constant\ninteger, a register variable, or a simple\nregister with constant offset\nexpression, where the\noffset is a constant expression that is either added or subtracted from a register variable. If more\ncomplicated indexing is desired, it must be written as an address calculation prior to use. Examples\nare:\nld.global.u32  s, a[0];\nld.global.u32  s, a[N-1];\nmov.u32        s, a[1];  // move address of a[1] into s\n6.4.3.\nVectors as Operands\n\nVector operands can be specified as source and destination operands for instructions. However, when\nspecified as destination operand, all elements in vector expression must be unique, otherwise behavior\nis undefined.\nVectors may also be passed as arguments to called functions.\nVector elements can be extracted from the vector with the suffixes\n.x\n,\n.y\n,\n.z\nand\n.w\n, as well as the typical color fields\n.r\n,\n.g\n,\n.b\nand\n.a\n.\nA brace-enclosed list is used for pattern matching to pull apart vectors.\n.reg .v4 .f32 V;\n.reg .f32     a, b, c, d;\n\nmov.v4.f32 {a,b,c,d}, V;\nVector loads and stores can be used to implement wide loads and stores, which may improve memory\nperformance. The registers in the load/store operations can be a vector, or a brace-enclosed list of\nsimilarly typed scalars. Here are examples:\nld.global.v4.f32  {a,b,c,d}, [addr+16];\nld.global.v2.u32  V2, [addr+8];\nElements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows:\nRa = V.x = V.r\nRb = V.y = V.g\nRc = V.z = V.b\nRd = V.w = V.a\n6.4.4.\nLabels and Function Names as Operands\n\nLabels and function names can be used only in\nbra\n/\nbrx.idx\nand\ncall\ninstructions\nrespectively. Function names can be used in\nmov\ninstruction to get the address of the function\ninto a register, for use in an indirect call.\nBeginning in PTX ISA version 3.1, the\nmov\ninstruction may be used to take the address of kernel\nfunctions, to be passed to a system call that initiates a kernel launch from the GPU. This feature\nis part of the support for CUDA Dynamic Parallelism. See the\nCUDA Dynamic Parallelism Programming\nGuide\nfor details.\n6.5.\nType Conversion\n\nAll operands to all arithmetic, logic, and data movement instruction must be of the same type and\nsize, except for operations where changing the size and/or type is part of the definition of the\ninstruction. Operands of different sizes or types must be converted prior to the operation.\n6.5.1.\nScalar Conversions\n\nTable 15\nand\nTable 16\nshow what\nprecision and format the cvt instruction uses given operands of differing types. For example, if a\ncvt.s32.u16\ninstruction is given a\nu16\nsource operand and\ns32\nas a destination operand,\nthe\nu16\nis zero-extended to\ns32\n.\nConversions to floating-point that are beyond the range of floating-point numbers are represented\nwith the maximum floating-point value (IEEE 754 Inf for\nf32\nand\nf64\n, and ~131,000 for\nf16\n).\nTable 15\nConvert Instruction Precision and Format Table 1\n\nDestination Format\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nbf16\ntf32\nSource\nFormat\ns8\n–\nsext\nsext\nsext\n–\nsext\nsext\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns16\nchop\n1\n–\nsext\nsext\nchop\n1\n–\nsext\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns32\nchop\n1\nchop\n1\n–\nsext\nchop\n1\nchop\n1\n–\nsext\ns2f\ns2f\ns2f\ns2f\n–\ns64\nchop\n1\nchop\n1\nchop\n1\n–\nchop\n1\nchop\n1\nchop\n1\n–\ns2f\ns2f\ns2f\ns2f\n–\nu8\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu16\nchop\n1\n–\nzext\nzext\nchop\n1\n–\nzext\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu32\nchop\n1\nchop\n1\n–\nzext\nchop\n1\nchop\n1\n–\nzext\nu2f\nu2f\nu2f\nu2f\n–\nu64\nchop\n1\nchop\n1\nchop\n1\n–\nchop\n1\nchop\n1\nchop\n1\n–\nu2f\nu2f\nu2f\nu2f\n–\nf16\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\n–\nf2f\nf2f\nf2f\n–\nf32\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\n–\nf2f\nf2f\nf2f\nf64\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\nf2f\n–\nf2f\n–\nbf16\nf2s\nf2s\nf2s\nf2s\nf2u\nf2u\nf2u\nf2u\nf2f\nf2f\nf2f\nf2f\n–\ntf32\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\nTable 16\nConvert Instruction Precision and Format Table 2\n\nDestination Format\nf16\nf32\nbf16\ne4m3\ne5m2\ne2m3\ne3m2\ne2m1\nue8m0\ns2f6\nSource\nFormat\nf16\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\n–\n–\nf32\nf2f\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nbf16\nf2f\nf2f\n–\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\nf2f\ne4m3\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne5m2\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne2m3\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne3m2\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\ne2m1\nf2f\n–\n–\n–\n–\n–\n–\n–\n–\n–\nue8m0\n–\n–\nf2f\n–\n–\n–\n–\n–\n–\n–\ns2f6\n–\n–\nf2f\n–\n–\n–\n–\n–\n–\n–\nNotes\nsext = sign-extend; zext = zero-extend; chop = keep only low bits that fit;\ns2f = signed-to-float; f2s = float-to-signed; u2f = unsigned-to-float;\nf2u = float-to-unsigned; f2f = float-to-float.\n1\nIf the destination register is wider than the destination format, the result is extended to the\ndestination register width after chopping. The type of extension (sign or zero) is based on the\ndestination format. For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then\nsign-extends to 32-bit.\n6.5.2.\nRounding Modifiers\n\nConversion instructions may specify a rounding modifier. In PTX, there are four integer rounding\nmodifiers and six floating-point rounding\nmodifiers.\nTable 17\nand\nTable 18\nsummarize the rounding modifiers.\nTable 17\nFloating-Point Rounding Modifiers\n\nModifier\nDescription\n.rn\nrounds to nearest even\n.rna\nrounds to nearest, ties away from zero\n.rz\nrounds towards zero\n.rm\nrounds towards negative infinity\n.rp\nrounds towards positive infinity\n.rs\nrounds either towards zero or away from zero based\non the carry out of the integer addition of random\nbits and the discarded bits of mantissa\nTable 18\nInteger Rounding Modifiers\n\nModifier\nDescription\n.rni\nround to nearest integer, choosing even integer if source is equidistant between two integers.\n.rzi\nround to nearest integer in the direction of zero\n.rmi\nround to nearest integer in direction of negative infinity\n.rpi\nround to nearest integer in direction of positive infinity\n6.6.\nOperand Costs\n\nOperands from different state spaces affect the speed of an operation. Registers are fastest, while\nglobal memory is slowest. Much of the delay to memory can be hidden in a number of ways. The first\nis to have multiple threads of execution so that the hardware can issue a memory operation and then\nswitch to other execution. Another way to hide latency is to issue the load instructions as early as\npossible, as execution is not blocked until the desired result is used in a subsequent (in time)\ninstruction. The register in a store operation is available much more\nquickly.\nTable 19\ngives estimates of the\ncosts of using different kinds of memory.\nTable 19\nCost Estimates for Accessing State-Spaces\n\nSpace\nTime\nNotes\nRegister\n0\nShared\n0\nConstant\n0\nAmortized cost is low, first access is high\nLocal\n> 100 clocks\nParameter\n0\nImmediate\n0\nGlobal\n> 100 clocks\nTexture\n> 100 clocks\nSurface\n> 100 clocks\n7.\nAbstracting the ABI\n\nRather than expose details of a particular calling convention, stack layout, and Application Binary\nInterface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI\nimplementations. In this section, we describe the features of PTX needed to achieve this hiding of\nthe ABI. These include syntax for function definitions, function calls, parameter passing, and\nmemory allocated on the stack (\nalloca\n).\nRefer to\nPTX Writers Guide to Interoperability\nfor details on generating PTX compliant with\nApplication Binary Interface (ABI) for the CUDA\n®\narchitecture.\n7.1.\nFunction Declarations and Definitions\n\nIn PTX, functions are declared and defined using the\n.func\ndirective. A function\ndeclaration\nspecifies an optional list of return parameters, the function name, and an optional list of input\nparameters; together these specify the function’s interface, or prototype. A function\ndefinition\nspecifies both the interface and the body of the function. A function must be declared or defined\nprior to being called.\nThe simplest function has no parameters or return values, and is represented in PTX as follows:\n.func foo\n{\n    ...\n    ret;\n}\n\n    ...\n    call foo;\n    ...\nHere, execution of the\ncall\ninstruction transfers control to\nfoo\n, implicitly saving the\nreturn address. Execution of the\nret\ninstruction within\nfoo\ntransfers control to the\ninstruction following the call.\nScalar and vector base-type input and return parameters may be represented simply as register\nvariables. At the call, arguments may be register variables or constants, and return values may be\nplaced directly into register variables. The arguments and return variables at the call must have\ntype and size that match the callee’s corresponding formal parameters.\nExample\n.func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc )\n{\n    add.u32 %res, %ptr, %inc;\n    ret;\n}\n\n    ...\n    call (%r1), inc_ptr, (%r1,4);\n    ...\nWhen using the ABI,\n.reg\nstate space parameters must be at least 32-bits in size. Subword scalar\nobjects in the source language should be promoted to 32-bit registers in PTX, or use\n.param\nstate space byte arrays described next.\nObjects such as C structures and unions are flattened into registers or byte arrays in PTX and are\nrepresented using\n.param\nspace memory. For example, consider the following C structure, passed\nby value to a function:\nstruct {\n    double dbl;\n    char   c[4];\n};\nIn PTX, this structure will be flattened into a byte array. Since memory accesses are required to be\naligned to a multiple of the access size, the structure in this example will be a 12 byte array with\n8 byte alignment so that accesses to the\n.f64\nfield are aligned. The\n.param\nstate space is\nused to pass the structure by value:\nExample\n.func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12])\n{\n    .reg .f64 f1;\n    .reg .b32 c1, c2, c3, c4;\n    ...\n    ld.param.f64 f1, [y+0];\n    ld.param.b8  c1, [y+8];\n    ld.param.b8  c2, [y+9];\n    ld.param.b8  c3, [y+10];\n    ld.param.b8  c4, [y+11];\n    ...\n    ... // computation using x,f1,c1,c2,c3,c4;\n}\n\n{\n     .param .b8 .align 8 py[12];\n     ...\n     st.param.b64 [py+ 0], %rd;\n     st.param.b8  [py+ 8], %rc1;\n     st.param.b8  [py+ 9], %rc2;\n     st.param.b8  [py+10], %rc1;\n     st.param.b8  [py+11], %rc2;\n     // scalar args in .reg space, byte array in .param space\n     call (%out), bar, (%x, py);\n     ...\nIn this example, note that\n.param\nspace variables are used in two ways. First, a\n.param\nvariable\ny\nis used in function definition bar to represent a formal parameter. Second, a\n.param\nvariable\npy\nis declared in the body of the calling function and used to set up the\nstructure being passed to bar.\nThe following is a conceptual way to think about the\n.param\nstate space use in device functions.\nFor a caller,\nThe\n.param\nstate space is used to set values that will be passed to a called function and/or\nto receive return values from a called function. Typically, a\n.param\nbyte array is used to\ncollect together fields of a structure being passed by value.\nFor a callee,\nThe\n.param\nstate space is used to receive parameter values and/or pass return values back to\nthe caller.\nThe following restrictions apply to parameter passing.\nFor a caller,\nArguments may be\n.param\nvariables,\n.reg\nvariables, or constants.\nIn the case of\n.param\nspace formal parameters that are byte arrays, the argument must also be\na\n.param\nspace byte array with matching type, size, and alignment. A\n.param\nargument must\nbe declared within the local scope of the caller.\nIn the case of\n.param\nspace formal parameters that are base-type scalar or vector variables,\nthe corresponding argument may be either a\n.param\nor\n.reg\nspace variable with matching\ntype and size, or a constant that can be represented in the type of the formal parameter.\nIn the case of\n.reg\nspace formal parameters, the corresponding argument may be either a\n.param\nor\n.reg\nspace variable of matching type and size, or a constant that can be\nrepresented in the type of the formal parameter.\nIn the case of\n.reg\nspace formal parameters, the register must be at least 32-bits in size.\nAll\nst.param\ninstructions used for passing arguments to function call must immediately precede\nthe corresponding\ncall\ninstruction and\nld.param\ninstruction used for collecting return\nvalue must immediately follow the\ncall\ninstruction without any control flow\nalteration.\nst.param\nand\nld.param\ninstructions used for argument passing cannot be\npredicated. This enables compiler optimization and ensures that the\n.param\nvariable does not\nconsume extra space in the caller’s frame beyond that needed by the ABI. The\n.param\nvariable\nsimply allows a mapping to be made at the call site between data that may be in multiple\nlocations (e.g., structure being manipulated by caller is located in registers and memory) to\nsomething that can be passed as a parameter or return value to the callee.\nFor a callee,\nInput and return parameters may be\n.param\nvariables or\n.reg\nvariables.\nParameters in\n.param\nmemory must be aligned to a multiple of 1, 2, 4, 8, or 16 bytes.\nParameters in the\n.reg\nstate space must be at least 32-bits in size.\nThe\n.reg\nstate space can be used to receive and return base-type scalar and vector values,\nincluding sub-word size objects when compiling in non-ABI mode. Supporting the\n.reg\nstate\nspace provides legacy support.\nNote that the choice of\n.reg\nor\n.param\nstate space for parameter passing has no impact on\nwhether the parameter is ultimately passed in physical registers or on the stack. The mapping of\nparameters to physical registers and stack locations depends on the ABI definition and the order,\nsize, and alignment of parameters.\n7.1.1.\nChanges from PTX ISA Version 1.x\n\nIn PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no\nsupport for array parameters. Objects such as C structures were flattened and passed or returned\nusing multiple registers. PTX ISA version 1.x supports multiple return values for this purpose.\nBeginning with PTX ISA version 2.0, formal parameters may be in either\n.reg\nor\n.param\nstate\nspace, and\n.param\nspace parameters support arrays. For targets\nsm_20\nor higher, PTX\nrestricts functions to a single return value, and a\n.param\nbyte array should be used to return\nobjects that do not fit into a register. PTX continues to support multiple return registers for\nsm_1x\ntargets.\nNote\nPTX implements a stack-based ABI only for targets\nsm_20\nor higher.\nPTX ISA versions prior to 3.0 permitted variables in\n.reg\nand\n.local\nstate spaces to be\ndefined at module scope. When compiling to use the ABI, PTX ISA version 3.0 and later disallows\nmodule-scoped\n.reg\nand\n.local\nvariables and restricts their use to within function\nscope. When compiling without use of the ABI, module-scoped\n.reg\nand\n.local\nvariables are\nsupported as before. When compiling legacy PTX code (ISA versions prior to 3.0) containing\nmodule-scoped\n.reg\nor\n.local\nvariables, the compiler silently disables use of the ABI.\n7.2.\nVariadic Functions\n\nNote\nSupport for variadic functions which was unimplemented has been removed from the spec.\nPTX version 6.0 supports passing unsized array parameter to a function which can be used to\nimplement variadic functions.\nRefer to\nKernel and Function Directives: .func\nfor details\n7.3.\nAlloca\n\nPTX provides\nalloca\ninstruction for allocating storage at runtime on the per-thread local memory\nstack. The allocated stack memory can be accessed with\nld.local\nand\nst.local\ninstructions\nusing the pointer returned by\nalloca\n.\nIn order to facilitate deallocation of memory allocated with\nalloca\n, PTX provides two additional\ninstructions:\nstacksave\nwhich allows reading the value of stack pointer in a local variable, and\nstackrestore\nwhich can restore the stack pointer with the saved value.\nalloca\n,\nstacksave\n, and\nstackrestore\ninstructions are described in\nStack Manipulation Instructions\n.\n8.\nMemory Consistency Model\n\nIn multi-threaded executions, the side-effects of memory operations performed by each thread become\nvisible to other threads in a partial and non-identical order. This means that any two operations\nmay appear to happen in no order, or in different orders, to different threads. The axioms\nintroduced by the memory consistency model specify exactly which contradictions are forbidden\nbetween the orders observed by different threads.\nIn the absence of any constraint, each read operation returns the value committed by some write\noperation to the same memory location, including the initial write to that memory location. The\nmemory consistency model effectively constrains the set of such candidate writes from which a read\noperation can return a value.\n8.1.\nScope and applicability of the model\n\nThe constraints specified under this model apply to PTX programs with any PTX ISA version number,\nrunning on\nsm_70\nor later architectures.\nThe memory consistency model does not apply to texture (including\nld.global.nc\n) and surface\naccesses.\n8.1.1.\nLimitations on atomicity at system scope\n\nWhen communicating with the host CPU, certain strong operations with system scope may not be\nperformed atomically on some systems. For more details on atomicity guarantees to host memory, see\nthe\nCUDA Atomicity Requirements\n.\n8.2.\nMemory operations\n\nThe fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits. Each state\nspace available to a PTX program is a sequence of contiguous bytes in memory. Every byte in a PTX\nstate space has a unique address relative to all threads that have access to the same state space.\nEach PTX memory instruction specifies an address operand and a data type. The address operand\ncontains a virtual address that gets converted to a physical address during memory access. The\nphysical address and the size of the data type together define a physical memory location, which is\nthe range of bytes starting from the physical address and extending up to the size of the data type\nin bytes.\nThe memory consistency model specification uses the terms “address” or “memory address” to indicate\na virtual address, and the term “memory location” to indicate a physical memory location.\nEach PTX memory instruction also specifies the operation — either a read, a write or an atomic\nread-modify-write — to be performed on all the bytes in the corresponding memory location.\n8.2.1.\nOverlap\n\nTwo memory locations are said to overlap when the starting address of one location is within the\nrange of bytes constituting the other location. Two memory operations are said to overlap when they\nspecify the same virtual address and the corresponding memory locations overlap. The overlap is said\nto be complete when both memory locations are identical, and it is said to be partial otherwise.\n8.2.2.\nAliases\n\nTwo distinct virtual addresses are said to be aliases if they map to the same memory location.\n8.2.3.\nMultimem Addresses\n\nA multimem address is a virtual address which points to multiple distinct memory locations across\ndevices.\nOnly\nmultimem.\n* operations are valid on multimem addresses. That is, the behavior of accessing\na multimem address in any other memory operation is undefined.\n8.2.4.\nMemory Operations on Vector Data Types\n\nThe memory consistency model relates operations executed on memory locations with scalar data types,\nwhich have a maximum size and alignment of 64 bits. Memory operations with a vector data type are\nmodelled as a set of equivalent memory operations with a scalar data type, executed in an\nunspecified order on the elements in the vector.\n8.2.5.\nMemory Operations on Packed Data Types\n\nA packed data type consists of two values of the same scalar data type, as described in\nPacked Data Types\n. These values are accessed in adjacent memory locations. A\nmemory operation on a packed data type is modelled as a pair of equivalent memory operations on the\nscalar data type, executed in an unspecified order on each element of the packed data.\n8.2.6.\nInitialization\n\nEach byte in memory is initialized by a hypothetical write\nW0\nexecuted before starting any thread\nin the program. If the byte is included in a program variable, and that variable has an initial\nvalue, then\nW0\nwrites the corresponding initial value for that byte; else\nW0\nis assumed to have\nwritten an unknown but constant value to the byte.\n8.3.\nState spaces\n\nThe relations defined in the memory consistency model are independent of state spaces. In\nparticular, causality order closes over all memory operations across all the state spaces. But the\nside-effect of a memory operation in one state space can be observed directly only by operations\nthat also have access to the same state space. This further constrains the synchronizing effect of a\nmemory operation in addition to scope. For example, the synchronizing effect of the PTX instruction\nld.relaxed.shared.sys\nis identical to that of\nld.relaxed.shared.cluster\n, since no thread\noutside the same cluster can execute an operation that accesses the same memory location.\n8.4.\nOperation types\n\nFor simplicity, the rest of the document refers to the following operation types, instead of\nmentioning specific instructions that give rise to them.\nTable 20\nOperation Types\n\nOperation Type\nInstruction/Operation\natomic operation\natom\nor\nred\ninstruction.\nread operation\nAll variants of\nld\ninstruction and\natom\ninstruction (but not\nred\ninstruction).\nwrite operation\nAll variants of\nst\ninstruction, and\natomic\noperations if they result\nin a write.\nmemory operation\nA\nread\nor\nwrite\noperation.\nvolatile operation\nAn instruction with\n.volatile\nqualifier.\nacquire operation\nA\nmemory\noperation with\n.acquire\nor\n.acq_rel\nqualifier.\nrelease operation\nA\nmemory\noperation with\n.release\nor\n.acq_rel\nqualifier.\nmmio operation\nAn\nld\nor\nst\ninstruction with\n.mmio\nqualifier.\nmemory fence operation\nA\nmembar\n,\nfence.sc\nor\nfence.acq_rel\ninstruction.\nproxy fence operation\nA\nfence.proxy\nor a\nmembar.proxy\ninstruction.\nstrong operation\nA\nmemory fence\noperation, or a\nmemory\noperation with a\n.relaxed\n,\n.acquire\n,\n.release\n,\n.acq_rel\n,\n.volatile\n, or\n.mmio\nqualifier.\nweak operation\nAn\nld\nor\nst\ninstruction with a\n.weak\nqualifier.\nsynchronizing operation\nA\nbarrier\ninstruction,\nfence\noperation,\nrelease\noperation or\nacquire\noperation.\n8.4.1.\nmmio Operation\n\nAn\nmmio\noperation is a memory operation with\n.mmio\nqualifier specified. It is usually performed\non a memory location which is mapped to the control registers of peer I/O devices. It can also be\nused for communication between threads but has poor performance relative to non-\nmmio\noperations.\nThe semantic meaning of\nmmio\noperations cannot be defined precisely as it is defined by the\nunderlying I/O device. For formal specification of semantics of\nmmio\noperation from Memory\nConsistency Model perspective, it is equivalent to the semantics of a\nstrong\noperation. But it\nfollows a few implementation-specific properties, if it meets the\nCUDA atomicity requirements\nat\nthe specified scope:\nWrites are always performed and are never combined within the scope specified.\nReads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any\ncache within the scope specified.\nAs an exception, in some implementations, the surrounding locations may also be loaded. In such\ncases the amount of data loaded is implementation specific and varies between 32 and 128 bytes\nin size.\n8.4.2.\nvolatile Operation\n\nA\nvolatile\noperation is a memory operation with\n.volatile\nqualifier specified.\nThe semantics of volatile operations are equivalent to a relaxed memory operation with system-scope\nbut with the following extra implementation-specific constraints:\nThe number of volatile\ninstructions\n(not operations) executed by a program is preserved.\nHardware may combine and merge volatile\noperations\nissued by multiple different volatile\ninstructions\n, that is, the number of volatile\noperations\nin the program is not preserved.\nVolatile\ninstructions\nare not re-ordered around other volatile\ninstructions\n, but the memory\noperations\nperformed by those\ninstructions\nmay be re-ordered around each other.\nNote\nPTX volatile operations are intended for compilers to lower volatile read and write operations from\nCUDA C++, and other programming languages sharing CUDA C++ volatile semantics, to PTX.\nSince volatile operations are relaxed at system-scope with extra constraints, prefer using other\nstrong\nread or write operations (e.g.\nld.relaxed.sys\nor\nst.relaxed.sys\n) for\nInter-Thread Synchronization\ninstead, which may deliver better performance.\nPTX volatile operations are not suited for\nMemory Mapped IO (MMIO)\nbecause volatile operations\ndo not preserve the number of memory operations performed, and may perform more or less operations\nthan requested in a non-deterministic way.\nUse\n.mmio operations\ninstead, which strictly preserve the number of operations\nperformed.\n8.5.\nScope\n\nEach\nstrong\noperation must specify a\nscope\n, which is the set of threads that may interact\ndirectly with that operation and establish any of the relations described in the memory consistency\nmodel. There are four scopes:\nTable 21\nScopes\n\nScope\nDescription\n.cta\nThe set of all threads executing in the same CTA as the current thread.\n.cluster\nThe set of all threads executing in the same cluster as the current thread.\n.gpu\nThe set of all threads in the current program executing on the same compute\ndevice as the current thread. This also includes other kernel grids invoked by\nthe host program on the same compute device.\n.sys\nThe set of all threads in the current program, including all kernel grids\ninvoked by the host program on all compute devices, and all threads\nconstituting the host program itself.\nNote that the warp is not a\nscope\n; the CTA is the smallest collection of threads that qualifies as\na\nscope\nin the memory consistency model.\n8.6.\nProxies\n\nA\nmemory proxy\n, or a\nproxy\nis an abstract label applied to a method of memory access. When two\nmemory operations use distinct methods of memory access, they are said to be different\nproxies\n.\nMemory operations as defined in\nOperation types\nuse\ngeneric\nmethod of memory access, i.e. a\ngeneric proxy\n. Other operations such as textures and surfaces all\nuse distinct methods of memory access, also distinct from the\ngeneric\nmethod.\nA\nproxy fence\nis required to synchronize memory operations across different\nproxies\n. Although\nvirtual aliases use the\ngeneric\nmethod of memory access, since using distinct virtual addresses\nbehaves as if using different\nproxies\n, they require a\nproxy fence\nto establish memory ordering.\n8.7.\nMorally strong operations\n\nTwo operations are said to be\nmorally strong\nrelative to each other if they satisfy all of the\nfollowing conditions:\nThe operations are related in\nprogram order\n(i.e, they are both executed by the same thread),\nor each operation is\nstrong\nand specifies a\nscope\nthat includes the thread executing the\nother operation.\nBoth operations are performed via the same\nproxy\n.\nIf both are memory operations, then they overlap completely.\nMost (but not all) of the axioms in the memory consistency model depend on relations between\nmorally strong\noperations.\n8.7.1.\nConflict and Data-races\n\nTwo\noverlapping\nmemory operations are said to\nconflict\nwhen at least one of them is a\nwrite\n.\nTwo\nconflicting\nmemory operations are said to be in a\ndata-race\nif they are not related in\ncausality order\nand they are not\nmorally strong\n.\n8.7.2.\nLimitations on Mixed-size Data-races\n\nA\ndata-race\nbetween operations that\noverlap\ncompletely is called a\nuniform-size data-race\n,\nwhile a\ndata-race\nbetween operations that\noverlap\npartially is called a\nmixed-size data-race\n.\nThe axioms in the memory consistency model do not apply if a PTX program contains one or more\nmixed-size data-races\n. But these axioms are sufficient to describe the behavior of a PTX program\nwith only\nuniform-size data-races\n.\nAtomicity of mixed-size RMW operations\nIn any program with or without\nmixed-size data-races\n, the following property holds for every pair\nof\noverlapping atomic\noperations A1 and A2 such that each specifies a\nscope\nthat includes the\nother: Either the\nread-modify-write\noperation specified by A1 is performed completely before A2 is\ninitiated, or vice versa. This property holds irrespective of whether the two operations A1 and A2\noverlap partially or completely.\n8.8.\nRelease and Acquire Patterns\n\nSome sequences of instructions give rise to patterns that participate in memory synchronization as\ndescribed later. The\nrelease\npattern makes prior operations from the current thread\n1\nvisible to some operations from other threads. The\nacquire\npattern makes some operations from\nother threads visible to later operations from the current thread.\nA\nrelease\npattern on a location M consists of one of the following:\nA\nrelease\noperation on M\nE.g.:\nst.release\n[M];\nor\natom.release\n[M];\nor\nmbarrier.arrive.release\n[M];\nOr a\nrelease\nor\nacquire-release\noperation on M followed by a\nstrong\nwrite on M in\nprogram order\nE.g.:\nst.release\n[M]\n;\nst.relaxed\n[M];\nOr a\nrelease\nor\nacquire-release\nmemory fence\nfollowed by a\nstrong\nwrite on M in\nprogram order\nE.g.:\nfence.release;\nst.relaxed\n[M];\nor\nfence.release;\natom.relaxed\n[M];\nAny\nmemory synchronization\nestablished by a\nrelease\npattern only affects operations occurring in\nprogram order\nbefore the first instruction in that pattern.\nAn\nacquire\npattern on a location M consists of one of the following:\nAn\nacquire\noperation on M\nE.g.:\nld.acquire\n[M];\nor\natom.acquire\n[M];\nor\nmbarrier.test_wait.acquire\n[M];\nOr a\nstrong\nread on M followed by an\nacquire\noperation on M in\nprogram order\nE.g.:\nld.relaxed\n[M];\nld.acquire\n[M];\nOr a\nstrong\nread on M followed by an acquire\nmemory fence\nin\nprogram order\nE.g.:\nld.relaxed\n[M];\nfence.acquire;\nor\natom.relaxed\n[M];\nfence.acquire;\nAny\nmemory synchronization\nestablished by an\nacquire\npattern only affects operations occurring\nin\nprogram order\nafter the last instruction in that pattern.\nNote that while atomic reductions conceptually perform a strong read as part of its\nread-modify-write sequence, this strong read does not form an acquire pattern.\nE.g.:\nred.add\n[M],\n1;\nfence.acquire;\nis not an acquire pattern.\n1\nFor both\nrelease\nand\nacquire\npatterns, this effect is further extended to operations in\nother threads through the transitive nature of\ncausality order\n.\n8.9.\nOrdering of memory operations\n\nThe sequence of operations performed by each thread is captured as\nprogram order\nwhile\nmemory\nsynchronization\nacross threads is captured as\ncausality order\n. The visibility of the side-effects\nof memory operations to other memory operations is captured as\ncommunication order\n. The memory\nconsistency model defines contradictions that are disallowed between communication order on the one\nhand, and\ncausality order\nand\nprogram order\non the other.\n8.9.1.\nProgram Order\n\nThe\nprogram order\nrelates all operations performed by a thread to the order in which a sequential\nprocessor will execute instructions in the corresponding PTX source. It is a transitive relation\nthat forms a total order over the operations performed by the thread, but does not relate operations\nfrom different threads.\n8.9.1.1.\nAsynchronous Operations\n\nSome PTX instructions (all variants of\ncp.async\n,\ncp.async.bulk\n,\ncp.reduce.async.bulk\n,\nwgmma.mma_async\n) perform operations that are asynchronous to the thread that executed the\ninstruction. These asynchronous operations are ordered after prior instructions in the same thread\n(except in the case of\nwgmma.mma_async\n), but they are not part of the program order for that\nthread. Instead, they provide weaker ordering guarantees as documented in the instruction\ndescription.\nFor example, the loads and stores performed as part of a\ncp.async\nare ordered with respect to\neach other, but not to those of any other\ncp.async\ninstructions initiated by the same thread,\nnor any other instruction subsequently issued by the thread with the exception of\ncp.async.commit_group\nor\ncp.async.mbarrier.arrive\n. The asynchronous mbarrier\narrive-on\noperation\nperformed by a\ncp.async.mbarrier.arrive\ninstruction is ordered with respect to the memory\noperations performed by all prior\ncp.async\noperations initiated by the same thread, but not to\nthose of any other instruction issued by the thread. The implicit mbarrier\ncomplete-tx\noperation that is part of all variants of\ncp.async.bulk\nand\ncp.reduce.async.bulk\ninstructions is ordered only with respect to the memory operations performed by the same\nasynchronous instruction, and in particular it does not transitively establish ordering with respect\nto prior instructions from the issuing thread.\n8.9.2.\nObservation Order\n\nObservation order\nrelates a write W to a read R through an optional sequence of atomic\nread-modify-write operations.\nA write W precedes a read R in\nobservation order\nif:\nR and W are\nmorally strong\nand R reads the value written by W, or\nFor some atomic operation Z, W precedes Z and Z precedes R in\nobservation order\n.\n8.9.3.\nFence-SC Order\n\nThe\nFence-SC\norder is an acyclic partial order, determined at runtime, that relates every pair of\nmorally strong fence.sc\noperations.\n8.9.4.\nMemory synchronization\n\nSynchronizing operations performed by different threads synchronize with each other at runtime as\ndescribed here. The effect of such synchronization is to establish\ncausality order\nacross threads.\nA\nfence.sc\noperation X\nsynchronizes\nwith a\nfence.sc\noperation Y if X precedes Y in the\nFence-SC\norder.\nA\nbar{.cta}.sync\nor\nbar{.cta}.red\nor\nbar{.cta}.arrive\noperation\nsynchronizes\nwith a\nbar{.cta}.sync\nor\nbar{.cta}.red\noperation executed on the same barrier.\nA\nbarrier.cluster.arrive\noperation synchronizes with a\nbarrier.cluster.wait\noperation.\nA\nrelease\npattern X\nsynchronizes\nwith an\nacquire\npattern Y, if a\nwrite\noperation in X\nprecedes a\nread\noperation in Y in\nobservation order\n, and the first operation in X and the\nlast operation in Y are\nmorally strong\n.\nAPI synchronization\nA\nsynchronizes\nrelation can also be established by certain CUDA APIs.\nCompletion of a task enqueued in a CUDA stream\nsynchronizes\nwith the start of the following\ntask in the same stream, if any.\nFor purposes of the above, recording or waiting on a CUDA event in a stream, or causing a\ncross-stream barrier to be inserted due to\ncudaStreamLegacy\n, enqueues tasks in the associated\nstreams even if there are no direct side effects. An event record task\nsynchronizes\nwith\nmatching event wait tasks, and a barrier arrival task\nsynchronizes\nwith matching barrier wait\ntasks.\nStart of a CUDA kernel\nsynchronizes\nwith start of all threads in the kernel. End of all threads\nin a kernel\nsynchronize\nwith end of the kernel.\nStart of a CUDA graph\nsynchronizes\nwith start of all source nodes in the graph. Completion of\nall sink nodes in a CUDA graph\nsynchronizes\nwith completion of the graph. Completion of a graph\nnode\nsynchronizes\nwith start of all nodes with a direct dependency.\nStart of a CUDA API call to enqueue a task\nsynchronizes\nwith start of the task.\nCompletion of the last task queued to a stream, if any,\nsynchronizes\nwith return from\ncudaStreamSynchronize\n. Completion of the most recently queued matching event record task, if\nany,\nsynchronizes\nwith return from\ncudaEventSynchronize\n. Synchronizing a CUDA device or\ncontext behaves as if synchronizing all streams in the context, including ones that have been\ndestroyed.\nReturning\ncudaSuccess\nfrom an API to query a CUDA handle, such as a stream or event, behaves\nthe same as return from the matching synchronization API.\nIn addition to establishing a\nsynchronizes\nrelation, the CUDA API synchronization mechanisms above\nalso participate in\nproxy-preserved base causality order\n.\n8.9.5.\nCausality Order\n\nCausality order\ncaptures how memory operations become visible across threads through synchronizing\noperations. The axiom “Causality” uses this order to constrain the set of write operations from\nwhich a read operation may read a value.\nRelations in the\ncausality order\nprimarily consist of relations in\nBase causality order\n1\n, which is a transitive order, determined at runtime.\nBase causality order\nAn operation X precedes an operation Y in\nbase causality order\nif:\nX precedes Y in\nprogram order\n, or\nX\nsynchronizes\nwith Y, or\nFor some operation Z,\nX precedes Z in\nprogram order\nand Z precedes Y in\nbase causality order\n, or\nX precedes Z in\nbase causality order\nand Z precedes Y in\nprogram order\n, or\nX precedes Z in\nbase causality order\nand Z precedes Y in\nbase causality order\n.\nProxy-preserved base causality order\nA memory operation X precedes a memory operation Y in\nproxy-preserved base causality order\nif X\nprecedes Y in\nbase causality order\n, and:\nX and Y are performed to the same address, using the\ngeneric proxy\n, or\nX and Y are performed to the same address, using the same\nproxy\n, and by the same thread block,\nor\nX and Y are aliases and there is an alias\nproxy fence\nalong the base causality path from X\nto Y.\nCausality order\nCausality order\ncombines\nbase causality order\nwith some non-transitive relations as follows:\nAn operation X precedes an operation Y in\ncausality order\nif:\nX precedes Y in\nproxy-preserved base causality order\n, or\nFor some operation Z, X precedes Z in observation order, and Z precedes Y in\nproxy-preserved\nbase causality order\n.\n1\nThe transitivity of\nbase causality order\naccounts for the “cumulativity” of synchronizing\noperations.\n8.9.6.\nCoherence Order\n\nThere exists a partial transitive order that relates\noverlapping\nwrite operations, determined at\nruntime, called the\ncoherence order\n1\n. Two\noverlapping\nwrite operations are related in\ncoherence order\nif they are\nmorally strong\nor if they are related in\ncausality order\n. Two\noverlapping\nwrites are unrelated in\ncoherence order\nif they are in a\ndata-race\n, which gives\nrise to the partial nature of\ncoherence order\n.\n1\nCoherence order\ncannot be observed directly since it consists entirely of write\noperations. It may be observed indirectly by its use in constraining the set of candidate\nwrites that a read operation may read from.\n8.9.7.\nCommunication Order\n\nThe\ncommunication order\nis a non-transitive order, determined at runtime, that relates write\noperations to other\noverlapping\nmemory operations.\nA write W precedes an\noverlapping\nread R in\ncommunication order\nif R returns the value of any\nbyte that was written by W.\nA write W precedes a write W’ in\ncommunication order\nif W precedes W’ in\ncoherence order\n.\nA read R precedes an\noverlapping\nwrite W in\ncommunication order\nif, for any byte accessed by\nboth R and W, R returns the value written by a write W’ that precedes W in\ncoherence order\n.\nCommunication order\ncaptures the visibility of memory operations — when a memory operation X1\nprecedes a memory operation X2 in\ncommunication order\n, X1 is said to be visible to X2.\n8.10.\nAxioms\n\n8.10.1.\nCoherence\n\nIf a write W precedes an\noverlapping\nwrite W’ in\ncausality order\n, then W must precede W’ in\ncoherence order\n.\n8.10.2.\nFence-SC\n\nFence-SC\norder cannot contradict\ncausality order\n. For a pair of\nmorally strong\nfence.sc\noperations F1 and F2, if F1 precedes F2 in\ncausality order\n, then F1 must precede F2 in\nFence-SC\norder.\n8.10.3.\nAtomicity\n\nSingle-Copy Atomicity\nConflicting\nmorally strong\noperations are performed with\nsingle-copy atomicity\n. When a read R\nand a write W are\nmorally strong\n, then the following two communications cannot both exist in the\nsame execution, for the set of bytes accessed by both R and W:\nR reads any byte from W.\nR reads any byte from any write W’ which precedes W in\ncoherence order\n.\nAtomicity of read-modify-write (RMW) operations\nWhen an\natomic\noperation A and a write W\noverlap\nand are\nmorally strong\n, then the following\ntwo communications cannot both exist in the same execution, for the set of bytes accessed by both A\nand W:\nA reads any byte from a write W’ that precedes W in\ncoherence order\n.\nA follows W in\ncoherence order\n.\nLitmus Test 1\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2\nA1\n:\natom\n.\nsys\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nA2\n:\natom\n.\nsys\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nFINAL\nSTATE\n:\nx\n==\n2\nAtomicity is guaranteed when the operations are\nmorally strong\n.\nLitmus Test 2\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2 (In a different CTA)\nA1\n:\natom\n.\ncta\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nA2\n:\natom\n.\ngpu\n.\ninc\n.\nu32\n%\nr0\n,\n[\nx\n];\nFINAL\nSTATE\n:\nx\n==\n1\nOR\nx\n==\n2\nAtomicity is not guaranteed if the operations are not\nmorally strong\n.\n8.10.4.\nNo Thin Air\n\nValues may not appear “out of thin air”: an execution cannot speculatively produce a value in such a\nway that the speculation becomes self-satisfying through chains of instruction dependencies and\ninter-thread communication. This matches both programmer intuition and hardware reality, but is\nnecessary to state explicitly when performing formal analysis.\nLitmus Test: Load Buffering\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\ny\n=\n0\n;\nT1\nT2\nA1\n:\nld\n.\nglobal\n.\nu32\n%\nr0\n,\n[\nx\n];\nB1\n:\nst\n.\nglobal\n.\nu32\n[\ny\n],\n%\nr0\n;\nA2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ny\n];\nB2\n:\nst\n.\nglobal\n.\nu32\n[\nx\n],\n%\nr1\n;\nFINAL\nSTATE\n:\nx\n==\n0\nAND\ny\n==\n0\nThe litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of\nthin air. Two threads T1 and T2 each read from a first variable and copy the observed result into a\nsecond variable, with the first and second variable exchanged between the threads. If each variable\nis initially zero, the final result shall also be zero. If A1 reads from B2 and A2 reads from B1,\nthen values passing through the memory operations in this example form a cycle:\nA1->B1->A2->B2->A1. Only the values x == 0 and y == 0 are allowed to satisfy this cycle. If any of\nthe memory operations in this example were to speculatively associate a different value with the\ncorresponding memory location, then such a speculation would become self-fulfilling, and hence\nforbidden.\n8.10.5.\nSequential Consistency Per Location\n\nWithin any set of\noverlapping\nmemory operations that are pairwise\nmorally strong\n,\ncommunication\norder\ncannot contradict\nprogram order\n, i.e., a concatenation of\nprogram order\nbetween\noverlapping\noperations and\nmorally strong\nrelations in\ncommunication order\ncannot result in a\ncycle. This ensures that each program slice of\noverlapping\npairwise morally\nstrong operations\nis\nstrictly\nsequentially-consistent\n.\nLitmus Test: CoRR\n.\nglobal\n.\nu32\nx\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n[\nx\n],\n1\n;\nR1\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr0\n,\n[\nx\n];\nR2\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr1\n,\n[\nx\n];\nIF\n%\nr0\n==\n1\nTHEN\n%\nr1\n==\n1\nThe litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee. A\nthread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence\nof) reads R1 and R2 on the same location x. No other writes are executed on x, except the one\nmodelling the initial value. The operations W1, R1 and R2 are pairwise\nmorally strong\n. If R1 reads\nfrom W1, then the subsequent read R2 must also observe the same value. If R2 observed the initial\nvalue of x instead, then this would form a sequence of\nmorally-strong\nrelations R2->W1->R1 in\ncommunication order\nthat contradicts the\nprogram order\nR1->R2 in thread T2. Hence R2 cannot read\nthe initial value of x in such an execution.\n8.10.6.\nCausality\n\nRelations in\ncommunication order\ncannot contradict\ncausality order\n. This constrains the set of\ncandidate write operations that a read operation may read from:\nIf a read R precedes an\noverlapping\nwrite W in\ncausality order\n, then R cannot read from W.\nIf a write W precedes an\noverlapping\nread R in\ncausality order\n, then for any byte accessed by\nboth R and W, R cannot read from any write W’ that precedes W in\ncoherence order\n.\nLitmus Test: Message Passing\n.\nglobal\n.\nu32\ndata\n=\n0\n;\n.\nglobal\n.\nu32\nflag\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nu32\n[\ndata\n],\n1\n;\nF1\n:\nfence\n.\nsys\n;\nW2\n:\nst\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n[\nflag\n],\n1\n;\nR1\n:\nld\n.\nglobal\n.\nrelaxed\n.\nsys\n.\nu32\n%\nr0\n,\n[\nflag\n];\nF2\n:\nfence\n.\nsys\n;\nR2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ndata\n];\nIF\n%\nr0\n==\n1\nTHEN\n%\nr1\n==\n1\nThe litmus test known as “MP” (Message Passing) represents the essence of typical synchronization\nalgorithms. A vast majority of useful programs can be reduced to sequenced applications of this\npattern.\nThread T1 first writes to a data variable and then to a flag variable while a second thread T2 first\nreads from the flag variable and then from the data variable. The operations on the flag are\nmorally strong\nand the memory operations in each thread are separated by a\nfence\n, and these\nfences\nare\nmorally strong\n.\nIf R1 observes W2, then the release pattern “F1; W2”\nsynchronizes\nwith the\nacquire pattern\n“R1;\nF2”. This establishes the\ncausality order\nW1 -> F1 -> W2 -> R1 -> F2 -> R2. Then axiom\ncausality\nguarantees that R2 cannot read from any write that precedes W1 in\ncoherence order\n. In the absence\nof any other writes in this example, R2 must read from W1.\nLitmus Test: CoWR\n// These addresses are aliases\n.\nglobal\n.\nu32\ndata_alias_1\n;\n.\nglobal\n.\nu32\ndata_alias_2\n;\nT1\nW1\n:\nst\n.\nglobal\n.\nu32\n[\ndata_alias_1\n],\n1\n;\nF1\n:\nfence\n.\nproxy\n.\nalias\n;\nR1\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\ndata_alias_2\n];\n%\nr1\n==\n1\nVirtual aliases require an alias\nproxy fence\nalong the synchronization path.\nLitmus Test: Store Buffering\nThe litmus test known as “SB” (Store Buffering) demonstrates the\nsequential consistency\nenforced\nby the\nfence.sc\n. A thread T1 writes to a first variable, and then reads the value of a second\nvariable, while a second thread T2 writes to the second variable and then reads the value of the\nfirst variable. The memory operations in each thread are separated by\nfence.\nsc instructions,\nand these\nfences\nare\nmorally strong\n.\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\ny\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nglobal\n.\nu32\n[\nx\n],\n1\n;\nF1\n:\nfence\n.\nsc\n.\nsys\n;\nR1\n:\nld\n.\nglobal\n.\nu32\n%\nr0\n,\n[\ny\n];\nW2\n:\nst\n.\nglobal\n.\nu32\n[\ny\n],\n1\n;\nF2\n:\nfence\n.\nsc\n.\nsys\n;\nR2\n:\nld\n.\nglobal\n.\nu32\n%\nr1\n,\n[\nx\n];\n%\nr0\n==\n1\nOR\n%\nr1\n==\n1\nIn any execution, either F1 precedes F2 in\nFence-SC\norder, or vice versa. If F1 precedes F2 in\nFence-SC\norder, then F1\nsynchronizes\nwith F2. This establishes the\ncausality order\nin W1 -> F1\n-> F2 -> R2. Axiom\ncausality\nensures that R2 cannot read from any write that precedes W1 in\ncoherence order\n. In the absence of any other write to that variable, R2 must read from\nW1. Similarly, in the case where F2 precedes F1 in\nFence-SC\norder, R1 must read from W2. If each\nfence.sc\nin this example were replaced by a\nfence.acq_rel\ninstruction, then this outcome is\nnot guaranteed. There may be an execution where the write from each thread remains unobserved from\nthe other thread, i.e., an execution is possible, where both R1 and R2 return the initial value “0”\nfor variables y and x respectively.\n8.11.\nSpecial Cases\n\n8.11.1.\nReductions do not form Acquire Patterns\n\nAtomic reduction operations like\nred\ndo not form acquire patterns with acquire fences.\nLitmus Test: Message Passing with a Red Instruction\n.\nglobal\n.\nu32\nx\n=\n0\n;\n.\nglobal\n.\nu32\nflag\n=\n0\n;\nT1\nT2\nW1\n:\nst\n.\nu32\n[\nx\n],\n42\n;\nW2\n:\nst\n.\nrelease\n.\ngpu\n.\nu32\n[\nflag\n],\n1\n;\nRMW1\n:\nred\n.\nsys\n.\nglobal\n.\nadd\n.\nu32\n[\nflag\n],\n1\n;\nF2\n:\nfence\n.\nacquire\n.\ngpu\n;\nR2\n:\nld\n.\nweak\n.\nu32\n%\nr1\n,\n[\nx\n];\n%\nr1\n==\n0\nAND\nflag\n==\n2\nThe litmus test known as “MP” (Message Passing) demonstrates the consequence\nof reductions being excluded from acquire patterns.\nIt is possible to observe the outcome where\nR2\nreads the value\n0\nfrom\nx\nand\nflag\nhas the final value of\n2\n.\nThis outcome is possible since the release pattern in\nT1\ndoes not synchronize\nwith any acquire pattern in\nT2\n.\nUsing the\natom\ninstruction instead of\nred\nforbids this outcome.\n9.\nInstruction Set\n\n9.1.\nFormat and Semantics of Instruction Descriptions\n\nThis section describes each PTX instruction. In addition to the name and the format of the\ninstruction, the semantics are described, followed by some examples that attempt to show several\npossible instantiations of the instruction.\n9.2.\nPTX Instructions\n\nPTX instructions generally have from zero to four operands, plus an optional guard predicate\nappearing after an\n@\nsymbol to the left of the\nopcode\n:\n@p\nopcode;\n@p\nopcode\na;\n@p\nopcode\nd,\na;\n@p\nopcode\nd,\na,\nb;\n@p\nopcode\nd,\na,\nb,\nc;\nFor instructions that create a result value, the\nd\noperand is the destination operand, while\na\n,\nb\n, and\nc\nare source operands.\nThe\nsetp\ninstruction writes two destination registers. We use a\n|\nsymbol to separate\nmultiple destination registers.\nsetp.lt.s32  p|q, a, b;  // p = (a < b); q = !(a < b);\nFor some instructions the destination operand is optional. A\nbit bucket\noperand denoted with an\nunderscore (\n_\n) may be used in place of a destination register.\n9.3.\nPredicated Execution\n\nIn PTX, predicate registers are virtual and have\n.pred\nas the type specifier. So, predicate\nregisters can be declared as\n.reg .pred p, q, r;\nAll instructions have an optional\nguard predicate\nwhich controls conditional execution of the\ninstruction. The syntax to specify conditional execution is to prefix an instruction with\n@{!}p\n,\nwhere\np\nis a predicate variable, optionally negated. Instructions without a guard predicate are\nexecuted unconditionally.\nPredicates are most commonly set as the result of a comparison performed by the\nsetp\ninstruction.\nAs an example, consider the high-level code\nif (i < n)\n    j = j + 1;\nThis can be written in PTX as\nsetp.lt.s32  p, i, n;    // p = (i < n)\n@p    add.s32      j, j, 1;    // if i < n, add 1 to j\nTo get a conditional branch or conditional function call, use a predicate to control the execution\nof the branch or call instructions. To implement the above example as a true conditional branch, the\nfollowing PTX instruction sequence might be used:\nsetp.lt.s32  p, i, n;    // compare i to n\n@!p   bra  L1;                 // if False, branch over\n      add.s32      j, j, 1;\nL1:     ...\n9.3.1.\nComparisons\n\n9.3.1.1.\nInteger and Bit-Size Comparisons\n\nThe signed integer comparisons are the traditional\neq\n(equal),\nne\n(not-equal),\nlt\n(less-than),\nle\n(less-than-or-equal),\ngt\n(greater-than), and\nge\n(greater-than-or-equal). The unsigned comparisons are\neq\n,\nne\n,\nlo\n(lower),\nls\n(lower-or-same),\nhi\n(higher), and\nhs\n(higher-or-same). The bit-size comparisons are\neq\nand\nne\n; ordering comparisons are not defined for bit-size types.\nTable 22\nshows the operators for signed integer, unsigned integer, and bit-size types.\nTable 22\nOperators for Signed Integer, Unsigned Integer, and Bit-Size Types\n\nMeaning\nSigned Operator\nUnsigned Operator\nBit-Size Operator\na\n==\nb\neq\neq\neq\na\n!=\nb\nne\nne\nne\na\n<\nb\nlt\nlo\nn/a\na\n<=\nb\nle\nls\nn/a\na\n>\nb\ngt\nhi\nn/a\na\n>=\nb\nge\nhs\nn/a\n9.3.1.2.\nFloating Point Comparisons\n\nThe ordered floating-point comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n, and\nge\n. If\neither operand is\nNaN\n, the result is\nFalse\n.\nTable 23\nlists the floating-point\ncomparison operators.\nTable 23\nFloating-Point Comparison Operators\n\nMeaning\nFloating-Point Operator\na\n==\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\neq\na\n!=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nne\na\n<\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nlt\na\n<=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nle\na\n>\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\ngt\na\n>=\nb\n&&\n!isNaN(a)\n&&\n!isNaN(b)\nge\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered floating-point comparisons\nare provided:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n, and\ngeu\n. If both operands are numeric\nvalues (not\nNaN\n), then the comparison has the same result as its ordered counterpart. If either\noperand is\nNaN\n, then the result of the comparison is\nTrue\n.\nTable 24\nlists the floating-point\ncomparison operators accepting\nNaN\nvalues.\nTable 24\nFloating-Point Comparison Operators Accepting NaN\n\nMeaning\nFloating-Point Operator\na\n==\nb\n||\nisNaN(a)\n||\nisNaN(b)\nequ\na\n!=\nb\n||\nisNaN(a)\n||\nisNaN(b)\nneu\na\n<\nb\n||\nisNaN(a)\n||\nisNaN(b)\nltu\na\n<=\nb\n||\nisNaN(a)\n||\nisNaN(b)\nleu\na\n>\nb\n||\nisNaN(a)\n||\nisNaN(b)\ngtu\na\n>=\nb\n||\nisNaN(a)\n||\nisNaN(b)\ngeu\nTo test for\nNaN\nvalues, two operators\nnum\n(\nnumeric\n) and\nnan\n(\nisNaN\n) are\nprovided.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nTable 25\nlists the\nfloating-point comparison operators testing for\nNaN\nvalues.\nTable 25\nFloating-Point Comparison Operators Testing for NaN\n\nMeaning\nFloating-Point Operator\n!isNaN(a)\n&&\n!isNaN(b)\nnum\nisNaN(a)\n||\nisNaN(b)\nnan\n9.3.2.\nManipulating Predicates\n\nPredicate values may be computed and manipulated using the following instructions:\nand\n,\nor\n,\nxor\n,\nnot\n, and\nmov\n.\nThere is no direct conversion between predicates and integer values, and no direct way to load or\nstore predicate register values. However,\nsetp\ncan be used to generate a predicate from an\ninteger, and the predicate-based select (\nselp\n) instruction can be used to generate an integer\nvalue based on the value of a predicate; for example:\nselp.u32 %r1,1,0,%p;    // convert predicate to 32-bit value\n9.4.\nType Information for Instructions and Operands\n\nTyped instructions must have a type-size modifier. For example, the\nadd\ninstruction requires\ntype and size information to properly perform the addition operation (signed, unsigned, float,\ndifferent sizes), and this information must be specified as a suffix to the opcode.\nExample\n.reg .u16 d, a, b;\n\nadd.u16 d, a, b;    // perform a 16-bit unsigned add\nSome instructions require multiple type-size modifiers, most notably the data conversion instruction\ncvt\n. It requires separate type-size modifiers for the result and source, and these are placed in\nthe same order as the operands. For example:\n.reg .u16 a;\n.reg .f32 d;\n\ncvt.f32.u16 d, a;   // convert 16-bit unsigned to 32-bit float\nIn general, an operand’s type must agree with the corresponding instruction-type modifier. The rules\nfor operand and instruction type conformance are as follows:\nBit-size types agree with any type of the same size.\nSigned and unsigned integer types agree provided they have the same size, and integer operands are\nsilently cast to the instruction type if needed. For example, an unsigned integer operand used in\na signed integer instruction will be treated as a signed integer by the instruction.\nFloating-point types agree only if they have the same size; i.e., they must match exactly.\nTable 26\nsummarizes these type\nchecking rules.\nTable 26\nType Checking Rules\n\nOperand Type\n.bX\n.sX\n.uX\n.fX\nInstruction Type\n.bX\nokay\nokay\nokay\nokay\n.sX\nokay\nokay\nokay\ninvalid\n.uX\nokay\nokay\nokay\ninvalid\n.fX\nokay\ninvalid\ninvalid\nokay\nNote\nSome operands have their type and size defined independently from the instruction type-size. For\nexample, the shift amount operand for left and right shift instructions always has type\n.u32\n,\nwhile the remaining operands have their type and size determined by the instruction type.\nExample\n// 64-bit arithmetic right shift; shift amount 'b' is .u32\n    shr.s64 d,a,b;\n9.4.1.\nOperand Size Exceeding Instruction-Type Size\n\nFor convenience,\nld\n,\nst\n, and\ncvt\ninstructions permit source and destination data\noperands to be wider than the instruction-type size, so that narrow values may be loaded, stored,\nand converted using regular-width registers. For example, 8-bit or 16-bit values may be held\ndirectly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and\nsizes. The operand type checking rules are relaxed for bit-size and integer (signed and unsigned)\ninstruction types; floating-point instruction types still require that the operand type-size matches\nexactly, unless the operand is of bit-size type.\nWhen a source operand has a size that exceeds the instruction-type size, the source data is\ntruncated (chopped) to the appropriate number of bits specified by the instruction type-size.\nTable 27\nsummarizes the relaxed type-checking rules for source operands. Note that some combinations may\nstill be invalid for a particular instruction; for example, the\ncvt\ninstruction does not support\n.bX\ninstruction types, so those rows are invalid for\ncvt\n.\nTable 27\nRelaxed Type-checking Rules for Source Operands\n\nSource Operand Type\nb8\nb16\nb32\nb64\nb128\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nInstruction Type\nb8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\nchop\nchop\nchop\nb16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\n–\nchop\nchop\nb32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\n–\nchop\nb64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\n–\nb128\ninv\ninv\ninv\ninv\n–\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ns8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\ninv\ninv\ninv\ns16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\ninv\ninv\ninv\ns32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\ninv\ninv\ns64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nu8\n–\nchop\nchop\nchop\nchop\n–\nchop\nchop\nchop\n–\nchop\nchop\nchop\ninv\ninv\ninv\nu16\ninv\n–\nchop\nchop\nchop\ninv\n–\nchop\nchop\ninv\n–\nchop\nchop\ninv\ninv\ninv\nu32\ninv\ninv\n–\nchop\nchop\ninv\ninv\n–\nchop\ninv\ninv\n–\nchop\ninv\ninv\ninv\nu64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nf16\ninv\n–\nchop\nchop\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\ninv\nf32\ninv\ninv\n–\nchop\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\nf64\ninv\ninv\ninv\n–\nchop\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\nNotes\nchop = keep only low bits that fit; “–” = allowed, but no conversion needed;\ninv = invalid, parse error.\nSource register size must be of equal or greater size than the instruction-type size.\nBit-size source registers may be used with any appropriately-sized instruction type. The data are\ntruncated (“chopped”) to the instruction-type size and interpreted according to the instruction\ntype.\nInteger source registers may be used with any appropriately-sized bit-size or integer instruction\ntype. The data are truncated to the instruction-type size and interpreted according to the\ninstruction type.\nFloating-point source registers can only be used with bit-size or floating-point instruction types.\nWhen used with a narrower bit-size instruction type, the data are truncated. When used with a\nfloating-point instruction type, the size must match exactly.\nWhen a destination operand has a size that exceeds the instruction-type size, the destination data\nis zero- or sign-extended to the size of the destination register. If the corresponding instruction\ntype is signed integer, the data is sign-extended; otherwise, the data is zero-extended.\nTable 28\nsummarizes the relaxed type-checking rules for destination operands.\nTable 28\nRelaxed Type-checking Rules for Destination Operands\n\nDestination Operand Type\nb8\nb16\nb32\nb64\nb128\ns8\ns16\ns32\ns64\nu8\nu16\nu32\nu64\nf16\nf32\nf64\nInstruction Type\nb8\n–\nzext\nzext\nzext\nzext\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\nzext\nzext\nzext\nb16\ninv\n–\nzext\nzext\nzext\ninv\n–\nzext\nzext\ninv\n–\nzext\nzext\n–\nzext\nzext\nb32\ninv\ninv\n–\nzext\nzext\ninv\ninv\n–\nzext\ninv\ninv\n–\nzext\ninv\n–\nzext\nb64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\n–\nb128\ninv\ninv\ninv\ninv\n–\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ns8\n–\nsext\nsext\nsext\nsext\n–\nsext\nsext\nsext\n–\nsext\nsext\nsext\ninv\ninv\ninv\ns16\ninv\n–\nsext\nsext\nsext\ninv\n–\nsext\nsext\ninv\n–\nsext\nsext\ninv\ninv\ninv\ns32\ninv\ninv\n–\nsext\nsext\ninv\ninv\n–\nsext\ninv\ninv\n–\nsext\ninv\ninv\ninv\ns64\ninv\ninv\ninv\n–\nsext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nu8\n–\nzext\nzext\nzext\nzext\n–\nzext\nzext\nzext\n–\nzext\nzext\nzext\ninv\ninv\ninv\nu16\ninv\n–\nzext\nzext\nzext\ninv\n–\nzext\nzext\ninv\n–\nzext\nzext\ninv\ninv\ninv\nu32\ninv\ninv\n–\nzext\nzext\ninv\ninv\n–\nzext\ninv\ninv\n–\nzext\ninv\ninv\ninv\nu64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\n–\ninv\ninv\ninv\n–\ninv\ninv\ninv\nf16\ninv\n–\nzext\nzext\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\ninv\nf32\ninv\ninv\n–\nzext\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\ninv\nf64\ninv\ninv\ninv\n–\nzext\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\ninv\n–\nNotes\nsext = sign-extend; zext = zero-extend; “–” = allowed, but no conversion needed;\ninv = invalid, parse error.\nDestination register size must be of equal or greater size than the instruction-type size.\nBit-size destination registers may be used with any appropriately-sized instruction type. The data\nare sign-extended to the destination register width for signed integer instruction types, and are\nzero-extended to the destination register width otherwise.\nInteger destination registers may be used with any appropriately-sized bit-size or integer\ninstruction type. The data are sign-extended to the destination register width for signed integer\ninstruction types, and are zero-extended to the destination register width for bit-size an d\nunsigned integer instruction types.\nFloating-point destination registers can only be used with bit-size or floating-point instruction\ntypes. When used with a narrower bit-size instruction type, the data are zero-extended. When used\nwith a floating-point instruction type, the size must match exactly.\n9.5.\nDivergence of Threads in Control Constructs\n\nThreads in a CTA execute together, at least in appearance, until they come to a conditional control\nconstruct such as a conditional branch, conditional function call, or conditional return. If threads\nexecute down different control flow paths, the threads are called\ndivergent\n. If all of the threads\nact in unison and follow a single control flow path, the threads are called\nuniform\n. Both\nsituations occur often in programs.\nA CTA with divergent threads may have lower performance than a CTA with uniformly executing threads,\nso it is important to have divergent threads re-converge as soon as possible. All control constructs\nare assumed to be divergent points unless the control-flow instruction is marked as uniform, using\nthe\n.uni\nsuffix. For divergent control flow, the optimizing code generator automatically\ndetermines points of re-convergence. Therefore, a compiler or code author targeting PTX can ignore\nthe issue of divergent threads, but has the opportunity to improve performance by marking branch\npoints as uniform when the compiler or author can guarantee that the branch point is non-divergent.\n9.6.\nSemantics\n\nThe goal of the semantic description of an instruction is to describe the results in all cases in as\nsimple language as possible. The semantics are described using C, until C is not expressive enough.\n9.6.1.\nMachine-Specific Semantics of 16-bit Code\n\nA PTX program may execute on a GPU with either a 16-bit or a 32-bit data path. When executing on a\n32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit\ncomputations are\npromoted\nto 32-bit computations. This can lead to computational differences\nbetween code run on a 16-bit machine versus the same code run on a 32-bit machine, since the\npromoted computation may have bits in the high-order half-word of registers that are not present in\n16-bit physical registers. These extra precision bits can become visible at the application level,\nfor example, by a right-shift instruction.\nAt the PTX language level, one solution would be to define semantics for 16-bit code that is\nconsistent with execution on a 16-bit data path. This approach introduces a performance penalty for\n16-bit code executing on a 32-bit data path, since the translated code would require many additional\nmasking instructions to suppress extra precision bits in the high-order half-word of 32-bit\nregisters.\nRather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of\n16-bit instructions in PTX is machine-specific. A compiler or programmer may chose to enforce\nportable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at\nappropriate points in the program to guarantee portability of the code. However, for many\nperformance-critical applications, this is not desirable, and for many applications the difference\nin execution is preferable to limiting performance.\n9.7.\nInstructions\n\nAll PTX instructions may be predicated. In the following descriptions, the optional guard predicate\nis omitted from the syntax.\n9.7.1.\nInteger Arithmetic Instructions\n\nInteger arithmetic instructions operate on the integer types in register and constant immediate\nforms. The integer arithmetic instructions are:\nadd\nsub\nmul\nmad\nmul24\nmad24\nsad\ndiv\nrem\nabs\nneg\nmin\nmax\npopc\nclz\nbfind\nfns\nbrev\nbfe\nbfi\nbmsk\nszext\ndp4a\ndp2a\n9.7.1.1.\nInteger Arithmetic Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd.type       d, a, b;\nadd{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64,\n          .u16x2, .s16x2 };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce\n.u16x2\n,\n.s16x2\nresult in\ndestination.\nOperands\nd\n,\na\nand\nb\nhave type\n.type\n. For instruction types\n.u16x2\n,\n.s16x2\n,\noperands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = iA[i] + iB[i];\n    }\n} else {\n    d = a + b;\n}\nNotes\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation. Applies only to\n.s32\ntype.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nadd.u16x2\nand\nadd.s16x2\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nadd.u16x2\nand\nadd.s16x2\nrequire\nsm_90\nor higher.\nExamples\n@p  add.u32     x,y,z;\n    add.sat.s32 c,c,1;\n    add.u16x2   u,v,w;\n9.7.1.2.\nInteger Arithmetic Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub.type       d, a, b;\nsub{.sat}.s32  d, a, b;     // .sat applies only to .s32\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nSemantics\nd = a - b;\nNotes\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation. Applies only to\n.s32\ntype.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsub.s32 c,a,b;\n9.7.1.3.\nInteger Arithmetic Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul.mode.type  d, a, b;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nCompute the product of two values.\nSemantics\nt = a * b;\nn = bitwidth of type;\nd = t;            // for .wide\nd = t<2n-1..n>;   // for .hi variant\nd = t<n-1..0>;    // for .lo variant\nNotes\nThe type of the operation represents the types of the\na\nand\nb\noperands. If\n.hi\nor\n.lo\nis specified, then\nd\nis the same size as\na\nand\nb\n, and either the upper or lower\nhalf of the result is written to the destination register. If\n.wide\nis specified, then\nd\nis\ntwice as wide as\na\nand\nb\nto receive the full result of the multiplication.\nThe\n.wide\nsuffix is supported only for 16- and 32-bit integer types.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmul.wide.s16 fa,fxs,fys;   // 16*16 bits yields 32 bits\nmul.lo.s16 fa,fxs,fys;     // 16*16 bits, save only the low 16 bits\nmul.wide.s32 z,x,y;        // 32*32 bits, creates 64 bit result\n9.7.1.4.\nInteger Arithmetic Instructions:\nmad\n\nmad\nMultiply two values, optionally extract the high or low half of the intermediate result, and add a third value.\nSyntax\nmad.mode.type  d, a, b, c;\nmad.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo, .wide };\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nMultiplies two values, optionally extracts the high or low half of the intermediate result, and adds\na third value. Writes the result into a destination register.\nSemantics\nt = a * b;\nn = bitwidth of type;\nd = t + c;           // for .wide\nd = t<2n-1..n> + c;  // for .hi variant\nd = t<n-1..0> + c;   // for .lo variant\nNotes\nThe type of the operation represents the types of the\na\nand\nb\noperands. If .hi or .lo is\nspecified, then\nd\nand\nc\nare the same size as\na\nand\nb\n, and either the upper or lower\nhalf of the result is written to the destination register. If\n.wide\nis specified, then\nd\nand\nc\nare twice as wide as\na\nand\nb\nto receive the result of the multiplication.\nThe\n.wide\nsuffix is supported only for 16-bit and 32-bit integer types.\nSaturation modifier:\n.sat\nlimits result to\nMININT..MAXINT\n(no overflow) for the size of the operation.\nApplies only to\n.s32\ntype in\n.hi\nmode.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n@p  mad.lo.s32 d,a,b,c;\n    mad.lo.s32 r,p,q,r;\n9.7.1.5.\nInteger Arithmetic Instructions:\nmul24\n\nmul24\nMultiply two 24-bit integer values.\nSyntax\nmul24.mode.type  d, a, b;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 };\nDescription\nCompute the product of two 24-bit integer values held in 32-bit source registers, and return either\nthe high or low 32-bits of the 48-bit result.\nSemantics\nt = a * b;\nd = t<47..16>;    // for .hi variant\nd = t<31..0>;     // for .lo variant\nNotes\nInteger multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.\nmul24.hi\nperforms a 24x24-bit multiply and returns the high 32 bits of the 48-bit result.\nmul24.lo\nperforms a 24x24-bit multiply and returns the low 32 bits of the 48-bit result.\nAll operands are of the same type and size.\nmul24.hi\nmay be less efficient on machines without hardware support for 24-bit multiply.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmul24.lo.s32 d,a,b;   // low 32-bits of 24x24-bit signed multiply.\n9.7.1.6.\nInteger Arithmetic Instructions:\nmad24\n\nmad24\nMultiply two 24-bit integer values and add a third value.\nSyntax\nmad24.mode.type  d, a, b, c;\nmad24.hi.sat.s32 d, a, b, c;\n\n.mode = { .hi, .lo };\n.type = { .u32, .s32 };\nDescription\nCompute the product of two 24-bit integer values held in 32-bit source registers, and add a third,\n32-bit value to either the high or low 32-bits of the 48-bit result. Return either the high or low\n32-bits of the 48-bit result.\nSemantics\nt = a * b;\nd = t<47..16> + c;   // for .hi variant\nd = t<31..0> + c;    // for .lo variant\nNotes\nInteger multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.\nmad24.hi\nperforms a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third\nvalue.\nmad24.lo\nperforms a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third\nvalue.\nAll operands are of the same type and size.\nSaturation modifier:\n.sat\nlimits result of 32-bit signed addition to\nMININT..MAXINT\n(no overflow). Applies only to\n.s32\ntype in .hi mode.\nmad24.hi\nmay be less efficient on machines without hardware support for 24-bit multiply.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmad24.lo.s32 d,a,b,c;   // low 32-bits of 24x24-bit signed multiply.\n9.7.1.7.\nInteger Arithmetic Instructions:\nsad\n\nsad\nSum of absolute differences.\nSyntax\nsad.type  d, a, b, c;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nAdds the absolute value of\na-b\nto\nc\nand writes the resulting value into\nd\n.\nSemantics\nd = c + ((a<b) ? b-a : a-b);\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsad.s32  d,a,b,c;\nsad.u32  d,a,b,d;  // running sum\n9.7.1.8.\nInteger Arithmetic Instructions:\ndiv\n\ndiv\nDivide one value by another.\nSyntax\ndiv.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nDivides\na\nby\nb\n, stores result in\nd\n.\nSemantics\nd = a / b;\nNotes\nDivision by zero yields an unspecified, machine-specific value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ndiv.s32  b,n,i;\n9.7.1.9.\nInteger Arithmetic Instructions:\nrem\n\nrem\nThe remainder of integer division.\nSyntax\nrem.type  d, a, b;\n\n.type = { .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nDivides\na\nby\nb\n, store the remainder in\nd\n.\nSemantics\nd = a % b;\nNotes\nThe behavior for negative numbers is machine-dependent and depends on whether divide rounds towards\nzero or negative infinity.\nDivision by zero yields an unspecified, machine-specific value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nrem.s32  x,x,8;    // x = x%8;\n9.7.1.10.\nInteger Arithmetic Instructions:\nabs\n\nabs\nAbsolute value.\nSyntax\nabs.type  d, a;\n\n.type = { .s16, .s32, .s64 };\nDescription\nTake the absolute value of\na\nand store it in\nd\n.\nSemantics\nd = |a|;\nNotes\nOnly for signed integers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nabs.s32  r0,a;\n9.7.1.11.\nInteger Arithmetic Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg.type  d, a;\n\n.type = { .s16, .s32, .s64 };\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nSemantics\nd = -a;\nNotes\nOnly for signed integers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nneg.s32  r0,a;\n9.7.1.12.\nInteger Arithmetic Instructions:\nmin\n\nmin\nFind the minimum of two values.\nSyntax\nmin.atype         d, a, b;\nmin{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 };\nDescription\nStore the minimum of\na\nand\nb\nin\nd\n.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce\n.u16x2\n,\n.s16x2\nresult\nin destination.\nOperands\nd\n,\na\nand\nb\nhave the same type as the instruction type. For instruction types\n.u16x2\n,\n.s16x2\n, operands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] < iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a < b) ? a : b; // Integer (signed and unsigned)\n}\nNotes\nSigned and unsigned differ.\nSaturation modifier:\nmin.relu.{s16x2,\ns32}\nclamps the result to 0 if negative.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmin.u16x2\n,\nmin{.relu}.s16x2\nand\nmin.relu.s32\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nmin.u16x2\n,\nmin{.relu}.s16x2\nand\nmin.relu.s32\nrequire\nsm_90\nor higher.\nExamples\nmin.s32  r0,a,b;\n@p  min.u16  h,i,j;\n    min.s16x2.relu u,v,w;\n9.7.1.13.\nInteger Arithmetic Instructions:\nmax\n\nmax\nFind the maximum of two values.\nSyntax\nmax.atype         d, a, b;\nmax{.relu}.btype  d, a, b;\n\n.atype = { .u16, .u32, .u64,\n           .u16x2, .s16, .s64 };\n.btype = { .s16x2, .s32 };\nDescription\nStore the maximum of\na\nand\nb\nin\nd\n.\nFor\n.u16x2\n,\n.s16x2\ninstruction types, forms input vectors by half word values from source\noperands. Half-word operands are then processed in parallel to produce\n.u16x2\n,\n.s16x2\nresult\nin destination.\nOperands\nd\n,\na\nand\nb\nhave the same type as the instruction type. For instruction types\n.u16x2\n,\n.s16x2\n, operands\nd\n,\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == u16x2 || type == s16x2) {\n    iA[0] = a[0:15];\n    iA[1] = a[16:31];\n    iB[0] = b[0:15];\n    iB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = (iA[i] > iB[i]) ? iA[i] : iB[i];\n    }\n} else {\n    d = (a > b) ? a : b; // Integer (signed and unsigned)\n}\nNotes\nSigned and unsigned differ.\nSaturation modifier:\nmax.relu.{s16x2,\ns32}\nclamps the result to 0 if negative.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmax.u16x2\n,\nmax{.relu}.s16x2\nand\nmax.relu.s32\nintroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on all target architectures.\nmax.u16x2\n,\nmax{.relu}.s16x2\nand\nmax.relu.s32\nrequire\nsm_90\nor higher.\nExamples\nmax.u32  d,a,b;\nmax.s32  q,q,0;\nmax.relu.s16x2 t,t,u;\n9.7.1.14.\nInteger Arithmetic Instructions:\npopc\n\npopc\nPopulation count.\nSyntax\npopc.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nCount the number of one bits in\na\nand place the resulting\npopulation count\nin 32-bit\ndestination register\nd\n. Operand\na\nhas the instruction type and destination\nd\nhas type\n.u32\n.\nSemantics\n.u32  d = 0;\nwhile (a != 0) {\n   if (a & 0x1)  d++;\n   a = a >> 1;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\npopc\nrequires\nsm_20\nor higher.\nExamples\npopc.b32  d, a;\npopc.b64  cnt, X;  // cnt is .u32\n9.7.1.15.\nInteger Arithmetic Instructions:\nclz\n\nclz\nCount leading zeros.\nSyntax\nclz.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nCount the number of leading zeros in\na\nstarting with the most-significant bit and place the\nresult in 32-bit destination register\nd\n. Operand\na\nhas the instruction type, and destination\nd\nhas type\n.u32\n. For\n.b32\ntype, the number of leading zeros is between 0 and 32,\ninclusively. For\n.b64\ntype, the number of leading zeros is between 0 and 64, inclusively.\nSemantics\n.u32  d = 0;\nif (.type == .b32)   { max = 32; mask = 0x80000000; }\nelse                 { max = 64; mask = 0x8000000000000000; }\n\nwhile (d < max && (a&mask == 0) ) {\n    d++;\n    a = a << 1;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nclz\nrequires\nsm_20\nor higher.\nExamples\nclz.b32  d, a;\nclz.b64  cnt, X;  // cnt is .u32\n9.7.1.16.\nInteger Arithmetic Instructions:\nbfind\n\nbfind\nFind most significant non-sign bit.\nSyntax\nbfind.type           d, a;\nbfind.shiftamt.type  d, a;\n\n.type = { .u32, .u64,\n          .s32, .s64 };\nDescription\nFind the bit position of the most significant non-sign bit in\na\nand place the result in\nd\n. Operand\na\nhas the instruction type, and destination\nd\nhas type\n.u32\n. For unsigned\nintegers,\nbfind\nreturns the bit position of the most significant\n1\n. For signed integers,\nbfind\nreturns the bit position of the most significant\n0\nfor negative inputs and the most\nsignificant\n1\nfor non-negative inputs.\nIf\n.shiftamt\nis specified,\nbfind\nreturns the shift amount needed to left-shift the found bit\ninto the most-significant bit position.\nbfind\nreturns\n0xffffffff\nif no non-sign bit is found.\nSemantics\nmsb = (.type==.u32 || .type==.s32) ? 31 : 63;\n// negate negative signed inputs\nif ( (.type==.s32 || .type==.s64) && (a & (1<<msb)) ) {\n    a = ~a;\n}\n.u32  d = 0xffffffff;\nfor (.s32 i=msb; i>=0; i--) {\n    if (a & (1<<i))  { d = i; break; }\n}\nif (.shiftamt && d != 0xffffffff)  { d = msb - d; }\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfind\nrequires\nsm_20\nor higher.\nExamples\nbfind.u32  d, a;\nbfind.shiftamt.s64  cnt, X;  // cnt is .u32\n9.7.1.17.\nInteger Arithmetic Instructions:\nfns\n\nfns\nFind the n-th set bit\nSyntax\nfns.b32 d, mask, base, offset;\nDescription\nGiven a 32-bit value\nmask\nand an integer value\nbase\n(between 0 and 31), find the n-th (given\nby offset) set bit in\nmask\nfrom the\nbase\nbit, and store the bit position in\nd\n. If not\nfound, store 0xffffffff in\nd\n.\nOperand\nmask\nhas a 32-bit type. Operand\nbase\nhas\n.b32\n,\n.u32\nor\n.s32\ntype. Operand offset has\n.s32\ntype. Destination\nd\nhas type\n.b32.\nOperand\nbase\nmust be <= 31, otherwise behavior is undefined.\nSemantics\nd = 0xffffffff;\nif (offset == 0) {\n    if (mask[base] == 1) {\n        d = base;\n    }\n} else {\n    pos = base;\n    count = |offset| - 1;\n    inc = (offset > 0) ? 1 : -1;\n\n    while ((pos >= 0) && (pos < 32)) {\n        if (mask[pos] == 1) {\n            if (count == 0) {\n              d = pos;\n              break;\n           } else {\n               count = count - 1;\n           }\n        }\n        pos = pos + inc;\n    }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nfns\nrequires\nsm_30\nor higher.\nExamples\nfns.b32 d, 0xaaaaaaaa, 3, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 3, -1;  // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, 1;   // d = 3\nfns.b32 d, 0xaaaaaaaa, 2, -1;  // d = 1\n9.7.1.18.\nInteger Arithmetic Instructions:\nbrev\n\nbrev\nBit reverse.\nSyntax\nbrev.type  d, a;\n\n.type = { .b32, .b64 };\nDescription\nPerform bitwise reversal of input.\nSemantics\nmsb = (.type==.b32) ? 31 : 63;\n\nfor (i=0; i<=msb; i++) {\n    d[i] = a[msb-i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbrev\nrequires\nsm_20\nor higher.\nExamples\nbrev.b32  d, a;\n9.7.1.19.\nInteger Arithmetic Instructions:\nbfe\n\nbfe\nBit Field Extract.\nSyntax\nbfe.type  d, a, b, c;\n\n.type = { .u32, .u64,\n          .s32, .s64 };\nDescription\nExtract bit field from\na\nand place the zero or sign-extended result in\nd\n. Source\nb\ngives\nthe bit field starting bit position, and source\nc\ngives the bit field length in bits.\nOperands\na\nand\nd\nhave the same type as the instruction type. Operands\nb\nand\nc\nare\ntype\n.u32\n, but are restricted to the 8-bit value range\n0..255\n.\nThe sign bit of the extracted field is defined as:\n.u32\n,\n.u64\n:\nzero\n.s32\n,\n.s64\n:\nmsb\nof input a if the extracted field extends beyond the\nmsb\nof a\nmsb\nof extracted\nfield, otherwise\nIf the bit field length is zero, the result is zero.\nThe destination\nd\nis padded with the sign bit of the extracted field. If the start position is\nbeyond the\nmsb\nof the input, the destination\nd\nis filled with the replicated sign bit of the\nextracted field.\nSemantics\nmsb = (.type==.u32 || .type==.s32) ? 31 : 63;\npos = b & 0xff;  // pos restricted to 0..255 range\nlen = c & 0xff;  // len restricted to 0..255 range\n\nif (.type==.u32 || .type==.u64 || len==0)\n    sbit = 0;\nelse\n    sbit = a[min(pos+len-1,msb)];\n\nd = 0;\nfor (i=0; i<=msb; i++) {\n    d[i] = (i<len && pos+i<=msb) ? a[pos+i] : sbit;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfe\nrequires\nsm_20\nor higher.\nExamples\nbfe.b32  d,a,start,len;\n9.7.1.20.\nInteger Arithmetic Instructions:\nbfi\n\nbfi\nBit Field Insert.\nSyntax\nbfi.type  f, a, b, c, d;\n\n.type = { .b32, .b64 };\nDescription\nAlign and insert a bit field from\na\ninto\nb\n, and place the result in\nf\n. Source\nc\ngives the starting bit position for the insertion, and source\nd\ngives the bit field length in\nbits.\nOperands\na\n,\nb\n, and\nf\nhave the same type as the instruction type. Operands\nc\nand\nd\nare type\n.u32\n, but are restricted to the 8-bit value range\n0..255\n.\nIf the bit field length is zero, the result is\nb\n.\nIf the start position is beyond the msb of the input, the result is\nb\n.\nSemantics\nmsb = (.type==.b32) ? 31 : 63;\npos = c & 0xff;  // pos restricted to 0..255 range\nlen = d & 0xff;  // len restricted to 0..255 range\n\nf = b;\nfor (i=0; i<len && pos+i<=msb; i++) {\n    f[pos+i] = a[i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nbfi\nrequires\nsm_20\nor higher.\nExamples\nbfi.b32  d,a,b,start,len;\n9.7.1.21.\nInteger Arithmetic Instructions:\nszext\n\nszext\nSign-extend or Zero-extend.\nSyntax\nszext.mode.type  d, a, b;\n\n.mode = { .clamp, .wrap };\n.type = { .u32, .s32 };\nDescription\nSign-extends or zero-extends an N-bit value from operand\na\nwhere N is specified in operand\nb\n. The resulting value is stored in the destination operand\nd\n.\nFor the\n.s32\ninstruction type, the value in\na\nis treated as an N-bit signed value and the\nmost significant bit of this N-bit value is replicated up to bit 31. For the\n.u32\ninstruction\ntype, the value in\na\nis treated as an N-bit unsigned number and is zero-extended to 32\nbits. Operand\nb\nis an unsigned 32-bit value.\nIf the value of N is 0, then the result of\nszext\nis 0. If the value of N is 32 or higher, then\nthe result of\nszext\ndepends upon the value of the\n.mode\nqualifier as follows:\nIf\n.mode\nis\n.clamp\n, then the result is the same as the source operand\na\n.\nIf\n.mode\nis\n.wrap\n, then the result is computed using the wrapped value of N.\nSemantics\nb1        = b & 0x1f;\ntoo_large = (b >= 32 && .mode == .clamp) ? true : false;\nmask      = too_large ? 0 : (~0) << b1;\nsign_pos  = (b1 - 1) & 0x1f;\n\nif (b1 == 0 || too_large || .type != .s32) {\n    sign_bit = false;\n} else {\n    sign_bit = (a >> sign_pos) & 1;\n}\nd = (a & ~mask) | (sign_bit ? mask | 0);\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nszext\nrequires\nsm_70\nor higher.\nExamples\nszext.clamp.s32 rd, ra, rb;\nszext.wrap.u32  rd, 0xffffffff, 0; // Result is 0.\n9.7.1.22.\nInteger Arithmetic Instructions:\nbmsk\n\nbmsk\nBit Field Mask.\nSyntax\nbmsk.mode.b32  d, a, b;\n\n.mode = { .clamp, .wrap };\nDescription\nGenerates a 32-bit mask starting from the bit position specified in operand\na\n, and of the width\nspecified in operand\nb\n. The generated bitmask is stored in the destination operand\nd\n.\nThe resulting bitmask is 0 in the following cases:\nWhen the value of\na\nis 32 or higher and\n.mode\nis\n.clamp\n.\nWhen either the specified value of\nb\nor the wrapped value of\nb\n(when\n.mode\nis\nspecified as\n.wrap\n) is 0.\nSemantics\na1    = a & 0x1f;\nmask0 = (~0) << a1;\nb1    = b & 0x1f;\nsum   = a1 + b1;\nmask1 = (~0) << sum;\n\nsum-overflow          = sum >= 32 ? true : false;\nbit-position-overflow = false;\nbit-width-overflow    = false;\n\nif (.mode == .clamp) {\n    if (a >= 32) {\n        bit-position-overflow = true;\n        mask0 = 0;\n    }\n    if (b >= 32) {\n        bit-width-overflow = true;\n    }\n}\n\nif (sum-overflow || bit-position-overflow || bit-width-overflow) {\n    mask1 = 0;\n} else if (b1 == 0) {\n    mask1 = ~0;\n}\nd = mask0 & ~mask1;\nNotes\nThe bitmask width specified by operand\nb\nis limited to range\n0..32\nin\n.clamp\nmode and to\nrange\n0..31\nin\n.wrap\nmode.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nbmsk\nrequires\nsm_70\nor higher.\nExamples\nbmsk.clamp.b32  rd, ra, rb;\nbmsk.wrap.b32   rd, 1, 2; // Creates a bitmask of 0x00000006.\n9.7.1.23.\nInteger Arithmetic Instructions:\ndp4a\n\ndp4a\nFour-way byte dot product-accumulate.\nSyntax\ndp4a.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\nDescription\nFour-way byte dot product which is accumulated in 32-bit result.\nOperand\na\nand\nb\nare 32-bit inputs which hold 4 byte inputs in packed form for dot product.\nOperand\nc\nhas type\n.u32\nif both\n.atype\nand\n.btype\nare\n.u32\nelse operand\nc\nhas type\n.s32\n.\nSemantics\nd = c;\n\n// Extract 4 bytes from a 32bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_4(a, .atype);\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nfor (i = 0; i < 4; ++i) {\n    d += Va[i] * Vb[i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\nRequires\nsm_61\nor higher.\nExamples\ndp4a.u32.u32           d0, a0, b0, c0;\ndp4a.u32.s32           d1, a1, b1, c1;\n9.7.1.24.\nInteger Arithmetic Instructions:\ndp2a\n\ndp2a\nTwo-way dot product-accumulate.\nSyntax\ndp2a.mode.atype.btype  d, a, b, c;\n\n.atype = .btype = { .u32, .s32 };\n.mode = { .lo, .hi };\nDescription\nTwo-way 16-bit to 8-bit dot product which is accumulated in 32-bit result.\nOperand\na\nand\nb\nare 32-bit inputs. Operand\na\nholds two 16-bits inputs in packed form and\noperand\nb\nholds 4 byte inputs in packed form for dot product.\nDepending on the\n.mode\nspecified, either lower half or upper half of operand\nb\nwill be used\nfor dot product.\nOperand\nc\nhas type\n.u32\nif both\n.atype\nand\n.btype\nare\n.u32\nelse operand\nc\nhas type\n.s32\n.\nSemantics\nd = c;\n// Extract two 16-bit values from a 32-bit input and sign or zero extend\n// based on input type.\nVa = extractAndSignOrZeroExt_2(a, .atype);\n\n// Extract four 8-bit values from a 32-bit input and sign or zer extend\n// based on input type.\nVb = extractAndSignOrZeroExt_4(b, .btype);\n\nb_select = (.mode == .lo) ? 0 : 2;\n\nfor (i = 0; i < 2; ++i) {\n    d += Va[i] * Vb[b_select + i];\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\nRequires\nsm_61\nor higher.\nExamples\ndp2a.lo.u32.u32           d0, a0, b0, c0;\ndp2a.hi.u32.s32           d1, a1, b1, c1;\n9.7.2.\nExtended-Precision Integer Arithmetic Instructions\n\nInstructions\nadd.cc\n,\naddc\n,\nsub.cc\n,\nsubc\n,\nmad.cc\nand\nmadc\nreference an\nimplicitly specified condition code register (\nCC\n) having a single carry flag bit (\nCC.CF\n)\nholding carry-in/carry-out or borrow-in/borrow-out. These instructions support extended-precision\ninteger addition, subtraction, and multiplication. No other instructions access the condition code,\nand there is no support for setting, clearing, or testing the condition code. The condition code\nregister is not preserved across calls and is mainly intended for use in straight-line code\nsequences for computing extended-precision integer addition, subtraction, and multiplication.\nThe extended-precision arithmetic instructions are:\nadd.cc\n,\naddc\nsub.cc\n,\nsubc\nmad.cc\n,\nmadc\n9.7.2.1.\nExtended-Precision Arithmetic Instructions:\nadd.cc\n\nadd.cc\nAdd two values with carry-out.\nSyntax\nadd.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer addition and writes the carry-out value into the condition code register.\nSemantics\nd = a + b;\ncarry-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nadd.cc\nintroduced in PTX ISA version 1.2.\n64-bit\nadd.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nadd.cc\nis supported on all target architectures.\n64-bit\nadd.cc\nrequires\nsm_20\nor higher.\nExamples\n@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4;\n9.7.2.2.\nExtended-Precision Arithmetic Instructions:\naddc\n\naddc\nAdd two values with carry-in and optional carry-out.\nSyntax\naddc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer addition with carry-in and optionally writes the carry-out value into the condition\ncode register.\nSemantics\nd = a + b + CC.CF;\nif\n.cc\nspecified, carry-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\naddc\nintroduced in PTX ISA version 1.2.\n64-bit\naddc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\naddc\nis supported on all target architectures.\n64-bit\naddc\nrequires\nsm_20\nor higher.\nExamples\n@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of\n@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values\n@p  addc.cc.u32  x3,y3,z3;\n@p  addc.u32     x4,y4,z4;\n9.7.2.3.\nExtended-Precision Arithmetic Instructions:\nsub.cc\n\nsub.cc\nSubtract one value from another, with borrow-out.\nSyntax\nsub.cc.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer subtraction and writes the borrow-out value into the condition code register.\nSemantics\nd = a - b;\nborrow-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nsub.cc\nintroduced in PTX ISA version 1.2.\n64-bit\nsub.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nsub.cc\nis supported on all target architectures.\n64-bit\nsub.cc\nrequires\nsm_20\nor higher.\nExamples\n@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4;\n9.7.2.4.\nExtended-Precision Arithmetic Instructions:\nsubc\n\nsubc\nSubtract one value from another, with borrow-in and optional borrow-out.\nSyntax\nsubc{.cc}.type  d, a, b;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nPerforms integer subtraction with borrow-in and optionally writes the borrow-out value into the\ncondition code register.\nSemantics\nd = a  - (b + CC.CF);\nif\n.cc\nspecified, borrow-out written to\nCC.CF\nNotes\nNo integer rounding modifiers.\nNo saturation.\nBehavior is the same for unsigned and signed integers.\nPTX ISA Notes\n32-bit\nsubc\nintroduced in PTX ISA version 1.2.\n64-bit\nsubc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\n32-bit\nsubc\nis supported on all target architectures.\n64-bit\nsubc\nrequires\nsm_20\nor higher.\nExamples\n@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction\n@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values\n@p  subc.cc.u32  x3,y3,z3;\n@p  subc.u32     x4,y4,z4;\n9.7.2.5.\nExtended-Precision Arithmetic Instructions:\nmad.cc\n\nmad.cc\nMultiply two values, extract high or low half of result, and add a third value with carry-out.\nSyntax\nmad{.hi,.lo}.cc.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nMultiplies two values, extracts either the high or low part of the result, and adds a third\nvalue. Writes the result to the destination register and the carry-out from the addition into the\ncondition code register.\nSemantics\nt = a * b;\nd = t<63..32> + c;    // for .hi variant\nd = t<31..0> + c;     // for .lo variant\ncarry-out from addition is written to\nCC.CF\nNotes\nGenerally used in combination with\nmadc\nand\naddc\nto implement extended-precision multi-word\nmultiplication. See\nmadc\nfor an example.\nPTX ISA Notes\n32-bit\nmad.cc\nintroduced in PTX ISA version 3.0.\n64-bit\nmad.cc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nRequires target\nsm_20\nor higher.\nExamples\n@p  mad.lo.cc.u32 d,a,b,c;\n    mad.lo.cc.u32 r,p,q,r;\n9.7.2.6.\nExtended-Precision Arithmetic Instructions:\nmadc\n\nmadc\nMultiply two values, extract high or low half of result, and add a third value with carry-in and\noptional carry-out.\nSyntax\nmadc{.hi,.lo}{.cc}.type  d, a, b, c;\n\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nMultiplies two values, extracts either the high or low part of the result, and adds a third value\nalong with carry-in. Writes the result to the destination register and optionally writes the\ncarry-out from the addition into the condition code register.\nSemantics\nt = a * b;\nd = t<63..32> + c + CC.CF;     // for .hi variant\nd = t<31..0> + c + CC.CF;      // for .lo variant\nif\n.cc\nspecified, carry-out from addition is written to\nCC.CF\nNotes\nGenerally used in combination with\nmad.cc\nand\naddc\nto implement extended-precision\nmulti-word multiplication. See example below.\nPTX ISA Notes\n32-bit\nmadc\nintroduced in PTX ISA version 3.0.\n64-bit\nmadc\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nRequires target\nsm_20\nor higher.\nExamples\n// extended-precision multiply:  [r3,r2,r1,r0] = [r5,r4] * [r7,r6]\nmul.lo.u32     r0,r4,r6;      // r0=(r4*r6).[31:0], no carry-out\nmul.hi.u32     r1,r4,r6;      // r1=(r4*r6).[63:32], no carry-out\nmad.lo.cc.u32  r1,r5,r6,r1;   // r1+=(r5*r6).[31:0], may carry-out\nmadc.hi.u32    r2,r5,r6,0;    // r2 =(r5*r6).[63:32]+carry-in,\n                              // no carry-out\nmad.lo.cc.u32   r1,r4,r7,r1;  // r1+=(r4*r7).[31:0], may carry-out\nmadc.hi.cc.u32  r2,r4,r7,r2;  // r2+=(r4*r7).[63:32]+carry-in,\n                              // may carry-out\naddc.u32        r3,0,0;       // r3 = carry-in, no carry-out\nmad.lo.cc.u32   r2,r5,r7,r2;  // r2+=(r5*r7).[31:0], may carry-out\nmadc.hi.u32     r3,r5,r7,r3;  // r3+=(r5*r7).[63:32]+carry-in\n9.7.3.\nFloating-Point Instructions\n\nFloating-point instructions operate on\n.f32\nand\n.f64\nregister operands and constant\nimmediate values. The floating-point instructions are:\ntestp\ncopysign\nadd\nsub\nmul\nfma\nmad\ndiv\nabs\nneg\nmin\nmax\nrcp\nsqrt\nrsqrt\nsin\ncos\nlg2\nex2\ntanh\nInstructions that support rounding modifiers are IEEE-754 compliant. Double-precision instructions\nsupport subnormal inputs and results. Single-precision instructions support subnormal inputs and\nresults by default for\nsm_20\nand subsequent targets, and flush subnormal inputs and results to\nsign-preserving zero for\nsm_1x\ntargets. The optional\n.ftz\nmodifier on single-precision\ninstructions provides backward compatibility with\nsm_1x\ntargets by flushing subnormal inputs and\nresults to sign-preserving zero regardless of the target architecture.\nSingle-precision\nadd\n,\nsub\n,\nmul\n, and\nmad\nsupport saturation of results to the range\n[0.0, 1.0], with\nNaN\ns being flushed to positive zero.\nNaN\npayloads are supported for\ndouble-precision instructions (except for\nrcp.approx.ftz.f64\nand\nrsqrt.approx.ftz.f64\n, which\nmaps input\nNaN\ns to a canonical\nNaN\n). Single-precision instructions return an unspecified\nNaN\n. Note that future implementations may support\nNaN\npayloads for single-precision\ninstructions, so PTX programs should not rely on the specific single-precision\nNaN\ns being\ngenerated.\nTable 29\nsummarizes\nfloating-point instructions in PTX.\nTable 29\nSummary of Floating-Point Instructions\n\nInstruction\n.rn\n.rz\n.rm\n.rp\n.ftz\n.sat\nNotes\n{add,sub,mul}.rnd.f32\nx\nx\nx\nx\nx\nx\nIf no rounding modifier is specified,\ndefault is\n.rn\nand instructions may\nbe folded into a multiply-add.\n{add,sub,mul}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\nIf no rounding modifier is specified,\ndefault is\n.rn\nand instructions may\nbe folded into a multiply-add.\nmad.f32\nn/a\nn/a\nn/a\nn/a\nx\nx\n.target\nsm_1x\nNo rounding modifier.\n{mad,fma}.rnd.f32\nx\nx\nx\nx\nx\nx\n.target\nsm_20\nor higher\nmad.f32\nand\nfma.f32\nare the same.\n{mad,fma}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\nmad.f64\nand\nfma.f64\nare the same.\ndiv.full.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nNo rounding modifier.\n{div,rcp,sqrt}.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nn/a\nrcp.approx.ftz.f64\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n.target\nsm_20\nor higher\n{div,rcp,sqrt}.rnd.f32\nx\nx\nx\nx\nx\nn/a\n.target\nsm_20\nor higher\n{div,rcp,sqrt}.rnd.f64\nx\nx\nx\nx\nn/a\nn/a\n.target\nsm_20\nor higher\n{abs,neg,min,max}.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n{abs,neg,min,max}.f64\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nrsqrt.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\nrsqrt.approx.f64\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nrsqrt.approx.ftz.f64\nn/a\nn/a\nn/a\nn/a\nx\nn/a\n.target\nsm_20\nor higher\n{sin,cos,lg2,ex2}.approx.f32\nn/a\nn/a\nn/a\nn/a\nx\nn/a\ntanh.approx.f32\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\n.target\nsm_75\nor higher\n9.7.3.1.\nFloating Point Instructions:\ntestp\n\ntestp\nTest floating-point property.\nSyntax\ntestp.op.type  p, a;  // result is .pred\n\n.op   = { .finite, .infinite,\n          .number, .notanumber,\n          .normal, .subnormal };\n.type = { .f32, .f64 };\nDescription\ntestp\ntests common properties of floating-point numbers and returns a predicate value of\n1\nif\nTrue\nand\n0\nif\nFalse\n.\ntestp.finite\nTrue\nif the input is not infinite or\nNaN\ntestp.infinite\nTrue\nif the input is positive or negative infinity\ntestp.number\nTrue\nif the input is not\nNaN\ntestp.notanumber\nTrue\nif the input is\nNaN\ntestp.normal\nTrue\nif the input is a normal number (not\nNaN\n, not infinity)\ntestp.subnormal\nTrue\nif the input is a subnormal number (not\nNaN\n, not infinity)\nAs a special case, positive and negative zero are considered normal numbers.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ntestp.notanumber.f32  isnan, f0;\ntestp.infinite.f64    p, X;\n9.7.3.2.\nFloating Point Instructions:\ncopysign\n\ncopysign\nCopy sign of one input to another.\nSyntax\ncopysign.type  d, a, b;\n\n.type = { .f32, .f64 };\nDescription\nCopy sign bit of\na\ninto value of\nb\n, and return the result as\nd\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ncopysign.f32  x, y, z;\ncopysign.f64  A, B, C;\n9.7.3.3.\nFloating Point Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd{.rnd}{.ftz}{.sat}.f32  d, a, b;\nadd{.rnd}{.ftz}.f32x2      d, a, b;\nadd{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values from\nsource operands. Single precision (\n.f32\n) operands are then added in parallel to produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a + b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] + fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nadd.ftz.f32\n,\nadd.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nadd.f64\nsupports subnormal numbers.\nadd.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nadd.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nadd.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nadd.f32\nsupported on all target architectures.\nadd.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nadd.f64\n, requires\nsm_13\nor higher.\nfor\nadd.f32\n, requires\nsm_20\nor higher.\nadd.f32x2\nrequires\nsm_100\nor higher.\nExamples\n@p  add.rz.ftz.f32  f1,f2,f3;\nadd.rp.ftz.f32x2    d, a, b;\n9.7.3.4.\nFloating Point Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub{.rnd}{.ftz}{.sat}.f32  d, a, b;\nsub{.rnd}{.ftz}.f32x2      d, a, b;\nsub{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values\nfrom source operands. Single precision (\n.f32\n) operands are then subtracted in parallel\nto produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a - b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] - fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that a\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsub.ftz.f32\n,\nsub.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nsub.f64\nsupports subnormal numbers.\nsub.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nsub.sat.f32\nclamps the result to [0.0, 1.0]. NaN results are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nsub.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nsub.f32\nsupported on all target architectures.\nsub.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nsub.f64\n, requires\nsm_13\nor higher.\nfor\nsub.f32\n, requires\nsm_20\nor higher.\nsub.f32x2\nrequires\nsm_100\nor higher.\nExamples\nsub.f32 c,a,b;\nsub.rn.ftz.f32  f1,f2,f3;\n9.7.3.5.\nFloating Point Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul{.rnd}{.ftz}{.sat}.f32  d, a, b;\nmul{.rnd}{.ftz}.f32x2      d, a, b;\nmul{.rnd}.f64              d, a, b;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute the product of two values.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values\nfrom source operands. Single precision (\n.f32\n) operands are then multiplied in parallel\nto produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a * b;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] * fB[i];\n    }\n}\nNotes\nFor floating-point multiplication, all operands must be the same size.\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that a\nmul\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nmul\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul/add\nand\nmul/sub\nsequences with no rounding modifiers may be\noptimized to use fused-multiply-add instructions on the target device.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmul.ftz.f32\n,\nmul.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmul.f64\nsupports subnormal numbers.\nmul.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmul.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmul.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nmul.f32\nsupported on all target architectures.\nmul.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\navailable for all targets\n.rm\n,\n.rp\nfor\nmul.f64\n, requires\nsm_13\nor higher.\nfor\nmul.f32\n, requires\nsm_20\nor higher.\nmul.f32x2\nrequires\nsm_100\nor higher.\nExamples\nmul.ftz.f32 circumf,radius,pi  // a single-precision multiply\n9.7.3.6.\nFloating Point Instructions:\nfma\n\nfma\nFused multiply-add.\nSyntax\nfma.rnd{.ftz}{.sat}.f32  d, a, b, c;\nfma.rnd{.ftz}.f32x2      d, a, b, c;\nfma.rnd.f64              d, a, b, c;\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nPerforms a fused multiply-add with no loss of precision in the intermediate product and addition.\nFor\n.f32x2\ninstruction type, forms input vectors of single precision (\n.f32\n) values from\nsource operands. Single precision (\n.f32\n) operands are then operated in parallel to produce\n.f32x2\nresult in destination.\nFor\n.f32x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b64\ntype.\nSemantics\nif (type == f32 || type == f64) {\n    d = a * b + c;\n} else if (type == f32x2) {\n    fA[0] = a[0:31];\n    fA[1] = a[32:63];\n    fB[0] = b[0:31];\n    fB[1] = b[32:63];\n    fC[0] = c[0:31];\n    fC[1] = c[32:63];\n    for (i = 0; i < 2; i++) {\n        d[i] = fA[i] * fB[i] + fC[i];\n    }\n}\nNotes\nfma.f32\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by\n.rnd\n.\nfma.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n.\nfma.f64\nis the same as\nmad.f64\n.\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nfma.ftz.f32\n,\nfma.ftz.f32x2\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nfma.f64\nsupports subnormal numbers.\nfma.f32\nis unimplemented for\nsm_1x\ntargets.\nSaturation:\nfma.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nfma.f64\nintroduced in PTX ISA version 1.4.\nfma.f32\nintroduced in PTX ISA version 2.0.\nfma.f32x2\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nfma.f32\nrequires\nsm_20\nor higher.\nfma.f64\nrequires\nsm_13\nor higher.\nfma.f32x2\nrequires\nsm_100\nor higher.\nExamples\nfma.rn.ftz.f32  w,x,y,z;\n@p  fma.rn.f64      d,a,b,c;\n    fma.rp.ftz.f32x2 p,q,r,s;\n9.7.3.7.\nFloating Point Instructions:\nmad\n\nmad\nMultiply two values and add a third value.\nSyntax\nmad{.ftz}{.sat}.f32      d, a, b, c;    // .target sm_1x\nmad.rnd{.ftz}{.sat}.f32  d, a, b, c;    // .target sm_20\nmad.rnd.f64              d, a, b, c;    // .target sm_13 and higher\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nMultiplies two values and adds a third, and then writes the resulting value into a destination\nregister.\nSemantics\nd = a*b + c;\nNotes\nFor\n.target\nsm_20\nand higher:\nmad.f32\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to single precision\nusing the rounding mode specified by\n.rnd\n.\nmad.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n.\nmad.{f32,f64}\nis the same as\nfma.{f32,f64}\n.\nFor\n.target\nsm_1x\n:\nmad.f32\ncomputes the product of\na\nand\nb\nat double precision, and then the mantissa is\ntruncated to 23 bits, but the exponent is preserved. Note that this is different from computing\nthe product with\nmul\n, where the mantissa can be rounded and the exponent will be clamped. The\nexception for\nmad.f32\nis when\nc\n=\n+/-0.0\n,\nmad.f32\nis identical to the result computed\nusing separate mul and add instructions. When JIT-compiled for SM 2.0 devices,\nmad.f32\nis\nimplemented as a fused multiply-add (i.e.,\nfma.rn.ftz.f32\n). In this case,\nmad.f32\ncan\nproduce slightly different numeric results and backward compatibility is not guaranteed in this\ncase.\nmad.f64\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto\nthis product, again in infinite precision. The resulting value is then rounded to double precision\nusing the rounding mode specified by\n.rnd\n. Unlike\nmad.f32\n, the treatment of subnormal\ninputs and output follows IEEE 754 standard.\nmad.f64\nis the same as\nfma.f64\n.\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmad.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmad.f64\nsupports subnormal numbers.\nmad.f32\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmad.sat.f32\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nIn PTX ISA versions 1.4 and later, a rounding modifier is required for\nmad.f64\n.\nLegacy\nmad.f64\ninstructions having no rounding modifier will map to\nmad.rn.f64\n.\nIn PTX ISA versions 2.0 and later, a rounding modifier is required for\nmad.f32\nfor\nsm_20\nand higher targets.\nErrata\nmad.f32\nrequires a rounding modifier for\nsm_20\nand higher targets. However for PTX ISA\nversion 3.0 and earlier, ptxas does not enforce this requirement and\nmad.f32\nsilently defaults\nto\nmad.rn.f32\n. For PTX ISA version 3.1, ptxas generates a warning and defaults to\nmad.rn.f32\n, and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later.\nTarget ISA Notes\nmad.f32\nsupported on all target architectures.\nmad.f64\nrequires\nsm_13\nor higher.\nRounding modifiers have the following target requirements:\n.rn\n,\n.rz\n,\n.rm\n,\n.rp\nfor\nmad.f64\n, requires\nsm_13\nor higher.\n.rn\n,\n.rz\n,\n.rm\n,\n.rp\nfor\nmad.f32\n, requires\nsm_20\nor higher.\nExamples\n@p  mad.f32  d,a,b,c;\n9.7.3.8.\nFloating Point Instructions:\ndiv\n\ndiv\nDivide one value by another.\nSyntax\ndiv.approx{.ftz}.f32  d, a, b;  // fast, approximate divide\ndiv.full{.ftz}.f32    d, a, b;  // full-range approximate divide\ndiv.rnd{.ftz}.f32     d, a, b;  // IEEE 754 compliant rounding\ndiv.rnd.f64           d, a, b;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nDivides\na\nby\nb\n, stores result in\nd\n.\nSemantics\nd = a / b;\nNotes\nFast, approximate single-precision divides:\ndiv.approx.f32\nimplements a fast approximation to divide, computed as\nd\n=\na\n*\n(1/b)\n. For\n|b|\nin [2\n-126\n, 2\n126\n], the maximum\nulp\nerror is 2. For 2\n126\n<\n|b|\n< 2\n128\n, if\na\nis infinity,\ndiv.approx.f32\nreturns\nNaN\n, otherwise it\nreturns a sign-preserving zero.\ndiv.full.f32\nimplements a relatively fast, full-range approximation that scales operands to\nachieve better accuracy, but is not fully IEEE 754 compliant and does not support rounding\nmodifiers. The maximum\nulp\nerror is 2 across the full range of inputs.\nDivide with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\ndiv.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\ndiv.f64\nsupports subnormal numbers.\ndiv.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\ndiv.f32\nand\ndiv.f64\nintroduced in PTX ISA version 1.0.\nExplicit modifiers\n.approx\n,\n.full\n,\n.ftz\n, and rounding introduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, one of\n.approx\n,\n.full\n, or\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\ndiv.f32\ndefaults to\ndiv.approx.ftz.f32\n, and\ndiv.f64\ndefaults to\ndiv.rn.f64\n.\nTarget ISA Notes\ndiv.approx.f32\nand\ndiv.full.f32\nsupported on all target architectures.\ndiv.rnd.f32\nrequires\nsm_20\nor higher.\ndiv.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32\n.\ndiv.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\ndiv.approx.ftz.f32  diam,circum,3.14159;\ndiv.full.ftz.f32    x, y, z;\ndiv.rn.f64          xd, yd, zd;\n9.7.3.9.\nFloating Point Instructions:\nabs\n\nabs\nAbsolute value.\nSyntax\nabs{.ftz}.f32  d, a;\nabs.f64        d, a;\nDescription\nTake the absolute value of\na\nand store the result in\nd\n.\nSemantics\nd = |a|;\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nabs.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nabs.f64\nsupports subnormal numbers.\nabs.f32\nflushes subnormal inputs and results to sign-preserving zero.\nFor\nabs.f32\n,\nNaN\ninput yields unspecified\nNaN\n. For\nabs.f64\n,\nNaN\ninput is passed\nthrough unchanged. Future implementations may comply with the IEEE 754 standard by preserving\npayload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nabs.f32\nsupported on all target architectures.\nabs.f64\nrequires\nsm_13\nor higher.\nExamples\nabs.ftz.f32  x,f0;\n9.7.3.10.\nFloating Point Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg{.ftz}.f32  d, a;\nneg.f64        d, a;\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nSemantics\nd = -a;\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nneg.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nneg.f64\nsupports subnormal numbers.\nneg.f32\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nneg.f32\nsupported on all target architectures.\nneg.f64\nrequires\nsm_13\nor higher.\nExamples\nneg.ftz.f32  x,f0;\n9.7.3.11.\nFloating Point Instructions:\nmin\n\nmin\nFind the minimum of given values.\nSyntax\nmin{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmin{.ftz}{.NaN}{.abs}.f32          d, a, b, c;\nmin.f64                            d, a, b;\nDescription\nStore the minimum of\na\n,\nb\nand optionally\nc\nin\nd\n.\nIf\n.NaN\nmodifier is specified, then the result is canonical\nNaN\nif any of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the minimum of\nabsolute values of both input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both inputs\na\nand\nb\n. The\n.xorsign\nqualifier cannot be specified for three\ninputs operation.\nQualifier\n.xorsign\nrequires qualifier\n.abs\nto be specified. In such cases,\n.xorsign\nconsiders the sign bit of both inputs before applying\n.abs\noperation.\nIf the result of\nmin\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\ndef min_num (z, x, y) {\n    if (isNaN(x) && isNaN(y))\n        z = NaN;\n    else if (isNaN(x))\n        z = y;\n    else if (isNaN(y))\n        z = x;\n    else\n        // note: -0.0 < +0.0 here\n        z = (x < y) ? x : y;\n    return z;\n}\n\ndef min_nan (z, x, y) {\n    if (isNaN(x) || isNaN(y))\n        z = NaN;\n    else\n        // note: -0.0 < +0.0 here\n        z = (x < y) ? x : y;\n    return z;\n}\n\ndef two_inputs_min (z, x, y) {\n    if (.NaN)\n        z = min_nan(z, x, y);\n    else\n        z = min_num(z, x, y);\n    return z;\n}\n\nif (.xorsign && !isPresent(c)) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n}\nif (.abs) {\n    a = |a|;\n    b = |b|;\n    if (isPresent(c)) {\n        c = |c|;\n    }\n}\n\nd = two_inputs_min(d, a, b)\nif (isPresent(c)) {\n    d = two_inputs_min(d, d, c)\n}\nif (.xorsign && !isPresent(c) && !isNaN(d)) {\n    setSignBit(d, xorsign);\n}\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmin.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmin.f64\nsupports subnormal numbers.\nmin.f32\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmin.NaN\nintroduced in PTX ISA version 7.0.\nmin.xorsign.abs\nintroduced in PTX ISA version 7.2.\nmin\nwith three input arguments introduced in PTX ISA version 8.8.\nTarget ISA Notes\nmin.f32\nsupported on all target architectures.\nmin.f64\nrequires\nsm_13\nor higher.\nmin.NaN\nrequires\nsm_80\nor higher.\nmin.xorsign.abs\nrequires\nsm_86\nor higher.\nmin\nwith three input arguments requires\nsm_100\nor higher.\nExamples\n@p  min.ftz.f32  z,z,x;\n    min.f64      a,b,c;\n    // fp32 min with .NaN\n    min.NaN.f32  f0,f1,f2;\n    // fp32 min with .xorsign.abs\n    min.xorsign.abs.f32 Rd, Ra, Rb;\n9.7.3.12.\nFloating Point Instructions:\nmax\n\nmax\nFind the maximum of given values.\nSyntax\nmax{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;\nmax{.ftz}{.NaN}{.abs}.f32          d, a, b, c;\nmax.f64                            d, a, b;\nDescription\nStore the maximum of\na\n,\nb\nand optionally\nc\nin\nd\n.\nIf\n.NaN\nmodifier is specified, the result is canonical\nNaN\nif any of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the maximum of\nabsolute values of the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of the inputs:\na\nand\nb\n. The\n.xorsign\nqualifier cannot be specified for three\ninputs operation.\nQualifier\n.xorsign\nrequires qualifier\n.abs\nto be specified. In such cases,\n.xorsign\nconsiders the sign bit of both inputs before applying\n.abs\noperation.\nIf the result of\nmax\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\ndef max_num (z, x, y) {\n    if (isNaN(x) && isNaN(y))\n        z = NaN;\n    else if (isNaN(x))\n        z = y;\n    else if (isNaN(y))\n        z = x;\n    else\n        // note: +0.0 > -0.0 here\n        z = (x > y) ? x : y;\n    return z;\n}\n\ndef max_nan (z, x, y) {\n    if (isNaN(x) || isNaN(y))\n        z = NaN;\n    else\n        // note: +0.0 > -0.0 here\n        z = (x > y) ? x : y;\n    return z;\n}\n\ndef two_inputs_max (z, x, y) {\n    if (.NaN)\n        z = max_nan(z, x, y);\n    else\n        z = max_num(z, x, y);\n    return z;\n}\n\nif (.xorsign && !isPresent(c)) {\n    xorsign = getSignBit(a) ^ getSignBit(b);\n}\nif (.abs) {\n    a = |a|;\n    b = |b|;\n    if (isPresent(c)) {\n        c = |c|;\n    }\n}\n\nd = two_inputs_max (d, a, b)\nif (isPresent(c)) {\n    d = two_inputs_max (d, d, c)\n}\n\nif (.xorsign && !isPresent(c) !isNaN(d)) {\n    setSignBit(d, xorsign);\n}\nNotes\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nmax.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nmax.f64\nsupports subnormal numbers.\nmax.f32\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nmax.NaN\nintroduced in PTX ISA version 7.0.\nmax.xorsign.abs\nintroduced in PTX ISA version 7.2.\nmax\nwith three input arguments introduced in PTX ISA version 8.8.\nTarget ISA Notes\nmax.f32\nsupported on all target architectures.\nmax.f64\nrequires\nsm_13\nor higher.\nmax.NaN\nrequires\nsm_80\nor higher.\nmax.xorsign.abs\nrequires\nsm_86\nor higher.\nmax\nwith three input arguments requires\nsm_100\nor higher.\nExamples\nmax.ftz.f32  f0,f1,f2;\nmax.f64      a,b,c;\n// fp32 max with .NaN\nmax.NaN.f32  f0,f1,f2;\n// fp32 max with .xorsign.abs\nmax.xorsign.abs.f32 Rd, Ra, Rb;\n9.7.3.13.\nFloating Point Instructions:\nrcp\n\nrcp\nTake the reciprocal of a value.\nSyntax\nrcp.approx{.ftz}.f32  d, a;  // fast, approximate reciprocal\nrcp.rnd{.ftz}.f32     d, a;  // IEEE 754 compliant rounding\nrcp.rnd.f64           d, a;  // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute\n1/a\n, store result in\nd\n.\nSemantics\nd = 1 / a;\nNotes\nFast, approximate single-precision reciprocal:\nrcp.approx.f32\nimplements a fast approximation to reciprocal.\nThe maximum ulp error is 1 across the full range of inputs.\nInput\nResult\n-Inf\n-0.0\n-0.0\n-Inf\n+0.0\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nReciprocal with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nrcp.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nrcp.f64\nsupports subnormal numbers.\nrcp.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nrcp.f32\nand\nrcp.f64\nintroduced in PTX ISA version 1.0.\nrcp.rn.f64\nand explicit modifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4. General rounding modifiers were\nadded in PTX ISA version 2.0.\nFor PTX ISA version 1.4 and later, one of\n.approx\nor\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\nrcp.f32\ndefaults to\nrcp.approx.ftz.f32\n, and\nrcp.f64\ndefaults to\nrcp.rn.f64\n.\nTarget ISA Notes\nrcp.approx.f32\nsupported on all target architectures.\nrcp.rnd.f32\nrequires\nsm_20\nor higher.\nrcp.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32.\nrcp.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\nrcp.approx.ftz.f32  ri,r;\nrcp.rn.ftz.f32      xi,x;\nrcp.rn.f64          xi,x;\n9.7.3.14.\nFloating Point Instructions:\nrcp.approx.ftz.f64\n\nrcp.approx.ftz.f64\nCompute a fast, gross approximation to the reciprocal of a value.\nSyntax\nrcp.approx.ftz.f64  d, a;\nDescription\nCompute a fast, gross approximation to the reciprocal as follows:\nextract the most-significant 32 bits of\n.f64\noperand\na\nin 1.11.20 IEEE floating-point\nformat (i.e., ignore the least-significant 32 bits of\na\n),\ncompute an approximate\n.f64\nreciprocal of this value using the most-significant 20 bits of\nthe mantissa of operand\na\n,\nplace the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits\nof destination\nd\n,and\nzero the least significant 32 mantissa bits of\n.f64\ndestination\nd\n.\nSemantics\ntmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / tmp;\nd[31:0] = 0x00000000;\nNotes\nrcp.approx.ftz.f64\nimplements a fast, gross approximation to reciprocal.\nInput a[63:32]\nResult d[63:32]\n-Inf\n-0.0\n-subnormal\n-Inf\n-0.0\n-Inf\n+0.0\n+Inf\n+subnormal\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nInput\nNaN\ns map to a canonical\nNaN\nwith encoding\n0x7fffffff00000000\n.\nSubnormal inputs and results are flushed to sign-preserving zero.\nPTX ISA Notes\nrcp.approx.ftz.f64\nintroduced in PTX ISA version 2.1.\nTarget ISA Notes\nrcp.approx.ftz.f64\nrequires\nsm_20\nor higher.\nExamples\nrcp.approx.ftz.f64  xi,x;\n9.7.3.15.\nFloating Point Instructions:\nsqrt\n\nsqrt\nTake the square root of a value.\nSyntax\nsqrt.approx{.ftz}.f32  d, a; // fast, approximate square root\nsqrt.rnd{.ftz}.f32     d, a; // IEEE 754 compliant rounding\nsqrt.rnd.f64           d, a; // IEEE 754 compliant rounding\n\n.rnd = { .rn, .rz, .rm, .rp };\nDescription\nCompute sqrt(\na\n) and store the result in\nd\n.\nSemantics\nd = sqrt(a);\nNotes\nsqrt.approx.f32\nimplements a fast approximation to square root.\nThe maximum relative error over the entire positive finite floating-point\nrange is 2\n-23\n.\nFor various corner-case inputs, results of\nsqrt\ninstruction are shown\nin below table:\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n+Inf\nNaN\nNaN\nSquare root with IEEE 754 compliant rounding:\nRounding modifiers (no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsqrt.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nsqrt.f64\nsupports subnormal numbers.\nsqrt.f32\nflushes subnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nsqrt.f32\nand\nsqrt.f64\nintroduced in PTX ISA version 1.0.\nsqrt.rn.f64\nand explicit\nmodifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4. General rounding\nmodifiers were added in PTX ISA version 2.0.\nFor PTX ISA version 1.4 and later, one of\n.approx\nor\n.rnd\nis required.\nFor PTX ISA versions 1.0 through 1.3,\nsqrt.f32\ndefaults to\nsqrt.approx.ftz.f32\n, and\nsqrt.f64\ndefaults to\nsqrt.rn.f64\n.\nTarget ISA Notes\nsqrt.approx.f32\nsupported on all target architectures.\nsqrt.rnd.f32\nrequires\nsm_20\nor higher.\nsqrt.rn.f64\nrequires\nsm_13\nor higher, or\n.target\nmap_f64_to_f32\n.\nsqrt.{rz,rm,rp}.f64\nrequires\nsm_20\nor higher.\nExamples\nsqrt.approx.ftz.f32  r,x;\nsqrt.rn.ftz.f32      r,x;\nsqrt.rn.f64          r,x;\n9.7.3.16.\nFloating Point Instructions:\nrsqrt\n\nrsqrt\nTake the reciprocal of the square root of a value.\nSyntax\nrsqrt.approx{.ftz}.f32  d, a;\nrsqrt.approx.f64        d, a;\nDescription\nCompute\n1/sqrt(a)\nand store the result in\nd\n.\nSemantics\nd = 1/sqrt(a);\nNotes\nrsqrt.approx\nimplements an approximation to the reciprocal square root.\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-Inf\n+0.0\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nThe maximum relative error for\nrsqrt.f32\nover the entire positive\nfinite floating-point range is 2\n-22.9\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nrsqrt.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nrsqrt.f64\nsupports subnormal numbers.\nrsqrt.f32\nflushes subnormal inputs and results to sign-preserving zero.\nNote that\nrsqrt.approx.f64\nis emulated in software and are relatively slow.\nPTX ISA Notes\nrsqrt.f32\nand\nrsqrt.f64\nwere introduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nwere introduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nrsqrt.f32\ndefaults to\nrsqrt.approx.ftz.f32\n, and\nrsqrt.f64\ndefaults to\nrsqrt.approx.f64\n.\nTarget ISA Notes\nrsqrt.f32\nsupported on all target architectures.\nrsqrt.f64\nrequires\nsm_13\nor higher.\nExamples\nrsqrt.approx.ftz.f32  isr, x;\nrsqrt.approx.f64      ISR, X;\n9.7.3.17.\nFloating Point Instructions:\nrsqrt.approx.ftz.f64\n\nrsqrt.approx.ftz.f64\nCompute an approximation of the square root reciprocal of a value.\nSyntax\nrsqrt.approx.ftz.f64 d, a;\nDescription\nCompute a double-precision (\n.f64\n) approximation of the square root reciprocal of a value. The\nleast significant 32 bits of the double-precision (\n.f64\n) destination\nd\nare all zeros.\nSemantics\ntmp = a[63:32]; // upper word of a, 1.11.20 format\nd[63:32] = 1.0 / sqrt(tmp);\nd[31:0] = 0x00000000;\nNotes\nrsqrt.approx.ftz.f64\nimplements a fast approximation of the square root reciprocal of a value.\nInput\nResult\n-Inf\nNaN\n-subnormal\n-Inf\n-0.0\n-Inf\n+0.0\n+Inf\n+subnormal\n+Inf\n+Inf\n+0.0\nNaN\nNaN\nInput\nNaN\ns map to a canonical\nNaN\nwith encoding\n0x7fffffff00000000\n.\nSubnormal inputs and results are flushed to sign-preserving zero.\nPTX ISA Notes\nrsqrt.approx.ftz.f64\nintroduced in PTX ISA version 4.0.\nTarget ISA Notes\nrsqrt.approx.ftz.f64\nrequires\nsm_20\nor higher.\nExamples\nrsqrt.approx.ftz.f64 xi,x;\n9.7.3.18.\nFloating Point Instructions:\nsin\n\nsin\nFind the sine of a value.\nSyntax\nsin.approx{.ftz}.f32  d, a;\nDescription\nFind the sine of the angle\na\n(in radians).\nSemantics\nd = sin(a);\nNotes\nsin.approx.f32\nimplements a fast approximation to sine.\nInput\nResult\n-Inf\nNaN\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\nNaN\nNaN\nNaN\nThe maximum absolute error over input range is as follows:\nRange\n[-2pi .. 2pi]\n[-100pi .. +100pi]\nError\n2\n-20.5\n2\n-14.7\nOutside of the range [-100pi .. +100pi], only best effort\nis provided. There are no defined error guarantees.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsin.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nsin.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the .approx modifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nsin.f32\ndefaults to\nsin.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsin.approx.ftz.f32  sa, a;\n9.7.3.19.\nFloating Point Instructions:\ncos\n\ncos\nFind the cosine of a value.\nSyntax\ncos.approx{.ftz}.f32  d, a;\nDescription\nFind the cosine of the angle\na\n(in radians).\nSemantics\nd = cos(a);\nNotes\ncos.approx.f32\nimplements a fast approximation to cosine.\nInput\nResult\n-Inf\nNaN\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\nNaN\nNaN\nNaN\nThe maximum absolute error over input range is as follows:\nRange\n[-2pi .. 2pi]\n[-100pi .. +100pi]\nError\n2\n-20.5\n2\n-14.7\nOutside of the range [-100pi .. +100pi], only best effort\nis provided. There are no defined error guarantees.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\ncos.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\ncos.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\ncos.f32\ndefaults to\ncos.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ncos.approx.ftz.f32  ca, a;\n9.7.3.20.\nFloating Point Instructions:\nlg2\n\nlg2\nFind the base-2 logarithm of a value.\nSyntax\nlg2.approx{.ftz}.f32  d, a;\nDescription\nDetermine the log\n2\nof\na\n.\nSemantics\nd = log(a) / log(2);\nNotes\nlg2.approx.f32\nimplements a fast approximation to log\n2\n(a).\nInput\nResult\n-Inf\nNaN\n-normal\nNaN\n-0.0\n-Inf\n+0.0\n-Inf\n+Inf\n+Inf\nNaN\nNaN\nThe maximum absolute error is 2\n-22\nwhen the input operand is in the\nrange (0.5, 2). For positive finite inputs outside of this interval, maximum\nrelative error is 2\n-22\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nlg2.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nlg2.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nlg2.f32\ndefaults to\nlg2.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nlg2.approx.ftz.f32  la, a;\n9.7.3.21.\nFloating Point Instructions:\nex2\n\nex2\nFind the base-2 exponential of a value.\nSyntax\nex2.approx{.ftz}.f32  d, a;\nDescription\nRaise 2 to the power\na\n.\nSemantics\nd = 2 ^ a;\nNotes\nex2.approx.f32\nimplements a fast approximation to 2\na\n.\nInput\nResult\n-Inf\n+0.0\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nThe maximum ulp error is 2 ulp from correctly rounded result across the\nfull range of inputs.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nex2.ftz.f32\nflushes subnormal inputs and results to sign-preserving zero.\nsm_1x\nSubnormal inputs and results to sign-preserving zero.\nPTX ISA Notes\nex2.f32\nintroduced in PTX ISA version 1.0. Explicit modifiers\n.approx\nand\n.ftz\nintroduced in PTX ISA version 1.4.\nFor PTX ISA version 1.4 and later, the\n.approx\nmodifier is required.\nFor PTX ISA versions 1.0 through 1.3,\nex2.f32\ndefaults to\nex2.approx.ftz.f32\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nex2.approx.ftz.f32  xa, a;\n9.7.3.22.\nFloating Point Instructions:\ntanh\n\ntanh\nFind the hyperbolic tangent of a value (in radians)\nSyntax\ntanh.approx.f32 d, a;\nDescription\nTake hyperbolic tangent value of\na\n.\nThe operands\nd\nand\na\nare of type\n.f32\n.\nSemantics\nd = tanh(a);\nNotes\ntanh.approx.f32\nimplements a fast approximation to FP32 hyperbolic-tangent.\nResults of\ntanh\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n-1.0\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n1.0\nNaN\nNaN\nThe maximum relative error over the entire floating point\nrange is 2\n-11\n.\nThe subnormal numbers are supported.\nNote\nThe subnormal inputs gets passed through to the output since the value of\ntanh(x)\nfor small\nvalues of\nx\nis approximately the same as\nx\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\ntanh.approx.f32 ta, a;\n9.7.4.\nHalf Precision Floating-Point Instructions\n\nHalf precision floating-point instructions operate on\n.f16\nand\n.f16x2\nregister operands. The\nhalf precision floating-point instructions are:\nadd\nsub\nmul\nfma\nneg\nabs\nmin\nmax\ntanh\nex2\nHalf-precision\nadd\n,\nsub\n,\nmul\n, and\nfma\nsupport saturation of results to the range\n[0.0, 1.0], with\nNaN\ns being flushed to positive zero. Half-precision instructions return an\nunspecified\nNaN\n.\n9.7.4.1.\nHalf Precision Floating Point Instructions:\nadd\n\nadd\nAdd two values.\nSyntax\nadd{.rnd}{.ftz}{.sat}.f16   d, a, b;\nadd{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nadd{.rnd}.bf16   d, a, b;\nadd{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms addition and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then added in parallel to produce\n.f16x2\nor\n.bf16x2\nresult\nin destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a + b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] + fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nadd.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nadd.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nadd{.rnd}.bf16\nand\nadd{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nadd{.rnd}.bf16\nand\nadd{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 additions\nadd.f16        d0, a0, b0;\nadd.rn.f16     d1, a1, b1;\nadd.bf16       bd0, ba0, bb0;\nadd.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 addition\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nadd.f16x2  p3, p1, p2;   // SIMD f16x2 addition\n\n// SIMD bf16 addition\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nadd.bf16x2  p6, p4, p5;       // SIMD bf16x2 addition\n\n// SIMD fp16 addition\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nadd.f16x2       f2, f0, f1;     // SIMD f16x2 addition\n\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nadd.bf16x2      f5, f3, f4;      // SIMD bf16x2 addition\n9.7.4.2.\nHalf Precision Floating Point Instructions:\nsub\n\nsub\nSubtract two values.\nSyntax\nsub{.rnd}{.ftz}{.sat}.f16   d, a, b;\nsub{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nsub{.rnd}.bf16   d, a, b;\nsub{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms subtraction and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then subtracted in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a - b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] - fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that a\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nsub.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nsub.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nsub{.rnd}.bf16\nand\nsub{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nsub{.rnd}.bf16\nand\nsub{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 subtractions\nsub.f16        d0, a0, b0;\nsub.rn.f16     d1, a1, b1;\nsub.bf16       bd0, ba0, bb0;\nsub.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 subtraction\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nsub.f16x2  p3, p1, p2;   // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nsub.bf16x2  p6, p4, p5;       // SIMD bf16x2 subtraction\n\n// SIMD fp16 subtraction\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nsub.f16x2       f2, f0, f1;     // SIMD f16x2 subtraction\n\n// SIMD bf16 subtraction\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nsub.bf16x2      f5, f3, f4;      // SIMD bf16x2 subtraction\n9.7.4.3.\nHalf Precision Floating Point Instructions:\nmul\n\nmul\nMultiply two values.\nSyntax\nmul{.rnd}{.ftz}{.sat}.f16   d, a, b;\nmul{.rnd}{.ftz}{.sat}.f16x2 d, a, b;\n\nmul{.rnd}.bf16   d, a, b;\nmul{.rnd}.bf16x2 d, a, b;\n\n.rnd = { .rn };\nDescription\nPerforms multiplication and writes the resulting value into a destination register.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then multiplied in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\nand\nb\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperands\nd\n,\na\n,\nb\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a * b;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i];\n    }\n}\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\nThe default value of rounding modifier is\n.rn\n. Note that a\nmul\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. A\nmul\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nand\nmul/sub\nsequences with no rounding modifiers may\nbe optimized to use fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmul.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nmul.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nmul{.rnd}.bf16\nand\nmul{.rnd}.bf16x2\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nmul{.rnd}.bf16\nand\nmul{.rnd}.bf16x2\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 multiplications\nmul.f16        d0, a0, b0;\nmul.rn.f16     d1, a1, b1;\nmul.bf16       bd0, ba0, bb0;\nmul.rn.bf16    bd1, ba1, bb1;\n\n// SIMD f16 multiplication\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2\nmul.f16x2  p3, p1, p2;   // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\ncvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2\ncvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2\nmul.bf16x2  p6, p4, p5;       // SIMD bf16x2 multiplication\n\n// SIMD fp16 multiplication\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nmul.f16x2       f2, f0, f1;     // SIMD f16x2 multiplication\n\n// SIMD bf16 multiplication\nld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2\nld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2\nmul.bf16x2      f5, f3, f4;      // SIMD bf16x2 multiplication\n9.7.4.4.\nHalf Precision Floating Point Instructions:\nfma\n\nfma\nFused multiply-add\nSyntax\nfma.rnd{.ftz}{.sat}.f16     d, a, b, c;\nfma.rnd{.ftz}{.sat}.f16x2   d, a, b, c;\nfma.rnd{.ftz}.relu.f16      d, a, b, c;\nfma.rnd{.ftz}.relu.f16x2    d, a, b, c;\nfma.rnd{.relu}.bf16         d, a, b, c;\nfma.rnd{.relu}.bf16x2       d, a, b, c;\nfma.rnd.oob.{relu}.type     d, a, b, c;\n\n.rnd = { .rn };\nDescription\nPerforms a fused multiply-add with no loss of precision in the intermediate product and addition.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vectors by half word values from source\noperands. Half-word operands are then operated in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type, operands\nd\n,\na\n,\nb\nand\nc\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = a * b + c;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    fC[0] = c[0:15];\n    fC[1] = c[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = fA[i] * fB[i] + fC[i];\n    }\n}\nNotes\nRounding modifiers (default is\n.rn\n):\n.rn\nmantissa LSB rounds to nearest even\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nfma.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nSaturation modifier:\nfma.sat.{f16,\nf16x2}\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nfma.relu.{f16,\nf16x2,\nbf16,\nbf16x2}\nclamps the result to 0 if negative.\nNaN\nresult is\nconverted to canonical\nNaN\n.\nOut Of Bounds modifier:\nfma.oob.{f16,\nf16x2,\nbf16,\nbf16x2}\nclamps the result to 0 if either of the operands\nis\nOOB\nNaN\n(defined under\nTensors\n) value. The test for the special\nNaN\nvalue\nand resultant forcing of the result to +0.0 is performed independently for each of the\ntwo SIMD operations.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nfma.relu.{f16,\nf16x2}\nand\nfma{.relu}.{bf16,\nbf16x2}\nintroduced in PTX ISA version 7.0.\nSupport for modifier\n.oob\nintroduced in PTX ISA version 8.1.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nfma.relu.{f16,\nf16x2}\nand\nfma{.relu}.{bf16,\nbf16x2}\nrequire\nsm_80\nor higher.\nfma{.oob}.{f16,\nf16x2,\nbf16,\nbf16x2}\nrequires\nsm_90\nor higher.\nExamples\n// scalar f16 fused multiply-add\nfma.rn.f16         d0, a0, b0, c0;\nfma.rn.f16         d1, a1, b1, c1;\nfma.rn.relu.f16    d1, a1, b1, c1;\nfma.rn.oob.f16      d1, a1, b1, c1;\nfma.rn.oob.relu.f16 d1, a1, b1, c1;\n\n// scalar bf16 fused multiply-add\nfma.rn.bf16        d1, a1, b1, c1;\nfma.rn.relu.bf16   d1, a1, b1, c1;\nfma.rn.oob.bf16       d1, a1, b1, c1;\nfma.rn.oob.relu.bf16  d1, a1, b1, c1;\n\n// SIMD f16 fused multiply-add\ncvt.rn.f16.f32 h0, f0;\ncvt.rn.f16.f32 h1, f1;\ncvt.rn.f16.f32 h2, f2;\ncvt.rn.f16.f32 h3, f3;\nmov.b32  p1, {h0, h1}; // pack two f16 to 32bit f16x2\nmov.b32  p2, {h2, h3}; // pack two f16 to 32bit f16x2\nfma.rn.f16x2  p3, p1, p2, p2;   // SIMD f16x2 fused multiply-add\nfma.rn.relu.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.f16x2 p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier and relu saturation mode\n\n// SIMD fp16 fused multiply-add\nld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2\nld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2\nfma.rn.f16x2    f2, f0, f1, f1; // SIMD f16x2 fused multiply-add\n\n// SIMD bf16 fused multiply-add\nfma.rn.bf16x2       f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add\nfma.rn.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with relu saturation mode\nfma.rn.oob.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier\nfma.rn.oob.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier and relu saturation mode\n9.7.4.5.\nHalf Precision Floating Point Instructions:\nneg\n\nneg\nArithmetic negate.\nSyntax\nneg{.ftz}.f16    d, a;\nneg{.ftz}.f16x2  d, a;\nneg.bf16         d, a;\nneg.bf16x2       d, a;\nDescription\nNegate the sign of\na\nand store the result in\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vector by extracting half word values\nfrom the source operand. Half-word operands are then negated in parallel to produce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.b32\ntype. For\n.bf16\ninstruction\ntype, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction type, operands\nd\nand\na\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = -a;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = -fA[i];\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nneg.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nneg.bf16\nand\nneg.bf16x2\nintroduced in PTX ISA 7.0.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nneg.bf16\nand\nneg.bf16x2\nrequires architecture\nsm_80\nor higher.\nExamples\nneg.ftz.f16  x,f0;\nneg.bf16     x,b0;\nneg.bf16x2   x1,b1;\n9.7.4.6.\nHalf Precision Floating Point Instructions:\nabs\n\nabs\nAbsolute value\nSyntax\nabs{.ftz}.f16    d, a;\nabs{.ftz}.f16x2  d, a;\nabs.bf16         d, a;\nabs.bf16x2       d, a;\nDescription\nTake absolute value of\na\nand store the result in\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, forms input vector by extracting half word values\nfrom the source operand. Absolute values of half-word operands are then computed in parallel to\nproduce\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nSemantics\nif (type == f16 || type == bf16) {\n    d = |a|;\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    for (i = 0; i < 2; i++) {\n         d[i] = |fA[i]|;\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nabs.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nNaN\ninputs yield an unspecified\nNaN\n. Future implementations may comply with the IEEE 754\nstandard by preserving payload and modifying only the sign bit.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nabs.bf16\nand\nabs.bf16x2\nintroduced in PTX ISA 7.0.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nabs.bf16\nand\nabs.bf16x2\nrequires architecture\nsm_80\nor higher.\nExamples\nabs.ftz.f16  x,f0;\nabs.bf16     x,b0;\nabs.bf16x2   x1,b1;\n9.7.4.7.\nHalf Precision Floating Point Instructions:\nmin\n\nmin\nFind the minimum of two values.\nSyntax\nmin{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmin{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmin{.NaN}{.xorsign.abs}.bf16x2         d, a, b;\nDescription\nStore the minimum of\na\nand\nb\nin\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nIf\n.NaN\nmodifier is specified, then the result is canonical\nNaN\nif either of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the minimum of\nabsolute values of both the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both the inputs.\nModifiers\n.abs\nand\n.xorsign\nmust be specified together and\n.xorsign\nconsiders the sign\nbit of both inputs before applying\n.abs\noperation.\nIf the result of\nmin\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\nif (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a < b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n               fA[i] = |fA[i]|;\n               fB[i] = |fB[i]|;\n           }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] < fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(d[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmin.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nmin.xorsign\nintroduced in PTX ISA version 7.2.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nmin.xorsign.abs\nsupport requires\nsm_86\nor higher.\nExamples\nmin.ftz.f16       h0,h1,h2;\nmin.f16x2         b0,b1,b2;\n// SIMD fp16 min with .NaN\nmin.NaN.f16x2     b0,b1,b2;\nmin.bf16          h0, h1, h2;\n// SIMD bf16 min with NaN\nmin.NaN.bf16x2    b0, b1, b2;\n// scalar bf16 min with xorsign.abs\nmin.xorsign.abs.bf16 Rd, Ra, Rb\n9.7.4.8.\nHalf Precision Floating Point Instructions:\nmax\n\nmax\nFind the maximum of two values.\nSyntax\nmax{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;\nmax{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16           d, a, b;\nmax{.NaN}{.xorsign.abs}.bf16x2         d, a, b;\nDescription\nStore the maximum of\na\nand\nb\nin\nd\n.\nFor\n.f16x2\nand\n.bf16x2\ninstruction types, input vectors are formed with half-word values\nfrom source operands. Half-word operands are then processed in parallel to store\n.f16x2\nor\n.bf16x2\nresult in destination.\nFor\n.f16\ninstruction type, operands\nd\nand\na\nhave\n.f16\nor\n.b16\ntype. For\n.f16x2\ninstruction type, operands\nd\nand\na\nhave\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operands\nd\nand\na\nhave\n.b16\ntype. For\n.bf16x2\ninstruction\ntype, operands\nd\nand\na\nhave\n.b32\ntype.\nIf\n.NaN\nmodifier is specified, the result is canonical\nNaN\nif either of the inputs is\nNaN\n.\nIf\n.abs\nmodifier is specified, the magnitude of destination operand\nd\nis the maximum of\nabsolute values of both the input arguments.\nIf\n.xorsign\nmodifier is specified, the sign bit of destination\nd\nis equal to the XOR of the\nsign bits of both the inputs.\nModifiers\n.abs\nand\n.xorsign\nmust be specified together and\n.xorsign\nconsiders the sign\nbit of both inputs before applying\n.abs\noperation.\nIf the result of\nmax\nis\nNaN\nthen the\n.xorsign\nand\n.abs\nmodifiers will be ignored.\nSemantics\nif (type == f16 || type == bf16) {\n    if (.xorsign) {\n        xorsign = getSignBit(a) ^ getSignBit(b);\n        if (.abs) {\n            a = |a|;\n            b = |b|;\n        }\n    }\n    if (isNaN(a) && isNaN(b))              d = NaN;\n    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;\n    else if (isNaN(a))                     d = b;\n    else if (isNaN(b))                     d = a;\n    else                                   d = (a > b) ? a : b;\n    if (.xorsign && !isNaN(d)) {\n         setSignBit(d, xorsign);\n    }\n} else if (type == f16x2 || type == bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    for (i = 0; i < 2; i++) {\n        if (.xorsign) {\n            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);\n            if (.abs) {\n                fA[i] = |fA[i]|;\n                fB[i] = |fB[i]|;\n            }\n        }\n        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;\n        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;\n        else if (isNaN(fA[i]))                         d[i] = fB[i];\n        else if (isNaN(fB[i]))                         d[i] = fA[i];\n        else                                           d[i] = (fA[i] > fB[i]) ? fA[i] : fB[i];\n        if (.xorsign && !isNaN(fA[i])) {\n            setSignBit(d[i], xorsign);\n        }\n    }\n}\nNotes\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nmax.ftz.{f16,\nf16x2}\nflushes subnormal inputs and results to sign-preserving zero.\nIf values of both inputs are 0.0, then +0.0 > -0.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nmax.xorsign.abs\nintroduced in PTX ISA version 7.2.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nmax.xorsign.abs\nsupport requires\nsm_86\nor higher.\nExamples\nmax.ftz.f16       h0,h1,h2;\nmax.f16x2         b0,b1,b2;\n// SIMD fp16 max with NaN\nmax.NaN.f16x2     b0,b1,b2;\n// scalar f16 max with xorsign.abs\nmax.xorsign.abs.f16 Rd, Ra, Rb;\nmax.bf16          h0, h1, h2;\n// scalar bf16 max and NaN\nmax.NaN.bf16x2    b0, b1, b2;\n// SIMD bf16 max with xorsign.abs\nmax.xorsign.abs.bf16x2 Rd, Ra, Rb;\n9.7.4.9.\nHalf Precision Floating Point Instructions:\ntanh\n\ntanh\nFind the hyperbolic tangent of a value (in radians)\nSyntax\ntanh.approx.type d, a;\n\n.type = {.f16, .f16x2, .bf16, .bf16x2}\nDescription\nTake hyperbolic tangent value of\na\n.\nThe type of operands\nd\nand\na\nare as specified by\n.type\n.\nFor\n.f16x2\nor\n.bf16x2\ninstruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a\n.f16x2\nor\n.bf16x2\n.\nSemantics\nif (.type == .f16 || .type == .bf16) {\n  d = tanh(a)\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = tanh(fA[0])\n  d[1] = tanh(fA[1])\n}\nNotes\ntanh.approx.{f16,\nf16x2,\nbf16,\nbf16x2}\nimplements an approximate hyperbolic tangent in the\ntarget format.\nResults of\ntanh\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n-1.0\n-0.0\n-0.0\n+0.0\n+0.0\n+Inf\n1.0\nNaN\nNaN\nThe maximum absolute error for\n.f16\ntype is 2-10.987. The maximum absolute error for\n.bf16\ntype is 2-8.\nThe subnormal numbers are supported.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\ntanh.approx.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\ntanh.approx.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\ntanh.approx.f16    h1, h0;\ntanh.approx.f16x2  hd1, hd0;\ntanh.approx.bf16   b1, b0;\ntanh.approx.bf16x2 hb1, hb0;\n9.7.4.10.\nHalf Precision Floating Point Instructions:\nex2\n\nex2\nFind the base-2 exponent of input.\nSyntax\nex2.approx.atype     d, a;\nex2.approx.ftz.btype d, a;\n\n.atype = { .f16,  .f16x2}\n.btype = { .bf16, .bf16x2}\nDescription\nRaise 2 to the power\na\n.\nThe type of operands\nd\nand\na\nare as specified by\n.type\n.\nFor\n.f16x2\nor\n.bf16x2\ninstruction type, each of the half-word operands are operated in\nparallel and the results are packed appropriately into a\n.f16x2\nor\n.bf16x2\n.\nSemantics\nif (.type == .f16 || .type == .bf16) {\n  d = 2 ^ a\n} else if (.type == .f16x2 || .type == .bf16x2) {\n  fA[0] = a[0:15];\n  fA[1] = a[16:31];\n  d[0] = 2 ^ fA[0]\n  d[1] = 2 ^ fA[1]\n}\nNotes\nex2.approx.{f16,\nf16x2,\nbf16,\nbf16x2}\nimplement a fast approximation to 2\na\n.\nFor the\n.f16\ntype, subnormal inputs are supported.\nex2.approx.ftz.bf16\nflushes subnormal\ninputs and results to sign-preserving zero.\nResults of\nex2.approx.ftz.bf16\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n+0.0\n-subnormal\n+1.0\n-0.0\n+1.0\n+0.0\n+1.0\n+subnormal\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nResults of\nex2.approx.f16\nfor various corner-case inputs are as follows:\nInput\nResult\n-Inf\n+0.0\n-0.0\n+1.0\n+0.0\n+1.0\n+Inf\n+Inf\nNaN\nNaN\nThe maximum relative error for\n.f16\ntype is 2-9.9. The maximum relative error for\n.bf16\ntype\nis 2-7.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nex2.approx.ftz.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nex2.approx.ftz.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\nex2.approx.f16         h1, h0;\nex2.approx.f16x2       hd1, hd0;\nex2.approx.ftz.bf16    b1, b2;\nex2.approx.ftz.bf16x2  hb1, hb2;\n9.7.5.\nMixed Precision Floating-Point Instructions\n\nMixed precision floating-point instructions operate on data with varied floating point precision.\nBefore executing the specified operation, operands with different precision needs to be converted\nsuch that all the instruction operands can be represented with a consistent floating-point precision.\nThe register variable to be used for holding a particular operand depends upon the combination of\nthe instruction types. Refer\nFundamental Types\nand\nAlternate Floating-Point Data Formats\nfor more details\naround exact register operand to be used for a given data type.\nThe mixed precision floating point instructions are:\nadd\nsub\nfma\nMixed precision\nadd\n,\nsub\n,\nfma\nsupport saturation of results to the range [0.0, 1.0],\nwith\nNaN\nbeing flushed to positive zero.\n9.7.5.1.\nMixed Precision Floating Point Instructions:\nadd\n\nadd\nAdd 2 values.\nSyntax\nadd{.rnd}{.sat}.f32.atype  d, a, c;\n\n.atype = { .f16, .bf16};\n.rnd   = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operand\na\nfrom\n.atype\ninto\n.f32\ntype. The converted value is then\nused for the addition. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) + c;\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nadd\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nadd\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nadd\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nadd.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nadd.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nadd.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .b16 ba;\nadd.rz.f32.bf16.sat   fd, fa, fc;\n9.7.5.2.\nMixed Precision Floating Point Instructions:\nsub\n\nsub\nSubtract one value from another.\nSyntax\nsub{.rnd}{.sat}.f32.atype  d, a, c;\n\n.atype = { .f16, .bf16};\n.rnd   = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operand\na\nfrom\n.atype\ninto\n.f32\ntype. The converted value is then\nused for the subtraction. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) - c;\nNotes\nRounding modifiers:\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe default value of rounding modifier is\n.rn\n. Note that an\nsub\ninstruction with an explicit\nrounding modifier is treated conservatively by the code optimizer. An\nsub\ninstruction with no\nrounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code\noptimizer. In particular,\nmul\n/\nsub\nsequences with no rounding modifiers may be optimized to\nuse fused-multiply-add instructions on the target device.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nsub.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nsub.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nsub.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .f16 ha;\nsub.rz.f32.f16.sat   fd, ha, fc;\n9.7.5.3.\nMixed Precision Floating Point Instructions:\nfma\n\nfma\nFused multiply-add.\nSyntax\nfma.rnd{.sat}.f32.abtype  d, a, b, c;\n\n.abtype = { .f16, .bf16};\n.rnd    = { .rn, .rz, .rm, .rp };\nDescription\nConverts input operands\na\nand\nb\nfrom\n.atype\ninto\n.f32\ntype. The converted values\nare then used to perform fused multiply-add operation with no loss of precision in the intermediate\nproduct and addition. The resulting value is stored in the destination operand\nd\n.\nSemantics\nd = convert(a) * convert(b) + c;\nNotes\nfma.f32.{f16/bf16}\ncomputes the product of\na\nand\nb\nto infinite precision and then adds\nc\nto this product, again in infinite precision. The resulting value is then rounded to single\nprecision using the rounding mode specified by\n.rnd\n.\nRounding modifiers(no default):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nSaturation modifier:\nfma.sat\nclamps the result to [0.0, 1.0].\nNaN\nresults are flushed to\n+0.0f\n.\nPTX ISA Notes\nfma.f32.{f16/bf16}\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nfma.f32.{f16/bf16}\nrequires\nsm_100\nor higher.\nExamples\n.reg .f32 fc, fd;\n.reg .f16 ha, hb;\nfma.rz.sat.f32.f16.sat   fd, ha, hb, fc;\n9.7.6.\nComparison and Selection Instructions\n\nThe comparison select instructions are:\nset\nsetp\nselp\nslct\nAs with single-precision floating-point instructions, the\nset\n,\nsetp\n, and\nslct\ninstructions support subnormal numbers for\nsm_20\nand higher targets and flush single-precision\nsubnormal inputs to sign-preserving zero for\nsm_1x\ntargets. The optional\n.ftz\nmodifier\nprovides backward compatibility with\nsm_1x\ntargets by flushing subnormal inputs and results to\nsign-preserving zero regardless of the target architecture.\n9.7.6.1.\nComparison and Selection Instructions:\nset\n\nset\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nset.CmpOp{.ftz}.dtype.stype         d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.stype  d, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.dtype  = { .u32, .s32, .f32 };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 };\nDescription\nCompares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator. If this result is\nTrue\n,\n1.0f\nis written for floating-point\ndestination types, and\n0xffffffff\nis written for integer destination types. Otherwise,\n0x00000000\nis written.\nOperand\nd\nhas type\n.dtype\n; operands\na\nand\nb\nhave type\n.stype\n; operand\nc\nhas\ntype\n.pred\n.\nSemantics\nt = (a CmpOp b) ? 1 : 0;\nif (isFloat(dtype))\n    d = BoolOp(t, c) ? 1.0f : 0x00000000;\nelse\n    d = BoolOp(t, c) ? 0xffffffff : 0x00000000;\nInteger Notes\nThe signed and unsigned comparison operators are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n.\nFor unsigned values, the comparison operators\nlo\n,\nls\n,\nhi\n, and\nhs\nfor lower,\nlower-or-same, higher, and higher-or-same may be used instead of\nlt\n,\nle\n,\ngt\n,\nge\n,\nrespectively.\nThe untyped, bit-size comparisons are\neq\nand\nne\n.\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nset.ftz.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nsm_1x\nset.dtype.f64\nsupports subnormal numbers.\nset.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nset\nwith\n.f64\nsource type requires\nsm_13\nor higher.\nExamples\n@p  set.lt.and.f32.s32  d,a,b,r;\n    set.eq.u32.u32      d,i,n;\n9.7.6.2.\nComparison and Selection Instructions:\nsetp\n\nsetp\nCompare two numeric values with a relational operator, and (optionally) combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nsetp.CmpOp{.ftz}.type         p[|q], a, b;\nsetp.CmpOp.BoolOp{.ftz}.type  p[|q], a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.type   = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n                  .f32, .f64 };\nDescription\nCompares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the first destination operand. A related value computed using\nthe complement of the compare result is written to the second destination operand.\nApplies to all numeric types. Operands\na\nand\nb\nhave type\n.type\n; operands\np\n,\nq\n,\nand\nc\nhave type\n.pred\n. The sink symbol ‘_’ may be used in place of any one of the\ndestination operands.\nSemantics\nt = (a CmpOp b) ? 1 : 0;\np = BoolOp(t, c);\nq = BoolOp(!t, c);\nInteger Notes\nThe signed and unsigned comparison operators are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n.\nFor unsigned values, the comparison operators\nlo\n,\nls\n,\nhi\n, and\nhs\nfor lower,\nlower-or-same, higher, and higher-or-same may be used instead of\nlt\n,\nle\n,\ngt\n,\nge\n,\nrespectively.\nThe untyped, bit-size comparisons are\neq\nand\nne\n.\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nsetp.ftz.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nsm_1x\nsetp.dtype.f64\nsupports subnormal numbers.\nsetp.dtype.f32\nflushes subnormal inputs to sign-preserving zero.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nsetp\nwith\n.f64\nsource type requires\nsm_13\nor higher.\nExamples\nsetp.lt.and.s32  p|q,a,b,r;\n@q  setp.eq.u32      p,i,n;\n9.7.6.3.\nComparison and Selection Instructions:\nselp\n\nselp\nSelect between source operands, based on the value of the predicate source operand.\nSyntax\nselp.type d, a, b, c;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 };\nDescription\nConditional selection. If\nc\nis\nTrue\n,\na\nis stored in\nd\n,\nb\notherwise. Operands\nd\n,\na\n, and\nb\nmust be of the same type. Operand\nc\nis a predicate.\nSemantics\nd = (c == 1) ? a : b;\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nselp.f64\nrequires\nsm_13\nor higher.\nExamples\nselp.s32  r0,r,g,p;\n@q  selp.f32  f0,t,x,xp;\n9.7.6.4.\nComparison and Selection Instructions:\nslct\n\nslct\nSelect one source operand, based on the sign of the third operand.\nSyntax\nslct.dtype.s32        d, a, b, c;\nslct{.ftz}.dtype.f32  d, a, b, c;\n\n.dtype = { .b16, .b32, .b64,\n           .u16, .u32, .u64,\n           .s16, .s32, .s64,\n                 .f32, .f64 };\nDescription\nConditional selection. If\nc\n>= 0,\na\nis stored in\nd\n, otherwise\nb\nis stored in\nd\n. Operands\nd\n,\na\n, and\nb\nare treated as a bitsize type of the same width as the first\ninstruction type; operand\nc\nmust match the second instruction type (\n.s32\nor\n.f32\n). The\nselected input is copied to the output without modification.\nSemantics\nd = (c >= 0) ? a : b;\nFloating Point Notes\nFor\n.f32\ncomparisons, negative zero equals zero.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nslct.ftz.dtype.f32\nflushes subnormal values of operand\nc\nto sign-preserving zero, and\noperand\na\nis selected.\nsm_1x\nslct.dtype.f32\nflushes subnormal values of operand\nc\nto sign-preserving zero, and operand\na\nis selected.\nModifier\n.ftz\napplies only to\n.f32\ncomparisons.\nIf operand\nc\nis\nNaN\n, the comparison is ordered and operand\nb\nis selected.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nslct.f64\nrequires\nsm_13\nor higher.\nExamples\nslct.u32.s32  x, y, z, val;\nslct.ftz.u64.f32  A, B, C, fval;\n9.7.7.\nHalf Precision Comparison Instructions\n\nThe comparison instructions are:\nset\nsetp\n9.7.7.1.\nHalf Precision Comparison Instructions:\nset\n\nset\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nset.CmpOp{.ftz}.f16.stype            d, a, b;\nset.CmpOp.BoolOp{.ftz}.f16.stype     d, a, b, {!}c;\n\nset.CmpOp.bf16.stype                 d, a, b;\nset.CmpOp.BoolOp.bf16.stype          d, a, b, {!}c;\n\nset.CmpOp{.ftz}.dtype.f16            d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16     d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp.dtype.bf16                 d, a, b;\nset.CmpOp.BoolOp.dtype.bf16          d, a, b, {!}c;\n.dtype  = { .u16, .s16, .u32, .s32}\n\nset.CmpOp{.ftz}.dtype.f16x2          d, a, b;\nset.CmpOp.BoolOp{.ftz}.dtype.f16x2   d, a, b, {!}c;\n.dtype  = { .f16x2, .u32, .s32}\n\nset.CmpOp.dtype.bf16x2               d, a, b;\nset.CmpOp.BoolOp.dtype.bf16x2        d, a, b, {!}c;\n.dtype  = { .bf16x2, .u32, .s32}\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\n.stype  = { .b16, .b32, .b64,\n            .u16, .u32, .u64,\n            .s16, .s32, .s64,\n            .f16, .f32, .f64};\nDescription\nCompares two numeric values and optionally combines the result with another predicate value by\napplying a Boolean operator.\nResult of this computation is written in destination register in the following way:\nIf result is\nTrue\n,\n0xffffffff\nis written for destination types\n.u32\n/\n.s32\n.\n0xffff\nis written for destination types\n.u16\n/\n.s16\n.\n1.0\nin target precision floating point format is written for destination type\n.f16\n,\n.bf16\n.\nIf result is\nFalse\n,\n0x0\nis written for all integer destination types.\n0.0\nin target precision floating point format is written for destination type\n.f16\n,\n.bf16\n.\nIf the source type is\n.f16x2\nor\n.bf16x2\nthen result of individual operations are packed in\nthe 32-bit destination operand.\nOperand\nc\nhas type\n.pred\n.\nSemantics\nif (stype == .f16x2 || stype == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0]   = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1]   = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    if (dtype == .f16x2 || stype == .bf16x2) {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 1.0 : 0.0;\n        }\n    } else {\n        for (i = 0; i < 2; i++) {\n            d[i] = BoolOp(t[i], c) ? 0xffff : 0;\n        }\n    }\n} else if (dtype == .f16 || stype == .bf16) {\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? 1.0 : 0.0;\n} else  { // Integer destination type\n    trueVal = (isU16(dtype) || isS16(dtype)) ?  0xffff : 0xffffffff;\n    t = (a CmpOp b) ? 1 : 0;\n    d = BoolOp(t, c) ? trueVal : 0;\n}\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nWhen\n.ftz\nmodifier is specified then subnormal inputs and results are flushed to sign\npreserving zero.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nset.{u16,\nu32,\ns16,\ns32}.f16\nand\nset.{u32,\ns32}.f16x2\nare introduced in PTX ISA version 6.5.\nset.{u16,\nu32,\ns16,\ns32}.bf16\n,\nset.{u32,\ns32,\nbf16x2}.bf16x2\n,\nset.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64}\nare introduced in PTX ISA version\n7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nset.{u16,\nu32,\ns16,\ns32}.bf16\n,\nset.{u32,\ns32,\nbf16x2}.bf16x2\n,\nset.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64}\nrequire\nsm_90\nor higher.\nExamples\nset.lt.and.f16.f16  d,a,b,r;\nset.eq.f16x2.f16x2  d,i,n;\nset.eq.u32.f16x2    d,i,n;\nset.lt.and.u16.f16  d,a,b,r;\nset.ltu.or.bf16.f16    d,u,v,s;\nset.equ.bf16x2.bf16x2  d,j,m;\nset.geu.s32.bf16x2     d,j,m;\nset.num.xor.s32.bf16   d,u,v,s;\n9.7.7.2.\nHalf Precision Comparison Instructions:\nsetp\n\nsetp\nCompare two numeric values with a relational operator, and optionally combine this result with a\npredicate value by applying a Boolean operator.\nSyntax\nsetp.CmpOp{.ftz}.f16           p, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16    p, a, b, {!}c;\n\nsetp.CmpOp{.ftz}.f16x2         p|q, a, b;\nsetp.CmpOp.BoolOp{.ftz}.f16x2  p|q, a, b, {!}c;\n\nsetp.CmpOp.bf16                p, a, b;\nsetp.CmpOp.BoolOp.bf16         p, a, b, {!}c;\n\nsetp.CmpOp.bf16x2              p|q, a, b;\nsetp.CmpOp.BoolOp.bf16x2       p|q, a, b, {!}c;\n\n.CmpOp  = { eq, ne, lt, le, gt, ge,\n            equ, neu, ltu, leu, gtu, geu, num, nan };\n.BoolOp = { and, or, xor };\nDescription\nCompares two values and combines the result with another predicate value by applying a Boolean\noperator. This result is written to the destination operand.\nOperand\nc\n,\np\nand\nq\nhas type\n.pred\n.\nFor instruction type\n.f16\n, operands\na\nand\nb\nhave type\n.b16\nor\n.f16\n.\nFor instruction type\n.f16x2\n, operands\na\nand\nb\nhave type\n.b32\n.\nFor instruction type\n.bf16\n, operands\na\nand\nb\nhave type\n.b16\n.\nFor instruction type\n.bf16x2\n, operands\na\nand\nb\nhave type\n.b32\n.\nSemantics\nif (type == .f16 || type == .bf16) {\n     t = (a CmpOp b) ? 1 : 0;\n     p = BoolOp(t, c);\n} else if (type == .f16x2 || type == .bf16x2) {\n    fA[0] = a[0:15];\n    fA[1] = a[16:31];\n    fB[0] = b[0:15];\n    fB[1] = b[16:31];\n    t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0;\n    t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0;\n    p = BoolOp(t[0], c);\n    q = BoolOp(t[1], c);\n}\nFloating Point Notes\nThe ordered comparisons are\neq\n,\nne\n,\nlt\n,\nle\n,\ngt\n,\nge\n. If either operand is\nNaN\n, the result is\nFalse\n.\nTo aid comparison operations in the presence of\nNaN\nvalues, unordered versions are included:\nequ\n,\nneu\n,\nltu\n,\nleu\n,\ngtu\n,\ngeu\n. If both operands are numeric values (not\nNaN\n), then these comparisons have the same result as their ordered counterparts. If either\noperand is\nNaN\n, then the result of these comparisons is\nTrue\n.\nnum\nreturns\nTrue\nif both operands are numeric values (not\nNaN\n), and\nnan\nreturns\nTrue\nif either operand is\nNaN\n.\nSubnormal numbers:\nBy default, subnormal numbers are supported.\nsetp.ftz.{f16,f16x2}\nflushes subnormal inputs to sign-preserving zero.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.2.\nsetp.{bf16/bf16x2}\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_53\nor higher.\nsetp.{bf16/bf16x2}\nrequires\nsm_90\nor higher.\nExamples\nsetp.lt.and.f16x2  p|q,a,b,r;\n@q  setp.eq.f16    p,i,n;\n\nsetp.gt.or.bf16x2  u|v,c,d,s;\n@q  setp.eq.bf16   u,j,m;\n9.7.8.\nLogic and Shift Instructions\n\nThe logic and shift instructions are fundamentally untyped, performing bit-wise operations on\noperands of any type, provided the operands are of the same size. This permits bit-wise operations\non floating point values without having to define a union to access the bits. Instructions\nand\n,\nor\n,\nxor\n, and\nnot\nalso operate on predicates.\nThe logical shift instructions are:\nand\nor\nxor\nnot\ncnot\nlop3\nshf\nshl\nshr\n9.7.8.1.\nLogic and Shift Instructions:\nand\n\nand\nBitwise AND.\nSyntax\nand.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise and operation for the bits in\na\nand\nb\n.\nSemantics\nd = a & b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nand.b32  x,q,r;\nand.b32  sign,fpvalue,0x80000000;\n9.7.8.2.\nLogic and Shift Instructions:\nor\n\nor\nBiwise OR.\nSyntax\nor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise or operation for the bits in\na\nand\nb\n.\nSemantics\nd = a | b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nor.b32  mask mask,0x00010001\nor.pred  p,q,r;\n9.7.8.3.\nLogic and Shift Instructions:\nxor\n\nxor\nBitwise exclusive-OR (inequality).\nSyntax\nxor.type d, a, b;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nCompute the bit-wise exclusive-or operation for the bits in\na\nand\nb\n.\nSemantics\nd = a ^ b;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicate registers.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nxor.b32  d,q,r;\nxor.b16  d,x,0x0001;\n9.7.8.4.\nLogic and Shift Instructions:\nnot\n\nnot\nBitwise negation; one’s complement.\nSyntax\nnot.type d, a;\n\n.type = { .pred, .b16, .b32, .b64 };\nDescription\nInvert the bits in\na\n.\nSemantics\nd = ~a;\nNotes\nThe size of the operands must match, but not necessarily the type.\nAllowed types include predicates.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nnot.b32  mask,mask;\nnot.pred  p,q;\n9.7.8.5.\nLogic and Shift Instructions:\ncnot\n\ncnot\nC/C++ style logical negation.\nSyntax\ncnot.type d, a;\n\n.type = { .b16, .b32, .b64 };\nDescription\nCompute the logical negation using C/C++ semantics.\nSemantics\nd = (a==0) ? 1 : 0;\nNotes\nThe size of the operands must match, but not necessarily the type.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ncnot.b32 d,a;\n9.7.8.6.\nLogic and Shift Instructions:\nlop3\n\nlop3\nArbitrary logical operation on 3 inputs.\nSyntax\nlop3.b32 d, a, b, c, immLut;\nlop3.BoolOp.b32 d|p, a, b, c, immLut, q;\n\n.BoolOp   = { .or , .and };\nDescription\nCompute bitwise logical operation on inputs\na\n,\nb\n,\nc\nand store the result in destination\nd\n.\nOptionally,\n.BoolOp\ncan be specified to compute the predicate result\np\nby performing a\nBoolean operation on the destination operand\nd\nwith the predicate\nq\nin the following manner:\np = (d != 0) BoolOp q;\nThe sink symbol ‘_’ may be used in place of the destination operand\nd\nwhen\n.BoolOp\nqualifier\nis specified.\nThe logical operation is defined by a look-up table which, for 3 inputs, can be represented as an\n8-bit value specified by operand\nimmLut\nas described below.\nimmLut\nis an integer constant\nthat can take values from 0 to 255, thereby allowing up to 256 distinct logical operations on inputs\na\n,\nb\n,\nc\n.\nFor a logical operation\nF(a,\nb,\nc)\nthe value of\nimmLut\ncan be computed by applying the same\noperation to three predefined constant values as follows:\nta = 0xF0;\ntb = 0xCC;\ntc = 0xAA;\n\nimmLut = F(ta, tb, tc);\nExamples:\nIf F = (a & b & c);\nimmLut = 0xF0 & 0xCC & 0xAA = 0x80\n\nIf F = (a | b | c);\nimmLut = 0xF0 | 0xCC | 0xAA = 0xFE\n\nIf F = (a & b & ~c);\nimmLut = 0xF0 & 0xCC & (~0xAA) = 0x40\n\nIf F = ((a & b | c) ^ a);\nimmLut = (0xF0 & 0xCC | 0xAA) ^ 0xF0 = 0x1A\nThe following table illustrates computation of\nimmLut\nfor various logical operations:\nta\ntb\ntc\nOper 0 (False)\nOper 1 (ta & tb & tc)\nOper 2 (ta & tb & ~tc)\n…\nOper 254 (ta | tb | tc)\nOper 255 (True)\n0\n0\n0\n0\n0\n0\n…\n0\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n0\n1\n1\nimmLut\n0x0\n0x80\n0x40\n…\n0xFE\n0xFF\nSemantics\nF = GetFunctionFromTable(immLut); // returns the function corresponding to immLut value\nd = F(a, b, c);\nif (BoolOp specified) {\n    p = (d != 0) BoolOp q;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 4.3.\nSupport for\n.BoolOp\nqualifier introduced in PTX ISA version 8.2.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nQualifier\n.BoolOp\nrequires\nsm_70\nor higher.\nExamples\nlop3.b32       d, a, b, c, 0x40;\nlop3.or.b32  d|p, a, b, c, 0x3f, q;\nlop3.and.b32 _|p, a, b, c, 0x3f, q;\n9.7.8.7.\nLogic and Shift Instructions:\nshf\n\nshf\nFunnel shift.\nSyntax\nshf.l.mode.b32  d, a, b, c;  // left shift\nshf.r.mode.b32  d, a, b, c;  // right shift\n\n.mode = { .clamp, .wrap };\nDescription\nShift the 64-bit value formed by concatenating operands\na\nand\nb\nleft or right by the amount\nspecified by the unsigned 32-bit value in\nc\n. Operand\nb\nholds bits\n63:32\nand operand a\nholds bits\n31:0\nof the 64-bit source value. The source is shifted left or right by the clamped\nor wrapped value in\nc\n. For\nshf.l\n, the most-significant 32-bits of the result are written\ninto\nd\n; for\nshf.r\n, the least-significant 32-bits of the result are written into\nd\n.\nSemantics\nu32  n = (.mode == .clamp) ? min(c, 32) : c & 0x1f;\nswitch (shf.dir) {  // shift concatenation of [b, a]\n    case shf.l:     // extract 32 msbs\n           u32  d = (b << n)      | (a >> (32-n));\n    case shf.r:     // extract 32 lsbs\n           u32  d = (b << (32-n)) | (a >> n);\n}\nNotes\nUse funnel shift for multi-word shift operations and for rotate operations. The shift amount is\nlimited to the range\n0..32\nin clamp mode and\n0..31\nin wrap mode, so shifting multi-word\nvalues by distances greater than 32 requires first moving 32-bit words, then using\nshf\nto shift\nthe remaining\n0..31\ndistance.\nTo shift data sizes greater than 64 bits to the right, use repeated\nshf.r\ninstructions applied\nto adjacent words, operating from least-significant word towards most-significant word. At each\nstep, a single word of the shifted result is computed. The most-significant word of the result is\ncomputed using a\nshr.{u32,s32}\ninstruction, which zero or sign fills based on the instruction\ntype.\nTo shift data sizes greater than 64 bits to the left, use repeated\nshf.l\ninstructions applied to\nadjacent words, operating from most-significant word towards least-significant word. At each step, a\nsingle word of the shifted result is computed. The least-significant word of the result is computed\nusing a\nshl\ninstruction.\nUse funnel shift to perform 32-bit left or right rotate by supplying the same value for source\narguments\na\nand\nb\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nRequires\nsm_32\nor higher.\nExample\nshf.l.clamp.b32  r3,r1,r0,16;\n\n// 128-bit left shift; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] << n\nshf.l.clamp.b32  r7,r2,r3,n;\nshf.l.clamp.b32  r6,r1,r2,n;\nshf.l.clamp.b32  r5,r0,r1,n;\nshl.b32          r4,r0,n;\n\n// 128-bit right shift, arithmetic; n < 32\n// [r7,r6,r5,r4] = [r3,r2,r1,r0] >> n\nshf.r.clamp.b32  r4,r0,r1,n;\nshf.r.clamp.b32  r5,r1,r2,n;\nshf.r.clamp.b32  r6,r2,r3,n;\nshr.s32          r7,r3,n;     // result is sign-extended\n\nshf.r.clamp.b32  r1,r0,r0,n;  // rotate right by n; n < 32\nshf.l.clamp.b32  r1,r0,r0,n;  // rotate left by n; n < 32\n\n// extract 32-bits from [r1,r0] starting at position n < 32\nshf.r.clamp.b32  r0,r0,r1,n;\n9.7.8.8.\nLogic and Shift Instructions:\nshl\n\nshl\nShift bits left, zero-fill on right.\nSyntax\nshl.type d, a, b;\n\n.type = { .b16, .b32, .b64 };\nDescription\nShift\na\nleft by the amount specified by unsigned 32-bit value in\nb\n.\nSemantics\nd = a << b;\nNotes\nShift amounts greater than the register width\nN\nare clamped to\nN\n.\nThe sizes of the destination and first source operand must match, but not necessarily the type. The\nb\noperand must be a 32-bit value, regardless of the instruction type.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExample\nshl.b32  q,a,2;\n9.7.8.9.\nLogic and Shift Instructions:\nshr\n\nshr\nShift bits right, sign or zero-fill on left.\nSyntax\nshr.type d, a, b;\n\n.type = { .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64 };\nDescription\nShift\na\nright by the amount specified by unsigned 32-bit value in\nb\n. Signed shifts fill with\nthe sign bit, unsigned and untyped shifts fill with\n0\n.\nSemantics\nd = a >> b;\nNotes\nShift amounts greater than the register width\nN\nare clamped to\nN\n.\nThe sizes of the destination and first source operand must match, but not necessarily the type. The\nb\noperand must be a 32-bit value, regardless of the instruction type.\nBit-size types are included for symmetry with\nshl\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExample\nshr.u16  c,a,2;\nshr.s32  i,i,1;\nshr.b16  k,i,j;\n9.7.9.\nData Movement and Conversion Instructions\n\nThese instructions copy data from place to place, and from state space to state space, possibly\nconverting it from one format to another.\nmov\n,\nld\n,\nldu\n, and\nst\noperate on both\nscalar and vector types. The\nisspacep\ninstruction is provided to query whether a generic address\nfalls within a particular state space window. The\ncvta\ninstruction converts addresses between\ngeneric\nand\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate spaces.\nInstructions\nld\n,\nst\n,\nsuld\n, and\nsust\nsupport optional cache operations.\nThe Data Movement and Conversion Instructions are:\nmov\nshfl.sync\nprmt\nld\nldu\nst\nst.async\nst.bulk\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nprefetch\n,\nprefetchu\nisspacep\ncvta\ncvt\ncvt.pack\ncp.async\ncp.async.commit_group\ncp.async.wait_group\n,\ncp.async.wait_all\ncp.async.bulk\ncp.reduce.async.bulk\ncp.async.bulk.prefetch\nmultimem.cp.async.bulk\nmultimem.cp.reduce.async.bulk\ncp.async.bulk.tensor\ncp.reduce.async.bulk.tensor\ncp.async.bulk.prefetch.tensor\ncp.async.bulk.commit_group\ncp.async.bulk.wait_group\ntensormap.replace\n9.7.9.1.\nCache Operators\n\nPTX ISA version 2.0 introduced optional cache operators on load and store instructions. The cache\noperators require a target architecture of\nsm_20\nor higher.\nCache operators on load or store instructions are treated as performance hints only. The use of a\ncache operator on an\nld\nor\nst\ninstruction does not change the memory consistency behavior of\nthe program.\nFor\nsm_20\nand higher, the cache operators have the following definitions and behavior.\nTable 30\nCache Operators for Memory Load Instructions\n\nOperator\nMeaning\n.ca\nCache at all levels, likely to be accessed again.\nThe default load instruction cache operation is ld.ca, which allocates cache lines in all\nlevels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but\nmultiple L1 caches are not coherent for global data. If one thread stores to global memory\nvia one L1 cache, and a second thread loads that address via a second L1 cache with\nld.ca\n,\nthe second thread may get stale L1 cache data, rather than the data stored by the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of parallel threads.\nStores by the first grid program are then correctly fetched by the second grid program issuing\ndefault\nld.ca\nloads cached in L1.\n.cg\nCache at global level (cache in L2 and below, not L1).\nUse\nld.cg\nto cache loads only globally, bypassing the L1 cache, and cache only in the L2\ncache.\n.cs\nCache streaming, likely to be accessed once.\nThe\nld.cs\nload cached streaming operation allocates global lines with evict-first policy in\nL1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or\ntwice. When\nld.cs\nis applied to a Local window address, it performs the\nld.lu\noperation.\n.lu\nLast use.\nThe compiler/programmer may use\nld.lu\nwhen restoring spilled registers and popping function\nstack frames to avoid needless write-backs of lines that will not be used again. The\nld.lu\ninstruction performs a load cached streaming operation (\nld.cs\n) on global addresses.\n.cv\nDon’t cache and fetch again (consider cached system memory lines stale, fetch again).\nThe ld.cv load operation applied to a global System Memory address invalidates (discards) a\nmatching L2 line and re-fetches the line on each new load.\nTable 31\nCache Operators for Memory Store Instructions\n\nOperator\nMeaning\n.wb\nCache write-back all coherent levels.\nThe default store instruction cache operation is\nst.wb\n, which writes back cache lines of\ncoherent cache levels with normal eviction policy.\nIf one thread stores to global memory, bypassing its L1 cache, and a second thread in a\ndifferent SM later loads from that address via a different L1 cache with\nld.ca\n, the second\nthread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored\nby the first thread.\nThe driver must invalidate global L1 cache lines between dependent grids of thread arrays.\nStores by the first grid program are then correctly missed in L1 and fetched by the second grid\nprogram issuing default\nld.ca\nloads.\n.cg\nCache at global level (cache in L2 and below, not L1).\nUse\nst.cg\nto cache global store data only globally, bypassing the L1 cache, and cache only\nin the L2 cache.\n.cs\nCache streaming, likely to be accessed once.\nThe\nst.cs\nstore cached-streaming operation allocates cache lines with evict-first policy to\nlimit cache pollution by streaming output data.\n.wt\nCache write-through (to system memory).\nThe\nst.wt\nstore write-through operation applied to a global System Memory address writes\nthrough the L2 cache.\n9.7.9.2.\nCache Eviction Priority Hints\n\nPTX ISA version 7.4 adds optional cache eviction priority hints on load and store\ninstructions. Cache eviction priority requires target architecture\nsm_70\nor higher.\nCache eviction priority on load or store instructions is treated as a performance hint. It is\nsupported for\n.global\nstate space and generic addresses where the address points to\n.global\nstate space.\nTable 32\nCache Eviction Priority Hints for Memory Load and Store Instructions\n\nCache Eviction Priority\nMeaning\nevict_normal\nCache data with normal eviction priority. This is the default eviction priority.\nevict_first\nData cached with this priority will be first in the eviction priority order and\nwill likely be evicted when cache eviction is required. This priority is suitable\nfor streaming data.\nevict_last\nData cached with this priority will be last in the eviction priority order and will\nlikely be evicted only after other data with\nevict_normal\nor\nevict_first\neviction priotity is already evicted. This priority is suitable for data that\nshould remain persistent in cache.\nevict_unchanged\nDo not change eviction priority order as part of this operation.\nno_allocate\nDo not allocate data to cache. This priority is suitable for streaming data.\n9.7.9.3.\nData Movement and Conversion Instructions:\nmov\n\nmov\nSet a register variable with the value of a register variable or an immediate value. Take the\nnon-generic address of a variable in global, local, or shared state space.\nSyntax\nmov.type  d, a;\nmov.type  d, sreg;\nmov.type  d, avar;       // get address of variable\nmov.type  d, avar+imm;   // get address of variable with offset\nmov.u32   d, fname;      // get address of device function\nmov.u64   d, fname;      // get address of device function\nmov.u32   d, kernel;     // get address of entry function\nmov.u64   d, kernel;     // get address of entry function\n\n.type = { .pred,\n          .b16, .b32, .b64,\n          .u16, .u32, .u64,\n          .s16, .s32, .s64,\n                .f32, .f64 };\nDescription\nWrite register\nd\nwith the value of\na\n.\nOperand\na\nmay be a register, special register, variable with optional offset in an addressable\nmemory space, or function name.\nFor variables declared in\n.const\n,\n.global\n,\n.local\n, and\n.shared\nstate spaces,\nmov\nplaces the non-generic address of the variable (i.e., the address of the variable in its state\nspace) into the destination register. The generic address of a variable in\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate space may be generated by first taking the address within the state\nspace with\nmov\nand then converting it to a generic address using the\ncvta\ninstruction;\nalternately, the generic address of a variable declared in\nconst\n,\nglobal\n,\nlocal\n, or\nshared\nstate space may be taken directly using the\ncvta\ninstruction.\nNote that if the address of a device function parameter is moved to a register, the parameter will\nbe copied onto the stack and the address will be in the local state space.\nSemantics\nd = a;\nd = sreg;\nd = &avar;        // address is non-generic; i.e., within the variable's declared state space\nd = &avar+imm;\nNotes\nAlthough only predicate and bit-size types are required, we include the arithmetic types for the\nprogrammer’s convenience: their use enhances program readability and allows additional type\nchecking.\nWhen moving address of a kernel or a device function, only\n.u32\nor\n.u64\ninstruction types\nare allowed. However, if a signed type is used, it is not treated as a compilation error. The\ncompiler issues a warning in this case.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTaking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function\naddresses should only be used in the context of CUDA Dynamic Parallelism system calls. See the\nCUDA\nDynamic Parallelism Programming Guide\nfor details.\nTarget ISA Notes\nmov.f64\nrequires\nsm_13\nor higher.\nTaking the address of kernel entry functions requires\nsm_35\nor higher.\nExamples\nmov.f32  d,a;\nmov.u16  u,v;\nmov.f32  k,0.1;\nmov.u32  ptr, A;        // move address of A into ptr\nmov.u32  ptr, A[5];     // move address of A[5] into ptr\nmov.u32  ptr, A+20;     // move address with offset into ptr\nmov.u32  addr, myFunc;  // get address of device function 'myFunc'\nmov.u64  kptr, main;    // get address of entry function 'main'\n9.7.9.4.\nData Movement and Conversion Instructions:\nmov\n\nmov\nMove vector-to-scalar (pack) or scalar-to-vector (unpack).\nSyntax\nmov.type  d, a;\n\n.type = { .b16, .b32, .b64, .b128 };\nDescription\nWrite scalar register\nd\nwith the packed value of vector register\na\n, or write vector register\nd\nwith the unpacked values from scalar register\na\n.\nWhen destination operand\nd\nis a vector register, the sink symbol\n'_'\nmay be used for one or\nmore elements provided that at least one element is a scalar register.\nFor bit-size types,\nmov\nmay be used to pack vector elements into a scalar register or unpack\nsub-fields of a scalar register into a vector. Both the overall size of the vector and the size of\nthe scalar must match the size of the instruction type.\nSemantics\n// pack two 8-bit elements into .b16\nd = a.x | (a.y << 8)\n// pack four 8-bit elements into .b32\nd = a.x | (a.y << 8)  | (a.z << 16) | (a.w << 24)\n// pack two 16-bit elements into .b32\nd = a.x | (a.y << 16)\n// pack four 16-bit elements into .b64\nd = a.x | (a.y << 16)  | (a.z << 32) | (a.w << 48)\n// pack two 32-bit elements into .b64\nd = a.x | (a.y << 32)\n// pack four 32-bit elements into .b128\nd = a.x | (a.y << 32)  | (a.z << 64) | (a.w << 96)\n// pack two 64-bit elements into .b128\nd = a.x | (a.y << 64)\n\n// unpack 8-bit elements from .b16\n{ d.x, d.y } = { a[0..7], a[8..15] }\n// unpack 8-bit elements from .b32\n{ d.x, d.y, d.z, d.w }\n        { a[0..7], a[8..15], a[16..23], a[24..31] }\n\n// unpack 16-bit elements from .b32\n{ d.x, d.y }  = { a[0..15], a[16..31] }\n// unpack 16-bit elements from .b64\n{ d.x, d.y, d.z, d.w } =\n        { a[0..15], a[16..31], a[32..47], a[48..63] }\n\n// unpack 32-bit elements from .b64\n{ d.x, d.y } = { a[0..31], a[32..63] }\n\n// unpack 32-bit elements from .b128\n{ d.x, d.y, d.z, d.w } =\n        { a[0..31], a[32..63], a[64..95], a[96..127] }\n// unpack 64-bit elements from .b128\n{ d.x, d.y } = { a[0..63], a[64..127] }\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nTarget ISA Notes\nSupported on all target architectures.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nExamples\nmov.b32 %r1,{a,b};      // a,b have type .u16\nmov.b64 {lo,hi}, %x;    // %x is a double; lo,hi are .u32\nmov.b32 %r1,{x,y,z,w};  // x,y,z,w have type .b8\nmov.b32 {r,g,b,a},%r1;  // r,g,b,a have type .u8\nmov.b64 {%r1, _}, %x;   // %x is.b64, %r1 is .b32\nmov.b128 {%b1, %b2}, %y;   // %y is.b128, %b1 and % b2 are .b64\nmov.b128 %y, {%b1, %b2};   // %y is.b128, %b1 and % b2 are .b64\n9.7.9.5.\nData Movement and Conversion Instructions:\nshfl\n(deprecated)\n\nshfl\n(deprecated)\nRegister data shuffle within threads of a warp.\nSyntax\nshfl.mode.b32  d[|p], a, b, c;\n\n.mode = { .up, .down, .bfly, .idx };\nDeprecation Note\nThe\nshfl\ninstruction without a\n.sync\nqualifier is deprecated in PTX ISA version 6.0.\nSupport for this instruction with\n.target\nlower than\nsm_70\nmay be removed in a future PTX ISA version.\nRemoval Note\nSupport for\nshfl\ninstruction without a\n.sync\nqualifier is removed in PTX ISA version 6.4 for\n.target\nsm_70\nor higher.\nDescription\nExchange register data between threads of a warp.\nEach thread in the currently executing warp will compute a source lane index\nj\nbased on input\noperands\nb\nand\nc\nand the\nmode\n. If the computed source lane index\nj\nis in range, the\nthread will copy the input operand\na\nfrom lane\nj\ninto its own destination register\nd\n;\notherwise, the thread will simply copy its own input\na\nto destination\nd\n. The optional\ndestination predicate\np\nis set to\nTrue\nif the computed source lane is in range, and\notherwise set to\nFalse\n.\nNote that an out of range value of\nb\nmay still result in a valid computed source lane index\nj\n. In this case, a data transfer occurs and the destination predicate\np\nis True.\nNote that results are undefined in divergent control flow within a warp, if an active thread sources\na register from an inactive thread.\nOperand\nb\nspecifies a source lane or source lane offset, depending on the mode.\nOperand\nc\ncontains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index.\nSemantics\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & mask[4:0]) | (cval[4:0] & ~mask[4:0]);\nminLane = (lane[4:0] & mask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~mask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval;\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nDeprecated in PTX ISA version 6.0 in favor of\nshfl.sync\n.\nNot supported in PTX ISA version 6.4 for .target\nsm_70\nor higher.\nTarget ISA Notes\nshfl\nrequires\nsm_30\nor higher.\nshfl\nis not supported on\nsm_70\nor higher starting PTX ISA version 6.4.\nExamples\n// Warp-level INCLUSIVE PLUS SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.up.b32  Ry|p, Rx, 0x1,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x2,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x4,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x8,  0x0;\n@p  add.f32      Rx, Ry, Rx;\n    shfl.up.b32  Ry|p, Rx, 0x10, 0x0;\n@p  add.f32      Rx, Ry, Rx;\n\n\n    // Warp-level INCLUSIVE PLUS REVERSE-SCAN:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.down.b32  Ry|p, Rx, 0x1,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x2,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x4,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x8,  0x1f;\n@p  add.f32        Rx, Ry, Rx;\n    shfl.down.b32  Ry|p, Rx, 0x10, 0x1f;\n@p  add.f32        Rx, Ry, Rx;\n\n\n    // BUTTERFLY REDUCTION:\n    //\n    // Assumes input in following registers:\n    //     - Rx  = sequence value for this thread\n    //\n    shfl.bfly.b32  Ry, Rx, 0x10, 0x1f;   // no predicate dest\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x8,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x4,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x2,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    shfl.bfly.b32  Ry, Rx, 0x1,  0x1f;\n    add.f32        Rx, Ry, Rx;\n    //\n    // All threads now hold sum in Rx\n9.7.9.6.\nData Movement and Conversion Instructions:\nshfl.sync\n\nshfl.sync\nRegister data shuffle within threads of a warp.\nSyntax\nshfl.sync.mode.b32  d[|p], a, b, c, membermask;\n\n.mode = { .up, .down, .bfly, .idx };\nDescription\nExchange register data between threads of a warp.\nshfl.sync\nwill cause executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nshfl.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s\nlaneid\n.\nshfl.sync\nexchanges register data between threads in\nmembermask\n.\nEach thread in the currently executing warp will compute a source lane index\nj\nbased on input\noperands\nb\nand\nc\nand the\nmode\n. If the computed source lane index\nj\nis in range, the\nthread will copy the input operand\na\nfrom lane\nj\ninto its own destination register\nd\n;\notherwise, the thread will simply copy its own input\na\nto destination\nd\n. The optional\ndestination predicate\np\nis set to\nTrue\nif the computed source lane is in range, and\notherwise set to\nFalse\n.\nNote that an out of range value of\nb\nmay still result in a valid computed source lane index\nj\n. In this case, a data transfer occurs and the destination predicate\np\nis True.\nNote that results are undefined if a thread sources a register from an inactive thread or a thread\nthat is not in\nmembermask\n.\nOperand\nb\nspecifies a source lane or source lane offset, depending on the mode.\nOperand\nc\ncontains two packed values specifying a mask for logically splitting warps into\nsub-segments and an upper bound for clamping the source lane index.\nThe behavior of\nshfl.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nshfl.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when\nthe\nshfl.sync\ninstruction is executed. Otherwise, the behavior is undefined.\nSemantics\n// wait for all threads in membermask to arrive\nwait_for_specified_threads(membermask);\n\nlane[4:0]  = [Thread].laneid;  // position of thread in warp\nbval[4:0] = b[4:0];            // source lane or lane offset (0..31)\ncval[4:0] = c[4:0];            // clamp value\nsegmask[4:0] = c[12:8];\n\n// get value of source register a if thread is active and\n// guard predicate true, else unpredictable\nif (isActive(Thread) && isGuardPredicateTrue(Thread)) {\n    SourceA[lane] = a;\n} else {\n    // Value of SourceA[lane] is unpredictable for\n    // inactive/predicated-off threads in warp\n}\nmaxLane = (lane[4:0] & segmask[4:0]) | (cval[4:0] & ~segmask[4:0]);\nminLane = (lane[4:0] & segmask[4:0]);\n\nswitch (.mode) {\n    case .up:    j = lane - bval; pval = (j >= maxLane); break;\n    case .down:  j = lane + bval; pval = (j <= maxLane); break;\n    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;\n    case .idx:   j = minLane  | (bval[4:0] & ~segmask[4:0]);\n                                 pval = (j <= maxLane); break;\n}\nif (!pval) j = lane;  // copy from own lane\nd = SourceA[j];       // copy input a from lane j\nif (dest predicate selected)\n    p = pval;\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nshfl.sync.up.b32  Ry|p, Rx, 0x1,  0x0, 0xffffffff;\n9.7.9.7.\nData Movement and Conversion Instructions:\nprmt\n\nprmt\nPermute bytes from register pair.\nSyntax\nprmt.b32{.mode}  d, a, b, c;\n\n.mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 };\nDescription\nPick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination\nregister.\nIn the generic form (no mode specified), the permute control consists of four 4-bit selection\nvalues. The bytes in the two source registers are numbered from 0 to 7:\n{b,\na}\n=\n{{b7,\nb6,\nb5,\nb4},\n{b3,\nb2,\nb1,\nb0}}\n. For each byte in the target register, a 4-bit selection value is defined.\nThe 3 lsbs of the selection value specify which of the 8 source bytes should be moved into the\ntarget position. The msb defines if the byte value should be copied, or if the sign (msb of the\nbyte) should be replicated over all 8 bits of the target position (sign extend of the byte value);\nmsb=0\nmeans copy the literal value;\nmsb=1\nmeans replicate the sign. Note that the sign\nextension is only performed as part of generic form.\nThus, the four 4-bit values fully specify an arbitrary byte permute, as a\n16b\npermute code.\ndefault mode\nd.b3\nsource select\nd.b2\nsource select\nd.b1\nsource select\nd.b0\nsource select\nindex\nc[15:12]\nc[11:8]\nc[7:4]\nc[3:0]\nThe more specialized form of the permute control uses the two lsb’s of operand\nc\n(which is\ntypically an address pointer) to control the byte extraction.\nmode\nselector\nc[1:0]\nd.b3\nsource\nd.b2\nsource\nd.b1\nsource\nd.b0\nsource\nf4e\n(forward 4 extract)\n0\n3\n2\n1\n0\n1\n4\n3\n2\n1\n2\n5\n4\n3\n2\n3\n6\n5\n4\n3\nb4e\n(backward 4 extract)\n0\n5\n6\n7\n0\n1\n6\n7\n0\n1\n2\n7\n0\n1\n2\n3\n0\n1\n2\n3\nrc8\n(replicate 8)\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\necl\n(edge clamp left)\n0\n3\n2\n1\n0\n1\n3\n2\n1\n1\n2\n3\n2\n2\n2\n3\n3\n3\n3\n3\necr\n(edge clamp right)\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n2\n2\n2\n1\n0\n3\n3\n2\n1\n0\nrc16\n(replicate 16)\n0\n1\n0\n1\n0\n1\n3\n2\n3\n2\n2\n1\n0\n1\n0\n3\n3\n2\n3\n2\nSemantics\ntmp64 = (b<<32) | a;  // create 8 byte source\n\nif ( ! mode ) {\n   ctl[0] = (c >>  0) & 0xf;\n   ctl[1] = (c >>  4) & 0xf;\n   ctl[2] = (c >>  8) & 0xf;\n   ctl[3] = (c >> 12) & 0xf;\n} else {\n   ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >>  0) & 0x3;\n}\n\ntmp[07:00] = ReadByte( mode, ctl[0], tmp64 );\ntmp[15:08] = ReadByte( mode, ctl[1], tmp64 );\ntmp[23:16] = ReadByte( mode, ctl[2], tmp64 );\ntmp[31:24] = ReadByte( mode, ctl[3], tmp64 );\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nprmt\nrequires\nsm_20\nor higher.\nExamples\nprmt.b32      r1, r2, r3, r4;\nprmt.b32.f4e  r1, r2, r3, r4;\n9.7.9.8.\nData Movement and Conversion Instructions:\nld\n\nld\nLoad a register variable from an addressable state space variable.\nSyntax\nld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};\n\nld.volatile{.ss}{.level::prefetch_size}{.vec}.type  d, [a];\n\nld.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.acquire.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};\n\nld.mmio.relaxed.sys{.global}.type  d, [a];\n\n.ss =                       { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} };\n.cop =                      { .ca, .cg, .cs, .lu, .cv };\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nLoad register variable\nd\nfrom the location specified by the source address operand\na\nin\nspecified state space. If no state space is given, perform the load using\nGeneric Addressing\n.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nIf no sub-qualifier is specified with\n.param\nstate space, then:\n::func\nis assumed when access is inside a device function.\n::entry\nis assumed when accessing kernel function parameters from entry function. Otherwise, when\naccessing device function parameters or any other\n.param\nvariables from entry function\n::func\nis assumed by default.\nFor\nld.param::entry\ninstruction, operand a must be a kernel parameter address, otherwise behavior\nis undefined. For\nld.param::func\ninstruction, operand a must be a device function parameter address,\notherwise behavior is undefined.\nInstruction\nld.param{::func}\nused for reading value returned from device function call cannot be\npredicated. See\nParameter State Space\nand\nFunction Declarations and Definitions\nfor descriptions\nof the proper use of\nld.param\n.\nThe\n.relaxed\nand\n.acquire\nqualifiers indicate memory synchronization as described in the\nMemory Consistency Model\n. The\n.scope\nqualifier\nindicates the set of threads with which an\nld.relaxed\nor\nld.acquire\ninstruction can directly\nsynchronize\n1\n. The\n.weak\nqualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means.\nThe semantic details of\n.mmio\nqualifier are described in the\nMemory Consistency Model\n.\nOnly\n.sys\nthread scope is valid for\nld.mmio\noperation. The\nqualifiers\n.mmio\nand\n.relaxed\nmust be specified together.\nThe semantic details of\n.volatile\nqualifier are described in the\nMemory Consistency Model\n.\nThe\n.weak\n,\n.volatile\n,\n.relaxed\nand\n.acquire\nqualifiers are mutually exclusive. When\nnone of these is specified, the\n.weak\nqualifier is assumed by default.\n.relaxed\nand\n.acquire\n:\nMay be used with\n.global\n,\n.shared\nspaces, or with generic addressing where the address\npoints to\n.global\nor\n.shared\nspace.\nCache operations are not allowed.\n.volatile\n:\nMay be used with\n.global\n,\n.shared\n,\n.local\nspaces, or with generic addressing where\nthe address points to\n.global\n,\n.shared\n, or\n.local\nspace.\nCache operations are not allowed.\n.mmio\n:\nMay be used only with\n.global\nspace or with generic addressing where the address points to\n.global\nspace.\nThe optional qualifier\n.unified\nmust be specified on operand\na\nif\na\nis the address of a\nvariable declared with\n.unified\nattribute as described in\nVariable and Function Attribute Directive: .attribute\n.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nd\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that data from corresponding memory location is not read.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe qualifier\n.level::prefetch_size\nmay only be used with\n.global\nstate space and with\ngeneric addressing where the address points to\n.global\nstate space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifiers\n.unified\nand\n.level::cache_hint\nare only supported for\n.global\nstate\nspace and for generic addressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\n1\nThis synchronization is further extended to other threads through the transitive nature of\ncausality order\n, as described in the memory consistency model.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See\nTable 28\nfor a description of these relaxed type-checking rules.\n.f16\ndata may be loaded using\nld.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\nor can be used in half precision floating point instructions.\n.f16x2\ndata may be loaded using\nld.b32\nand then used in half precision floating point\ninstructions.\nPTX ISA Notes\nld introduced in PTX ISA version 1.0.\nld.volatile\nintroduced in PTX ISA version 1.1.\nGeneric addressing and cache operations introduced in PTX ISA version 2.0.\nSupport for scope qualifier,\n.relaxed\n,\n.acquire\n,\n.weak\nqualifiers introduced in PTX ISA\nversion 6.0.\nSupport for generic addressing of .const space added in PTX ISA version 3.1.\nSupport for\n.level1::eviction_priority\n,\n.level::prefetch_size\nand\n.level::cache_hint\nqualifiers introduced in PTX ISA version 7.4.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for\n.unified\nqualifier introduced in PTX ISA version 8.0.\nSupport for\n.mmio\nqualifier introduced in PTX ISA version 8.2.\nSupport for\n::entry\nand\n::func\nsub-qualifiers on\n.param\nspace introduced in PTX ISA\nversion 8.3.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nSupport for\n.volatile\nqualifier with\n.local\nstate space introduced in PTX ISA version 9.1.\nTarget ISA Notes\nld.f64\nrequires\nsm_13\nor higher.\nSupport for scope qualifier,\n.relaxed\n,\n.acquire\n,\n.weak\nqualifiers require\nsm_70\nor\nhigher.\nGeneric addressing requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nSupport for\n.level::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::prefetch_size\nqualifier requires\nsm_75\nor higher.\nSupport for\n.L2::256B\nand\n.L2::cache_hint\nqualifiers requires\nsm_80\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.unified\nqualifier requires\nsm_90\nor higher.\nSupport for\n.mmio\nqualifier requires\nsm_70\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nld.global.f32    d,[a];\nld.shared.v4.b32 Q,[p];\nld.const.s32     d,[p+4];\nld.local.b32     x,[p+-8]; // negative offset\nld.local.b64     x,[240];  // immediate address\n\nld.global.b16    %r,[fs];  // load .f16 data into 32-bit reg\ncvt.f32.f16      %r,%r;    // up-convert f16 data to f32\n\nld.global.b32    %r0, [fs];     // load .f16x2 data in 32-bit reg\nld.global.b32    %r1, [fs + 4]; // load .f16x2 data in 32-bit reg\nadd.rn.f16x2     %d0, %r0, %r1; // addition of f16x2 data\nld.global.relaxed.gpu.u32 %r0, [gbl];\nld.shared.acquire.gpu.u32 %r1, [sh];\nld.global.relaxed.cluster.u32 %r2, [gbl];\nld.shared::cta.acquire.gpu.u32 %r2, [sh + 4];\nld.shared::cluster.u32 %r3, [sh + 8];\nld.global.mmio.relaxed.sys.u32 %r3, [gbl];\nld.local.volatile.u32 %r4, [lcl];\n\nld.global.f32    d,[ugbl].unified;\nld.b32           %r0, [%r1].unified;\n\nld.global.L1::evict_last.u32  d, [p];\n\nld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2\nld.L2::128B.f64         %r1, [gbl]; // Prefetch 128B to L2\nld.global.L2::256B.f64  %r2, [gbl]; // Prefetch 256B to L2\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1;\nld.global.L2::cache_hint.b64  x, [p], cache-policy;\nld.param::entry.b32 %rp1, [kparam1];\n\nld.global.b128   %r0, [gbl];   // 128-bit load\n\n// 256-bit load\nld.global.L2::evict_last.v8.f32 { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7}, [addr];\nld.global.L2::evict_last.L1::evict_last.v4.u64 { %reg0, %reg1, %reg2, %reg3}, [addr];\n9.7.9.9.\nData Movement and Conversion Instructions:\nld.global.nc\n\nld.global.nc\nLoad a register variable from global state space via non-coherent cache.\nSyntax\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type                 d, [a]{, cache-policy};\nld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type             d, [a]{, cache-policy};\n\nld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type      d, [a]{, cache-policy};\nld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type  d, [a]{, cache-policy};\n\n.cop  =                     { .ca, .cg, .cs };     // cache operation\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate};\n.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};\n.level::cache_hint =        { .L2::cache_hint };\n.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }\n.vec  =                     { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nLoad register variable\nd\nfrom the location specified by the source address operand\na\nin the\nglobal state space, and optionally cache in non-coherent read-only cache.\nNote\nOn some architectures, the texture cache is larger, has higher bandwidth, and longer latency than\nthe global memory cache. For applications with sufficient parallelism to cover the longer\nlatency,\nld.global.nc\nshould offer better performance than\nld.global\non such\narchitectures.\nThe address operand\na\nshall contain a global address.\nSupported addressing modes for operand\na\nand alignment requirements are\ndescribed in\nAddresses as Operands\n.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\n,\n.s32\n,\n.u32\n, or\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nd\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that data from corresponding memory location is not read.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types.\n.f16\ndata may be loaded using\nld.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nSupport for\n.level::eviction_priority\n,\n.level::prefetch_size\nand\n.level::cache_hint\nqualifiers introduced in PTX ISA version 7.4.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nRequires\nsm_32\nor higher.\nSupport for\n.level1::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::prefetch_size\nqualifier requires\nsm_75\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nld.global.nc.f32           d, [a];\nld.gloal.nc.L1::evict_last.u32 d, [a];\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.5;\nld.global.nc.L2::cache_hint.f32  d, [a], cache-policy;\n\nld.global.nc.L2::64B.b32      d,  [a];     // Prefetch 64B to L2\nld.global.nc.L2::256B.f64     d,  [a];     // Prefetch 256B to L2\n\nld.global.nc.b128             d,  [a];\n\nld.global.nc.L2::evict_first.v4.f64 {%reg0, %reg1. %reg2, %reg3}. [a]; // 256-bit load\n9.7.9.10.\nData Movement and Conversion Instructions:\nldu\n\nldu\nLoad read-only data from an address that is common across threads in the warp.\nSyntax\nldu{.ss}.type      d, [a];       // load from address\nldu{.ss}.vec.type  d, [a];       // vec load from address\n\n.ss   = { .global };             // state space\n.vec  = { .v2, .v4 };\n.type = { .b8, .b16, .b32, .b64, .b128,\n          .u8, .u16, .u32, .u64,\n          .s8, .s16, .s32, .s64,\n                     .f32, .f64 };\nDescription\nLoad\nread-only\ndata into register variable\nd\nfrom the location specified by the source address\noperand\na\nin the global state space, where the address is guaranteed to be the same across all\nthreads in the warp. If no state space is given, perform the load using\nGeneric Addressing\n.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\n.\nSemantics\nd = a;             // named variable a\nd = *(&a+immOff)   // variable-plus-offset\nd = *a;            // register\nd = *(a+immOff);   // register-plus-offset\nd = *(immAddr);    // immediate address\nNotes\nDestination\nd\nmust be in the\n.reg\nstate space.\nA destination register wider than the specified type may be used. The value loaded is sign-extended\nto the destination register width for signed integers, and is zero-extended to the destination\nregister width for unsigned and bit-size types. See\nTable 28\nfor a description of these relaxed type-checking rules.\n.f16\ndata may be loaded using\nldu.b16\n, and then converted to\n.f32\nor\n.f64\nusing\ncvt\nor can be used in half precision floating point instructions.\n.f16x2\ndata may be loaded using\nldu.b32\nand then used in half precision floating point\ninstructions.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nTarget ISA Notes\nldu.f64\nrequires\nsm_13\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nExamples\nldu.global.f32    d,[a];\nldu.global.b32    d,[p+4];\nldu.global.v4.f32 Q,[p];\nldu.global.b128   d,[a];\n9.7.9.11.\nData Movement and Conversion Instructions:\nst\n\nst\nStore data to an addressable state space variable.\nSyntax\nst{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type   [a], b{, cache-policy};\nst{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.volatile{.ss}{.vec}.type                           [a], b;\nst.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.release.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type\n                                                      [a], b{, cache-policy};\nst.mmio.relaxed.sys{.global}.type         [a], b;\n\n.ss =                       { .global, .local, .param{::func}, .shared{::cta, ::cluster} };\n.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,\n                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };\n.level2::eviction_priority = { .L2::evict_normal, .L2::evict_first, .L2::evict_last };\n.level::cache_hint =        { .L2::cache_hint };\n.cop =                      { .wb, .cg, .cs, .wt };\n.sem =                      { .relaxed, .release };\n.scope =                    { .cta, .cluster, .gpu, .sys };\n.vec =                      { .v2, .v4, .v8 };\n.type =                     { .b8, .b16, .b32, .b64, .b128,\n                              .u8, .u16, .u32, .u64,\n                              .s8, .s16, .s32, .s64,\n                              .f32, .f64 };\nDescription\nStore the value of operand\nb\nin the location specified by the destination address\noperand\na\nin specified state space. If no state space is given, perform the store using\nGeneric Addressing\n. Stores to const memory are illegal.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\n.\nIf\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::func\n.\nInstruction\nst.param{::func}\nused for passing arguments to device function cannot be predicated.\nSee\nParameter State Space\nand\nFunction Declarations and Definitions\nfor descriptions of the proper use\nof\nst.param\n.\nThe qualifiers\n.relaxed\nand\n.release\nindicate memory synchronization as described in the\nMemory Consistency Model\n. The\n.scope\nqualifier\nindicates the set of threads with which an\nst.relaxed\nor\nst.release\ninstruction can directly\nsynchronize\n1\n. The\n.weak\nqualifier indicates a memory instruction with no synchronization.\nThe effects of this instruction become visible to other threads only when synchronization is established\nby other means.\nThe semantic details of\n.mmio\nqualifier are described in the\nMemory Consistency Model\n.\nOnly\n.sys\nthread scope is valid for\nst.mmio\noperation. The\nqualifiers\n.mmio\nand\n.relaxed\nmust be specified together.\nThe semantic details of\n.volatile\nqualifier are described in the\nMemory Consistency Model\n.\nThe\n.weak\n,\n.volatile\n,\n.relaxed\nand\n.release\nqualifiers are mutually exclusive. When\nnone of these is specified, the\n.weak\nqualifier is assumed by default.\n.relaxed\nand\n.release\n:\nMay be used with\n.global\n,\n.shared\nspaces or with generic addressing where the address points\nto\n.global\nor\n.shared\nspace.\nCache operations are not allowed.\n.volatile\n:\nMay be used with\n.global\n,\n.shared\n,\n.local\nspaces or with generic addressing where the\naddress points to\n.global\n,\n.shared\n, or\n.local\nspace.\nCache operations are not allowed.\n.mmio\n:\nMay be used only with\n.global\nspace or with generic addressing where the address points to\n.global\nspace.\nThe\n.v8\n(\n.vec\n) qualifier is supported if:\n.type\nis\n.b32\n,\n.s32\n,\n.u32\n, or\n.f32\nAND\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nThe\n.v4\n(\n.vec\n) qualifier with type\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nis supported if:\nState space is\n.global\nor with generic addressing where address points to\n.global\nstate space\nQualifiers\n.level1::eviction_priority\nand\n.level2::eviction_priority\nspecify the eviction policy\nfor L1 and L2 cache respectively which may be applied during memory access.\nQualifier\n.level2::eviction_priority\nis supported if:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nAND Operand\nd\nis vector of 8 registers with type specified with\n.type\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nAND Operand\nd\nis vector of 4 registers with type specified with\n.type\nOptionally, sink symbol ‘_’ can be used in vector expression\nb\nwhen:\n.vec\nis\n.v8\nand\n.type\nis\n.b32\nor\n.s32\nor\n.u32\nor\n.f32\nOR\n.vec\nis\n.v4\nand\n.type\nis\n.b64\nor\n.s64\nor\n.u64\nor\n.f64\nwhich indicates that no data is being written at the corresponding destination address.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\n1\nThis synchronization is further extended to other threads through the transitive nature of\ncausality order\n, as described in the memory consistency model.\nSemantics\nd = a;                // named variable d\n*(&a+immOffset) = b;            // variable-plus-offset\n*a = b;               // register\n*(a+immOffset) = b;   // register-plus-offset\n*(immAddr) = b;       // immediate address\nNotes\nOperand\nb\nmust be in the\n.reg\nstate space.\nA source register wider than the specified type may be used. The lower\nn\nbits corresponding to\nthe instruction-type width are stored to memory. See\nTable 27\nfor a description of these relaxed type-checking rules.\n.f16\ndata resulting from a\ncvt\ninstruction may be stored using\nst.b16\n.\n.f16x2\ndata may be stored using\nst.b32\n.\nPTX ISA Notes\nst introduced in PTX ISA version 1.0.\nst.volatile\nintroduced in PTX ISA version 1.1.\nGeneric addressing and cache operations introduced in PTX ISA version 2.0.\nSupport for scope qualifier,\n.relaxed\n,\n.release\n,\n.weak\nqualifiers introduced in PTX ISA\nversion 6.0.\nSupport for\n.level1::eviction_priority\nand\n.level::cache_hint\nqualifiers introduced in PTX\nISA version 7.4.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for\n.mmio\nqualifier introduced in PTX ISA version 8.2.\nSupport for\n::func\nsub-qualifier on\n.param\nspace introduced in PTX ISA version 8.3.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nintroduced in PTX ISA version 8.8.\nSupport for\n.volatile\nqualifier with\n.local\nstate space introduced in PTX ISA version 9.1.\nTarget ISA Notes\nst.f64\nrequires\nsm_13\nor higher.\nSupport for scope qualifier,\n.relaxed\n,\n.release\n,\n.weak\nqualifiers require\nsm_70\nor\nhigher.\nGeneric addressing requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nSupport for\n.level1::eviction_priority\nqualifier requires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.mmio\nqualifier requires\nsm_70\nor higher.\nSupport for\n.b128\ntype requires\nsm_70\nor higher.\nSupport for\n.level2::eviction_priority\nqualifier and\n.v8.b32\n/\n.v4.b64\nrequire\nsm_100\nor higher.\nExamples\nst.global.f32    [a],b;\nst.local.b32     [q+4],a;\nst.global.v4.s32 [p],Q;\nst.local.b32     [q+-8],a; // negative offset\nst.local.s32     [100],r7; // immediate address\n\ncvt.f16.f32      %r,%r;    // %r is 32-bit register\nst.b16           [fs],%r;  // store lower\nst.global.relaxed.sys.u32 [gbl], %r0;\nst.shared.release.cta.u32 [sh], %r1;\nst.global.relaxed.cluster.u32 [gbl], %r2;\nst.shared::cta.release.cta.u32 [sh + 4], %r1;\nst.shared::cluster.u32 [sh + 8], %r1;\nst.global.mmio.relaxed.sys.u32 [gbl], %r1;\nst.local.volatile.u32 [lcl], %r2;\n\nst.global.L1::no_allocate.f32 [p], a;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nst.global.L2::cache_hint.b32  [a], b, cache-policy;\n\nst.param::func.b64 [param1], %rp1;\n\nst.global.b128  [a], b;  // 128-bit store\n\n// 256-bit store\nst.global.L2::evict_last.v8.f32 [addr], { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7};\n9.7.9.12.\nData Movement and Conversion Instructions:\nst.async\n\nst.async\nAsynchronous store operation.\nSyntax\nst.async{.weak}{.ss}.completion_mechanism{.vec}.type [a], b, [mbar];\nst.async{.scope}{.ss}.completion_mechanism{.vec}.type [a], b, [mbar];\n\n.scope =                { .cluster };\n.ss   =                 { .shared::cluster };\n.type =                 { .b32, .b64,\n                          .u32, .u64,\n                          .s32, .s64,\n                          .f32, .f64 };\n.vec  =                 { .v2, .v4 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\nst.async{.mmio}.sem.scope{.ss}.type [a], b;\n\n.sem =                  { .release };\n.scope =                { .gpu, .sys };\n.ss =                   { .global };\n.type =                 { .b8, .b16, .b32, .b64,\n                          .u8, .u16, .u32, .u64,\n                          .s8, .s16, .s32, .s64,\n                                     .f32, .f64 };\nDescription\nst.async\nis a non-blocking instruction which initiates an asynchronous store operation that\nstores the value specified by source operand\nb\nto the destination memory location\nspecified by operand\na\n.\nOperands\na\nis a destination address, and must be either a register, or of the form\nregister\n+\nimmOff\n,\nas described in\nAddresses as Operands\n.\nb\nis a source value, of the type indicated by qualifier\n.type\n.\nmbar\nis an mbarrier object address.\nQualifiers\n.mmio\nindicates whether this is an\nmmio Operation\n.\n.sem\nspecifies the memory ordering semantics as described in the\nMemory Consistency Model\n.\nIf\n.sem\nis not specified, it defaults to\n.weak\n.\n.scope\nspecifies the set of threads with which this instruction can directly synchronize.\n.ss\nspecifies the state space of the destination operand\na\nand the mbarrier\noperand\nmbar\n.\nIf\n.ss\nis not specified,\nGeneric Addressing\nis used.\n.completion_mechanism\nspecifies the mechanism for observing the completion of the\nasynchronous operation.\nWhen\n.completion_mechanism\nis\n.mbarrier::complete_tx::bytes\n: upon completion of the\nasynchronous operation, a\ncomplete-tx\noperation will be performed on the mbarrier object specified by the operand\nmbar\n, with\ncompleteCount\nargument equal to the amount of data stored in bytes.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\n.type\nspecifies the type of the source operand\nb\n.\nConditions\nWhen\n.sem\nis\n.weak\n:\nThis is a weak store to shared memory, which signals its completion through an mbarrier object.\nThe store operation is treated as a weak memory operation.\nThe complete-tx operation on the mbarrier has\n.release\nsemantics at\n.cluster\nscope.\nRequires:\nThe shared memory addresses of destination operand\na\nand the\nmbarrier object\nmbar\nbelong\nto the same CTA within the same cluster as the executing thread.\nThe number of CTAs within the cluster is strictly greater than one;\n%cluster_nctarank\n>\n1\nis true.\nOtherwise, the behavior is undefined.\n.mmio\nmust not be specified.\nIf\n.ss\nis specified, it must be\n.shared::cluster\n.\nIf\n.ss\nis not specified, generic addressing is used for operands\na\nand\nmbar\n.\nIf the generic addresses specified do not fall within the address window of\n.shared::cluster\nstate space, the behavior is undefined.\n.completion_mechanism\nmust be specified and must be\n.mbarrier::complete_tx::bytes\n.\nWhen\n.sem\nis\n.release\n:\nThis is a release store to global memory.\nThe store operation is a strong memory operation with\n.release\nsemantics at the\nscope specified by\n.scope\n.\nIf\n.mmio\nis specified,\n.scope\nmust be\n.sys\n.\nIf\n.ss\nis specified, it must be\n.global\n.\nIf\n.ss\nis not specified, generic addressing is used for operand\na\n.\nIf the generic address specified does not fall within the address window of\n.global\nstate space, the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space, and\n.scope\nqualifier introduced in PTX ISA version 8.7.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space, and\n.scope\nqualifier require\nsm_100\nor higher.\nExamples\nst.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr];\n\nst.async.sys.release.global.u32 [addr], b;\n9.7.9.13.\nData Movement and Conversion Instructions:\nst.bulk\n\nst.bulk\nInitializes a region of memory as specified by state space.\nSyntax\nst.bulk{.weak}{.shared::cta}  [a], size, initval; // initval must be zero\nDescription\nst.bulk\ninstruction initializes a region of shared memory starting from the location specified\nby destination address operand\na\n.\nThe 32-bit or 64-bit integer operand\nsize\nspecifies the amount of memory to be initialized in terms of\nnumber of bytes.\nsize\nmust be a multiple of 8. If the value is not a multiple of 8, then the\nbehavior is undefined. The maximum value of\nsize\noperand can be 16777216.\nThe integer immediate operand\ninitval\nspecifies the initialization value for the memory\nlocations. The only numeric value allowed for operand\ninitval\nis 0.\nIf no state space is specified then\nGeneric Addressing\nis used. If the\naddress specified by\na\ndoes not fall within the address window of\n.shared\nstate space then\nthe behavior is undefined.\nThe optional qualifier\n.weak\nspecify the memory synchronizing effect of the\nst.bulk\ninstruction as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nSupport for\nsize\noperand with 32-bit length is introduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nExamples\nst.bulk.weak.shared::cta  [dst], n, 0;\n\nst.bulk                   [gdst], 4096, 0;\n9.7.9.14.\nData Movement and Conversion Instructions:\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\n\nThe multimem.* operations operate on multimem addresses and accesses all of the multiple memory\nlocations which the multimem address points to.\nMultimem addresses can only be accessed only by multimem.* operations. Accessing a multimem address\nwith\nld\n,\nst\nor any other memory operations results in undefined behavior.\nRefer to\nCUDA programming guide\nfor creation and management of the multimem addresses.\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nPerform memory operations on the multimem address.\nSyntax\n// Integer type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type      d, [a];\nmultimem.ld_reduce.weak{.ss}.op.type                 d, [a];\n\nmultimem.st{.stsem}{.scope}{.ss}.type                [a], b;\nmultimem.st.weak{.ss}.type                           [a], b;\n\nmultimem.red{.redsem}{.scope}{.ss}.op.type           [a], b;\n\n.ss =       { .global }\n.ldsem =    { .relaxed, .acquire }\n.stsem =    { .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add, .and, .or, .xor }\n.type =     { .b32, .b64,  .u32, .u64, .s32, .s64 }\n\n// Floating point type:\n\nmultimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type    d, [a];\nmultimem.ld_reduce.weak{.ss}.op{.acc_prec}{.vec}.type               d, [a];\n\nmultimem.st{.stsem}{.scope}{.ss}{.vec}.type                         [a], b;\nmultimem.st.weak{.ss}{.vec}.type                                    [a], b;\n\nmultimem.red{.redsem}{.scope}{.ss}.redop{.vec}.redtype              [a], b;\n\n.ss =       { .global }\n.ldsem =    { .relaxed, .acquire }\n.stsem =    { .relaxed, .release }\n.redsem =   { .relaxed, .release }\n.scope =    { .cta, .cluster, .gpu, .sys }\n.op  =      { .min, .max, .add }\n.redop  =   { .add }\n.acc_prec = { .acc::f32, .acc::f16 }\n.vec =      { .v2, .v4, .v8 }\n.type=      { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64, .e5m2, .e5m2x2, .e5m2x4, .e4m3, .e4m3x2, .e4m3x4 }\n.redtype =  { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 }\nDescription\nInstruction\nmultimem.ld_reduce\nperforms the following operations:\nload operation on the multimem address\na\n, which involves loading of data from all of the\nmultiple memory locations pointed to by the multimem address\na\n,\nreduction operation specified by\n.op\non the multiple data loaded from the multimem address\na\n.\nThe result of the reduction operation in returned in register\nd\n.\nInstruction\nmultimem.st\nperforms a store operation of the input operand\nb\nto all the memory\nlocations pointed to by the multimem address\na\n.\nInstruction\nmultimem.red\nperforms a reduction operation on all the memory locations pointed to\nby the multimem address\na\n, with operand\nb\n.\nInstruction\nmultimem.ld_reduce\nperforms reduction on the values loaded from all the memory\nlocations that the multimem address points to. In contrast, the\nmultimem.red\nperform reduction\non all the memory locations that the multimem address points to.\nAddress operand\na\nmust be a multimem address. Otherwise, the behavior is undefined.  Supported\naddressing modes for operand a and alignment requirements are described in\nAddresses as Operands\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\na\ndoes not fall within the address window of\n.global\nstate\nspace then the behavior is undefined.\nFor floating-point type multi- operations, the size of the specified type along with\n.vec\nmust\nequal either 32-bits or 64-bits or 128-bits. No other combinations of\n.vec\nand type are\nallowed. Type\n.f64\ncannot be used with\n.vec\nqualifier.\nThe following table describes the valid usage of\n.vec\nand base floating-point type:\n.vec\nBase float-type supported\nNo\n.vec\nspecified\n.f16x2\n,\n.bf16x2\n,\n.f32\n,\n.f64\n,\n.e5m2x4\n,\n.e4m3x4\n.v2\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3x2\n,\n.e4m3x4\n.v4\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\n.v8\n.f16\n,\n.bf16\n,\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\nThe following table describes the valid combinations of\n.op\nand base type:\nop\nBase type\n.add\n.u32\n,\n.u64\n,\n.s32\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.f32\n,\n.f64\n,\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nFor\nmultimem.ld_reduce\n, the default precision of the intermediate accumulation is same as the\nspecified type.\nOptionally,\n.acc_prec\nqualifier can be specified to change the precision of intermediate\naccumulation as follows:\n.type\n.acc::prec\nChanges precision to\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n.acc::f32\n.f32\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\n,\n.e4m3x4\n,\n.e5m2x4\n.acc::f16\n.f16\nOptional qualifiers\n.ldsem\n,\n.stsem\nand\n.redsem\nspecify the memory synchronizing effect\nof the\nmultimem.ld_reduce\n,\nmultimem.st\nand\nmultimem.red\nrespectively, as described in\nMemory Consistency Model\n. If explicit semantics qualifiers\nare not specified, then\nmultimem.ld_reduce\nand\nmultimem.st\ndefault to\n.weak\nand\nmultimem.red\ndefaults to\n.relaxed\n.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in\nMemory Consistency Model\n. If the\n.scope\nqualifier is not specified for\nmultimem.red\nthen\n.sys\nscope is assumed by default.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.acc::f32\nqualifier introduced in PTX ISA version 8.2.\nSupport for types\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nintroduced in PTX ISA version 8.6.\nSupport for\n.acc::f16\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nTypes\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nsm_121a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.acc::f16\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nsm_121a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nmultimem.ld_reduce.and.b32                    val1_b32, [addr1];\nmultimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2];\n\nmultimem.st.relaxed.gpu.b32                [addr3], val3_b32;\nmultimem.st.release.cta.global.u32         [addr4], val4_u32;\n\nmultimem.red.relaxed.gpu.max.f64           [addr5], val5_f64;\nmultimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9};\nmultimem.ld_reduce.add.acc::f32.v2.f16x2   {val_10, val_11}, [addr7];\n\nmultimem.ld_reduce.relaxed.cta.min.v2.e4m3x2 {val_12, val_13}, [addr8];\nmultimem.ld_reduce.relaxed.cta.add.v4.e4m3   {val_14, val_15, val_16, val_17}, [addr9];\nmultimem.ld_reduce.add.acc::f16.v4.e5m2      {val_18, val_19, val_20, val_21}, [addr10];\n9.7.9.15.\nData Movement and Conversion Instructions:\nprefetch\n,\nprefetchu\n\nprefetch\n,\nprefetchu\nPrefetch line containing a generic address at a specified level of memory hierarchy, in specified\nstate space.\nSyntax\nprefetch{.space}.level                    [a];   // prefetch to data cache\nprefetch.global.level::eviction_priority  [a];   // prefetch to data cache\n\nprefetchu.L1  [a];             // prefetch to uniform cache\n\nprefetch{.tensormap_space}.tensormap [a];  // prefetch the tensormap\n\n.space =                    { .global, .local };\n.level =                    { .L1, .L2 };\n.level::eviction_priority = { .L2::evict_last, .L2::evict_normal };\n.tensormap_space =          { .const, .param };\nDescription\nThe\nprefetch\ninstruction brings the cache line containing the specified address in global or\nlocal memory state space into the specified cache level.\nIf the\n.tensormap\nqualifier is specified then the\nprefetch\ninstruction brings the cache line\ncontaining the specified address in the\n.const\nor\n.param\nmemory state space for subsequent\nuse by the\ncp.async.bulk.tensor\ninstruction.\nIf no state space is given, the\nprefetch\nuses\nGeneric Addressing\n.\nOptionally, the eviction priority to be applied on the prefetched cache line can be specified by the\nmodifier\n.level::eviction_priority\n.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe\nprefetchu\ninstruction brings the cache line containing the specified generic address into\nthe specified uniform cache level.\nA\nprefetch\nto a shared memory location performs no operation.\nA\nprefetch\ninto the uniform cache requires a generic address, and no operation occurs if the\naddress maps to a\nconst\n,\nlocal\n, or\nshared\nmemory location.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nSupport for\n.level::eviction_priority\nqualifier introduced in PTX ISA version 7.4.\nSupport for the\n.tensormap\nqualifier is introduced in PTX ISA version 8.0.\nTarget ISA Notes\nprefetch\nand\nprefetchu\nrequire\nsm_20\nor higher.\nSupport for\n.level::eviction_priority\nqualifier requires\nsm_80\nor higher.\nSupport for the\n.tensormap\nqualifier requires\nsm_90\nor higher.\nExamples\nprefetch.global.L1             [ptr];\nprefetch.global.L2::evict_last [ptr];\nprefetchu.L1  [addr];\nprefetch.const.tensormap       [ptr];\n9.7.9.16.\nData Movement and Conversion Instructions:\napplypriority\n\napplypriority\nApply the cache eviction priority to the specified address in the specified cache level.\nSyntax\napplypriority{.global}.level::eviction_priority  [a], size;\n\n.level::eviction_priority = { .L2::evict_normal };\nDescription\nThe\napplypriority\ninstruction applies the cache eviction priority specified by the\n.level::eviction_priority\nqualifier to the address range\n[a..a+size)\nin the specified cache\nlevel.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nThe operand\nsize\nis an integer constant that specifies the amount of data, in bytes, in the\nspecified cache level on which the priority is to be applied. The only supported value for the\nsize\noperand is 128.\nSupported addressing modes for operand\na\nare described in\nAddresses as Operands\n.\na\nmust be aligned to 128 bytes.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\napplypriority.global.L2::evict_normal [ptr], 128;\n9.7.9.17.\nData Movement and Conversion Instructions:\ndiscard\n\ndiscard\nDiscard the data at the specified address range and cache level.\nSyntax\ndiscard{.global}.level  [a], size;\n\n.level = { .L2 };\nDescription\nSemantically, this behaves like a weak write of an\nunstable indeterminate value\n:\nreads of memory locations with\nunstable indeterminate values\nmay return different\nbit patterns each time until the memory is overwritten.\nThis operation\nhints\nto the implementation that data in the specified cache\n.level\ncan be destructively discarded without writing it back to memory.\nThe operand\nsize\nis an integer constant that specifies the length in bytes of the\naddress range\n[a,\na\n+\nsize)\nto write\nunstable indeterminate values\ninto.\nThe only supported value for the\nsize\noperand is\n128\n.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nSupported addressing modes for address operand\na\nare described in\nAddresses as Operands\n.\na\nmust be aligned to 128 bytes.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\ndiscard.global.L2 [ptr], 128;\nld.weak.u32 r0, [ptr];\nld.weak.u32 r1, [ptr];\n// The values in r0 and r1 may differ!\n9.7.9.18.\nData Movement and Conversion Instructions:\ncreatepolicy\n\ncreatepolicy\nCreate a cache eviction policy for the specified cache level.\nSyntax\n// Range-based policy\ncreatepolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy, [a], primary-size, total-size;\n\n// Fraction-based policy\ncreatepolicy.fractional.level::primary_priority{.level::secondary_priority}.b64\n                                   cache-policy{, fraction};\n\n// Converting the access property from CUDA APIs\ncreatepolicy.cvt.L2.b64            cache-policy, access-property;\n\n.level::primary_priority =   { .L2::evict_last, .L2::evict_normal,\n                               .L2::evict_first, .L2::evict_unchanged };\n.level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged };\nDescription\nThe\ncreatepolicy\ninstruction creates a cache eviction policy for the specified cache level in an\nopaque 64-bit register specified by the destination operand\ncache-policy\n. The cache eviction\npolicy specifies how cache eviction priorities are applied to global memory addresses used in memory\noperations with\n.level::cache_hint\nqualifier.\nThere are two types of cache eviction policies:\nRange-based policy\nThe cache eviction policy created using\ncreatepolicy.range\nspecifies the cache eviction\nbehaviors for the following three address ranges:\n[a\n..\na\n+\n(primary-size\n-\n1)]\nreferred to as primary range.\n[a\n+\nprimary-size\n..\na\n+\n(total-size\n-\n1)]\nreferred to as trailing secondary range.\n[a\n-\n(total-size\n-\nprimary-size)\n..\n(a\n-\n1)]\nreferred to as preceding secondary range.\nWhen a range-based cache eviction policy is used in a memory operation with\n.level::cache_hint\nqualifier, the eviction priorities are applied as follows:\nIf the memory address falls in the primary range, the eviction priority specified by\n.L2::primary_priority\nis applied.\nIf the memory address falls in any of the secondary ranges, the eviction priority specified by\n.L2::secondary_priority\nis applied.\nIf the memory address does not fall in either of the above ranges, then the applied eviction\npriority is unspecified.\nThe 32-bit operand\nprimary-size\nspecifies the size, in bytes, of the primary range. The\n32-bit operand\ntotal-size\nspecifies the combined size, in bytes, of the address range\nincluding primary and secondary ranges. The value of\nprimary-size\nmust be less than or equal\nto the value of\ntotal-size\n. Maximum allowed value of\ntotal-size\nis 4GB.\nIf\n.L2::secondary_priority\nis not specified, then it defaults to\n.L2::evict_unchanged\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the specified address does not fall within the address window of\n.global\nstate space\nthen the behavior is undefined.\nFraction-based policy\nA memory operation with\n.level::cache_hint\nqualifier can use the fraction-based cache\neviction policy to request the cache eviction priority specified by\n.L2:primary_priority\nto\nbe applied to a fraction of cache accesses specified by the 32-bit floating point operand\nfraction\n. The remainder of the cache accesses get the eviction priority specified by\n.L2::secondary_priority\n. This implies that in a memory operation that uses a fraction-based\ncache policy, the memory access has a probability specified by the operand\nfraction\nof\ngetting the cache eviction priority specified by\n.L2::primary_priority\n.\nThe valid range of values for the operand\nfraction\nis\n(0.0,..,\n1.0]\n. If the operand\nfraction\nis not specified, it defaults to 1.0.\nIf\n.L2::secondary_priority\nis not specified, then it defaults to\n.L2::evict_unchanged\n.\nThe access property created using the CUDA APIs can be converted into cache eviction policy by the\ninstruction\ncreatepolicy.cvt\n. The source operand\naccess-property\nis a 64-bit opaque\nregister. Refer to\nCUDA programming guide\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.4.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\ncreatepolicy.fractional.L2::evict_last.b64                      policy, 1.0;\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64  policy, 0.5;\n\ncreatepolicy.range.L2::evict_last.L2::evict_first.b64\n                                            policy, [ptr], 0x100000, 0x200000;\n\n// access-prop is created by CUDA APIs.\ncreatepolicy.cvt.L2.b64 policy, access-prop;\n9.7.9.19.\nData Movement and Conversion Instructions:\nisspacep\n\nisspacep\nQuery whether a generic address falls within a specified state space window.\nSyntax\nisspacep.space  p, a;    // result is .pred\n\n.space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\nDescription\nWrite predicate register\np\nwith\n1\nif generic address a falls within the specified state\nspace window and with\n0\notherwise. Destination\np\nhas type\n.pred\n; the source address\noperand must be of type\n.u32\nor\n.u64\n.\nisspacep.param{::entry}\nreturns\n1\nif the generic address falls within the window of\nKernel Function Parameters\n, otherwise returns\n0\n. If\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::entry\n.\nisspacep.global\nreturns\n1\nfor\nKernel Function Parameters\nas\n.param\nwindow is contained within the\n.global\nwindow.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nNote\nispacep.shared::cluster\nwill return 1 for every shared memory address that is accessible to\nthe threads in the cluster, whereas\nispacep.shared::cta\nwill return 1 only if the address is\nof a variable declared in the executing CTA.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nisspacep.const\nintroduced in PTX ISA version 3.1.\nisspacep.param\nintroduced in PTX ISA version 7.7.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::entry\non\n.param\nspace introduced in PTX ISA version 8.3.\nTarget ISA Notes\nisspacep\nrequires\nsm_20\nor higher.\nisspacep.param{::entry}\nrequires\nsm_70\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nExamples\nisspacep.const           iscnst, cptr;\nisspacep.global          isglbl, gptr;\nisspacep.local           islcl,  lptr;\nisspacep.shared          isshrd, sptr;\nisspacep.param::entry    isparam, pptr;\nisspacep.shared::cta     isshrdcta, sptr;\nisspacep.shared::cluster ishrdany sptr;\n9.7.9.20.\nData Movement and Conversion Instructions:\ncvta\n\ncvta\nConvert address from\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space to generic, or vice-versa. Take the generic address of a variable declared in\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space.\nSyntax\n// convert const, global, local, or shared address to generic address\ncvta.space.size  p, a;        // source address in register a\ncvta.space.size  p, var;      // get generic address of var\ncvta.space.size  p, var+imm;  // generic address of var+offset\n\n// convert generic address to const, global, local, or shared address\ncvta.to.space.size  p, a;\n\n.space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };\n.size  = { .u32, .u64 };\nDescription\nConvert a\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\naddress to a generic address, or vice-versa. The\nsource and destination addresses must be the same size. Use\ncvt.u32.u64\nor\ncvt.u64.u32\nto\ntruncate or zero-extend addresses.\nFor variables declared in\n.const\n,\nKernel Function Parameters\n(\n.param\n),\n.global\n,\n.local\n, or\n.shared\nstate space, the generic address of the variable may be taken using\ncvta\n. The source is either a\nregister or a variable defined in\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\nmemory\nwith an optional offset.\nWhen converting a generic address into a\nconst\n,\nKernel Function Parameters\n(\n.param\n),\nglobal\n,\nlocal\n, or\nshared\naddress, the resulting address is undefined in cases where the generic address does not fall within\nthe address window of the specified state space. A program may use\nisspacep\nto guard against\nsuch incorrect behavior.\nFor\ncvta\nwith\n.shared\nstate space, the address must belong to the space specified by\n::cta\nor\n::cluster\nsub-qualifier, otherwise the behavior is undefined. If no sub-qualifier\nis specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nIf\n.param\nis specified without any sub-qualifiers then it defaults to\n.param::entry\n. For\n.param{::entry}\nstate space, operand\na\nmust be a kernel parameter address, otherwise\nbehavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\ncvta.const\nand\ncvta.to.const\nintroduced in PTX ISA version 3.1.\ncvta.param\nand\ncvta.to.param\nintroduced in PTX ISA version 7.7.\nNote:\nThe current implementation does not allow generic pointers to\nconst\nspace variables in\nprograms that contain pointers to constant buffers passed as kernel parameters.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::entry\non\n.param\nspace introduced in PTX ISA version 8.3.\nTarget ISA Notes\ncvta\nrequires\nsm_20\nor higher.\ncvta.param{::entry}\nand\ncvta.to.param{::entry}\nrequires\nsm_70\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nExamples\ncvta.const.u32   ptr,cvar;\ncvta.local.u32   ptr,lptr;\ncvta.shared::cta.u32  p,As+4;\ncvta.shared::cluster.u32 ptr, As;\ncvta.to.global.u32  p,gptr;\ncvta.param.u64   ptr,pvar;\ncvta.to.param::entry.u64  epptr, ptr;\n9.7.9.21.\nData Movement and Conversion Instructions:\ncvt\n\ncvt\nConvert a value from one type to another.\nSyntax\ncvt{.irnd}{.ftz}{.sat}.dtype.atype         d, a;  // integer rounding\ncvt{.frnd}{.ftz}{.sat}.dtype.atype         d, a;  // fp rounding\n\ncvt.frnd2{.relu}{.satfinite}.f16.f32       d, a;\ncvt.frnd2{.relu}{.satfinite}.f16x2.f32     d, a, b;\ncvt.rs{.relu}{.satfinite}.f16x2.f32        d, a, b, rbits;\n\ncvt.frnd2{.relu}{.satfinite}.bf16.f32      d, a;\ncvt.frnd2{.relu}{.satfinite}.bf16x2.f32    d, a, b;\ncvt.rs{.relu}{.satfinite}.bf16x2.f32       d, a, b, rbits;\n\ncvt.rna{.satfinite}.tf32.f32               d, a;\ncvt.frnd2{.satfinite}{.relu}.tf32.f32      d, a;\n\ncvt.rn.satfinite{.relu}.f8x2type.f32       d, a, b;\ncvt.rn.satfinite{.relu}.f8x2type.fp16x2    d, a;\ncvt.rn.{.relu}.f16x2.f8x2type              d, a;\ncvt.rs{.relu}.satfinite.f8x4type.f32       d, {a, b, e, f}, rbits;\n\ncvt.rn.satfinite{.relu}.f4x2type.f32        d, a, b;\ncvt.rn.satfinite{.relu}.f4x2type.fp16x2type d, a;\ncvt.rn{.relu}.f16x2.f4x2type                d, a;\ncvt.rs{.relu}.satfinite.f4x4type.f32        d, {a, b, e, f}, rbits;\n\ncvt.rn.satfinite{.relu}.f6x2type.f32        d, a, b;\ncvt.rn.satfinite{.relu}.f6x2type.fp16x2type d, a;\ncvt.rn{.relu}.f16x2.f6x2type                d, a;\ncvt.rs{.relu}.satfinite.f6x4type.f32        d, {a, b, e, f}, rbits;\n\ncvt.frnd3{.satfinite}.ue8m0x2.f32          d, a, b;\ncvt.frnd3{.satfinite}.ue8m0x2.bf16x2       d, a;\ncvt.rn.bf16x2.ue8m0x2                      d, a;\n\ncvt.rn.satfinite{.relu}{.scaled::n2::ue8m0}.s2f6x2.f32      d, a, b{, scale-factor};\ncvt.rn.satfinite{.relu}{.scaled::n2::ue8m0}.s2f6x2.bf16x2   d, a{, scale-factor};\ncvt.rn{.satfinite}{.relu}{.scaled::n2::ue8m0}.bf16x2.s2f6x2 d, a{, scale-factor};\n\n.irnd   = { .rni, .rzi, .rmi, .rpi };\n.frnd   = { .rn,  .rz,  .rm,  .rp  };\n.frnd2  = { .rn,  .rz };\n.frnd3  = { .rz,  .rp };\n.dtype = .atype = { .u8,   .u16, .u32, .u64,\n                    .s8,   .s16, .s32, .s64,\n                    .bf16, .f16, .f32, .f64 };\n.f8x2type = { .e4m3x2, .e5m2x2 };\n.f4x2type = { .e2m1x2 };\n.f6x2type = { .e2m3x2, .e3m2x2 };\n.f4x4type = { .e2m1x4 };\n.f8x4type = { .e4m3x4, .e5m2x4 };\n.f6x4type = { .e2m3x4, .e3m2x4 };\n.fp16x2type = { .f16x2, .bf16x2 };\nDescription\nConvert between different types and sizes.\nFor\n.f16x2\nand\n.bf16x2\ninstruction type, two inputs\na\nand\nb\nof\n.f32\ntype are\nconverted into\n.f16\nor\n.bf16\ntype and the converted values are packed in the destination\nregister\nd\n, such that the value converted from input\na\nis stored in the upper half of\nd\nand the value converted from input\nb\nis stored in the lower half of\nd\nFor\n.f16x2\ninstruction type, destination operand\nd\nhas\n.f16x2\nor\n.b32\ntype. For\n.bf16\ninstruction type, operand\nd\nhas\n.b16\ntype. For\n.bf16x2\ninstruction type,\noperand\nd\nhas\n.b32\ntype. For\n.tf32\ninstruction type, operand\nd\nhas\n.b32\ntype.\nWhen converting to\n.e4m3x2\n/\n.e5m2x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When converting two\n.f32\ninputs to\n.e4m3x2\n/\n.e5m2x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nand the value converted from\ninput\nb\nis stored in the lower 8 bits of\nd\n. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e4m3x2\n/\n.e5m2x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified\nformat. The converted values are packed in the destination operand\nd\nsuch that the value\nconverted from the upper 16 bits of input\na\nis stored in the upper 8 bits of\nd\nand the value\nconverted from the lower 16 bits of input\na\nis stored in the lower 8 bits of\nd\n.\nWhen converting from\n.e4m3x2\n/\n.e5m2x2\nto\n.f16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit input value in operand\na\nis converted to\n.f16\ntype. The converted values\nare packed in the destination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in the upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the lower 16 bits of\nd\n.\nWhen converting to\n.e2m1x2\ndata formats, the destination operand\nd\nhas\n.b8\ntype.\nWhen converting two\n.f32\ninputs to\n.e2m1x2\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom input\na\nis stored in the upper 4 bits of\nd\nand the value converted from input\nb\nis\nstored in the lower 4 bits of\nd\n. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e2m1x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified format.\nThe converted values are packed in\nd\nso that the value from the upper 16 bits of\na\nis stored\nin the upper 4 bits of\nd\n, and the value from the lower 16 bits of\na\nis stored in the lower 4\nbits of\nd\n.\nWhen converting from\n.e2m1x2\nto\n.f16x2\n, source operand\na\nhas\n.b8\ntype. Each 4-bit\ninput value in operand\na\nis converted to\n.f16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 4 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 4 bits of\na\nis stored in the\nlower 16 bits of\nd\n.\nWhen converting to\n.e2m1x4\ndata format, the destination operand\nd\nhas\n.b16\ntype. When\nconverting four\n.f32\ninputs to\n.e2m1x4\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom inputs\na\n,\nb\n,\ne\n,\nf\nare stored in each 4 bits starting from upper bits of\nd\n.\nWhen converting to\n.e2m3x2\n/\n.e3m2x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When converting two\n.f32\ninputs to\n.e2m3x2\n/\n.e3m2x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nwith 2 MSB bits padded with\nzeros and the value converted from input\nb\nis stored in the lower 8 bits of\nd\nwith 2 MSB bits\npadded with zeros. When converting an\n.f16x2\n/\n.bf16x2\ninput to\n.e2m3x2\n/\n.e3m2x2\n, each\n.f16\n/\n.bf16\ninput from operand\na\nis converted to the specified format. The converted\nvalues are packed in the destination operand\nd\nso that the value from the upper 16 bits of input\na\nis stored in the upper 8 bits of\nd\nwith 2 MSB bits padded with zeros, and the value from\nthe lower 16 bits of input\na\nis stored in the lower 8 bits of\nd\nwith 2 MSB bits padded with\nzeros.\nWhen converting from\n.e2m3x2\n/\n.e3m2x2\nto\n.f16x2\n, source operand\na\nhas\n.b16\ntype.\nEach 8-bit input value with 2 MSB bits 0 in operand\na\nis converted to\n.f16\ntype. The converted\nvalues are packed in the destination operand\nd\nsuch that the value converted from the upper 8 bits\nof\na\nis stored in the upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the lower 16 bits of\nd\n.\nWhen converting to\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\ndata format, the destination\noperand\nd\nhas\n.b32\ntype. When converting four\n.f32\ninputs to\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\n, each input is converted to the specified format,\nand the converted values are packed in the destination operand\nd\nsuch that the value converted\nfrom inputs\na\n,\nb\n,\ne\n,\nf\nare stored in each 8 bits starting from upper bits of\nd\n.\nFor\n.e3m2x4\n/\n.e2m3x4\n, each 8-bit output will have 2 MSB bits padded with zeros.\nWhen converting to\n.ue8m0x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When\nconverting two\n.f32\nor two packed\n.bf16\ninputs to\n.ue8m0x2\n, each input is converted to the\nspecified format, and the converted values are packed in the destination operand\nd\nsuch that the\nvalue converted from input\na\nis stored in the upper 8 bits of\nd\nand the value converted from\ninput\nb\nis stored in the lower 8 bits of\nd\n.\nWhen converting from\n.ue8m0x2\nto\n.bf16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit\ninput value in operand\na\nis converted to\n.bf16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the\nlower 16 bits of\nd\n.\nWhen converting to\n.s2f6x2\ndata formats, the destination operand\nd\nhas\n.b16\ntype. When\nconverting two\n.f32\ninputs to\n.s2f6x2\n, each input is converted to the\n.s2f6\nvalue, and\nthe converted values are packed in the destination operand\nd\nsuch that the value converted from\ninput\na\nis stored in the upper 8 bits of\nd\nand the value converted from input\nb\nis\nstored in the lower 8 bits of\nd\n. When converting from\n.bf16x2\n, each input packed in operand\na\nis converted to\n.s2f6\nvalue and converted values are packed in the destination operand\nd\nsuch that value converted from upper 8 bits of input\na\nis stored in upper 8 bits of\noperand\nd\nand the value converted from lower 8 bits of input\na\nis stored in lower 8 bits of\noperand\nd\n. Optional operand\nscale-factor\nhas type\n.b16\nand stores two packed scaling\nfactors of type\n.ue8m0\n. For down conversion, inputs are divided by\nscale-factor\nand then\nconversion is performed. The scaling factor of input\na\n/\n.bf16\ninput from upper 16 bits of\na\nis stored in the upper 8 bits of operand\nscale-factor\nand the scaling factor of input\nb\n/\n.bf16\ninput from lower 16 bits of\na\nis stored in the lower 8 bits of operand\nscale-factor\n.\nIf not specified explicitly, value of\nscale-factor\nwill be assumed to be\n0x7f7f\n,\nthat is, assumed as value\n1\nfor both input scale factors.\nWhen converting from\n.s2f6x2\nto\n.bf16x2\n, source operand\na\nhas\n.b16\ntype. Each 8-bit\ninput value in operand\na\nis converted to\n.bf16\ntype. The converted values are packed in the\ndestination operand\nd\nsuch that the value converted from the upper 8 bits of\na\nis stored in\nthe upper 16 bits of\nd\nand the value converted from the lower 8 bits of\na\nis stored in the\nlower 16 bits of\nd\n. Optional operand\nscale-factor\nhas type\n.b16\nand stores two packed\nscaling factors of type\n.ue8m0\n. For up-conversion, inputs are converted to destination type and\nthen multiplied by\nscale-factor\n. The scaling factor of\n.s2f6\ninput from upper 8 bits of\na\nis stored in the upper 8 bits of operand\nscale-factor\nand the scaling factor of\n.s2f6\ninput\nfrom lower 8 bits of\na\nis stored in the lower 8 bits of operand\nscale-factor\n.\nIf not specified explicitly, value of\nscale-factor\nwill be assumed to\nbe\n0x7f7f\n, that is, assumed as value\n1\nfor both input scale factors.\nOptional qualifier\n.scaled::n2::ue8m0\nspecifies that the instruction uses packed\nscale-factor with 2 scale values of\nue8m0\ntype. Operand\nscale-factor\nand qualifier\n.scaled::n2::ue8m0\nmust be used together.\nrbits\nis a\n.b32\ntype register operand used for providing random bits for\n.rs\nrounding mode.\nWhen converting to\n.f16x2\n, two 16-bit values are provided from\nrbits\nwhere 13 LSBs from\nupper 16-bits are used as random bits for operand\na\nwith 3 MSBs are 0 and 13 LSBs from lower\n16-bits are used as random bits for operand\nb\nwith 3 MSBs are 0.\nWhen converting to\n.bf16x2\n, two 16-bit values are provided from\nrbits\nwhere upper 16-bits\nare used as random bits for operand\na\nand lower 16-bits are used as random bits for operand\nb\n.\nWhen converting to\n.e4m3x4\n/\n.e5m2x4\n/\n.e2m3x4\n/\n.e3m2x4\n, two 16-bit values are provided\nfrom\nrbits\nwhere lower 16-bits are used for operands\ne\n,\nf\nand upper 16 bits are used\nfor operands\na\n,\nb\n.\nWhen converting to\n.e2m1x4\n, two 16-bit values are provided from\nrbits\nwhere lower 8-bits\nfrom both 16-bits half of\nrbits\nare used for operands\ne\n,\nf\nand upper 8-bits from both\n16-bits half of\nrbits\nare used for operands\na\n,\nb\n.\nRounding modifier is mandatory in all of the following cases:\nfloat-to-float conversions, when destination type is smaller than source type\nAll float-to-int conversions\nAll int-to-float conversions\nAll conversions involving\n.f16x2\n,\n.e4m3x2,\n.e5m2x2,\n,\n.bf16x2\n,\n.tf32\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\nand\n.ue8m0x2\ninstruction types.\n.satfinite\nmodifier is only supported for conversions involving the following types:\n.e4m3x2\n,\n.e5m2x2\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\ndestination types.\n.satfinite\nmodifier is mandatory for such conversions.\n.f16\n,\n.bf16\n,\n.f16x2\n,\n.bf16x2\n,\n.tf32\n,\n.ue8m0x2\nas destination types.\nSemantics\nif (/* inst type is .f16x2 or .bf16x2 */) {\n    d[31:16] = convert(a);\n    d[15:0]  = convert(b);\n} else if (/* inst destination type is .e5m2x2 or .e4m3x2 or .ue8m0x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:8] = convert(a);\n        d[7:0]  = convert(b);\n    } else {\n        d[15:8] = convert(a[31:16]);\n        d[7:0]  = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .s2f6x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:8] = convert(a / scale-factor[15:8]);\n        d[7:0]  = convert(b / scale-factor[7:0]);\n    } else {\n        d[15:8] = convert(a[15:8] / scale-factor[15:8]);\n        d[7:0]  = convert(a[7:0] / scale-factor[7:0]);\n    }\n} else if (/* inst source type is .s2f6x2 */) {\n        d[31:16] = convert(a[15:8]) * scale-factor[15:8];\n        d[15:0]  = convert(a[7:0]) * scale-factor[7:0];\n} else if (/* inst destination type is .e2m1x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[7:4] = convert(a);\n        d[3:0] = convert(b);\n    } else {\n        d[7:4] = convert(a[31:16]);\n        d[3:0]  = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .e2m3x2 or .e3m2x2 */) {\n    if (/* inst source type is .f32 */) {\n        d[15:14] = 0;\n        d[13:8] = convert(a);\n        d[7:6] = 0;\n        d[5:0] = convert(b);\n    } else {\n        d[15:14] = 0;\n        d[13:8] = convert(a[31:16]);\n        d[7:6] = 0;\n        d[5:0] = convert(a[15:0]);\n    }\n} else if (/* inst destination type is .e2m1x4 */) {\n    d[15:12] = convert(a);\n    d[11:8] = convert(b);\n    d[7:4] = convert(e);\n    d[3:0] = convert(f);\n} else if (/* inst destination type is .e4m3x4 or .e5m2x4 */) {\n    d[31:24] = convert(a);\n    d[23:16] = convert(b);\n    d[15:8] = convert(e);\n    d[7:0] = convert(f);\n} else if (/* inst destination type is .e2m3x4 or .e3m2x4 */) {\n    d[31:30] = 0;\n    d[29:24] = convert(a);\n    d[23:22] = 0;\n    d[21:16] = convert(b);\n    d[15:14] = 0;\n    d[13:8] = convert(e);\n    d[7:6] = 0;\n    d[5:0] = convert(f);\n} else {\n    d = convert(a);\n}\n// Random bits\nrbits\nsemantics for\n.rs\nrounding:\nDestination type\n.f16\n:\nRefer\nFigure 38\nfor random bits layout details.\nFigure 38\nRandom bits layout for\n.rs\nrounding with\n.f16\ndestination type\n\nDestination type\n.bf16\n:\nRefer\nFigure 39\nfor random bits layout details.\nFigure 39\nRandom bits layout for\n.rs\nrounding with\n.bf16\ndestination type\n\nDestination type\n.e2m1x4\n:\nRefer\nFigure 40\nfor random bits layout details.\nFigure 40\nRandom bits layout for\n.rs\nrounding with\n.e2m1x4\ndestination type\n\nDestination type\n.e5m2x4\n,\n.e4m3x4\n,\n.e3m2x4\n,\n.e2m3x4\n:\nRefer\nFigure 41\nfor random bits layout details.\nFigure 41\nRandom bits layout for\n.rs\nrounding with\n.e5m2x4\n/\n.e4m3x4\n/\n.e3m2x4\n/\n.e2m3x4\ndestination type\n\nInteger Notes\nInteger rounding is required for float-to-integer conversions, and for same-size float-to-float\nconversions where the value is rounded to an integer. Integer rounding is illegal in all other\ninstances.\nInteger rounding modifiers:\n.rni\nround to nearest integer, choosing even integer if source is equidistant between two integers\n.rzi\nround to nearest integer in the direction of zero\n.rmi\nround to nearest integer in direction of negative infinity\n.rpi\nround to nearest integer in direction of positive infinity\nIn float-to-integer conversions, depending upon conversion types,\nNaN\ninput results in following\nvalue:\nZero if source is not\n.f64\nand destination is not\n.s64\n,\n.u64\n.\nOtherwise 1 << (BitWidth(dst) - 1) corresponding to the value of (\nMAXINT\n>> 1) + 1 for unsigned type\nor\nMININT\nfor signed type.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported.\nFor\ncvt.ftz.dtype.f32\nfloat-to-integer conversions and\ncvt.ftz.f32.f32\nfloat-to-float\nconversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier\n.ftz\ncan only be specified when either\n.dtype\nor\n.atype\nis\n.f32\nand applies only\nto single precision (\n.f32\n) inputs and results.\nsm_1x\nFor\ncvt.ftz.dtype.f32\nfloat-to-integer conversions and\ncvt.ftz.f32.f32\nfloat-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving\nzero. The optional\n.ftz\nmodifier may be specified in these cases for clarity.\nNote:\nIn PTX ISA versions 1.4 and earlier, the\ncvt\ninstruction did not flush single-precision\nsubnormal inputs or results to zero if the destination type size was 64-bits. The compiler will\npreserve this behavior for legacy PTX code.\nSaturation modifier:\n.sat\nFor integer destination types,\n.sat\nlimits the result to\nMININT..MAXINT\nfor the size of\nthe operation. Note that saturation applies to both signed and unsigned integer types.\nThe saturation modifier is allowed only in cases where the destination type’s value range is not\na superset of the source type’s value range; i.e., the\n.sat\nmodifier is illegal in cases\nwhere saturation is not possible based on the source and destination types.\nFor float-to-integer conversions, the result is clamped to the destination range by default; i.e,\n.sat\nis redundant.\nFloating Point Notes\nFloating-point rounding is required for float-to-float conversions that result in loss of precision,\nand for integer-to-float conversions. Floating-point rounding is illegal in all other instances.\nFloating-point rounding modifiers:\n.rn\nrounding to nearest, with ties to even\n.rna\nrounding to nearest, with ties away from zero\n.rz\nrounding toward zero\n.rm\nrounding toward negative infinity\n.rp\nrounding toward positive infinity\n.rs\nStochastic rounding is achieved through the use of the supplied random bits. Operation’s result\nis rounded in the direction toward zero or away from zero based on the carry out of the integer\naddition of the supplied random bits (\nrbits\n) to the truncated off (discarded) bits of\nmantissa from the input.\nA floating-point value may be rounded to an integral value using the integer rounding modifiers (see\nInteger Notes). The operands must be of the same size. The result is an integral value, stored in\nfloating-point format.\nSubnormal numbers:\nsm_20+\nBy default, subnormal numbers are supported. Modifier\n.ftz\nmay be specified to flush\nsingle-precision subnormal inputs and results to sign-preserving zero. Modifier\n.ftz\ncan only\nbe specified when either\n.dtype\nor\n.atype\nis\n.f32\nand applies only to single\nprecision (\n.f32\n) inputs and results.\nsm_1x\nSingle-precision subnormal inputs and results are flushed to sign-preserving zero. The optional\n.ftz\nmodifier may be specified in these cases for clarity.\nNote:\nIn PTX ISA versions 1.4 and earlier, the\ncvt\ninstruction did not flush\nsingle-precision subnormal inputs or results to zero if either source or destination type was\n.f64\n. The compiler will preserve this behavior for legacy PTX code. Specifically, if the PTX\nISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to\nsign-preserving zero only for\ncvt.f32.f16\n,\ncvt.f16.f32\n, and\ncvt.f32.f32\ninstructions.\nSaturation modifier:\n.sat\n:\nFor floating-point destination types,\n.sat\nlimits the result to the range [0.0, 1.0].\nNaN\nresults are flushed to positive zero. Applies to\n.f16\n,\n.f32\n, and\n.f64\ntypes.\n.relu\n:\nFor\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.e4m3x2\n,\n.e5m2x2\n,\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\nand\n.tf32\ndestination types,\n.relu\nclamps the result to 0 if negative.\nNaN\nresults are converted\nto canonical\nNaN\n.\n.satfinite\n:\nFor\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.e4m3x2\n,\n.e5m2x2\n,\n.ue8m0x2\n,\n.e4m3x4\n,\n.e5m2x4\nand\n.tf32\ndestination formats, if the input value is\nNaN\n, then the result is\nNaN\nin the specified destination format. For\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e2m1x4\n,\n.e2m3x4\n,\n.e3m2x4\n,\n.s2f6x2\ndestination formats\nNaN\nresults are converted to positive\nMAX_NORM\n.\nIf the absolute value of input (ignoring sign) is greater than\nMAX_NORM\nof the specified destination\nformat, then the result is sign-preserved\nMAX_NORM\nof the destination format and a positive\nMAX_NORM\nin\n.ue8m0x2\nfor which the destination sign is not supported.\nNotes\nA source register wider than the specified type may be used, except when the source operand has\n.bf16\nor\n.bf16x2\nformat. The lower\nn\nbits corresponding to the instruction-type width\nare used in the conversion. See\nOperand Size Exceeding Instruction-Type Size\nfor a description of these relaxed\ntype-checking rules.\nA destination register wider than the specified type may be used, except when the destination\noperand has\n.bf16\n,\n.bf16x2\nor\n.tf32\nformat. The result of conversion is sign-extended to\nthe destination register width for signed integers, and is zero-extended to the destination register\nwidth for unsigned, bit-size, and floating-point types. See\nOperand Size Exceeding Instruction-Type Size\nfor a description of these relaxed\ntype-checking rules.\nFor\ncvt.f32.bf16\n,\nNaN\ninput yields unspecified\nNaN\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\n.relu\nmodifier and {\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.tf32\n} destination formats\nintroduced in PTX ISA version 7.0.\ncvt.f32.bf16\nintroduced in PTX ISA version 7.1.\ncvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}\n,\ncvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16\n, and\ncvt.tf32.f32.{relu}.{rn/rz}\nintroduced\nin PTX ISA version 7.8.\n.ftz\nqualifier for\ncvt.f32.bf16\nintroduced in PTX ISA version 7.8.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nfor\nsm_90\nor higher introduced in PTX ISA version 7.8.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nfor\nsm_90\nor higher introduced in PTX ISA version 7.8.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nfor\nsm_89\nintroduced in PTX ISA version 8.1.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nfor\nsm_89\nintroduced in PTX ISA version 8.1.\ncvt.satfinite.{f16,\nbf16,\nf16x2,\nbf16x2,\ntf32}.f32\nintroduced in PTX ISA version 8.1.\ncvt.{rn/rz}.satfinite.tf32.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}\nintroduced in PTX ISA version 8.6.\ncvt.{rp/rz}{.satfinite}{.relu}.ue8m0x2.bf16x2\nintroduced in PTX ISA version 8.6.\ncvt.{rz/rp}.satfinite.ue8m0x2.f32\nintroduced in PTX ISA version 8.6.\ncvt.rn.bf16x2.ue8m0x2\nintroduced in PTX ISA version 8.6.\n.rs\nrounding mode introduced in PTX ISA version 8.7.\ncvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32\nintroduced in PTX ISA version 8.7.\ncvt.rn.satfinite{.relu}{.e5m2x2/.e4m3x2}{.bf16x2}\nintroduced in PTX ISA version 9.1.\ncvt.rn.satfinite{.relu}{.e2m3x2/.e3m2x2/.e2m1x2}{.f16x2/.bf16x2}\nintroduced in PTX ISA\nversion 9.1.\ncvt\nwith\n.s2f6x2\ninstruction type introduced in PTX ISA version 9.1.\nTarget ISA Notes\ncvt\nto or from\n.f64\nrequires\nsm_13\nor higher.\n.relu\nmodifier and {\n.f16x2\n,\n.bf16\n,\n.bf16x2\n,\n.tf32\n} destination formats require\nsm_80\nor higher.\ncvt.f32.bf16\nrequires\nsm_80\nor higher.\ncvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}\n,\ncvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16\n, and\ncvt.tf32.f32.{relu}.{rn/rz}\nrequire\nsm_90\nor higher.\n.ftz\nqualifier for\ncvt.f32.bf16\nrequires\nsm_90\nor higher.\ncvt\nwith\n.e4m3x2\n/\n.e5m2x2\nrequires\nsm89\nor higher.\ncvt.satfinite.{e4m3x2,\ne5m2x2}.{f32,\nf16x2}\nrequires\nsm_89\nor higher.\ncvt.{rn/rz}.satfinite.tf32.f32\nrequires\nsm_100\nor higher.\ncvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.{rz/rp}{.satfinite}{.relu}.ue8m0x2.bf16x2\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.{rz/rp}.satfinite.ue8m0x2.f32\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\ncvt.rn.bf16x2.ue8m0x2\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.rs\nrounding mode is supported on following architectures:\nsm_100a\nsm_103a\ncvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32\nis supported on following architectures:\nsm_100a\nsm_103a\ncvt.rn.satfinite{.relu}{.e5m2x2/.e4m3x2}{.bf16x2}\nis supported on following family-specific\narchitectures:\nsm_100f\nor higher in the same family\nsm_110f\nor higher in the same family\nsm_120f\nor higher in the same family\ncvt.rn.satfinite{.relu}{.e2m3x2/.e3m2x2/.e2m1x2}{.f16x2/.bf16x2}\nis supported on following\nfamily-specific architectures:\nsm_100f\nor higher in the same family\nsm_110f\nor higher in the same family\nsm_120f\nor higher in the same family\ncvt\nwith\n.s2f6x2\ninstruction type is supported on following architectures:\nsm_100a\nsm_103a\nsm_110a\nsm_120a\nsm_121a\nExamples\ncvt.f32.s32 f,i;\ncvt.s32.f64 j,r;     // float-to-int saturates by default\ncvt.rni.f32.f32 x,y; // round to nearest int, result is fp\ncvt.f32.f32 x,y;     // note .ftz behavior for sm_1x targets\ncvt.rn.relu.f16.f32      b, f;        // result is saturated with .relu saturation mode\ncvt.rz.f16x2.f32         b1, f, f1;   // convert two fp32 values to packed fp16 outputs\ncvt.rn.relu.satfinite.f16x2.f32    b1, f, f1;   // convert two fp32 values to packed fp16 outputs with .relu saturation on each output\ncvt.rn.bf16.f32          b, f;        // convert fp32 to bf16\ncvt.rz.relu.satfinite.bf16.f3 2    b, f;        // convert fp32 to bf16 with .relu and .satfinite saturation\ncvt.rz.satfinite.bf16x2.f32        b1, f, f1;   // convert two fp32 values to packed bf16 outputs\ncvt.rn.relu.bf16x2.f32   b1, f, f1;   // convert two fp32 values to packed bf16 outputs with .relu saturation on each output\ncvt.rna.satfinite.tf32.f32         b1, f;       // convert fp32 to tf32 format\ncvt.rn.relu.tf32.f32     d, a;        // convert fp32 to tf32 format\ncvt.f64.bf16.rp          f, b;        // convert bf16 to f64 format\ncvt.bf16.f16.rz          b, f         // convert f16 to bf16 format\ncvt.bf16.u64.rz          b, u         // convert u64 to bf16 format\ncvt.s8.bf16.rpi          s, b         // convert bf16 to s8 format\ncvt.bf16.bf16.rpi        b1, b2       // convert bf16 to corresponding int represented in bf16 format\ncvt.rn.satfinite.e4m3x2.f32 d, a, b;  // convert a, b to .e4m3 and pack as .e4m3x2 output\ncvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu\n                                         // saturation on each output and pack as .e5m2x2\ncvt.rn.f16x2.e4m3x2 d, a;             // unpack a, convert two .e4m3 values to packed f16x2 output\ncvt.rn.satfinite.tf32.f32 d, a;       // convert fp32 to tf32 format\ncvt.rn.relu.f16x2.e2m1x2 d, a;        // unpack a, convert two .e2m1 values to packed f16x2 output\ncvt.rn.satfinite.e2m3x2.f32 d, a, b;  // convert a, b to .e2m3 and pack as .e2m3x2 output\ncvt.rn.relu.f16x2.e3m2x2 d, a;        // unpack a, convert two .e3m2 values to packed f16x2 output\n\ncvt.rs.f16x2.f32    d, a, b, rbits;  // convert 2 fp32 values to packed fp16 with applying .rs rounding\ncvt.rs.satfinite.e2m1x4.f32  d, {a, b, e, f}, rbits; // convert 4 fp32 values to packed 4 e2m1 values with applying .rs rounding\n\ncvt.rn.satfinite.relu.e2m1x2type.f16x2  d, a; // unpack a and covert to two .e2m1 values\ncvt.rn.satfinite.e2m3x2type.bf16x2  d, a; // unpack a and covert to two .e2m3 values\n\n// Convert 2 f32 values to s2f6 after applying scale factor for dividing the value\ncvt.rn.satfinite.scaled::n2::ue8m0.s2f6x2.f32 d, a, b, scale-factor;\n9.7.9.22.\nData Movement and Conversion Instructions:\ncvt.pack\n\ncvt.pack\nConvert two integer values from one integer type to another and pack the results.\nSyntax\ncvt.pack.sat.convertType.abType  d, a, b;\n    .convertType  = { .u16, .s16 }\n    .abType       = { .s32 }\n\ncvt.pack.sat.convertType.abType.cType  d, a, b, c;\n    .convertType  = { .u2, .s2, .u4, .s4, .u8, .s8 }\n    .abType       = { .s32 }\n    .cType        = { .b32 }\nDescription\nConvert two 32-bit integers\na\nand\nb\ninto specified type and pack the results into\nd\n.\nDestination\nd\nis an unsigned 32-bit integer. Source operands\na\nand\nb\nare integers of\ntype\n.abType\nand the source operand\nc\nis an integer of type\n.cType\n.\nThe inputs\na\nand\nb\nare converted to values of type specified by\n.convertType\nwith\nsaturation and the results after conversion are packed into lower bits of\nd\n.\nIf operand\nc\nis specified then remaining bits of\nd\nare copied from lower bits of\nc\n.\nSemantics\nta = a < MIN(convertType) ? MIN(convertType) : a;\nta = a > MAX(convertType) ? MAX(convertType) : a;\ntb = b < MIN(convertType) ? MIN(convertType) : b;\ntb = b > MAX(convertType) ? MAX(convertType) : b;\n\nsize = sizeInBits(convertType);\ntd = tb ;\nfor (i = size; i <= 2 * size - 1; i++) {\n    td[i] = ta[i - size];\n}\n\nif (isU16(convertType) || isS16(convertType)) {\n    d = td;\n} else {\n    for (i = 0; i < 2 * size; i++) {\n        d[i] = td[i];\n    }\n    for (i = 2 * size; i <= 31; i++) {\n        d[i] = c[i - 2 * size];\n    }\n}\n.sat\nmodifier limits the converted values to\nMIN(convertType)\n..\nMAX(convertedType)\n(no\noverflow) if the corresponding inputs are not in the range of datatype specified as\n.convertType\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nTarget ISA Notes\nRequires\nsm_72\nor higher.\nSub byte types (\n.u4\n/\n.s4\nand\n.u2\n/\n.s2\n) requires\nsm_75\nor higher.\nExamples\ncvt.pack.sat.s16.s32      %r1, %r2, %r3;           // 32-bit to 16-bit conversion\ncvt.pack.sat.u8.s32.b32   %r4, %r5, %r6, 0;        // 32-bit to 8-bit conversion\ncvt.pack.sat.u8.s32.b32   %r7, %r8, %r9, %r4;      // %r7 = { %r5, %r6, %r8, %r9 }\ncvt.pack.sat.u4.s32.b32   %r10, %r12, %r13, %r14;  // 32-bit to 4-bit conversion\ncvt.pack.sat.s2.s32.b32   %r15, %r16, %r17, %r18;  // 32-bits to 2-bit conversion\n9.7.9.23.\nData Movement and Conversion Instructions:\nmapa\n\nmapa\nMap the address of the shared variable in the target CTA.\nSyntax\nmapa{.space}.type          d, a, b;\n\n// Maps shared memory address in register a into CTA b.\nmapa.shared::cluster.type  d, a, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh, b;\n\n// Maps shared memory variable into CTA b.\nmapa.shared::cluster.type  d, sh + imm, b;\n\n// Maps generic address in register a into CTA b.\nmapa.type                  d, a, b;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 }\nDescription\nGet address in the CTA specified by operand\nb\nwhich corresponds to the address specified by\noperand\na\n.\nInstruction type\n.type\nindicates the type of the destination operand\nd\nand the source\noperand\na\n.\nWhen space is\n.shared::cluster\n, source\na\nis either a shared memory variable or a register\ncontaining a valid shared memory address and register\nd\ncontains a shared memory address. When\nthe optional qualifier\n.space\nis not specified, both\na\nand\nd\nare registers containing\ngeneric addresses pointing to shared memory.\nb\nis a 32-bit integer operand representing the rank of the target CTA.\nDestination register\nd\nwill hold an address in CTA\nb\ncorresponding to operand\na\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmapa.shared::cluster.u64 d1, %reg1, cta;\nmapa.shared::cluster.u32 d2, sh, 3;\nmapa.u64                 d3, %reg2, cta;\n9.7.9.24.\nData Movement and Conversion Instructions:\ngetctarank\n\ngetctarank\nGenerate the CTA rank of the address.\nSyntax\ngetctarank{.space}.type d, a;\n\n// Get cta rank from source shared memory address in register a.\ngetctarank.shared::cluster.type d, a;\n\n// Get cta rank from shared memory variable.\ngetctarank.shared::cluster.type d, var;\n\n// Get cta rank from shared memory variable+offset.\ngetctarank.shared::cluster.type d, var + imm;\n\n// Get cta rank from generic address of shared memory variable in register a.\ngetctarank.type d, a;\n\n.space = { .shared::cluster }\n.type  = { .u32, .u64 }\nDescription\nWrite the destination register\nd\nwith the rank of the CTA which contains the address specified\nin operand\na\n.\nInstruction type\n.type\nindicates the type of source operand\na\n.\nWhen space is\n.shared::cluster\n, source\na\nis either a shared memory variable or a register\ncontaining a valid shared memory address. When the optional qualifier\n.space\nis not specified,\na\nis a register containing a generic addresses pointing to shared memory. Destination\nd\nis\nalways a 32-bit register which holds the rank of the CTA.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ngetctarank.shared::cluster.u32 d1, addr;\ngetctarank.shared::cluster.u64 d2, sh + 4;\ngetctarank.u64                 d3, src;\n9.7.9.25.\nData Movement and Conversion Instructions: Asynchronous copy\n\nAn asynchronous copy operation performs the underlying operation asynchronously in the background,\nthus allowing the issuing threads to perform subsequent tasks.\nAn asynchronous copy operation can be a\nbulk\noperation that operates on a large amount of data, or\na\nnon-bulk\noperation that operates on smaller sized data. The amount of data handled by a bulk\nasynchronous operation must be a multiple of 16 bytes.\nAn asynchronous copy operation typically includes the following sequence:\nOptionally, reading from the tensormap.\nReading data from the source location(s).\nWriting data to the destination location(s).\nWrites being made visible to the executing thread or other threads.\n9.7.9.25.1.\nCompletion Mechanisms for Asynchronous Copy Operations\n\nA thread must explicitly wait for the completion of an asynchronous copy operation in order to\naccess the result of the operation. Once an asynchronous copy operation is initiated, modifying the\nsource memory location or tensor descriptor or reading from the destination memory location before\nthe asynchronous operation completes, exhibits undefined behavior.\nThis section describes two asynchronous copy operation completion mechanisms supported in PTX:\nAsync-group mechanism and mbarrier-based mechanism.\nAsynchronous operations may be tracked by either of the completion mechanisms or both mechanisms.\nThe tracking mechanism is instruction/instruction-variant specific.\n9.7.9.25.1.1.\nAsync-group mechanism\n\nWhen using the async-group completion mechanism, the issuing thread specifies a group of\nasynchronous operations, called\nasync-group\n, using a\ncommit\noperation and tracks the completion\nof this group using a\nwait\noperation. The thread issuing the asynchronous operation must create\nseparate\nasync-groups\nfor bulk and non-bulk asynchronous operations.\nA\ncommit\noperation creates a per-thread\nasync-group\ncontaining all prior asynchronous operations\ntracked by\nasync-group\ncompletion and initiated by the executing thread but none of the asynchronous\noperations following the commit operation. A committed asynchronous operation belongs to a single\nasync-group\n.\nWhen an\nasync-group\ncompletes, all the asynchronous operations belonging to that group are\ncomplete and the executing thread that initiated the asynchronous operations can read the result of\nthe asynchronous operations. All\nasync-groups\ncommitted by an executing thread always complete in\nthe order in which they were committed. There is no ordering between asynchronous operations within\nan\nasync-group\n.\nA typical pattern of using\nasync-group\nas the completion mechanism is as follows:\nInitiate the asynchronous operations.\nGroup the asynchronous operations into an\nasync-group\nusing a\ncommit\noperation.\nWait for the completion of the async-group using the wait operation.\nOnce the\nasync-group\ncompletes, access the results of all asynchronous operations in that\nasync-group\n.\n9.7.9.25.1.2.\nMbarrier-based mechanism\n\nA thread can track the completion of one or more asynchronous operations using the current phase of\nan\nmbarrier object\n. When the current phase of the\nmbarrier object\nis complete, it implies that\nall asynchronous operations tracked by this phase are complete, and all threads participating in\nthat\nmbarrier object\ncan access the result of the asynchronous operations.\nThe\nmbarrier object\nto be used for tracking the completion of an asynchronous operation can be\neither specified along with the asynchronous operation as part of its syntax, or as a separate\noperation. For a bulk asynchronous operation, the\nmbarrier object\nmust be specified in the\nasynchronous operation, whereas for non-bulk operations, it can be specified after the asynchronous\noperation.\nA typical pattern of using mbarrier-based completion mechanism is as follows:\nInitiate the asynchronous operations.\nSet up an\nmbarrier object\nto track the asynchronous operations in its current phase, either as\npart of the asynchronous operation or as a separate operation.\nWait for the\nmbarrier object\nto complete its current phase using\nmbarrier.test_wait\nor\nmbarrier.try_wait\n.\nOnce the\nmbarrier.test_wait\nor\nmbarrier.try_wait\noperation returns\nTrue\n, access the\nresults of the asynchronous operations tracked by the\nmbarrier object\n.\n9.7.9.25.2.\nAsync Proxy\n\nThe\ncp{.reduce}.async.bulk\noperations are performed in the\nasynchronous proxy\n(or\nasync\nproxy\n).\nAccessing the same memory location across multiple proxies needs a cross-proxy fence. For the\nasync proxy\n,\nfence.proxy.async\nshould be used to synchronize memory between\ngeneric\nproxy\nand the\nasync proxy\n.\nThe completion of a\ncp{.reduce}.async.bulk\noperation is followed by an implicit\ngeneric-async\nproxy fence. So the result of the asynchronous operation is made visible to the generic proxy as\nsoon as its completion is observed.\nAsync-group\nOR\nmbarrier-based\ncompletion mechanism must\nbe used to wait for the completion of the\ncp{.reduce}.async.bulk\ninstructions.\n9.7.9.25.3.\nData Movement and Conversion Instructions: Non-bulk copy\n\n9.7.9.25.3.1.\nData Movement and Conversion Instructions:\ncp.async\n\ncp.async\nInitiates an asynchronous copy operation from one state space to another.\nSyntax\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, src-size}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, src-size}{, cache-policy} ;\ncp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], cp-size{, ignore-src}{, cache-policy} ;\ncp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}\n                         [dst], [src], 16{, ignore-src}{, cache-policy} ;\n\n.level::cache_hint =     { .L2::cache_hint }\n.level::prefetch_size =  { .L2::64B, .L2::128B, .L2::256B }\ncp-size =                { 4, 8, 16 }\nDescription\ncp.async\nis a non-blocking instruction which initiates an asynchronous copy operation of data\nfrom the location specified by source address operand\nsrc\nto the location specified by\ndestination address operand\ndst\n. Operand\nsrc\nspecifies a location in the global state space\nand\ndst\nspecifies a location in the shared state space.\nOperand\ncp-size\nis an integer constant which specifies the size of data in bytes to be copied to\nthe destination\ndst\n.\ncp-size\ncan only be 4, 8 and 16.\nInstruction\ncp.async\nallows optionally specifying a 32-bit integer operand\nsrc-size\n. Operand\nsrc-size\nrepresents the size of the data in bytes to be copied from\nsrc\nto\ndst\nand must\nbe less than\ncp-size\n. In such case, remaining bytes in destination\ndst\nare filled with\nzeros. Specifying\nsrc-size\nlarger than\ncp-size\nresults in undefined behavior.\nThe optional and non-immediate predicate argument\nignore-src\nspecifies whether the data from the\nsource location\nsrc\nshould be ignored completely. If the source data is ignored then zeros will\nbe copied to destination\ndst\n. If the argument\nignore-src\nis not specified then it defaults\nto\nFalse\n.\nSupported alignment requirements and addressing modes for operand\nsrc\nand\ndst\nare described\nin\nAddresses as Operands\n.\nThe mandatory\n.async\nqualifier indicates that the\ncp\ninstruction will initiate the memory\ncopy operation asynchronously and control will return to the executing thread before the copy\noperation is complete. The executing thread can then use\nasync-group based completion mechanism\nor the\nmbarrier based completion mechanism\nto wait for completion of the asynchronous copy operation.\nNo other synchronization mechanism guarantees the completion of the asynchronous\ncopy operations.\nThere is no ordering guarantee between two\ncp.async\noperations if they are not explicitly\nsynchronized using\ncp.async.wait_all\nor\ncp.async.wait_group\nor\nmbarrier instructions\n.\nAs described in\nCache Operators\n, the\n.cg\nqualifier indicates\ncaching of data only at global level cache L2 and not at L1 whereas\n.ca\nqualifier indicates\ncaching of data at all levels including L1 cache. Cache operator are treated as performance hints\nonly.\ncp.async\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nThe\n.level::prefetch_size\nqualifier is a hint to fetch additional data of the specified size\ninto the respective cache level.The sub-qualifier\nprefetch_size\ncan be set to either of\n64B\n,\n128B\n,\n256B\nthereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes\nrespectively.\nThe qualifier\n.level::prefetch_size\nmay only be used with\n.global\nstate space and with\ngeneric addressing where the address points to\n.global\nstate space. If the generic address does\nnot fall within the address window of the global memory, then the prefetching behavior is undefined.\nThe\n.level::prefetch_size\nqualifier is treated as a performance hint only.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for\n.level::cache_hint\nand\n.level::prefetch_size\nqualifiers introduced in PTX ISA\nversion 7.4.\nSupport for\nignore-src\noperand introduced in PTX ISA version 7.5.\nSupport for sub-qualifier\n::cta\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nExamples\ncp.async.ca.shared.global  [shrd],    [gbl + 4], 4;\ncp.async.ca.shared::cta.global  [%r0 + 8], [%r1],     8;\ncp.async.cg.shared.global  [%r2],     [%r3],     16;\n\ncp.async.cg.shared.global.L2::64B   [%r2],      [%r3],     16;\ncp.async.cg.shared.global.L2::128B  [%r0 + 16], [%r1],     16;\ncp.async.cg.shared.global.L2::256B  [%r2 + 32], [%r3],     16;\n\ncreatepolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 0.25;\ncp.async.ca.shared.global.L2::cache_hint [%r2], [%r1], 4, cache-policy;\n\ncp.async.ca.shared.global                   [shrd], [gbl], 4, p;\ncp.async.cg.shared.global.L2::cache_hint   [%r0], [%r2], 16, q, cache-policy;\n9.7.9.25.3.2.\nData Movement and Conversion Instructions:\ncp.async.commit_group\n\ncp.async.commit_group\nCommits all prior initiated but uncommitted\ncp.async\ninstructions into a\ncp.async-group\n.\nSyntax\ncp.async.commit_group ;\nDescription\ncp.async.commit_group\ninstruction creates a new\ncp.async-group\nper thread and batches all\nprior\ncp.async\ninstructions initiated by the executing thread but not committed to any\ncp.async-group\ninto the new\ncp.async-group\n. If there are no uncommitted\ncp.async\ninstructions then\ncp.async.commit_group\nresults in an empty\ncp.async-group.\nAn executing thread can wait for the completion of all\ncp.async\noperations in a\ncp.async-group\nusing\ncp.async.wait_group\n.\nThere is no memory ordering guarantee provided between any two\ncp.async\noperations within the\nsame\ncp.async-group\n. So two or more\ncp.async\noperations within a\ncp.async-group\ncopying data\nto the same location results in undefined behavior.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example 1:\ncp.async.ca.shared.global [shrd], [gbl], 4;\ncp.async.commit_group ; // Marks the end of a cp.async group\n\n// Example 2:\ncp.async.ca.shared.global [shrd1],   [gbl1],   8;\ncp.async.ca.shared.global [shrd1+8], [gbl1+8], 8;\ncp.async.commit_group ; // Marks the end of cp.async group 1\n\ncp.async.ca.shared.global [shrd2],    [gbl2],    16;\ncp.async.cg.shared.global [shrd2+16], [gbl2+16], 16;\ncp.async.commit_group ; // Marks the end of cp.async group 2\n9.7.9.25.3.3.\nData Movement and Conversion Instructions:\ncp.async.wait_group\n/\ncp.async.wait_all\n\ncp.async.wait_group\n,\ncp.async.wait_all\nWait for completion of prior asynchronous copy operations.\nSyntax\ncp.async.wait_group N;\ncp.async.wait_all ;\nDescription\ncp.async.wait_group\ninstruction will cause executing thread to wait till only\nN\nor fewer of\nthe most recent\ncp.async-group\ns are pending and all the prior\ncp.async-group\ns committed by\nthe executing threads are complete. For example, when\nN\nis 0, the executing thread waits on all\nthe prior\ncp.async-group\ns to complete. Operand\nN\nis an integer constant.\ncp.async.wait_all\nis equivalent to :\ncp.async.commit_group;\ncp.async.wait_group 0;\nAn empty\ncp.async-group\nis considered to be trivially complete.\nWrites performed by\ncp.async\noperations are made visible to the executing thread only after:\nThe completion of\ncp.async.wait_all\nor\nThe completion of\ncp.async.wait_group\non the\ncp.async-group\nin which the\ncp.async\nbelongs to or\nmbarrier.test_wait\nreturns\nTrue\non an\nmbarrier object\nwhich is tracking the completion of the\ncp.async\noperation.\nThere is no ordering between two\ncp.async\noperations that are not synchronized with\ncp.async.wait_all\nor\ncp.async.wait_group\nor\nmbarrier objects\n.\ncp.async.wait_group\nand\ncp.async.wait_all\ndoes not provide any ordering and visibility\nguarantees for any other memory operation apart from\ncp.async\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example of .wait_all:\ncp.async.ca.shared.global [shrd1], [gbl1], 4;\ncp.async.cg.shared.global [shrd2], [gbl2], 16;\ncp.async.wait_all;  // waits for all prior cp.async to complete\n\n// Example of .wait_group :\ncp.async.ca.shared.global [shrd3], [gbl3], 8;\ncp.async.commit_group;  // End of group 1\n\ncp.async.cg.shared.global [shrd4], [gbl4], 16;\ncp.async.commit_group;  // End of group 2\n\ncp.async.cg.shared.global [shrd5], [gbl5], 16;\ncp.async.commit_group;  // End of group 3\n\ncp.async.wait_group 1;  // waits for group 1 and group 2 to complete\n9.7.9.25.4.\nData Movement and Conversion Instructions: Bulk copy\n\n9.7.9.25.4.1.\nData Movement and Conversion Instructions:\ncp.async.bulk\n\ncp.async.bulk\nInitiates an asynchronous copy operation from one state space to another.\nSyntax\n// global -> shared::cta\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, cache-policy}\n\n.dst =                  { .shared::cta }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n\n\n// global -> shared::cluster\ncp.async.bulk.dst.src.completion_mechanism{.multicast}{.level::cache_hint}\n                      [dstMem], [srcMem], size, [mbar] {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> shared::cluster\ncp.async.bulk.dst.src.completion_mechanism [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n\n\n// shared::cta -> global\ncp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}{.cp_mask}\n                      [dstMem], [srcMem], size {, cache-policy} {, byteMask}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint =    { .L2::cache_hint }\nDescription\ncp.async.bulk\nis a non-blocking instruction which initiates an asynchronous bulk-copy operation\nfrom the location specified by source address operand\nsrcMem\nto the location specified by\ndestination address operand\ndstMem\n.\nThe direction of bulk-copy is from the state space specified by the\n.src\nmodifier to the state\nspace specified by the\n.dst\nmodifiers.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied, in terms of number of\nbytes.\nsize\nmust be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined. The memory range\n[dstMem,\ndstMem\n+\nsize\n-\n1]\nmust not overflow the destination memory\nspace and the memory range\n[srcMem,\nsrcMem\n+\nsize\n-\n1]\nmust not overflow the source memory\nspace. Otherwise, the behavior is undefined. The addresses\ndstMem\nand\nsrcMem\nmust be aligned\nto 16 bytes.\nWhen the destination of the copy is\n.shared::cta\nthe destination address has to be in the shared\nmemory of the executing CTA within the cluster, otherwise the behavior is undefined.\nWhen the source of the copy is\n.shared::cta\nand the destination is\n.shared::cluster\n, the\ndestination has to be in the shared memory of a different CTA within the cluster.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\n.mbarrier::...\n.shared::cta\n.global\nmbarrier based\n.shared::cluster\n.global\n.shared::cluster\n.shared::cta\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.async.bulk\nvariant uses\nmbarrier based completion mechanism. The\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.bulk_group\nspecifies that the\ncp.async.bulk\nvariant uses\nbulk async-group\nbased completion mechanism.\nThe optional modifier\n.multicast::cluster\nallows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand\nctaMask\nspecifies the destination CTAs in the\ncluster such that each bit position in the 16-bit\nctaMask\noperand corresponds to the\n%ctaid\nof the destination CTA. The source data is multicast to the same CTA-relative offset as\ndstMem\nin the shared memory of each destination CTA. The mbarrier signal is also multicast to the same\nCTA-relative offset as\nmbar\nin the shared memory of the destination CTA.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nWhen the optional qualifier\n.cp_mask\nis specified, the argument\nbyteMask\nis required.\nThe i-th bit in the 16-bit wide\nbyteMask\noperand specifies whether the i-th byte of each 16-byte\nwide chunk of source data is copied to the destination. If the bit is set, the byte is copied.\nThe copy operation in\ncp.async.bulk\nis treated as a weak memory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nNotes\n.multicast::cluster\nqualifier is optimized for target architecture\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\nand may have substantially reduced performance on other\ntargets and hence\n.multicast::cluster\nis advised to be used with\n.target\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.shared::cta\nas destination state space is introduced in PTX ISA version 8.6.\nSupport for\n.cp_mask\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.multicast::cluster\nqualifier advised to be used with\n.target\nsm_90a\nor\nsm_100f\nor\nsm_100a\nor\nsm_103f\nor\nsm_103a\nor\nsm_110f\nor\nsm_110a\n.\nSupport for\n.cp_mask\nqualifier requires\nsm_100\nor higher.\nExamples\n// .global -> .shared::cta (strictly non-remote):\ncp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n// .global -> .shared::cluster:\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                                             [dstMem], [srcMem], size, [mbar], ctaMask;\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                                             [dstMem], [srcMem], size, [mbar], cache-policy;\n\n\n// .shared::cta -> .shared::cluster (strictly remote):\ncp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];\n\n// .shared::cta -> .global:\ncp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy;\n\n// .shared::cta -> .global with .cp_mask:\ncp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.cp_mask [dstMem], [srcMem], size, cache-policy, byteMask;\n9.7.9.25.4.2.\nData Movement and Conversion Instructions:\ncp.reduce.async.bulk\n\ncp.reduce.async.bulk\nInitiates an asynchronous reduction operation.\nSyntax\ncp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type\n              [dstMem], [srcMem], size, [mbar]\n\n.dst =                  { .shared::cluster }\n.src =                  { .shared::cta }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .b32, .u32, .s32, .b64, .u64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type\n               [dstMem], [srcMem], size{, cache-policy}\n\n.dst =                  { .global      }\n.src =                  { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.level::cache_hint    = { .L2::cache_hint }\n.redOp=                 { .and, .or, .xor,\n                          .add, .inc, .dec,\n                          .min, .max }\n.type =                 { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 }\n\n\ncp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type\n               [dstMem], [srcMem], size{, cache-policy}\n.dst  =                 { .global }\n.src  =                 { .shared::cta }\n.completion_mechanism = { .bulk_group }\n.type =                 { .f16, .bf16 }\nDescription\ncp.reduce.async.bulk\nis a non-blocking instruction which initiates an asynchronous reduction\noperation on an array of memory locations specified by the destination address operand\ndstMem\nwith the source array whose location is specified by the source address operand\nsrcMem\n. The size\nof the source and the destination array must be the same and is specified by the operand\nsize\n.\nEach data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier\n.redOp\n. The type of each\ndata element in the source and the destination array is specified by the modifier\n.type\n.\nThe source address operand\nsrcMem\nis located in the state space specified by\n.src\nand the\ndestination address operand\ndstMem\nis located in the state specified by the\n.dst\n.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied from the source location and\nused in the reduction operation, in terms of number of bytes.\nsize\nmust be a multiple of 16. If\nthe value is not a multiple of 16, then the behavior is undefined. The memory range\n[dstMem,\ndstMem\n+\nsize\n-\n1]\nmust not overflow the destination memory space and the memory range\n[srcMem,\nsrcMem\n+\nsize\n-\n1]\nmust not overflow the source memory space. Otherwise, the behavior is\nundefined. The addresses\ndstMem\nand\nsrcMem\nmust be aligned to 16 bytes.\nThe operations supported by\n.redOp\nare classified as follows:\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n, and\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..x]\nwhere\nx\nis the value at the source\nstate space.\nThe floating point operation\n.add\nrounds to the nearest even. The current implementation of\ncp.reduce.async.bulk.add.f32\nflushes subnormal inputs and results to sign-preserving zero. The\ncp.reduce.async.bulk.add.f16\nand\ncp.reduce.async.bulk.add.bf16\noperations require\n.noftz\nqualifier. It preserves input and result subnormals, and does not flush them to zero.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.dst\n.redOp\nElement type\n.shared::cluster\n.add\n.u32\n,\n.s32\n,\n.u64\n.min\n,\n.max\n.u32\n,\n.s32\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n.global\n.add\n.u32\n,\n.s32\n,\n.u64\n,\n.f32\n,\n.f64\n,\n.f16\n,\n.bf16\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n,\n.f16\n,\n.bf16\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\n.mbarrier::...\n.shared::cluster\n.global\nmbarrier based\n.shared::cluster\n.shared::cta\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.reduce.async.bulk\nvariant\nuses mbarrier based completion mechanism. The\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.bulk_group\nspecifies that the\ncp.reduce.async.bulk\nvariant uses\nbulk\nasync-group\nbased completion mechanism.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nEach reduction operation performed by the\ncp.reduce.async.bulk\nhas individually\n.relaxed.gpu\nmemory ordering semantics. The load operations in\ncp.reduce.async.bulk\nare treated as weak\nmemory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32\n                                                                  [dstMem], [srcMem], size, [mbar];\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy;\n\ncp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;\n9.7.9.25.4.3.\nData Movement and Conversion Instructions:\ncp.async.bulk.prefetch\n\ncp.async.bulk.prefetch\nProvides a hint to the system to initiate the asynchronous prefetch of data to the cache.\nSyntax\ncp.async.bulk.prefetch.L2.src{.level::cache_hint}   [srcMem], size {, cache-policy}\n\n.src =                { .global }\n.level::cache_hint =  { .L2::cache_hint }\nDescription\ncp.async.bulk.prefetch\nis a non-blocking instruction which may initiate an asynchronous prefetch\nof data from the location specified by source address operand\nsrcMem\n, in\n.src\nstatespace, to\nthe L2 cache.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be prefetched in terms of number of\nbytes.\nsize\nmust be a multiple of 16. If the value is not a multiple of 16, then the behavior is\nundefined.  The address\nsrcMem\nmust be aligned to 16 bytes.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\ncp.async.bulk.prefetch\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.prefetch.L2.global                 [srcMem], size;\n\ncp.async.bulk.prefetch.L2.global.L2::cache_hint  [srcMem], size, policy;\n9.7.9.26.\nData Movement and Conversion Instructions:\nmultimem.cp.async.bulk\n\nmultimem.cp.async.bulk\nInitiates an asynchronous copy operation to a multimem address range.\nSyntax\nmultimem.cp.async.bulk.dst.src.completion_mechanism{.cp_mask}\n    [dstMem], [srcMem], size{, byteMask};\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\nDescription\nInstruction\nmultimem.cp.async.bulk\ninitiates an asynchronous bulk-copy operation from source\naddress range\n[srcMem,\nsrcMem\n+\nsize)\nto memory locations residing on each GPU’s memory referred\nto by the destination multimem address range\n[dstMem,\ndstMem\n+\nsize)\n. The direction of\nbulk-copy is from the state space specified by the\n.src\nmodifier to the state space specified\nby the\n.dst\nmodifiers.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied, in terms of number of\nbytes. Operand\nsize\nmust be a multiple of\n16\n. The memory range\n[dstMem,\ndstMem\n+\nsize)\nmust not overflow the destination multimem memory space. The memory range\n[srcMem,\nsrcMem\n+\nsize)\nmust not overflow the source memory space. The addresses\ndstMem\nand\nsrcMem\nmust be aligned\nto\n16\nbytes. If any of these pre-conditions is not met, the behavior is undefined.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported by the\ninstruction. The modifier\n.bulk_group\nspecifies that the\nmultimem.cp.async.bulk\ninstruction\nuses bulk async-group based completion mechanism.\nWhen the optional modifier\n.cp_mask\nis specified, the argument\nbyteMask\nis required. The\ni-th bit in the 16-bit wide\nbyteMask\noperand specifies whether the i-th byte of each 16-byte\nwide chunk of source data is copied to the destination. If the bit is set, the byte is copied.\nThe reads and writes of the copy operation in\nmultimem.cp.async.bulk\nare weak memory operations\nas described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nSupport for\n.cp_mask\nqualifier requires\nsm_100\nor higher.\nExamples\nmultimem.cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;\n\nmultimem.cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], 512;\n\nmultimem.cp.async.bulk.global.shared::cta.bulk_group.cp_mask [dstMem], [srcMem], size, byteMask;\n9.7.9.27.\nData Movement and Conversion Instructions:\nmultimem.cp.reduce.async.bulk\n\nmultimem.cp.reduce.async.bulk\nInitiates an asynchronous reduction operation to a multimem address range.\nSyntax\nmultimem.cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type  [dstMem], [srcMem], size;\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\n    .redOp                = { .and, .or, .xor,\n                              .add, .inc, .dec,\n                              .min, .max }\n    .type                 = { .f16, .bf16,\n                              .b32, .u32, .s32,\n                              .b64, .u64, .s64,\n                              .f32, .f64 }\n\nmultimem.cp.reduce.async.bulk.dst.src.completion_mechanism.add.noftz.type  [dstMem], [srcMem], size;\n\n    .dst                  = { .global }\n    .src                  = { .shared::cta }\n    .completion_mechanism = { .bulk_group }\n    .type                 = { .f16, .bf16 }\nDescription\nInstruction\nmultimem.cp.reduce.async.bulk\ninitiates an element-wise asynchronous reduction\noperation with elements from source memory address range\n[srcMem,\nsrcMem\n+\nsize)\nto memory\nlocations residing on each GPU’s memory referred to by the multimem destination address range\n[dstMem,\ndstMem\n+\nsize)\n.\nEach data element in the destination array is reduced inline with the corresponding data element in\nthe source array with the reduction operation specified by the modifier\n.redOp\n. The type of each\ndata element in the source and the destination array is specified by the modifier\n.type\n.\nThe source address operand\nsrcMem\nis in the state space specified by\n.src\nand the\ndestination address operand\ndstMem\nis in the state specified by the\n.dst\n.\nThe 32-bit operand\nsize\nspecifies the amount of memory to be copied from the source location\nand used in the reduction operation, in terms of number of bytes. Operand\nsize\nmust be a\nmultiple of 16. The memory range\n[dstMem,\ndstMem\n+\nsize)\nmust not overflow the destination\nmultimem memory space. The memory range\n[srcMem,\nsrcMem\n+\nsize)\nmust not overflow the source\nmemory space. The addresses\ndstMem\nand\nsrcMem\nmust be aligned to 16 bytes. If any of these\npreconditions is not met, the behavior is undefined.\nThe operations supported by\n.redOp\nare classified as follows:\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n, and\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..x]\nwhere\nx\nis the value at the source\nstate space.\nThe floating point operation\n.add\nrounds to the nearest even, preserve input and result\nsubnormals, and does not flush them to zero, except for the current implementation of\nmultimem.cp.reduce.async.bulk.add.f32\nwhich flushes subnormal inputs and results to\nsign-preserving zero. The\nmultimem.cp.reduce.async.bulk.add.f16\nand\nmultimem.cp.reduce.async.bulk.add.bf16\noperations require\n.noftz\nqualifier. It preserves\ninput and result subnormals, and does not flush them to zero.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.redOp\nelement type\n.add\n.u32, .s32, .u64, .f32, .f64,\n.f16, .bf16\n.min,\n.max\n.u32, .s32, .u64, .s64, .f16,\n.bf16\n.inc,\n.dec\n.u32\n.and,\n.or,\n.xor\n.b32, .b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported by the\ninstruction. The modifier\n.bulk_group\nspecifies that the\nmultimem.cp.reduce.async.bulk\nuses\nbulk async-group based completion mechanism.\nEach reduction operation performed by the\nmultimem.cp.reduce.async.bulk\nhas individually\n.relaxed.sys\nmemory ordering semantics. The load operations in\nmultimem.cp.reduce.async.bulk\nare treated as weak memory operations as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.xor.b64 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.inc.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.dec.u32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.max.s32 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.min.bf16 [dstMem], [srcMem], size;\n\nmultimem.cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.bf16 [dstMem], [srcMem], size;\n9.7.9.27.1.\nData Movement and Conversion Instructions: Tensor copy\n\n9.7.9.27.1.1.\nRestriction on Tensor Copy instructions\n\nFollowing are the restrictions on the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n:\ncp.reduce.async.bulk\ndoesn’t support the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\ncp.async.bulk.tensor\nwith the direction\n.global.shared::cta\ndoesn’t support the\ntype\n.b4x16_p64\n.\ncp.async.bulk.tensor\nwith the direction\n.shared::cluster.global\ndoesn’t support\nthe sub-byte types on\nsm_120a\n.\nOOB-NaN fill mode doesn’t support the types\n.b4x16\n,\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\nBox-Size[0] must be exactly:\n96B for\nb6x16_p32\nand\n.b6p2x16\n.\n64B for\nb4x16_p64\n.\nTensor-Size[0] must be a multiple of:\n96B for\nb6x16_p32\nand\n.b6p2x16\n.\n64B for\nb4x16_p64\n.\nFor\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n, the first coordinate in the tensorCoords\nargument vector must be a multiple of 128.\nFor\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n, the global memory address must be 32B aligned.\nAdditionally, tensor stride in every dimension must be 32B aligned.\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\nsupports the following swizzling modes:\nNone.\n128B (With all potential swizzle atomicity values except: 32B with 8B flip)\nFollowing are the restrictions on the 96B swizzle mode:\nThe\n.swizzle_atomicity\nmust be 16B.\nThe\n.interleave_layout\nmust not be set.\nBox-Size[0] must be less than or equal to 96B.\nThe type must not be among following:\n.b4x16_p64\n,\n.b6x16_p32\nand\n.b6p2x16\n.\nThe\n.load_mode\nmust not be set to\n.im2col::w::128\n.\nFollowing are the restrictions on the\n.global.shared::cta\ndirection:\nStarting co-ordinates for Bounding Box (\ntensorCoords\n) must be non-negative.\nThe bounding box along the D, W and H dimensions must stay within the tensor boundaries.\nThis implies:\nBounding-Box Lower-Corner must be non-negative.\nBounding-Box Upper-Corner must be non-positive.\nFollowing are the restrictions for\nsm_120a\n:\ncp.async.bulk.tensor\nwith the direction\n.shared::cluster.global\ndoesn’t support:\nthe sub-byte types\nthe qualifier\n.swizzle_atomicity\nFollowing are the restrictions for\nsm_103a\nwhile using type\n.b6p2x16\non\ncp.async.bulk.tensor\nwith the direction\n.global.shared::cta\n:\nBox-Size[0] must be exactly either of 48B or 96B.\nThe global memory address must be 16B aligned.\nTensor Stride in every dimension must be 16B aligned.\nThe first coordinate in the tensorCoords argument vector must be a multiple of 64.\nTensor-Size[0] must be a multiple of 48B.\nThe following swizzle modes are supported:\nNone.\n128B (With all potential swizzle atomicity values except: 32B with 8B flip)\n64B swizzle with 16B swizzle atomicity\n9.7.9.27.1.2.\nData Movement and Conversion Instructions:\ncp.async.bulk.tensor\n\ncp.async.bulk.tensor\nInitiates an asynchronous copy operation on the tensor data from one state space to another.\nSyntax\n// global -> shared::cta\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.cta_group}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo} {, cache-policy}\n\n.dst =                  { .shared::cta }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.cta_group =            { .cta_group::1, .cta_group::2 }\n.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =    { .L2::cache_hint }\n\n\n// global -> shared::cluster\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.cta_group}{.level::cache_hint}\n                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo}\n                                   {, ctaMask} {, cache-policy}\n\n.dst =                  { .shared::cluster }\n.src =                  { .global }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .mbarrier::complete_tx::bytes }\n.cta_group =            { .cta_group::1, .cta_group::2 }\n.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =    { .L2::cache_hint }\n.multicast =            { .multicast::cluster  }\n\n\n// shared::cta -> global\ncp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint}\n                                   [tensorMap, tensorCoords], [srcMem] {, cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .tile::scatter4, .im2col_no_offs }\n.level::cache_hint =    { .L2::cache_hint }\nDescription\ncp.async.bulk.tensor\nis a non-blocking instruction which initiates an asynchronous copy\noperation of tensor data from the location in\n.src\nstate space to the location in the\n.dst\nstate space.\nThe operand\ndstMem\nspecifies the location in the\n.dst\nstate space into which the tensor data\nhas to be copied and\nsrcMem\nspecifies the location in the\n.src\nstate space from which the\ntensor data has to be copied.\nWhen\n.dst\nis specified as\n.shared::cta\n, the address\ndstMem\nmust be in the shared memory\nof the executing CTA within the cluster, otherwise the behavior is undefined.\nWhen\n.dst\nis specified as\n.shared::cluster\n, the address\ndstMem\ncan be in the shared memory\nof any of the CTAs within the current cluster.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nThe dimension of the tensor data is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates in the tensor data in the\nglobal memory from or to which the copy operation has to be performed. The individual tensor\ncoordinates in\ntensorCoords\nare of type\n.s32\n. The format of vector argument\ntensorCoords\nis dependent on\n.load_mode\nspecified and is as follows:\n.load_mode\ntensorCoords\nSemantics\n.tile::scatter4\n{col_idx, row_idx0, row_idx1, row_idx2, row_idx3}\nFixed length vector of size 5.\nThe five elements together specify the start\nco-ordinates of the four rows.\n.tile::gather4\nRest all\n{d0, .., dn}\nfor n = .dim\nVector of n elements where n = .dim.\nThe elements indicate the offset in each of the\ndimension.\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. The completion mechanisms that are supported for different variants are\nsummarized in the following table:\n.completion-mechanism\n.dst\n.src\nCompletion mechanism\nNeeded for completion of\nentire Async operation\noptionally can be used for the completion of\nreading of the tensormap object\n.mbarrier::...\n.shared::cta\n.global\nmbarrier based\nBulk async-group\nbased\n.shared::cluster\n.global\n.bulk_group\n.global\n.shared::cta\nBulk async-group\nbased\nThe modifier\n.mbarrier::complete_tx::bytes\nspecifies that the\ncp.async.bulk.tensor\nvariant\nuses mbarrier based completion mechanism. Upon the completion of the asynchronous copy operation, the\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data copied in bytes, will be\nperformed on the mbarrier object specified by the operand\nmbar\n.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe modifier\n.cta_group\ncan only be specified with the mbarrier based completion mechanism. The\nmodifier\n.cta_group\nis used to signal either the odd numbered CTA or the even numbered CTA among\nthe\nCTA-Pair\n. When\n.cta_group::1\nis specified, the mbarrier object\nmbar\nthat is specified must be in the shared memory of the same CTA as the shared memory destination\ndstMem\n.\nWhen\n.cta_group::2\nis specified, the mbarrier object\nmbar\ncan be in shared memory of either the\nsame CTA as the shared memory destination\ndstMem\nor in its\npeer-CTA\n. If\n.cta_group\nis not specified, then it defaults to\n.cta_group::1\n.\nThe modifier\n.bulk_group\nspecifies that the\ncp.async.bulk.tensor\nvariant uses\nbulk\nasync-group\nbased completion mechanism.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n.\nIn\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination.\nIn\n.tile::gather4\nmode, four rows in 2-dimnesional source tensor are combined to form a single 2-dimensional\ndestination tensor. In\n.tile::scatter4\nmode, single 2-dimensional source tensor is divided into four rows\nin 2-dimensional destination tensor. Details of\n.tile::scatter4\n/\n.tile::gather4\nmodes are described\nin\n.tile::scatter4 and .tile::gather4 modes\n.\nIn\n.im2col\nand\n.im2col::*\nmodes, some dimensions of the source tensors are unrolled in a single\ndimensional column at the destination. Details of the\nim2col\nand\n.im2col::*\nmodes are described\nin\nim2col mode\nand\nim2col::w and im2col::w::128 modes\nrespectively. In\n.im2col\nand\n.im2col::*\nmodes, the tensor has to be at least 3-dimensional. The vector\noperand\nim2colInfo\ncan be specified only when\n.load_mode\nis\n.im2col\nor\n.im2col::w\nor\n.im2col::w::128\n. The format of the vector argument\nim2colInfo\nis dependent on the exact im2col mode\nand is as follows:\nExact im2col mode\nim2colInfo argument\nSemantics\n.im2col\n{ i2cOffW , i2cOffH , i2cOffD }\nfor\n.dim\n=\n.5d\nA vector of im2col offsets whose vector size is two\nless than number of dimensions .dim.\n.im2col::w\n{ wHalo, wOffset }\nA vector of 2 arguments containing\nwHalo\nand\nwOffset\narguments.\n.im2col::w::128\n.im2col_no_offs\nim2colInfo\nis not applicable.\nim2colInfo\nis not applicable.\nArgument\nwHalo\nis a 16bit unsigned integer whose valid set of values differs on the load-mode and is as follows:\n- Im2col::w mode : valid range is [0, 512).\n- Im2col::w::128 mode : valid range is [0, 32).\nArgument\nwOffset\nis a 16bit unsigned integer whose valid range of values is [0, 32).\nThe optional modifier\n.multicast::cluster\nallows copying of data from global memory to shared\nmemory of multiple CTAs in the cluster. Operand\nctaMask\nspecifies the destination CTAs in the\ncluster such that each bit position in the 16-bit\nctaMask\noperand corresponds to the\n%ctaid\nof the destination CTA. The source data is multicast to the same offset as\ndstMem\nin the shared\nmemory of each destination CTA. When\n.cta_group\nis specified as:\n.cta_group::1\n: The mbarrier signal is also multicasted to the same offset as\nmbar\nin\nthe shared memory of the destination CTA.\n.cta_group::2\n: The mbarrier signal is multicasted either to all the odd numbered CTAs or the\neven numbered CTAs within the corresponding\nCTA-Pair\n. For each destination\nCTA specified in the\nctaMask\n, the mbarrier signal is sent either to the destination CTA or its\npeer-CTA\nbased on CTAs\n%cluster_ctarank\nparity of shared memory where\nthe mbarrier object\nmbar\nresides.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nThe copy operation in\ncp.async.bulk.tensor\nis treated as a weak memory operation and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nNotes\n.multicast::cluster\nqualifier is optimized for target architecture\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\nand may have substantially reduced performance on other\ntargets and hence\n.multicast::cluster\nis advised to be used with\n.target\nsm_90a\n/\nsm_100f\n/\nsm_100a\n/\nsm_103f\n/\nsm_103a\n/\nsm_110f\n/\nsm_110a\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.shared::cta\nas destination state space is introduced in PTX ISA version 8.6.\nSupport for qualifiers\n.tile::gather4\nand\n.tile::scatter4\nintroduced in PTX ISA version 8.6.\nSupport for qualifiers\n.im2col::w\nand\n.im2col::w::128\nintroduced in PTX ISA version 8.6.\nSupport for qualifier\n.cta_group\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.multicast::cluster\nqualifier advised to be used with\n.target\nsm_90a\nor\nsm_100f\nor\nsm_100a\nor\nsm_103f\nor\nsm_103a\nor\nsm_110f\nor\nsm_110a\n.\nQualifiers\n.tile::gather4\nand\n.im2col::w\nrequire:\nsm_100a\nwhen destination state space is\n.shared::cluster\nand is supported on\nsm_100f\nfrom PTX ISA version 8.8.\nsm_100\nor higher when destination state space is\n.shared::cta\n.\nQualifier\n.tile::scatter4\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.im2col::w::128\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.cta_group\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n.reg .b16 ctaMask;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.tensor.1d.shared::cta.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.5d.shared::cta.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\ncp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];\n\n@p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster\n                     [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask;\n\n@p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes\n                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n                     [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy;\n\n@p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group  [tensorMap3, {tc0}], [sMem3];\n\ncp.async.bulk.tensor.2d.tile::gather4.shared::cluster.global.mbarrier::complete_tx::bytes\n                     [sMem5], [tensorMap6, {x0, y0, y1, y2, y3}], [mbar5];\n\ncp.async.bulk.tensor.3d.im2col::w.shared::cluster.global.mbarrier::complete_tx::bytes\n                     [sMem4], [tensorMap5, {t0, t1, t2}], [mbar4], {im2colwHalo, im2colOff};\n\ncp.async.bulk.tensor.1d.shared::cluster.global.tile.cta_group::2\n                     [sMem6], [tensorMap7, {tc0}], [peerMbar];\n9.7.9.27.1.3.\nData Movement and Conversion Instructions:\ncp.reduce.async.bulk.tensor\n\ncp.reduce.async.bulk.tensor\nInitiates an asynchronous reduction operation on the tensor data.\nSyntax\n// shared::cta -> global:\ncp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint}\n                                          [tensorMap, tensorCoords], [srcMem] {,cache-policy}\n\n.dst =                  { .global }\n.src =                  { .shared::cta }\n.dim =                  { .1d, .2d, .3d, .4d, .5d }\n.completion_mechanism = { .bulk_group }\n.load_mode =            { .tile, .im2col_no_offs }\n.redOp =                { .add, .min, .max, .inc, .dec, .and, .or, .xor}\nDescription\ncp.reduce.async.bulk.tensor\nis a non-blocking instruction which initiates an asynchronous\nreduction operation of tensor data in the\n.dst\nstate space with tensor data in the\n.src\nstate space.\nThe operand\nsrcMem\nspecifies the location of the tensor data in the\n.src\nstate space using\nwhich the reduction operation has to be performed.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nEach element of the tensor data in the\n.dst\nstate space is reduced inline with the corresponding\nelement from the tensor data in the\n.src\nstate space. The modifier\n.redOp\nspecifies the\nreduction operation used for the inline reduction. The type of each tensor data element in the\nsource and the destination tensor is specified in\nTensor-map\n.\nThe dimension of the tensor is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates of the tensor data in the\nglobal memory on which the reduce operation is to be performed. The number of tensor coordinates in\nthe vector argument\ntensorCoords\nshould be equal to the dimension specified by the modifier\n.dim\n. The individual tensor coordinates are of the type\n.s32\n.\nThe following table describes the valid combinations of\n.redOp\nand element type:\n.redOp\nElement type\n.add\n.u32\n,\n.s32\n,\n.u64\n,\n.f32\n,\n.f16\n,\n.bf16\n.min\n,\n.max\n.u32\n,\n.s32\n,\n.u64\n,\n.s64\n,\n.f16\n,\n.bf16\n.inc\n,\n.dec\n.u32\n.and\n,\n.or\n,\n.xor\n.b32\n,\n.b64\nThe modifier\n.completion_mechanism\nspecifies the completion mechanism that is supported on the\ninstruction variant. Value\n.bulk_group\nof the modifier\n.completion_mechanism\nspecifies that\ncp.reduce.async.bulk.tensor\ninstruction uses\nbulk async-group\nbased completion mechanism.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n. In\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination. In\n.im2col_no_offs\nmode, some dimensions of the source tensors are unrolled in a single dimensional\ncolumn at the destination. Details of the\nim2col\nmode are described in\nim2col mode\n. In\n.im2col\nmode, the tensor has to be at least\n3-dimensional.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program. The\nqualifier\n.level::cache_hint\nis only supported when at least one of the\n.src\nor\n.dst\nstatespaces is\n.global\nstate space.\nEach reduction operation performed by\ncp.reduce.async.bulk.tensor\nhas individually\n.relaxed.gpu\nmemory ordering semantics. The load operations in\ncp.reduce.async.bulk.tensor\nare treated as weak memory operations and the\ncomplete-tx\noperation on the mbarrier has\n.release\nsemantics at the\n.cluster\nscope as described in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group\n                                             [tensorMap0, {tc0}], [sMem0];\n\ncp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint\n                                             [tensorMap1, {tc0, tc1}], [sMem1] , policy;\n\ncp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group\n                                             [tensorMap2, {tc0, tc1, tc2}], [sMem2]\n9.7.9.27.1.4.\nData Movement and Conversion Instructions:\ncp.async.bulk.prefetch.tensor\n\ncp.async.bulk.prefetch.tensor\nProvides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache.\nSyntax\n// global -> shared::cluster:\ncp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords]\n                                                             {, im2colInfo } {, cache-policy}\n\n.src =                { .global }\n.dim =                { .1d, .2d, .3d, .4d, .5d }\n.load_mode =          { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }\n.level::cache_hint =  { .L2::cache_hint }\nDescription\ncp.async.bulk.prefetch.tensor\nis a non-blocking instruction which may initiate an asynchronous\nprefetch of tensor data from the location in\n.src\nstatespace to the L2 cache.\nThe operand\ntensorMap\nis the generic address of the opaque tensor-map object which resides\nin\n.param\nspace or\n.const\nspace or\n.global\nspace. The operand\ntensorMap\nspecifies\nthe properties of the tensor copy operation, as described in\nTensor-map\n.\nThe\ntensorMap\nis accessed in tensormap proxy. Refer to the\nCUDA programming guide\nfor creating\nthe tensor-map objects on the host side.\nThe dimension of the tensor data is specified by the\n.dim\nmodifier.\nThe vector operand\ntensorCoords\nspecifies the starting coordinates in the tensor data in the\nglobal memory from which the copy operation has to be performed. The individual tensor\ncoordinates in\ntensorCoords\nare of type\n.s32\n. The format of vector argument\ntensorCoords\nis dependent on\n.load_mode\nspecified and is as follows:\n.load_mode\ntensorCoords\nSemantics\n.tile::gather4\n{col_idx, row_idx0, row_idx1, row_idx2, row_idx3}\nFixed length vector of size 5.\nThe five elements together specify the start\nco-ordinates of the four rows.\nRest all\n{d0, .., dn}\nfor n = .dim\nVector of n elements where n = .dim.\nThe elements indicate the offset in each of the\ndimension.\nThe qualifier\n.load_mode\nspecifies how the data in the source location is copied into the\ndestination location. If\n.load_mode\nis not specified, it defaults to\n.tile\n.\nIn\n.tile\nmode, the multi-dimensional layout of the source tensor is preserved at the destination.\nIn\n.tile::gather4\nmode, four rows in the 2-dimnesional source tensor are fetched to L2 cache.\nDetails of\n.tile::gather4\nmodes are described\nin\n.tile::scatter4 and .tile::gather4 modes\n.\nIn\n.im2col\nand\n.im2col::*\nmodes, some dimensions of the source tensors are unrolled in a single\ndimensional column at the destination. Details of the\nim2col\nand\n.im2col::*\nmodes are described in\nim2col mode\nand\nim2col::w and im2col::w::128 modes\nrespectively. In\n.im2col\nand\n.im2col::*\nmodes, the tensor has to be at least 3-dimensional. The vector\noperand\nim2colInfo\ncan be specified only when\n.load_mode\nis\n.im2col\nor\n.im2col::w\nor\n.im2col::w::128\n. The format of the vector argument\nim2colInfo\nis dependent on the exact im2col mode\nand is as follows:\nExact im2col mode\nim2colInfo argument\nSemantics\n.im2col\n{ i2cOffW , i2cOffH , i2cOffD }\nfor\n.dim\n=\n.5d\nA vector of im2col offsets whose vector size is two\nless than number of dimensions .dim.\n.im2col::w\n{ wHalo, wOffset }\nA vector of 2 arguments containing\nwHalo\nand\nwOffset\narguments.\n.im2col::w::128\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\ncp.async.bulk.prefetch.tensor\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for qualifier\n.tile::gather4\nintroduced in PTX ISA version 8.6.\nSupport for qualifiers\n.im2col::w\nand\n.im2col::w::128\nintroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nQualifier\n.tile::gather4\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifiers\n.im2col::w\nand\n.im2col::w::128\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n.reg .b16 ctaMask, im2colwHalo, im2colOff;\n.reg .u16 i2cOffW, i2cOffH, i2cOffD;\n.reg .b64 l2CachePolicy;\n\ncp.async.bulk.prefetch.tensor.1d.L2.global.tile  [tensorMap0, {tc0}];\n\n@p cp.async.bulk.prefetch.tensor.2d.L2.global    [tensorMap1, {tc0, tc1}];\n\n@p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col\n                      [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD};\n\n@p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint\n                      [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy;\n\ncp.async.bulk.prefetch.tensor.2d.L2.global.tile::gather4 [tensorMap5, {col_idx, row_idx0, row_idx1, row_idx2, row_idx3}];\n\ncp.async.bulk.prefetch.tensor.4d.L2.global.im2col::w::128\n                      [tensorMap4, {t0, t1, t2, t3}], {im2colwHalo, im2colOff};\n9.7.9.27.2.\nData Movement and Conversion Instructions: Bulk and Tensor copy completion instructions\n\n9.7.9.27.2.1.\nData Movement and Conversion Instructions:\ncp.async.bulk.commit_group\n\ncp.async.bulk.commit_group\nCommits all prior initiated but uncommitted\ncp.async.bulk\ninstructions into a\ncp.async.bulk-group\n.\nSyntax\ncp.async.bulk.commit_group;\nDescription\ncp.async.bulk.commit_group\ninstruction creates a new per-thread\nbulk async-group\nand batches\nall prior\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions satisfying the following\nconditions into the new\nbulk async-group\n:\nThe prior\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions use\nbulk_group\nbased\ncompletion mechanism, and\nThey are initiated by the executing thread but not committed to any\nbulk async-group\n.\nIf there are no uncommitted\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\ninstructions then\ncp.async.bulk.commit_group\nresults in an empty\nbulk async-group\n.\nAn executing thread can wait for the completion of all\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\noperations in a\nbulk async-group\nusing\ncp.async.bulk.wait_group\n.\nThere is no memory ordering guarantee provided between any two\ncp{.reduce}.async.bulk.{.prefetch}{.tensor}\noperations within the same\nbulk async-group\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.commit_group;\n9.7.9.27.2.2.\nData Movement and Conversion Instructions:\ncp.async.bulk.wait_group\n\ncp.async.bulk.wait_group\nWait for completion of\nbulk async-groups\n.\nSyntax\ncp.async.bulk.wait_group{.read} N;\nDescription\ncp.async.bulk.wait_group\ninstruction will cause the executing thread to wait until only N or\nfewer of the most recent\nbulk async-groups\nare pending and all the prior\nbulk async-groups\ncommitted by the executing threads are complete. For example, when N is 0, the executing thread\nwaits on all the prior\nbulk async-groups\nto complete. Operand N is an integer constant.\nBy default,\ncp.async.bulk.wait_group\ninstruction will cause the executing thread to wait until\ncompletion of all the bulk async operations in the specified\nbulk async-group\n. A bulk async\noperation includes the following:\nOptionally, reading from the tensormap.\nReading from the source locations.\nWriting to their respective destination locations.\nWrites being made visible to the executing thread.\nThe optional\n.read\nmodifier indicates that the waiting has to be done until all the bulk\nasync operations in the specified\nbulk async-group\nhave completed:\nreading from the tensormap\nthe reading from their source locations.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ncp.async.bulk.wait_group.read   0;\ncp.async.bulk.wait_group        2;\n9.7.9.28.\nData Movement and Conversion Instructions:\ntensormap.replace\n\ntensormap.replace\nModifies the field of a tensor-map object.\nSyntax\ntensormap.replace.mode.field1{.ss}.b1024.type  [addr], new_val;\ntensormap.replace.mode.field2{.ss}.b1024.type  [addr], ord, new_val;\ntensormap.replace.mode.field3{.ss}.b1024.type  [addr], new_val;\n\n.mode    = { .tile }\n.field1  = { .global_address, .rank }\n.field2  = { .box_dim, .global_dim, .global_stride, .element_stride  }\n.field3  = { .elemtype,  .interleave_layout, .swizzle_mode, .swizzle_atomicity, .fill_mode }\n.ss      = { .global, .shared::cta }\n.type    = { .b32, .b64 }\nDescription\nThe\ntensormap.replace\ninstruction replaces the field, specified by\n.field\nqualifier,\nof the tensor-map object at the location specified by the address operand\naddr\nwith a\nnew value. The new value is specified by the argument\nnew_val\n.\nQualifier\n.mode\nspecifies the mode of the\ntensor-map\nobject\nlocated at the address operand\naddr\n.\nInstruction type\n.b1024\nindicates the size of the\ntensor-map\nobject, which is 1024 bits.\nOperand\nnew_val\nhas the type\n.type\n. When\n.field\nis specified as\n.global_address\nor\n.global_stride\n,\n.type\nmust be\n.b64\n. Otherwise,\n.type\nmust be\n.b32\n.\nThe immediate integer operand\nord\nspecifies the ordinal of the field across the rank of the\ntensor which needs to be replaced in the\ntensor-map\nobject.\nFor field\n.rank\n, the operand\nnew_val\nmust be ones less than the desired tensor rank as\nthis field uses zero-based numbering.\nWhen\n.field3\nis specified, the operand\nnew_val\nmust be an immediate and the\nTable 33\nshows the mapping of the operand\nnew_val\nacross various fields.\nTable 33\nTensormap new_val validity\n\nnew_val\n.field3\n.elemtype\n.interleave_layout\n.swizzle_mode\n.swizzle_atomicity\n.fill_mode\n0\n.u8\nNo interleave\nNo swizzling\n16B\nZero fill\n1\n.u16\n16B interleave\n32B swizzling\n32B\nOOB-NaN fill\n2\n.u32\n32B interleave\n64B swizzling\n32B + 8B flip\nx\n3\n.s32\nx\n128B swizzling\n64B\nx\n4\n.u64\nx\n96B swizzling\nx\nx\n5\n.s64\nx\nx\nx\nx\n6\n.f16\nx\nx\nx\nx\n7\n.f32\nx\nx\nx\nx\n8\n.f32.ftz\nx\nx\nx\nx\n9\n.f64\nx\nx\nx\nx\n10\n.bf16\nx\nx\nx\nx\n11\n.tf32\nx\nx\nx\nx\n12\n.tf32.ftz\nx\nx\nx\nx\n13\n.b4x16\nx\nx\nx\nx\n14\n.b4x16_p64\nx\nx\nx\nx\n15\n.b6x16_p32\nor\n.b6p2x16\nx\nx\nx\nx\nNote\nThe values of\n.elemtype\ndo not correspond to the values of the\nCUtensorMapDataType\nenum used in the driver API.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the address specified by\naddr\ndoes not fall within the address window of\n.global\nor\n.shared::cta\nstate space then the behavior is undefined.\ntensormap.replace\nis treated as a weak memory operation, on the entire 1024-bit opaque\ntensor-map\nobject, in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nQualifier\n.swizzle_atomicity\nintroduced in PTX ISA version 8.6.\nQualifier\n.elemtype\nwith values from\n13\nto\n15\n, both inclusive, is\nsupported in PTX ISA version 8.7 onwards.\nQualifier\n.swizzle_mode\nwith value\n4\nis supported from PTX ISA version 8.8 onwards.\nTarget ISA Notes\nSupported on following architectures:\nsm_90a\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nQualifier\n.swizzle_atomicity\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\n(refer to\nsection\nfor restrictions on sm_120a)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.field3\nvariant\n.elemtype\ncorresponding to\nnew_val\nvalues\n13\n,\n14\nand\n15\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\n(refer to\nsection\nfor restrictions on sm_120a)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\n.field3\nvariant\n.swizzle_mode\ncorresponding to\nnew_val\nvalue\n4\nis supported on\nfollowing architectures:\nsm_103a\n(refer to\nsection\nfor restrictions on sm_103a)\nExamples\ntensormap.replace.tile.global_address.shared::cta.b1024.b64   [sMem], new_val;\n9.7.10.\nTexture Instructions\n\nThis section describes PTX instructions for accessing textures and samplers. PTX supports the\nfollowing operations on texture and sampler descriptors:\nStatic initialization of texture and sampler descriptors.\nModule-scope and per-entry scope definitions of texture and sampler descriptors.\nAbility to query fields within texture and sampler descriptors.\n9.7.10.1.\nTexturing Modes\n\nFor working with textures and samplers, PTX has two modes of operation. In the\nunified mode,\ntexture and sampler information is accessed through a single\n.texref\nhandle. In the\nindependent\nmode\n, texture and sampler information each have their own handle, allowing them to be defined\nseparately and combined at the site of usage in the program.\nThe advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior\nto\nsm_3x\n), with the restriction that they correspond 1-to-1 with the 256 possible textures per\nkernel (128 for architectures prior to\nsm_3x\n). The advantage of independent mode is that\ntextures and samplers can be mixed and matched, but the number of samplers is greatly restricted to\n32 per kernel (16 for architectures prior to\nsm_3x\n).\nTable 34\nsummarizes the number of textures, samplers and\nsurfaces available in different texturing modes.\nTable 34\nTexture, sampler and surface limits\n\nTexturing mode\nResource\nsm_1x\n,\nsm_2x\nsm_3x+\nUnified mode\nTextures\n128\n256\nSamplers\n128\n256\nSurfaces\n8\n16\nIndependent mode\nTextures\n128\n256\nSamplers\n16\n32\nSurfaces\n8\n16\nThe texturing mode is selected using\n.target\noptions\ntexmode_unified\nand\ntexmode_independent\n. A PTX module may declare only one texturing mode. If no texturing mode is\ndeclared, the module is assumed to use unified mode.\nExample\n: calculate an element’s power contribution as element’s power/total number of elements.\n.target texmode_independent\n.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,\n                               filter_mode = nearest\n                             };\n...\n.entry compute_power\n  ( .param .texref tex1 )\n{\n  txq.width.b32  r6, [tex1]; // get tex1's width\n  txq.height.b32 r5, [tex1]; // get tex1's height\n  tex.2d.v4.f32.f32  {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}];\n  mul.u32 r5, r5, r6;\n  add.f32 r1, r1, r2;\n  add.f32 r3, r3, r4;\n  add.f32 r1, r1, r3;\n  cvt.f32.u32 r5, r5;\n  div.f32 r1, r1, r5;\n}\n9.7.10.2.\nMipmaps\n\nA\nmipmap\nis a sequence of textures, each of which is a progressively lower resolution\nrepresentation of the same image. The height and width of each image, or level of detail (LOD), in\nthe mipmap is a power of two smaller than the previous level. Mipmaps are used in graphics\napplications to improve rendering speed and reduce aliasing artifacts. For example, a\nhigh-resolution mipmap image is used for objects that are close to the user; lower-resolution images\nare used as the object appears farther away. Mipmap filtering modes are provided when switching\nbetween two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity.\nExample:\nIf the texture has a basic size of 256 by 256 pixels, then the associated mipmap set\nmay contain a series of eight images, each one-fourth the total area of the previous one: 128x128\npixels, 64x64, 32x32, 16x16, 8x8, 4x4, 2x2, 1x1 (a single pixel). If, for example, a scene is\nrendering this texture in a space of 40x40 pixels, then either a scaled up version of the 32x32\n(without trilinear interpolation) or an interpolation of the 64x64 and the 32x32 mipmaps (with\ntrilinear interpolation) would be used.\nThe total number of LODs in a complete mipmap pyramid is calculated through the following equation:\nnumLODs = 1 + floor(log2(max(w, h, d)))\nThe finest LOD is called the base level and is the 0th level. The next (coarser) level is the 1st\nlevel, and so on. The coarsest level is the level of size (1 x 1 x 1). Each successively smaller\nmipmap level has half the {width, height, depth} of the previous level, but if this half value is a\nfractional value, it’s rounded down to the next largest integer. Essentially, the size of a mipmap\nlevel can be specified as:\nmax(1, floor(w_b / 2^i)) x\nmax(1, floor(h_b / 2^i)) x\nmax(1, floor(d_b / 2^i))\nwhere\ni\nis the ith level beyond the 0th level (the base level). And\nw_b\n,\nh_b\nand\nd_b\nare the\nwidth, height and depth of the base level respectively.\nPTX support for mipmaps\nThe PTX\ntex\ninstruction supports three modes for specifying the LOD:\nbase\n,\nlevel\n, and\ngrad\nient. In base mode, the instruction always picks level 0. In level mode, an additional\nargument is provided to specify the LOD to fetch from. In gradmode, two floating-point vector\narguments provide\npartials\n(e.g.,\n{ds/dx,\ndt/dx}\nand\n{ds/dy,\ndt/dy}\nfor a 2d texture),\nwhich the\ntex\ninstruction uses to compute the LOD.\nThese instructions provide access to texture memory.\ntex\ntld4\ntxq\n9.7.10.3.\nTexture Instructions:\ntex\n\ntex\nPerform a texture memory lookup.\nSyntax\ntex.geom.v4.dtype.ctype  d, [a, c] {, e} {, f};\ntex.geom.v4.dtype.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\ntex.geom.v2.f16x2.ctype  d[|p], [a, c] {, e} {, f};\ntex.geom.v2.f16x2.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n// mipmaps\ntex.base.geom.v4.dtype.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v4.dtype.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v4.dtype.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\ntex.base.geom.v2.f16x2.ctype   d[|p], [a, {b,} c] {, e} {, f};\ntex.level.geom.v2.f16x2.ctype  d[|p], [a, {b,} c], lod {, e} {, f};\ntex.grad.geom.v2.f16x2.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms };\n.dtype = { .u32, .s32, .f16,  .f32 };\n.ctype = {       .s32, .f32 };          // .cube, .acube require .f32\n                                        // .2dms, .a2dms require .s32\nDescription\ntex.{1d,2d,3d}\nTexture lookup using a texture coordinate vector. The instruction loads data from the texture named\nby operand\na\nat coordinates given by operand\nc\ninto destination\nd\n. Operand\nc\nis a\nscalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a\nfour-element vector for 3d textures, where the fourth element is ignored. An optional texture\nsampler\nb\nmay be specified. If no sampler is specified, the sampler behavior is a property of\nthe named texture. The optional destination predicate\np\nis set to\nTrue\nif data from texture\nat specified coordinates is resident in memory,\nFalse\notherwise. When optional destination\npredicate\np\nis set to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data\nat specified coordinates is dependent on execution environment setup using Driver API calls, prior\nto kernel launch. Refer to Driver API documentation for more details including any\nsystem/implementation specific behavior.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nvalues that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand\ne\nis a singleton tuple for 1d textures; is a two\nelement vector 2d textures; and is four-element vector for 3d textures, where the fourth element is\nignored.\nAn optional operand\nf\nmay be specified for\ndepth\ntextures\n. Depth textures are special type\nof textures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in\nf\noperand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering. When using depth compare operand, the elements in texture coordinate vector\nc\nhave\n.f32\ntype.\nDepth compare operand is not supported for\n3d\ntextures.\nThe instruction returns a two-element vector for destination type\n.f16x2\n. For all other\ndestination types, the instruction returns a four-element vector. Coordinates may be given in either\nsigned 32-bit integer or 32-bit floating point form.\nA texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\ntex.{a1d,a2d}\nTexture array selection, followed by texture lookup. The instruction first selects a texture from\nthe texture array named by operand\na\nusing the index given by the first element of the array\ncoordinate vector\nc\n. The instruction then loads data from the selected texture at coordinates\ngiven by the remaining elements of operand\nc\ninto destination\nd\n. Operand\nc\nis a bit-size\ntype vector or tuple containing an index into the array of textures followed by coordinates within\nthe selected texture, as follows:\nFor 1d texture arrays, operand\nc\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the second element is interpreted as\na 1d texture coordinate of type\n.ctype\n.\nFor 2d texture arrays, operand\nc\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the next two elements are\ninterpreted as 2d texture coordinates of type\n.ctype\n. The fourth element is ignored.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nvalues that\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7. Operand\ne\nis a singleton tuple for 1d texture arrays; and is\na two element vector 2d texture arrays.\nAn optional operand\nf\nmay be specified for depth textures arrays. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth textures. When using depth compare\noperand, the coordinates in texture coordinate vector\nc\nhave\n.f32\ntype.\nThe instruction returns a two-element vector for destination type\n.f16x2\n. For all other\ndestination types, the instruction returns a four-element vector. The texture array index is a\n32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point\nvalues.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.cube\nCubemap\ntexture lookup. The instruction loads data from the cubemap texture named by operand\na\nat coordinates given by operand\nc\ninto destination\nd\n. Cubemap textures are special\ntwo-dimensional layered textures consisting of six layers that represent the faces of a cube. All\nlayers in a cubemap are of the same size and are square (i.e., width equals height).\nWhen accessing a cubemap, the texture coordinate vector\nc\nhas type\n.v4.f32\n, and comprises\nthree floating-point coordinates (\ns\n,\nt\n,\nr\n) and a fourth padding argument which is\nignored. Coordinates (\ns\n,\nt\n,\nr\n) are projected onto one of the six cube faces. The (\ns\n,\nt\n,\nr\n) coordinates can be thought of as a direction vector emanating from the center of the\ncube. Of the three coordinates (\ns\n,\nt\n,\nr\n), the coordinate of the largest magnitude (the\nmajor axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by\nthe absolute value of the major axis to produce a new (\ns\n,\nt\n) coordinate pair to lookup into\nthe selected cube face.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nOffset vector operand\ne\nis not supported for cubemap textures.\nan optional operand\nf\nmay be specified for cubemap depth textures. operand\nf\nis\n.f32\nscalar value that specifies depth compare value for cubemap depth textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.acube\nCubemap array selection, followed by cubemap lookup. The instruction first selects a cubemap texture\nfrom the cubemap array named by operand\na\nusing the index given by the first element of the\narray coordinate vector\nc\n. The instruction then loads data from the selected cubemap texture at\ncoordinates given by the remaining elements of operand\nc\ninto destination\nd\n.\nCubemap array\ntextures consist of an array of cubemaps, i.e., the total number of layers is a\nmultiple of six. When accessing a cubemap array texture, the coordinate vector\nc\nhas type\n.v4.b32\n. The first element is interpreted as an unsigned integer index (\n.u32\n) into the\ncubemap array, and the remaining three elements are interpreted as floating-point cubemap\ncoordinates (\ns\n,\nt\n,\nr\n), used to lookup in the selected cubemap as described above.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nOffset vector operand\ne\nis not supported for cubemap texture arrays.\nAn optional operand\nf\nmay be specified for cubemap depth texture arrays. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for cubemap depth textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.2dms\nMulti-sample texture lookup using a texture coordinate vector. Multi-sample textures consist of\nmultiple samples per data element. The instruction loads data from the texture named by operand\na\nfrom sample number given by first element of the operand\nc\n, at coordinates given by\nremaining elements of operand\nc\ninto destination\nd\n. When accessing a multi-sample texture,\ntexture coordinate vector\nc\nhas type\n.v4.b32\n. The first element in operand\nc\nis\ninterpreted as unsigned integer sample number (\n.u32\n), and the next two elements are interpreted\nas signed integer (\n.s32\n) 2d texture coordinates. The fourth element is ignored. An optional\ntexture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior is a\nproperty of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset\nvalue is in the range of -8 to +7.\nDepth compare operand\nf\nis not supported for multi-sample textures.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\ntex.a2dms\nMulti-sample texture array selection, followed by multi-sample texture lookup. The instruction first\nselects a multi-sample texture from the multi-sample texture array named by operand a using the\nindex given by the first element of the array coordinate vector\nc\n. The instruction then loads\ndata from the selected multi-sample texture from sample number given by second element of the\noperand\nc\n, at coordinates given by remaining elements of operand\nc\ninto destination\nd\n. When accessing a multi-sample texture array, texture coordinate vector\nc\nhas type\n.v4.b32\n. The first element in operand c is interpreted as unsigned integer sampler number, the\nsecond element is interpreted as unsigned integer index (\n.u32\n) into the multi-sample texture\narray and the next two elements are interpreted as signed integer (\n.s32\n) 2d texture\ncoordinates. An optional texture sampler\nb\nmay be specified. If no sampler is specified, the\nsampler behavior is a property of the named texture.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nvalues\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7.\nDepth compare operand\nf\nis not supported for multi-sample texture arrays.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nMipmaps\n.base\n(lod zero)\nPick level 0 (base level). This is the default if no mipmap mode is specified. No additional arguments.\n.level\n(lod explicit)\nRequires an additional 32-bit scalar argument,\nlod\n, which contains the LOD to fetch from. The\ntype of\nlod\nfollows\n.ctype\n(either\n.s32\nor\n.f32\n). Geometries\n.2dms\nand\n.a2dms\nare not supported in this mode.\n.grad\n(lod gradient)\nRequires two\n.f32\nvectors,\ndPdx\nand\ndPdy\n, that specify the partials. The vectors are\nsingletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are\nfour-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d\nand cube geometries. Geometries\n.2dms\nand\n.a2dms\nare not supported in this mode.\nFor mipmap texture lookup, an optional operand\ne\nmay be specified. Operand\ne\nis a vector of\n.s32\nthat specifies coordinate offset. Offset is applied to coordinates before doing texture\nlookup. Offset value is in the range of -8 to +7. Offset vector operand is not supported for cube\nand cubemap geometries.\nAn optional operand\nf\nmay be specified for mipmap textures. Operand\nf\nis\n.f32\nscalar\nvalue that specifies depth compare value for depth textures. When using depth compare operand, the\ncoordinates in texture coordinate vector\nc\nhave\n.f32\ntype.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nDepth compare operand is not supported for\n3d\ntextures.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nNotes\nFor compatibility with prior versions of PTX, the square brackets are not required and\n.v4\ncoordinate vectors are allowed for any geometry, with the extra elements being ignored.\nPTX ISA Notes\nUnified mode texturing introduced in PTX ISA version 1.0. Extension using opaque\n.texref\nand\n.samplerref\ntypes and independent mode texturing introduced in PTX ISA version 1.5.\nTexture arrays\ntex.{a1d,a2d}\nintroduced in PTX ISA version 2.3.\nCubemaps and cubemap arrays introduced in PTX ISA version 3.0.\nSupport for mipmaps introduced in PTX ISA version 3.1.\nIndirect texture access introduced in PTX ISA version 3.1.\nMulti-sample textures and multi-sample texture arrays introduced in PTX ISA version 3.2.\nSupport for textures returning\n.f16\nand\n.f16x2\ndata introduced in PTX ISA version 4.2.\nSupport for\ntex.grad.{cube,\nacube}\nintroduced in PTX ISA version 4.3.\nOffset vector operand introduced in PTX ISA version 4.3.\nDepth compare operand introduced in PTX ISA version 4.3.\nSupport for optional destination predicate introduced in PTX ISA version 7.1.\nTarget ISA Notes\nSupported on all target architectures.\nThe cubemap array geometry (\n.acube\n) requires\nsm_20\nor higher.\nMipmaps require\nsm_20\nor higher.\nIndirect texture access requires\nsm_20\nor higher.\nMulti-sample textures and multi-sample texture arrays require\nsm_30\nor higher.\nTexture fetch returning\n.f16\nand\n.f16x2\ndata require\nsm_53\nor higher.\ntex.grad.{cube,\nacube}\nrequires\nsm_20\nor higher.\nOffset vector operand requires\nsm_30\nor higher.\nDepth compare operand requires\nsm_30\nor higher.\nSupport for optional destination predicate requires\nsm_60\nor higher.\nExamples\n// Example of unified mode texturing\n // - f4 is required to pad four-element tuple and is ignored\n tex.3d.v4.s32.s32  {r1,r2,r3,r4}, [tex_a,{f1,f2,f3,f4}];\n\n // Example of independent mode texturing\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,smpl_x,{f1}];\n\n // Example of 1D texture array, independent texturing mode\n tex.a1d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,smpl_x,{idx,s1}];\n\n // Example of 2D texture array, unified texturing mode\n // - f3 is required to pad four-element tuple and is ignored\n tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2,f3}];\n\n // Example of cubemap array, unified textureing mode\n tex.acube.v4.f32.f32 {r0,r1,r2,r3}, [tex_cuarray,{idx,f1,f2,f3}];\n\n // Example of multi-sample texture, unified texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture, independent texturing mode\n tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms, smpl_x,{sample,r6,r7,r8}];\n\n // Example of multi-sample texture array, unified texturing mode\n tex.a2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ams,{idx,sample,r6,r7}];\n\n // Example of texture returning .f16 data\n tex.1d.v4.f16.f32  {h1,h2,h3,h4}, [tex_a,smpl_x,{f1}];\n\n // Example of texture returning .f16x2 data\n tex.1d.v2.f16x2.f32  {h1,h2}, [tex_a,smpl_x,{f1}];\n\n // Example of 3d texture array access with tex.grad,unified texturing mode\n tex.grad.3d.v4.f32.f32 {%f4,%f5,%f6,%f7},[tex_3d,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n// Example of cube texture array access with tex.grad,unified texturing mode\n tex.grad.cube.v4.f32.f32{%f4,%f5,%f6,%f7},[tex_cube,{%f0,%f0,%f0,%f0}],\n                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};\n\n // Example of 1d texture lookup with offset, unified texturing mode\n tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a, {f1}], {r5};\n\n // Example of 2d texture array lookup with offset, unified texturing mode\n tex.a2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{idx,f1,f2}], {f5,f6};\n\n // Example of 2d mipmap texture lookup with offset, unified texturing mode\n tex.level.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}],\n                          flvl, {r7, r8};\n\n // Example of 2d depth texture lookup with compare, unified texturing mode\n tex.1d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a, {f1}], f0;\n\n // Example of depth 2d texture array lookup with offset, compare\n tex.a2d.v4.s32.f32  {f0,f1,f2,f3}, [tex_a,{idx,f4,f5}], {r5,r6}, f6;\n\n // Example of destination predicate use\n tex.3d.v4.s32.s32 {r1,r2,r3,r4}|p, [tex_a,{f1,f2,f3,f4}];\n9.7.10.4.\nTexture Instructions:\ntld4\n\ntld4\nPerform a texture fetch of the 4-texel bilerp footprint.\nSyntax\ntld4.comp.2d.v4.dtype.f32    d[|p], [a, c] {, e} {, f};\ntld4.comp.geom.v4.dtype.f32  d[|p], [a, b, c] {, e} {, f};  // explicit sampler\n\n.comp  = { .r, .g, .b, .a };\n.geom  = { .2d, .a2d, .cube, .acube };\n.dtype = { .u32, .s32, .f32 };\nDescription\nTexture fetch of the 4-texel bilerp footprint using a texture coordinate vector. The instruction\nloads the bilerp footprint from the texture named by operand\na\nat coordinates given by operand\nc\ninto vector destination\nd\n. The texture component fetched for each texel sample is\nspecified by\n.comp\n. The four texel samples are placed into destination vector\nd\nin\ncounter-clockwise order starting at lower left.\nAn optional texture sampler\nb\nmay be specified. If no sampler is specified, the sampler behavior\nis a property of the named texture.\nThe optional destination predicate\np\nis set to\nTrue\nif data from texture at specified\ncoordinates is resident in memory,\nFalse\notherwise. When optional destination predicate\np\nis\nset to\nFalse\n, data loaded will be all zeros. Memory residency of Texture Data at specified\ncoordinates is dependent on execution environment setup using Driver API calls, prior to kernel\nlaunch. Refer to Driver API documentation for more details including any system/implementation\nspecific behavior.\nAn optional operand\nf\nmay be specified for\ndepth textures\n. Depth textures are special type of\ntextures which hold data from the depth buffer. Depth buffer contains depth information of each\npixel. Operand\nf\nis\n.f32\nscalar value that specifies depth compare value for depth\ntextures. Each element fetched from texture is compared against value given in\nf\noperand. If\ncomparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are\nused for the filtering.\nA texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\ntld4.2d\nFor 2D textures, operand\nc\nspecifies coordinates as a two-element, 32-bit floating-point vector.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7.\ntld4.a2d\nTexture array selection, followed by\ntld4\ntexture fetch of 2d texture. For 2d texture arrays\noperand\nc\nis a four element, 32-bit vector. The first element in operand c is interpreted as an\nunsigned integer index (\n.u32\n) into the texture array, and the next two elements are interpreted\nas 32-bit floating point coordinates of 2d texture. The fourth element is ignored.\nAn optional operand\ne\nmay be specified. Operand\ne\nis a vector of type\n.v2.s32\nthat\nspecifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset\nvalue is in the range of -8 to +7.\ntld4.cube\nFor cubemap textures, operand\nc\nspecifies four-element vector which comprises three\nfloating-point coordinates (s, t, r) and a fourth padding argument which is ignored.\nCubemap textures are special two-dimensional layered textures consisting of six layers that\nrepresent the faces of a cube. All layers in a cubemap are of the same size and are square (i.e.,\nwidth equals height).\nCoordinates (s, t, r) are projected onto one of the six cube faces. The (s, t, r) coordinates can be\nthought of as a direction vector emanating from the center of the cube. Of the three coordinates (s,\nt, r), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the\nother two coordinates (the minor axes) are divided by the absolute value of the major axis to\nproduce a new (s, t) coordinate pair to lookup into the selected cube face.\nOffset vector operand\ne\nis not supported for cubemap textures.\ntld4.acube\nCubemap array selection, followed by\ntld4\ntexture fetch of cubemap texture. The first element in\noperand\nc\nis interpreted as an unsigned integer index (\n.u32\n) into the cubemap texture array,\nand the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r),\nused to lookup in the selected cubemap.\nOffset vector operand\ne\nis not supported for cubemap texture arrays.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.2.\nIndirect texture access introduced in PTX ISA version 3.1.\ntld4.{a2d,cube,acube}\nintroduced in PTX ISA version 4.3.\nOffset vector operand introduced in PTX ISA version 4.3.\nDepth compare operand introduced in PTX ISA version 4.3.\nSupport for optional destination predicate introduced in PTX ISA version 7.1.\nTarget ISA Notes\ntld4\nrequires\nsm_20\nor higher.\nIndirect texture access requires\nsm_20\nor higher.\ntld4.{a2d,cube,acube}\nrequires\nsm_30\nor higher.\nOffset vector operand requires\nsm_30\nor higher.\nDepth compare operand requires\nsm_30\nor higher.\nSupport for optional destination predicate requires\nsm_60\nor higher.\nExamples\n//Example of unified mode texturing\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}];\n\n// Example of independent mode texturing\ntld4.r.2d.v4.u32.f32  {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}];\n\n// Example of unified mode texturing using offset\ntld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};\n\n// Example of unified mode texturing using compare\ntld4.r.2d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7;\n\n// Example of optional destination predicate\ntld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7;\n9.7.10.5.\nTexture Instructions:\ntxq\n\ntxq\nQuery texture and sampler attributes.\nSyntax\ntxq.tquery.b32         d, [a];       // texture attributes\ntxq.level.tlquery.b32  d, [a], lod;  // texture attributes\ntxq.squery.b32         d, [a];       // sampler attributes\n\n.tquery  = { .width, .height, .depth,\n             .channel_data_type, .channel_order,\n             .normalized_coords, .array_size,\n             .num_mipmap_levels, .num_samples};\n\n.tlquery = { .width, .height, .depth };\n\n.squery  = { .force_unnormalized_coords, .filter_mode,\n             .addr_mode_0, addr_mode_1, addr_mode_2 };\nDescription\nQuery an attribute of a texture or sampler. Operand\na\nis either a\n.texref\nor\n.samplerref\nvariable, or a\n.u64\nregister.\nQuery\nReturns\n.width\n.height\n.depth\nvalue in elements\n.channel_data_type\nUnsigned integer corresponding to source language’s channel data type\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both\nchannel_data_type\nand channel_order queries.\n.channel_order\nUnsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and channel\norder into a single enumeration type, that value is returned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.normalized_coords\n1\n(\nTrue\n) or\n0\n(\nFalse\n).\n.force_unnormalized_coords\n1\n(\nTrue)\nor\n0\n(\nFalse).\nDefined only for\n.samplerref\nvariables in independent texture mode. Overrides the\nnormalized_coords\nfield of a\n.texref\nvariable used with a\n.samplerref\nin a\ntex\ninstruction.\n.filter_mode\nInteger from\nenum\n{\nnearest,\nlinear\n}\n.addr_mode_0\n.addr_mode_1\n.addr_mode_2\nInteger from\nenum\n{\nwrap,\nmirror,\nclamp_ogl,\nclamp_to_edge,\nclamp_to_border\n}\n.array_size\nFor a texture array, number of textures in array, 0 otherwise.\n.num_mipmap_levels\nFor a mipmapped texture, number of levels of details (LOD), 0 otherwise.\n.num_samples\nFor a multi-sample texture, number of samples, 0 otherwise.\nTexture attributes are queried by supplying a\n.texref\nargument to\ntxq\n. In unified mode,\nsampler attributes are also accessed via a\n.texref\nargument, and in independent mode sampler\nattributes are accessed via a separate\n.samplerref\nargument.\ntxq.level\ntxq.level\nrequires an additional 32bit integer argument,\nlod\n, which specifies LOD and\nqueries requested attribute for the specified LOD.\nIndirect texture access\nBeginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target\narchitecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding\nthe address of a\n.texref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.5.\nChannel data type and channel order queries were added in PTX ISA version 2.1.\nThe\n.force_unnormalized_coords\nquery was added in PTX ISA version 2.2.\nIndirect texture access introduced in PTX ISA version 3.1.\n.array_size\n,\n.num_mipmap_levels\n,\n.num_samples\nsamples queries were added in PTX ISA\nversion 4.1.\ntxq.level\nintroduced in PTX ISA version 4.3.\nTarget ISA Notes\nSupported on all target architectures.\nIndirect texture access requires\nsm_20\nor higher.\nQuerying the number of mipmap levels requires\nsm_20\nor higher.\nQuerying the number of samples requires\nsm_30\nor higher.\ntxq.level\nrequires\nsm_30\nor higher.\nExamples\ntxq.width.b32       %r1, [tex_A];\ntxq.filter_mode.b32 %r1, [tex_A];   // unified mode\ntxq.addr_mode_0.b32 %r1, [smpl_B];  // independent mode\ntxq.level.width.b32 %r1, [tex_A], %r_lod;\n9.7.10.6.\nTexture Instructions:\nistypep\n\nistypep\nQuery whether a register points to an opaque variable of a specified type.\nSyntax\nistypep.type   p, a;  // result is .pred\n\n.type = { .texref, .samplerref, .surfref };\nDescription\nWrite predicate register\np\nwith 1 if register\na\npoints to an opaque variable of the\nspecified type, and with 0 otherwise. Destination\np\nhas type\n.pred\n; the source address\noperand must be of type\n.u64\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.0.\nTarget ISA Notes\nistypep requires\nsm_30\nor higher.\nExamples\nistypep.texref istex, tptr;\nistypep.samplerref issampler, sptr;\nistypep.surfref issurface, surfptr;\n9.7.11.\nSurface Instructions\n\nThis section describes PTX instructions for accessing surfaces. PTX supports the following\noperations on surface descriptors:\nStatic initialization of surface descriptors.\nModule-scope and per-entry scope definitions of surface descriptors.\nAbility to query fields within surface descriptors.\nThese instructions provide access to surface memory.\nsuld\nsust\nsured\nsuq\n9.7.11.1.\nSurface Instructions:\nsuld\n\nsuld\nLoad from surface memory.\nSyntax\nsuld.b.geom{.cop}.vec.dtype.clamp  d, [a, b];  // unformatted\n\n.geom  = { .1d, .2d, .3d, .a1d, .a2d };\n.cop   = { .ca, .cg, .cs, .cv };               // cache operation\n.vec   = { none, .v2, .v4 };\n.dtype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero };\nDescription\nsuld.b.{1d,2d,3d}\nLoad from surface memory using a surface coordinate vector. The instruction loads data from the\nsurface named by operand\na\nat coordinates given by operand\nb\ninto destination\nd\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a scalar or singleton tuple\nfor 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d\nsurfaces, where the fourth element is ignored. Coordinate elements are of type\n.s32\n.\nsuld.b\nperforms an unformatted load of binary data. The lowest dimension coordinate represents a\nbyte offset into the surface and is not scaled, and the size of the data transfer matches the size\nof destination operand\nd\n.\nsuld.b.{a1d,a2d}\nSurface layer selection, followed by a load from the selected surface. The instruction first selects\na surface layer from the surface array named by operand\na\nusing the index given by the first\nelement of the array coordinate vector\nb\n. The instruction then loads data from the selected\nsurface at coordinates given by the remaining elements of operand\nb\ninto destination\nd\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a bit-size\ntype vector or tuple containing an index into the array of surfaces followed by coordinates within\nthe selected surface, as follows:\nFor 1d surface arrays, operand\nb\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the second element is interpreted as a\n1d surface coordinate of type\n.s32\n.\nFor 2d surface arrays, operand\nb\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the next two elements are interpreted\nas 2d surface coordinates of type\n.s32\n. The fourth element is ignored.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nloads data at the nearest surface location (sized appropriately)\n.zero\nloads zero for out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nsuld.b.trap\nintroduced in PTX ISA version 1.5.\nAdditional clamp modifiers and cache operations introduced in PTX ISA version 2.0.\nsuld.b.3d\nand\nsuld.b.{a1d,a2d}\nintroduced in PTX ISA version 3.0.\nIndirect surface access introduced in PTX ISA version 3.1.\nTarget ISA Notes\nsuld.b\nsupported on all target architectures.\nsm_1x\ntargets support only the\n.trap\nclamping modifier.\nsuld.3d\nand\nsuld.{a1d,a2d}\nrequire\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nExamples\nsuld.b.1d.v4.b32.trap  {s1,s2,s3,s4}, [surf_B, {x}];\nsuld.b.3d.v2.b64.trap  {r1,r2}, [surf_A, {x,y,z,w}];\nsuld.b.a1d.v2.b32      {r0,r1}, [surf_C, {idx,x}];\nsuld.b.a2d.b32         r0, [surf_D, {idx,x,y,z}];  // z ignored\n9.7.11.2.\nSurface Instructions:\nsust\n\nsust\nStore to surface memory.\nSyntax\nsust.b.{1d,2d,3d}{.cop}.vec.ctype.clamp  [a, b], c;  // unformatted\nsust.p.{1d,2d,3d}.vec.b32.clamp          [a, b], c;  // formatted\n\nsust.b.{a1d,a2d}{.cop}.vec.ctype.clamp   [a, b], c;  // unformatted\n\n.cop   = { .wb, .cg, .cs, .wt };                     // cache operation\n.vec   = { none, .v2, .v4 };\n.ctype = { .b8 , .b16, .b32, .b64 };\n.clamp = { .trap, .clamp, .zero };\nDescription\nsust.{1d,2d,3d}\nStore to surface memory using a surface coordinate vector. The instruction stores data from operand\nc\nto the surface named by operand\na\nat coordinates given by operand\nb\n. Operand\na\nis\na\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a scalar or singleton tuple for 1d\nsurfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces,\nwhere the fourth element is ignored. Coordinate elements are of type\n.s32\n.\nsust.b\nperforms an unformatted store of binary data. The lowest dimension coordinate represents\na byte offset into the surface and is not scaled. The size of the data transfer matches the size of\nsource operand\nc\n.\nsust.p\nperforms a formatted store of a vector of 32-bit data values to a surface sample. The\nsource vector elements are interpreted left-to-right as\nR\n,\nG\n,\nB\n, and\nA\nsurface\ncomponents. These elements are written to the corresponding surface sample components. Source\nelements that do not occur in the surface sample are ignored. Surface sample components that do not\noccur in the source vector will be written with an unpredictable value. The lowest dimension\ncoordinate represents a sample offset rather than a byte offset.\nThe source data interpretation is based on the surface sample format as follows: If the surface\nformat contains\nUNORM\n,\nSNORM\n, or\nFLOAT\ndata, then\n.f32\nis assumed; if the surface\nformat contains\nUINT\ndata, then\n.u32\nis assumed; if the surface format contains\nSINT\ndata, then\n.s32\nis assumed. The source data is then converted from this type to the surface\nsample format.\nsust.b.{a1d,a2d}\nSurface layer selection, followed by an unformatted store to the selected surface. The instruction\nfirst selects a surface layer from the surface array named by operand\na\nusing the index given by\nthe first element of the array coordinate vector\nb\n. The instruction then stores the data in\noperand\nc\nto the selected surface at coordinates given by the remaining elements of operand\nb\n. Operand\na\nis a .surfref variable or\n.u64\nregister. Operand\nb\nis a bit-size type\nvector or tuple containing an index into the array of surfaces followed by coordinates within the\nselected surface, as follows:\nFor 1d surface arrays, operand\nb\nhas type\n.v2.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the second element is interpreted as\na 1d surface coordinate of type\n.s32\n.\nFor 2d surface arrays, operand\nb\nhas type\n.v4.b32\n. The first element is interpreted as an\nunsigned integer index (\n.u32\n) into the surface array, and the next two elements are\ninterpreted as 2d surface coordinates of type\n.s32\n. The fourth element is ignored.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nstores data at the nearest surface location (sized appropriately)\n.zero\ndrops stores to out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nsust.b.trap\nintroduced in PTX ISA version 1.5.\nsust.p\n, additional clamp modifiers, and\ncache operations introduced in PTX ISA version 2.0.\nsust.b.3d\nand\nsust.b.{a1d,a2d}\nintroduced in PTX ISA version 3.0.\nIndirect surface access introduced in PTX ISA version 3.1.\nTarget ISA Notes\nsust.b\nsupported on all target architectures.\nsm_1x\ntargets support only the\n.trap\nclamping modifier.\nsust.3d\nand\nsust.{a1d,a2d}\nrequire\nsm_20\nor higher.\nsust.p\nrequires\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\nCache operations require\nsm_20\nor higher.\nExamples\nsust.p.1d.v4.b32.trap  [surf_B, {x}], {f1,f2,f3,f4};\nsust.b.3d.v2.b64.trap  [surf_A, {x,y,z,w}], {r1,r2};\nsust.b.a1d.v2.b64      [surf_C, {idx,x}], {r1,r2};\nsust.b.a2d.b32         [surf_D, {idx,x,y,z}], r0;  // z ignored\n9.7.11.3.\nSurface Instructions:\nsured\n\nsured\nReduce surface memory.\nSyntax\nsured.b.op.geom.ctype.clamp  [a,b],c; // byte addressing\nsured.p.op.geom.ctype.clamp  [a,b],c; // sample addressing\n\n.op    = { .add, .min, .max, .and, .or };\n.geom  = { .1d, .2d, .3d };\n.ctype = { .u32, .u64, .s32, .b32, .s64 };  // for sured.b\n.ctype = { .b32, .b64 };                    // for sured.p\n.clamp = { .trap, .clamp, .zero };\nDescription\nReduction to surface memory using a surface coordinate vector. The instruction performs a reduction\noperation with data from operand\nc\nto the surface named by operand\na\nat coordinates given by\noperand\nb\n. Operand\na\nis a\n.surfref\nvariable or\n.u64\nregister. Operand\nb\nis a\nscalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a\nfour-element vector for 3d surfaces, where the fourth element is ignored. Coordinate elements are of\ntype\n.s32\n.\nsured.b\nperforms an unformatted reduction on\n.u32\n,\n.s32\n,\n.b32\n,\n.u64\n, or\n.s64\ndata. The lowest dimension coordinate represents a byte offset into the surface and is not\nscaled. Operation\nadd\napplies to\n.u32\n,\n.u64\n, and\n.s32\ntypes;\nmin\nand\nmax\napply to\n.u32\n,\n.s32\n,\n.u64\nand\n.s64\ntypes; operations\nand\nand\nor\napply to\n.b32\ntype.\nsured.p\nperforms an unformatted reduction on sample-addressed data. The lowest dimension coordinate\nrepresents a sample offset rather than a byte offset and is scaled by the data-size given by\n.ctype\n.\nThe instruction type\n.b64\nis restricted to\nmin\nand\nmax\noperations. For type\n.b32\n,\nthe data is interpreted as\n.u32\nor\n.s32\nbased on the surface sample format as follows:\nif the surface format contains\nUINT\ndata, then\n.u32\nis assumed; if the surface format contains\nSINT\ndata, then\n.s32\nis assumed. For type\n.b64\n, if the surface format contains\nUINT\ndata,\nthen\n.u64\nis assumed; if the surface format contains\nSINT\ndata, then\n.s64\nis assumed.\nA surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the\ncoordinate vector must be naturally aligned to a multiple of the access size. If an address is not\nproperly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently\nmasking off low-order address bits to achieve proper rounding, or the instruction may fault.\nThe\n.clamp\nfield specifies how to handle out-of-bounds addresses:\n.trap\ncauses an execution trap on out-of-bounds addresses\n.clamp\nstores data at the nearest surface location (sized appropriately)\n.zero\ndrops stores to out-of-bounds addresses\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nIndirect surface access introduced in PTX ISA version 3.1.\n.u64\n/\n.s64\n/\n.b64\ntypes with\n.min\n/\n.max\noperations introduced in PTX ISA version\n8.1.\nTarget ISA Notes\nsured requires\nsm_20\nor higher.\nIndirect surface access requires\nsm_20\nor higher.\n.u64\n/\n.s64\n/\n.b64\ntypes with\n.min\n/\n.max\noperations requires\nsm_50\nor higher.\nExamples\nsured.b.add.2d.u32.trap  [surf_A, {x,y}], r1;\nsured.p.min.1d.u32.trap  [surf_B, {x}], r1;\nsured.b.max.1d.u64.trap  [surf_C, {x}], r1;\nsured.p.min.1d.b64.trap  [surf_D, {x}], r1;\n9.7.11.4.\nSurface Instructions:\nsuq\n\nsuq\nQuery a surface attribute.\nSyntax\nsuq.query.b32   d, [a];\n\n.query = { .width, .height, .depth,\n           .channel_data_type, .channel_order,\n           .array_size, .memory_layout };\nDescription\nQuery an attribute of a surface. Operand\na\nis a\n.surfref\nvariable or a\n.u64\nregister.\nQuery\nReturns\n.width\n.height\n.depth\nvalue in elements\n.channel_data_type\nUnsigned integer corresponding to source language’s channel data\ntype enumeration. If the source language combines channel data\ntype and channel order into a single enumeration type, that value\nis returned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.channel_order\nUnsigned integer corresponding to source language’s channel order\nenumeration. If the source language combines channel data type and\nchannel order into a single enumeration type, that value is\nreturned for both\nchannel_data_type\nand\nchannel_order\nqueries.\n.array_size\nFor a surface array, number of surfaces in array, 0 otherwise.\n.memory_layout\n1\nfor surface with linear memory layout;\n0\notherwise\nIndirect surface access\nBeginning with PTX ISA version 3.1, indirect surface access is supported for target architecture\nsm_20\nor higher. In indirect access, operand\na\nis a\n.u64\nregister holding the address of\na\n.surfref\nvariable.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.5.\nChannel data type and channel order queries added in PTX ISA version 2.1.\nIndirect surface access introduced in PTX ISA version 3.1.\nThe\n.array_size\nquery was added in PTX ISA version 4.1.\nThe\n.memory_layout\nquery was added in PTX ISA version 4.2.\nTarget ISA Notes\nSupported on all target architectures.\nIndirect surface access requires\nsm_20\nor higher.\nExamples\nsuq.width.b32       %r1, [surf_A];\n9.7.12.\nControl Flow Instructions\n\nThe following PTX instructions and syntax are for controlling execution in a PTX program:\n{}\n@\nbra\ncall\nret\nexit\n9.7.12.1.\nControl Flow Instructions:\n{}\n\n{}\nInstruction grouping.\nSyntax\n{ instructionList }\nDescription\nThe curly braces create a group of instructions, used primarily for defining a function body. The\ncurly braces also provide a mechanism for determining the scope of a variable: any variable declared\nwithin a scope is not available outside the scope.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n{ add.s32  a,b,c; mov.s32  d,a; }\n9.7.12.2.\nControl Flow Instructions:\n@\n\n@\nPredicated execution.\nSyntax\n@{!}p    instruction;\nDescription\nExecute an instruction or instruction block for threads that have the guard predicate\nTrue\n. Threads with a\nFalse\nguard predicate do nothing.\nSemantics\nIf\n{!}p\nthen instruction\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nsetp.eq.f32  p,y,0;     // is y zero?\n@!p div.f32      ratio,x,y  // avoid division by zero\n\n@q  bra L23;                // conditional branch\n9.7.12.3.\nControl Flow Instructions:\nbra\n\nbra\nBranch to a target and continue execution there.\nSyntax\n@p   bra{.uni}  tgt;           // tgt is a label\n     bra{.uni}  tgt;           // unconditional branch\nDescription\nContinue execution at the target. Conditional branches are specified by using a guard predicate. The\nbranch target must be a label.\nbra.uni\nis guaranteed to be non-divergent, i.e. all active threads in a warp that are currently\nexecuting this instruction have identical values for the guard predicate and branch target.\nSemantics\nif (p) {\n    pc = tgt;\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nUnimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nbra.uni  L_exit;    // uniform unconditional jump\n@q  bra      L23;   // conditional branch\n9.7.12.4.\nControl Flow Instructions:\nbrx.idx\n\nbrx.idx\nBranch to a label indexed from a list of potential branch targets.\nSyntax\n@p    brx.idx{.uni} index, tlist;\n      brx.idx{.uni} index, tlist;\nDescription\nIndex into a list of possible destination labels, and continue execution from the chosen\nlabel. Conditional branches are specified by using a guard predicate.\nbrx.idx.uni\nguarantees that the branch is non-divergent, i.e. all active threads in a warp that\nare currently executing this instruction have identical values for the guard predicate and the\nindex\nargument.\nThe\nindex\noperand is a\n.u32\nregister. The\ntlist\noperand must be the label of a\n.branchtargets\ndirective. It is accessed as a zero-based sequence using\nindex\n. Behaviour is\nundefined if the value of\nindex\nis greater than or equal to the length of\ntlist\n.\nThe\n.branchtargets\ndirective must be defined in the local function scope before it is used. It\nmust refer to labels within the current function.\nSemantics\nif (p) {\n    if (index < length(tlist)) {\n      pc = tlist[index];\n    } else {\n      pc = undefined;\n    }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\n.function foo () {\n    .reg .u32 %r0;\n    ...\n    L1:\n    ...\n    L2:\n    ...\n    L3:\n    ...\n    ts: .branchtargets L1, L2, L3;\n    @p brx.idx %r0, ts;\n    ...\n}\n9.7.12.5.\nControl Flow Instructions:\ncall\n\ncall\nCall a function, recording the return location.\nSyntax\n// direct call to named function, func is a symbol\ncall{.uni} (ret-param), func, (param-list);\ncall{.uni} func, (param-list);\ncall{.uni} func;\n\n// indirect call via pointer, with full list of call targets\ncall{.uni} (ret-param), fptr, (param-list), flist;\ncall{.uni} fptr, (param-list), flist;\ncall{.uni} fptr, flist;\n\n// indirect call via pointer, with no knowledge of call targets\ncall{.uni} (ret-param), fptr, (param-list), fproto;\ncall{.uni} fptr, (param-list), fproto;\ncall{.uni} fptr, fproto;\nDescription\nThe\ncall\ninstruction stores the address of the next instruction, so execution can resume at that\npoint after executing a\nret\ninstruction. A\ncall\nis assumed to be divergent unless the\n.uni\nsuffix is present. The\n.uni\nsuffix indicates that the\ncall\nis guaranteed to be\nnon-divergent, i.e. all active threads in a warp that are currently executing this instruction have\nidentical values for the guard predicate and\ncall\ntarget.\nFor direct calls, the called location\nfunc\nmust be a symbolic function name; for indirect calls,\nthe called location\nfptr\nmust be an address of a function held in a register. Input arguments\nand return values are optional. Arguments may be registers, immediate constants, or variables in\n.param\nspace. Arguments are pass-by-value.\nIndirect calls require an additional operand,\nflist\nor\nfproto\n, to communicate the list of\npotential\ncall\ntargets or the common function prototype of all\ncall\ntargets,\nrespectively. In the first case,\nflist\ngives a complete list of potential\ncall\ntargets and\nthe optimizing backend is free to optimize the calling convention. In the second case, where the\ncomplete list of potential\ncall\ntargets may not be known, the common function prototype is given\nand the\ncall\nmust obey the ABI’s calling convention.\nThe\nflist\noperand is either the name of an array (call table) initialized to a list of function\nnames; or a label associated with a\n.calltargets\ndirective, which declares a list of potential\ncall\ntargets. In both cases the fptr register holds the address of a function listed in the call\ntable or\n.calltargets\nlist, and the\ncall\noperands are type-checked against the type\nsignature of the functions indicated by\nflist\n.\nThe fproto operand is the name of a label associated with a\n.callprototype\ndirective. This\noperand is used when a complete list of potential targets is not known. The\ncall\noperands are\ntype-checked against the prototype, and code generation will follow the ABI calling convention. If a\nfunction that doesn’t match the prototype is called, the behavior is undefined.\nCall tables may be declared at module scope or local scope, in either the constant or global state\nspace. The\n.calltargets\nand\n.callprototype\ndirectives must be declared within a function\nbody. All functions must be declared prior to being referenced in a\ncall\ntable initializer or\n.calltargets\ndirective.\nPTX ISA Notes\nDirect\ncall\nintroduced in PTX ISA version 1.0. Indirect\ncall\nintroduced in PTX ISA version 2.1.\nTarget ISA Notes\nDirect\ncall\nsupported on all target architectures. Indirect\ncall\nrequires\nsm_20\nor higher.\nExamples\n// examples of direct call\n    call     init;    // call function 'init'\n    call.uni g, (a);  // call function 'g' with parameter 'a'\n@p  call     (d), h, (a, b);  // return value into register d\n\n// call-via-pointer using jump table\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n\n.global .u32 jmptbl[5] = { foo, bar, baz };\n      ...\n@p    ld.global.u32  %r0, [jmptbl+4];\n@p    ld.global.u32  %r0, [jmptbl+8];\n      call  (retval), %r0, (x, y), jmptbl;\n\n// call-via-pointer using .calltargets directive\n.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...\n.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...\n      ...\n@p    mov.u32  %r0, foo;\n@q    mov.u32  %r0, baz;\nFtgt: .calltargets foo, bar, baz;\n      call  (retval), %r0, (x, y), Ftgt;\n\n// call-via-pointer using .callprototype directive\n.func dispatch (.reg .u32 fptr, .reg .u32 idx)\n{\n...\nFproto: .callprototype _ (.param .u32 _, .param .u32 _);\n      call  %fptr, (x, y), Fproto;\n...\n9.7.12.6.\nControl Flow Instructions:\nret\n\nret\nReturn from function to instruction after call.\nSyntax\nret{.uni};\nDescription\nReturn execution to caller’s environment. A divergent return suspends threads until all threads are\nready to return to the caller. This allows multiple divergent\nret\ninstructions.\nA\nret\nis assumed to be divergent unless the\n.uni\nsuffix is present, indicating that the\nreturn is guaranteed to be non-divergent.\nAny values returned from a function should be moved into the return parameter variables prior to\nexecuting the\nret\ninstruction.\nA return instruction executed in a top-level entry routine will terminate thread execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nret;\n@p  ret;\n9.7.12.7.\nControl Flow Instructions:\nexit\n\nexit\nTerminate a thread.\nSyntax\nexit;\nDescription\nEnds execution of a thread.\nBarriers exclusively waiting on arrivals from exited threads are always released.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nexit;\n@p  exit;\n9.7.13.\nParallel Synchronization and Communication Instructions\n\nThese instructions are:\nbar{.cta}\n,\nbarrier{.cta}\nbar.warp.sync\nbarrier.cluster\nmembar\natom\nred\nred.async\nvote\nmatch.sync\nactivemask\nredux.sync\ngriddepcontrol\nelect.sync\nmbarrier.init\nmbarrier.inval\nmbarrier.arrive\nmbarrier.arrive_drop\nmbarrier.test_wait\nmbarrier.try_wait\nmbarrier.pending_count\ncp.async.mbarrier.arrive\ntensormap.cp_fenceproxy\nclusterlaunchcontrol.try_cancel\nclusterlaunchcontrol.query_cancel\n9.7.13.1.\nParallel Synchronization and Communication Instructions:\nbar\n,\nbarrier\n\nbar\n,\nbar.cta\n,\nbarrier\n,\nbarrier.cta\nBarrier synchronization.\nSyntax\nbarrier{.cta}.sync{.aligned}      a{, b};\nbarrier{.cta}.arrive{.aligned}    a, b;\n\nbarrier{.cta}.red.popc{.aligned}.u32  d, a{, b}, {!}c;\nbarrier{.cta}.red.op{.aligned}.pred   p, a{, b}, {!}c;\n\nbar{.cta}.sync      a{, b};\nbar{.cta}.arrive    a, b;\n\nbar{.cta}.red.popc.u32  d, a{, b}, {!}c;\nbar{.cta}.red.op.pred   p, a{, b}, {!}c;\n\n.op = { .and, .or };\nDescription\nPerforms barrier synchronization and communication within a CTA. Each CTA instance has sixteen\nbarriers numbered\n0..15\n.\nbarrier{.cta}\ninstructions can be used by the threads within the CTA for synchronization and\ncommunication.\nOperands\na\n,\nb\n, and\nd\nhave type\n.u32\n; operands\np\nand\nc\nare predicates. Source\noperand\na\nspecifies a logical barrier resource as an immediate constant or register with value\n0\nthrough\n15\n. Operand\nb\nspecifies the number of threads participating in the barrier. If\nno thread count is specified, all threads in the CTA participate in the barrier. When specifying a\nthread count, the value must be a multiple of the warp size. Note that a non-zero thread count is\nrequired for\nbarrier{.cta}.arrive\n.\nDepending on operand\nb\n, either specified number of threads (in multiple of warp size) or all\nthreads in the CTA participate in\nbarrier{.cta}\ninstruction. The\nbarrier{.cta}\ninstructions\nsignal the arrival of the executing threads at the named barrier.\nbarrier{.cta}\ninstruction causes executing thread to wait for all non-exited threads from its\nwarp and marks warps’ arrival at barrier. In addition to signaling its arrival at the barrier, the\nbarrier{.cta}.red\nand\nbarrier{.cta}.sync\ninstructions causes executing thread to wait for\nnon-exited threads of all other warps participating in the barrier to\narrive.\nbarrier{.cta}.arrive\ndoes not cause executing thread to wait for threads of other\nparticipating warps.\nWhen a barrier completes, the waiting threads are restarted without delay, and the barrier is\nreinitialized so that it can be immediately reused.\nThe\nbarrier{.cta}.sync\nor\nbarrier{.cta}.red\nor\nbarrier{.cta}.arrive\ninstruction\nguarantees that when the barrier completes, prior memory accesses requested by this thread are\nperformed relative to all threads participating in the barrier. The\nbarrier{.cta}.sync\nand\nbarrier{.cta}.red\ninstruction further guarantees that no new memory access is requested by this\nthread before the barrier completes.\nA memory read (e.g., by\nld\nor\natom\n) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread participating in the barrier. A\nmemory write (e.g., by\nst\n,\nred\nor\natom\n) has been performed when the value written has\nbecome visible to other threads participating in the barrier, that is, when the previous value can\nno longer be read.\nbarrier{.cta}.red\nperforms a reduction operation across threads. The\nc\npredicate (or its\ncomplement) from all threads in the CTA are combined using the specified reduction operator. Once\nthe barrier count is reached, the final value is written to the destination register in all threads\nwaiting at the barrier.\nThe reduction operations for\nbarrier{.cta}.red\nare population-count (\n.popc\n),\nall-threads-True (\n.and\n), and any-thread-True (\n.or\n). The result of\n.popc\nis the number of\nthreads with a\nTrue\npredicate, while\n.and\nand\n.or\nindicate if all the threads had a\nTrue\npredicate or if any of the threads had a\nTrue\npredicate.\nInstruction\nbarrier{.cta}\nhas optional\n.aligned\nmodifier. When specified, it indicates that\nall threads in CTA will execute the same\nbarrier{.cta}\ninstruction. In conditionally executed\ncode, an aligned\nbarrier{.cta}\ninstruction should only be used if it is known that all threads\nin CTA evaluate the condition identically, otherwise behavior is undefined.\nDifferent warps may execute different forms of the\nbarrier{.cta}\ninstruction using the same\nbarrier name and thread count. One example mixes\nbarrier{.cta}.sync\nand\nbarrier{.cta}.arrive\nto implement producer/consumer models. The producer threads execute\nbarrier{.cta}.arrive\nto\nannounce their arrival at the barrier and continue execution without delay to produce the next\nvalue, while the consumer threads execute the\nbarrier{.cta}.sync\nto wait for a resource to be\nproduced. The roles are then reversed, using a different barrier, where the producer threads execute\na\nbarrier{.cta}.sync\nto wait for a resource to consumed, while the consumer threads announce\nthat the resource has been consumed with\nbarrier{.cta}.arrive\n. Care must be taken to keep a warp\nfrom executing more\nbarrier{.cta}\ninstructions than intended (\nbarrier{.cta}.arrive\nfollowed\nby any other\nbarrier{.cta}\ninstruction to the same barrier) prior to the reset of the\nbarrier.\nbarrier{.cta}.red\nshould not be intermixed with\nbarrier{.cta}.sync\nor\nbarrier{.cta}.arrive\nusing the same active barrier. Execution in this case is unpredictable.\nThe optional\n.cta\nqualifier simply indicates CTA-level applicability of the barrier and it\ndoesn’t change the semantics of the instruction.\nbar{.cta}.sync\nis equivalent to\nbarrier{.cta}.sync.aligned\n.\nbar{.cta}.arrive\nis\nequivalent to\nbarrier{.cta}.arrive.aligned\n.\nbar{.cta}.red\nis equivalent to\nbarrier{.cta}.red.aligned\n.\nNote\nFor .target\nsm_6x\nor below,\nbarrier{.cta}\ninstruction without\n.aligned\nmodifier is equivalent to\n.aligned\nvariant and has the same restrictions as of\n.aligned\nvariant.\nAll threads in warp (except for those have exited) must execute\nbarrier{.cta}\ninstruction\nin convergence.\nPTX ISA Notes\nbar.sync\nwithout a thread count introduced in PTX ISA version 1.0.\nRegister operands, thread count, and\nbar.{arrive,red}\nintroduced in PTX ISA version 2.0.\nbarrier\ninstruction introduced in PTX ISA version 6.0.\n.cta\nqualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nRegister operands, thread count, and\nbar{.cta}.{arrive,red}\nrequire\nsm_20\nor higher.\nOnly\nbar{.cta}.sync\nwith an immediate barrier number is supported for\nsm_1x\ntargets.\nbarrier{.cta}\ninstruction requires\nsm_30\nor higher.\nExamples\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for all threads in CTA to also arrive:\n    st.shared [r0],r1;  // write my result to shared memory\n    bar.cta.sync  1;    // arrive, wait for others to arrive\n    ld.shared r2,[r3];  // use shared results from other threads\n\n// Use bar.sync to arrive at a pre-computed barrier number and\n// wait for fixed number of cooperating threads to arrive:\n    #define CNT1 (8*12) // Number of cooperating threads\n\n    st.shared [r0],r1;     // write my result to shared memory\n    bar.cta.sync  1, CNT1; // arrive, wait for others to arrive\n    ld.shared r2,[r3];     // use shared results from other threads\n\n// Use bar.red.and to compare results across the entire CTA:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.and.pred r3,1,p; // r3=AND(p) forall threads in CTA\n\n// Use bar.red.popc to compute the size of a group of threads\n// that have a specific condition True:\n    setp.eq.u32 p,r1,r2;         // p is True if r1==r2\n    bar.cta.red.popc.u32 r3,1,p; // r3=SUM(p) forall threads in CTA\n\n// Examples of barrier.cta.sync\n    st.shared         [r0],r1;\n    barrier.cta.sync  0;\n    ld.shared         r1, [r0];\n\n/* Producer/consumer model. The producer deposits a value in\n * shared memory, signals that it is complete but does not wait\n * using bar.arrive, and begins fetching more data from memory.\n * Once the data returns from memory, the producer must wait\n * until the consumer signals that it has read the value from\n * the shared memory location. In the meantime, a consumer\n * thread waits until the data is stored by the producer, reads\n * it, and then signals that it is done (without waiting).\n */\n    // Producer code places produced value in shared memory.\n    st.shared   [r0],r1;\n    bar.arrive  0,64;\n    ld.global   r1,[r2];\n    bar.sync    1,64;\n    ...\n\n    // Consumer code, reads value from shared memory\n    bar.sync   0,64;\n    ld.shared  r1,[r0];\n    bar.arrive 1,64;\n    ...\n9.7.13.2.\nParallel Synchronization and Communication Instructions:\nbar.warp.sync\n\nbar.warp.sync\nBarrier synchronization for threads in a warp.\nSyntax\nbar.warp.sync      membermask;\nDescription\nbar.warp.sync\nwill cause executing thread to wait until all threads corresponding to\nmembermask\nhave executed a\nbar.warp.sync\nwith the same\nmembermask\nvalue before resuming\nexecution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin barrier where the bit position corresponds to thread’s\nlaneid\n.\nThe behavior of\nbar.warp.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nbar.warp.sync\nalso guarantee memory ordering among threads participating in barrier. Thus,\nthreads within warp that wish to communicate via memory can store to memory, execute\nbar.warp.sync\n, and then safely read values stored by other threads in warp.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nbar.warp.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when the\nbar.warp.sync\ninstruction is executed. Otherwise, the behavior is\nundefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nst.shared.u32 [r0],r1;         // write my result to shared memory\nbar.warp.sync  0xffffffff;     // arrive, wait for others to arrive\nld.shared.u32 r2,[r3];         // read results written by other threads\n9.7.13.3.\nParallel Synchronization and Communication Instructions:\nbarrier.cluster\n\nbarrier.cluster\nBarrier synchronization within a cluster.\nSyntax\nbarrier.cluster.arrive{.sem}{.aligned};\nbarrier.cluster.wait{.acquire}{.aligned};\n\n.sem = {.release, .relaxed}\nDescription\nPerforms barrier synchronization and communication within a cluster.\nbarrier.cluster\ninstructions can be used by the threads within the cluster for synchronization\nand communication.\nbarrier.cluster.arrive\ninstruction marks warps’ arrival at barrier without causing executing\nthread to wait for threads of other participating warps.\nbarrier.cluster.wait\ninstruction causes the executing thread to wait for all non-exited threads\nof the cluster to perform\nbarrier.cluster.arrive\n.\nIn addition,\nbarrier.cluster\ninstructions cause the executing thread to wait for all non-exited\nthreads from its warp.\nWhen all non-exited threads in the cluster have executed\nbarrier.cluster.arrive\n, the barrier\ncompletes and is automatically reinitialized. After using\nbarrier.cluster.wait\nto detect completion\nof the barrier, a thread may immediately arrive at the barrier once again.\nEach thread must arrive at the barrier only once before the barrier completes.\nThe\nbarrier.cluster.wait\ninstruction guarantees that when it completes the execution, memory\naccesses (except asynchronous operations) requested, in program order, prior to the preceding\nbarrier.cluster.arrive\nby all threads in the cluster are complete and visible to the executing\nthread.\nThere is no memory ordering and visibility guarantee for memory accesses requested by the executing\nthread, in program order, after\nbarrier.cluster.arrive\nand prior to\nbarrier.cluster.wait\n.\nThe optional\n.relaxed\nqualifier on\nbarrier.cluster.arrive\nspecifies that there are no memory\nordering and visibility guarantees provided for the memory accesses performed prior to\nbarrier.cluster.arrive\n.\nThe optional\n.sem\nand\n.acquire\nqualifiers on instructions\nbarrier.cluster.arrive\nand\nbarrier.cluster.wait\nspecify the memory synchronization as described in the\nMemory Consistency Model\n. If the optional\n.sem\nqualifier is absent for\nbarrier.cluster.arrive\n,\n.release\nis assumed by default. If the optional\n.acquire\nqualifier is absent for\nbarrier.cluster.wait\n,\n.acquire\nis assumed by default.\nThe optional\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nbarrier.cluster\ninstruction. In conditionally executed code, an aligned\nbarrier.cluster\ninstruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nSupport for\n.acquire\n,\n.relaxed\n,\n.release\nqualifiers introduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n// use of arrive followed by wait\nld.shared::cluster.u32 r0, [addr];\nbarrier.cluster.arrive.aligned;\n...\nbarrier.cluster.wait.aligned;\nst.shared::cluster.u32 [addr], r1;\n\n// use memory fence prior to arrive for relaxed barrier\n@cta0 ld.shared::cluster.u32 r0, [addr];\nfence.cluster.acq_rel;\nbarrier.cluster.arrive.relaxed.aligned;\n...\nbarrier.cluster.wait.aligned;\n@cta1 st.shared::cluster.u32 [addr], r1;\n9.7.13.4.\nParallel Synchronization and Communication Instructions:\nmembar\n/\nfence\n\nmembar\n,\nfence\nEnforce an ordering of memory operations.\nSyntax\n// Thread fence:\nfence{.sem}.scope;\n\n// Thread fence:\nfence.acquire.sync_restrict::shared::cluster.cluster;\nfence.release.sync_restrict::shared::cta.cluster;\n\n// Operation fence:\nfence.op_restrict.release.cluster;\n\n// Proxy fence (bi-directional):\nfence.proxy.proxykind;\n\n// Proxy fence (uni-directional):\nfence.proxy.to_proxykind::from_proxykind.release.scope;\nfence.proxy.to_proxykind::from_proxykind.acquire.scope  [addr], size;\nfence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\nfence.proxy.async::generic.release.sync_restrict::shared::cta.cluster;\n\n// Old style membar:\nmembar.level;\nmembar.proxy.proxykind;\n\n.sem       = { .sc, .acq_rel, .acquire, .release };\n.scope     = { .cta, .cluster, .gpu, .sys };\n.level     = { .cta, .gl, .sys };\n.proxykind = { .alias, .async, .async.global, .async.shared::{cta, cluster} };\n.op_restrict = { .mbarrier_init };\n.to_proxykind::from_proxykind = {.tensormap::generic};\nDescription\nThe\nmembar\ninstruction guarantees that prior memory accesses requested by this thread (\nld\n,\nst\n,\natom\nand\nred\ninstructions) are performed at the specified\nlevel\n, before later\nmemory operations requested by this thread following the\nmembar\ninstruction. The\nlevel\nqualifier specifies the set of threads that may observe the ordering effect of this operation.\nA memory read (e.g., by\nld\nor\natom\n) has been performed when the value read has been\ntransmitted from memory and cannot be modified by another thread at the indicated level. A memory\nwrite (e.g., by\nst\n,\nred\nor\natom\n) has been performed when the value written has become\nvisible to other threads at the specified level, that is, when the previous value can no longer be\nread.\nThe\nfence\ninstruction establishes an ordering between memory accesses requested by this thread\n(\nld\n,\nst\n,\natom\nand\nred\ninstructions) as described in the\nMemory Consistency Model\n. The scope qualifier specifies the set of threads that may\nobserve the ordering effect of this operation.\nfence.acq_rel\nis a light-weight fence that is sufficient for memory synchronization in most\nprograms. Instances of\nfence.acq_rel\nsynchronize when combined with additional memory operations\nas described in\nacquire\nand\nrelease\npatterns in the\nMemory Consistency Model\n.\nIf the optional\n.sem\nqualifier is absent,\n.acq_rel\nis assumed by default.\nfence.sc\nis a slower fence that can restore\nsequential consistency\nwhen used in sufficient\nplaces, at the cost of performance. Instances of\nfence.sc\nwith sufficient scope always\nsynchronize by forming a total order per scope, determined at runtime. This total order can be\nconstrained further by other synchronization in the program.\nQualifiers\n.op_restrict\nand\n.sync_restrict\nrestrict the class of memory operations\nfor which the\nfence\ninstruction provides the memory ordering guarantees. When\n.op_restrict\nis\n.mbarrier_init\n, the synchronizing effect of the fence only applies to the prior\nmbarrier.init\noperations executed by the same thread on\nmbarrier objects\nin\n.shared::cta\nstate space. When\n.sync_restrict\nis\n.sync_restrict::shared::cta\n,\n.sem\nmust be\n.release\n, and the effect of the fence only applies to operations performed on objects in\n.shared::cta\nstate space. Likewise, when\n.sync_restrict\nis\n.sync_restrict::shared::cluster\n,\n.sem\nmust be\n.acquire\n, and the effect of the fence only applies to operations performed on\nobjects in\n.shared::cluster\nstate space. When either\n.sync_restrict::shared::cta\nor\n.sync_restrict::shared::cluster\nis present, the\n.scope\nmust be specified as\n.cluster\n.\nThe address operand\naddr\nand the operand\nsize\ntogether specify the memory range\n[addr,\naddr+size-1]\non which the ordering guarantees on the memory accesses across the proxies is to be\nprovided. The only supported value for the\nsize\noperand is 128, which must be a constant integer literal.\nGeneric Addressing\nis used unconditionally, and the address specified by\nthe operand\naddr\nmust fall within the\n.global\nstate space. Otherwise, the behavior is undefined.\nOn\nsm_70\nand higher\nmembar\nis a synonym for\nfence.sc\n1\n, and the\nmembar\nlevels\ncta\n,\ngl\nand\nsys\nare synonymous with the\nfence\nscopes\ncta\n,\ngpu\nand\nsys\nrespectively.\nmembar.proxy\nand\nfence.proxy\ninstructions establish an ordering between memory accesses that\nmay happen through different\nproxies\n.\nA\nuni-directional\nproxy ordering from the\nfrom-proxykind\nto the\nto-proxykind\nestablishes\nordering between a prior memory access performed via the\nfrom-proxykind\nand a subsequent memory access\nperformed via the\nto-proxykind\n.\nA\nbi-directional\nproxy ordering between two proxykinds establishes two\nuni-directional\nproxy orderings\n: one from the first proxykind to the second proxykind and the other from the second proxykind to the first\nproxykind.\nThe\n.proxykind\nqualifier indicates the\nbi-directional\nproxy ordering that is established between the memory\naccesses done between the generic proxy and the proxy specified by\n.proxykind\n.\nValue\n.alias\nof the\n.proxykind\nqualifier refers to memory accesses performed using virtually\naliased addresses to the same memory location. Value\n.async\nof the\n.proxykind\nqualifier specifies\nthat the memory ordering is established between the async proxy and the generic proxy. The memory\nordering is limited only to operations performed on objects in the state space specified. If no state space\nis specified, then the memory ordering applies on all state spaces.\nA\n.release\nproxy fence can form a release sequence that synchronizes with an acquire\nsequence that contains a\n.acquire\nproxy fence. The\n.to_proxykind\nand\n.from_proxykind\nqualifiers indicate the\nuni-directional\nproxy ordering that is established.\nOn\nsm_70\nand higher,\nmembar.proxy\nis a synonym for\nfence.proxy\n.\n1\nThe semantics of\nfence.sc\nintroduced with\nsm_70\nis a superset of the semantics of\nmembar\nand the two are compatible; when executing on\nsm_70\nor later architectures,\nmembar\nacquires the full semantics of\nfence.sc\n.\nPTX ISA Notes\nmembar.{cta,gl}\nintroduced in PTX ISA version 1.4.\nmembar.sys\nintroduced in PTX ISA version 2.0.\nfence\nintroduced in PTX ISA version 6.0.\nmembar.proxy\nand\nfence.proxy\nintroduced in PTX ISA version 7.5.\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\n.op_restrict\nqualifier introduced in PTX ISA version 8.0.\nfence.proxy.async\nis introduced in PTX ISA version 8.0.\n.to_proxykind::from_proxykind\nqualifier introduced in PTX ISA version 8.3.\n.acquire\nand\n.release\nqualifiers for\nfence\ninstruction introduced in PTX ISA version 8.6.\n.sync_restrict\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nmembar.{cta,gl}\nsupported on all target architectures.\nmembar.sys\nrequires\nsm_20\nor higher.\nfence\nrequires\nsm_70\nor higher.\nmembar.proxy\nrequires\nsm_60\nor higher.\nfence.proxy\nrequires\nsm_70\nor higher.\n.cluster\nscope qualifier requires\nsm_90\nor higher.\n.op_restrict\nqualifier requires\nsm_90\nor higher.\nfence.proxy.async\nrequires\nsm_90\nor higher.\n.to_proxykind::from_proxykind\nqualifier requires\nsm_90\nor higher.\n.acquire\nand\n.release\nqualifiers for\nfence\ninstruction require\nsm_90\nor higher..\n.sync_restrict\nqualifier requires\nsm_90\nor higher..\nExamples\nmembar.gl;\nmembar.cta;\nmembar.sys;\nfence.sc.cta;\nfence.sc.cluster;\nfence.proxy.alias;\nmembar.proxy.alias;\nfence.mbarrier_init.release.cluster;\nfence.proxy.async;\nfence.proxy.async.shared::cta;\nfence.proxy.async.shared::cluster;\nfence.proxy.async.global;\n\ntensormap.replace.tile.global_address.global.b1024.b64   [gbl], new_addr;\nfence.proxy.tensormap::generic.release.gpu;\ncvta.global.u64  tmap, gbl;\nfence.proxy.tensormap::generic.acquire.gpu [tmap], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [tmap, {tc0}], [mbar0];\n\n// Acquire remote barrier state via async proxy.\nbarrier.cluster.wait.acquire;\nfence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\n\n// Release local barrier state via generic proxy.\nmbarrier.init [bar];\nfence.mbarrier_init.release.cluster;\nbarrier.cluster.arrive.relaxed;\n\n// Acquire local shared memory via generic proxy.\nmbarrier.try_wait.relaxed.cluster.shared::cta.b64 complete, [addr], parity;\nfence.acquire.sync_restrict::shared::cluster.cluster;\n\n// Release local shared memory via generic proxy.\nfence.release.sync_restrict::shared::cta.cluster;\nmbarrier.arrive.relaxed.cluster.shared::cluster.b64 state, [bar];\n9.7.13.5.\nParallel Synchronization and Communication Instructions:\natom\n\natom\nAtomic reduction operations for thread-to-thread communication.\nSyntax\nAtomic operation with scalar type:\natom{.sem}{.scope}{.space}.op{.level::cache_hint}.type d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.op.type d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b16 d, [a], b, c;\n\natom{.sem}{.scope}{.space}.cas.b128 d, [a], b, c;\natom{.sem}{.scope}{.space}.exch{.level::cache_hint}.b128 d, [a], b {, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16     d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2   d, [a], b{, cache-policy};\n\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16    d, [a], b{, cache-policy};\natom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2  d, [a], b{, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                { .relaxed, .acquire, .release, .acq_rel };\n.scope =              { .cta, .cluster, .gpu, .sys };\n\n.op =                 { .and, .or, .xor,\n                        .cas, .exch,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };\nAtomic operation with vector type:\natom{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32                  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_16_bit.half_word_type  d, [a], b{, cache-policy};\natom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type     d, [a], b{, cache-policy};\n\n.sem =               { .relaxed, .acquire, .release, .acq_rel };\n.scope =             { .cta, .cluster, .gpu, .sys };\n.op =                { .add, .min, .max };\n.half_word_type =    { .f16, .bf16 };\n.packed_type =       { .f16x2, .bf16x2 };\n.vec_16_bit =        { .v2, .v4, .v8 }\n.vec_32_bit =        { .v2, .v4 };\n.level::cache_hint = { .L2::cache_hint }\nDescription\nAtomically loads the original value at location\na\ninto destination register\nd\n, performs a\nreduction operation with operand\nb\nand the value in location\na\n, and stores the result of the\nspecified operation at location\na\n, overwriting the original value. Operand\na\nspecifies a\nlocation in the specified state space. If no state space is given, perform the memory accesses using\nGeneric Addressing\n.\natom\nwith scalar type may be used only\nwith\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\natom\nwith vector type may be used only with\n.global\nspace\nand with generic addressing where the address points to\n.global\nspace.\nFor\natom\nwith vector type, operands\nd\nand\nb\nare brace-enclosed vector expressions, size\nof which is equal to the size of vector qualifier.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.relaxed\nis assumed by default.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is absent,\n.gpu\nscope is\nassumed by default.\nFor\natom\nwith vector type, the supported combinations of vector qualifier and types, and atomic\noperations supported on these combinations are depicted in the following table:\nVector qualifier\nTypes\n.f16\n/\nbf16\n.f16x2\n/\nbf16x2\n.f32\n.v2\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v4\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v8\n.add\n,\n.min\n,\n.max\nNot supported\nNot Supported\nTwo atomic operations (\natom\nor\nred\n) are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write.\natom\ninstruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch cases, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire\natom\nis not guaranteed to be atomic as a single access.\nFor\nsm_6x\nand earlier architectures,\natom\noperations on\n.shared\nstate space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory atomic\ninstructions, e.g., by inserting barriers between normal stores and atomic operations to a common\naddress, or by using atom.exch to store to locations accessed by other atomic operations.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe bit-size operations are\n.and\n,\n.or\n,\n.xor\n,\n.cas\n(compare-and-swap), and\n.exch\n(exchange).\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n,\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\nThe floating-point operation\n.add\noperation rounds to nearest even. Current implementation of\natom.add.f32\non global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas\natom.add.f32\non shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero.\natom.add.f16\n,\natom.add.f16x2\n,\natom.add.bf16\nand\natom.add.bf16x2\noperation requires\nthe\n.noftz\nqualifier; it preserves subnormal inputs and results, and does not flush them to\nzero.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\natomic {\n    d = *a;\n    *a = (operation == cas) ? operation(*a, b, c)\n                            : operation(*a, b);\n}\nwhere\n    inc(r, s)  = (r >= s) ? 0 : r+1;\n    dec(r, s)  = (r==0 || r > s)  ? s : r-1;\n    exch(r, s) =  s;\n    cas(r,s,t) = (r == s) ? t : r;\nNotes\nSimple reductions may be specified by using the\nbit bucket\ndestination operand\n_\n.\nPTX ISA Notes\n32-bit atom.global introduced in PTX ISA version 1.1.\natom.shared\nand 64-bit\natom.global.{add,cas,exch}\nintroduced in PTX ISA 1.2.\natom.add.f32\nand 64-bit\natom.shared.{add,cas,exch}\nintroduced in PTX ISA 2.0.\n64-bit\natom.{and,or,xor,min,max}\nintroduced in PTX ISA 3.1.\natom.add.f64\nintroduced in PTX ISA 5.0.\n.scope\nqualifier introduced in PTX ISA 5.0.\n.sem\nqualifier introduced in PTX ISA version 6.0.\natom.add.noftz.f16x2\nintroduced in PTX ISA 6.2.\natom.add.noftz.f16\nand\natom.cas.b16\nintroduced in PTX ISA 6.3.\nPer-element atomicity of\natom.f16x2\nclarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2.\nSupport for\n.level::cache_hint\nqualifier introduced in PTX ISA version 7.4.\natom.add.noftz.bf16\nand\natom.add.noftz.bf16x2\nintroduced in PTX ISA 7.8.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for vector types introduced in PTX ISA version 8.1.\nSupport for\n.b128\ntype introduced in PTX ISA version 8.3.\nSupport for\n.sys\nscope with\n.b128\ntype introduced in PTX ISA version 8.4.\nTarget ISA Notes\natom.global\nrequires\nsm_11\nor higher.\natom.shared\nrequires\nsm_12\nor higher.\n64-bit\natom.global.{add,cas,exch}\nrequire\nsm_12\nor higher.\n64-bit\natom.shared.{add,cas,exch}\nrequire\nsm_20\nor higher.\n64-bit\natom.{and,or,xor,min,max}\nrequire\nsm_32\nor higher.\natom.add.f32\nrequires\nsm_20\nor higher.\natom.add.f64\nrequires\nsm_60\nor higher.\n.scope\nqualifier requires\nsm_60\nor higher.\n.sem\nqualifier requires\nsm_70\nor higher.\nUse of generic addressing requires\nsm_20\nor higher.\natom.add.noftz.f16x2\nrequires\nsm_60\nor higher.\natom.add.noftz.f16\nand\natom.cas.b16\nrequires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\natom.add.noftz.bf16\nand\natom.add.noftz.bf16x2\nrequire\nsm_90\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for vector types requires\nsm_90\nor higher.\nSupport for\n.b128\ntype requires\nsm_90\nor higher.\nExamples\natom.global.add.s32  d,[a],1;\natom.shared::cta.max.u32  d,[x+4],0;\n@p  atom.global.cas.b32  d,[p],my_val,my_new_val;\natom.global.sys.add.u32 d, [a], 1;\natom.global.acquire.sys.inc.u32 ans, [gbl], %r0;\natom.add.noftz.f16x2 d, [a], b;\natom.add.noftz.f16   hd, [ha], hb;\natom.global.cas.b16  hd, [ha], hb, hc;\natom.add.noftz.bf16   hd, [a], hb;\natom.add.noftz.bf16x2 bd, [b], bb;\natom.add.shared::cluster.noftz.f16   hd, [ha], hb;\natom.shared.b128.cas d, a, b, c; // 128-bit atom\natom.global.b128.exch d, a, b;   // 128-bit atom\n\natom.global.cluster.relaxed.add.u32 d, [a], 1;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\natom.global.add.L2::cache_hint.s32  d, [a], 1, cache-policy;\n\natom.global.v8.f16.max.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v8.bf16.add.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],\n                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\natom.global.v2.f16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v2.bf16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};\natom.global.v4.b16x2.min.noftz  {%hd0, %hd1, %hd2, %hd3}, [gbl], {%h0, %h1, %h2, %h3};\natom.global.v4.f32.add  {%f0, %f1, %f2, %f3}, [gbl], {%f0, %f1, %f2, %f3};\natom.global.v2.f16x2.min.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.bf16x2.max.noftz  {%bd0, %bd1}, [g], {%b0, %b1};\natom.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};\n9.7.13.6.\nParallel Synchronization and Communication Instructions:\nred\n\nred\nReduction operations on global and shared memory.\nSyntax\nReduction operation with scalar type:\nred{.sem}{.scope}{.space}.op{.level::cache_hint}.type          [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16    [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2  [a], b{, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16\n                                                      [a], b {, cache-policy};\n\nred{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2\n                                                      [a], b {, cache-policy};\n\n.space =              { .global, .shared{::cta, ::cluster} };\n.sem =                {.relaxed, .release};\n.scope =              {.cta, .cluster, .gpu, .sys};\n\n.op =                 { .and, .or, .xor,\n                        .add, .inc, .dec,\n                        .min, .max };\n.level::cache_hint =  { .L2::cache_hint };\n.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };\nReduction operation with vector type:\nred{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32 [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}. vec_16_bit.half_word_type [a], b{, cache-policy};\nred{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy};\n\n.sem =                { .relaxed, .release };\n.scope =              { .cta, .cluster, .gpu, .sys };\n.op =                 { .add, .min, .max };\n.half_word_type =     { .f16, .bf16 };\n.packed_type =        { .f16x2,.bf16x2 };\n.vec_16_bit =         { .v2, .v4, .v8 }\n.vec_32_bit =         { .v2, .v4 };\n.level::cache_hint =  { .L2::cache_hint }\nDescription\nPerforms a reduction operation with operand\nb\nand the value in location\na\n, and stores the\nresult of the specified operation at location\na\n, overwriting the original value. Operand\na\nspecifies a location in the specified state space. If no state space is given, perform the memory\naccesses using\nGeneric Addressing\n.\nred\nwith scalar type may\nbe used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address\npoints to\n.global\nor\n.shared\nspace.\nred\nwith vector type may be used only with\n.global\nspace and with generic addressing where the address points to\n.global\nspace.\nFor\nred\nwith vector type, operand\nb\nis brace-enclosed vector expressions, size of which is\nequal to the size of vector qualifier.\nIf no sub-qualifier is specified with\n.shared\nstate space, then\n::cta\nis assumed by default.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.relaxed\nis assumed by default.\nThe optional\n.scope\nqualifier specifies the set of threads that can directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is absent,\n.gpu\nscope is\nassumed by default.\nFor\nred\nwith vector type, the supported combinations of vector qualifier, types and reduction\noperations supported on these combinations are depicted in following table:\nVector qualifier\nTypes\n.f16\n/\nbf16\n.f16x2\n/\nbf16x2\n.f32\n.v2\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v4\n.add\n,\n.min\n,\n.max\n.add\n,\n.min\n,\n.max\n.add\n.v8\n.add\n,\n.min\n,\n.max\nNot supported\nNot Supported\nTwo atomic operations (\natom\nor\nred\n) are performed atomically with respect to each other only\nif each operation specifies a scope that includes the other. When this condition is not met, each\noperation observes the other operation being performed as if it were split into a read followed by a\ndependent write.\nred\ninstruction on packed type or vector type, accesses adjacent scalar elements in memory. In\nsuch case, the atomicity is guaranteed separately for each of the individual scalar elements; the\nentire\nred\nis not guaranteed to be atomic as a single access.\nFor\nsm_6x\nand earlier architectures,\nred\noperations on\n.shared\nstate space do not\nguarantee atomicity with respect to normal store instructions to the same address. It is the\nprogrammer’s responsibility to guarantee correctness of programs that use shared memory reduction\ninstructions, e.g., by inserting barriers between normal stores and reduction operations to a common\naddress, or by using\natom.exch\nto store to locations accessed by other reduction operations.\nSupported addressing modes for operand\na\nand alignment requirements are described in\nAddresses as Operands\nThe bit-size operations are\n.and\n,\n.or\n, and\n.xor\n.\nThe integer operations are\n.add\n,\n.inc\n,\n.dec\n,\n.min\n,\n.max\n. The\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\nThe floating-point operation\n.add\noperation rounds to nearest even. Current implementation of\nred.add.f32\non global memory flushes subnormal inputs and results to sign-preserving zero;\nwhereas\nred.add.f32\non shared memory supports subnormal inputs and results and doesn’t flush\nthem to zero.\nred.add.f16\n,\nred.add.f16x2\n,\nred.add.bf16\nand\nred.add.bf16x2\noperation requires the\n.noftz\nqualifier; it preserves subnormal inputs and results, and does not flush them to zero.\nWhen the optional argument\ncache-policy\nis specified, the qualifier\n.level::cache_hint\nis\nrequired. The 64-bit operand\ncache-policy\nspecifies the cache eviction policy that may be used\nduring the memory access.\nThe qualifier\n.level::cache_hint\nis only supported for\n.global\nstate space and for generic\naddressing where the address points to the\n.global\nstate space.\ncache-policy\nis a hint to the cache subsystem and may not always be respected. It is treated as\na performance hint only, and does not change the memory consistency behavior of the program.\nSemantics\n*a = operation(*a, b);\n\nwhere\n    inc(r, s) = (r >= s) ? 0 : r+1;\n    dec(r, s) = (r==0 || r > s)  ? s : r-1;\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2.\nred.add.f32\nand\nred.shared.add.u64\nintroduced in PTX ISA 2.0.\n64-bit\nred.{and,or,xor,min,max}\nintroduced in PTX ISA 3.1.\nred.add.f64\nintroduced in PTX ISA 5.0.\n.scope\nqualifier introduced in PTX ISA 5.0.\n.sem\nqualifier introduced in PTX ISA version 6.0.\nred.add.noftz.f16x2\nintroduced in PTX ISA 6.2.\nred.add.noftz.f16\nintroduced in PTX ISA 6.3.\nPer-element atomicity of\nred.f16x2\nclarified in PTX ISA version 6.3, with retrospective effect\nfrom PTX ISA version 6.2\nSupport for\n.level::cache_hint\nqualifier introduced in PTX ISA version 7.4.\nred.add.noftz.bf16\nand\nred.add.noftz.bf16x2\nintroduced in PTX ISA 7.8.\nSupport for\n.cluster\nscope qualifier introduced in PTX ISA version 7.8.\nSupport for\n::cta\nand\n::cluster\nsub-qualifiers introduced in PTX ISA version 7.8.\nSupport for vector types introduced in PTX ISA version 8.1.\nTarget ISA Notes\nred.global\nrequires\nsm_11\nor higher\nred.shared\nrequires\nsm_12\nor higher.\nred.global.add.u64\nrequires\nsm_12\nor higher.\nred.shared.add.u64\nrequires\nsm_20\nor higher.\n64-bit\nred.{and,or,xor,min,max}\nrequire\nsm_32\nor higher.\nred.add.f32\nrequires\nsm_20\nor higher.\nred.add.f64\nrequires\nsm_60\nor higher.\n.scope\nqualifier requires\nsm_60\nor higher.\n.sem\nqualifier requires\nsm_70\nor higher.\nUse of generic addressing requires\nsm_20\nor higher.\nred.add.noftz.f16x2\nrequires\nsm_60\nor higher.\nred.add.noftz.f16\nrequires\nsm_70\nor higher.\nSupport for\n.level::cache_hint\nqualifier requires\nsm_80\nor higher.\nred.add.noftz.bf16\nand\nred.add.noftz.bf16x2\nrequire\nsm_90\nor higher.\nSupport for\n.cluster\nscope qualifier requires\nsm_90\nor higher.\nSub-qualifier\n::cta\nrequires\nsm_30\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for vector types requires\nsm_90\nor higher.\nExamples\nred.global.add.s32  [a],1;\nred.shared::cluster.max.u32  [x+4],0;\n@p  red.global.and.b32  [p],my_val;\nred.global.sys.add.u32 [a], 1;\nred.global.acquire.sys.add.u32 [gbl], 1;\nred.add.noftz.f16x2 [a], b;\nred.add.noftz.bf16   [a], hb;\nred.add.noftz.bf16x2 [b], bb;\nred.global.cluster.relaxed.add.u32 [a], 1;\nred.shared::cta.min.u32  [x+4],0;\n\ncreatepolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;\nred.global.and.L2::cache_hint.b32 [a], 1, cache-policy;\n\nred.global.v8.f16.add.noftz  [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v8.bf16.min.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};\nred.global.v2.f16.add.noftz [gbl], {%h0, %h1};\nred.global.v2.bf16.add.noftz [gbl], {%h0, %h1};\nred.global.v4.f16x2.max.noftz [gbl], {%h0, %h1, %h2, %h3};\nred.global.v4.f32.add  [gbl], {%f0, %f1, %f2, %f3};\nred.global.v2.f16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.bf16x2.add.noftz {%bd0, %bd1}, [g], {%b0, %b1};\nred.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};\n9.7.13.7.\nParallel Synchronization and Communication Instructions:\nred.async\n\nred.async\nAsynchronous reduction operation.\nSyntax\n// Increment and Decrement reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  =                 { .relaxed };\n.scope =                { .cluster };\n.ss   =                 { .shared::cluster };\n.op   =                 { .inc, .dec };\n.type =                 { .u32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n\n// MIN and MAX reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.op   = { .min, .max };\n.type = { .u32, .s32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// Bitwise AND, OR and XOR reductions\nred.async.sem.scope{.ss}.completion_mechanism.op.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.op   = { .and, .or, .xor };\n.type = { .b32 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\n// ADD reductions\nred.async.sem.scope{.ss}.completion_mechanism.add.type [a], b, [mbar];\n\n.sem  = { .relaxed };\n.scope = { .cluster };\n.ss   = { .shared::cluster };\n.type = { .u32, .s32, .u64 };\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n\nred.async{.mmio}.sem.scope{.ss}.add.type [a], b;\n\n.sem  = { .release };\n.scope = { .gpu, .cluster };\n.ss   = { .global };\n.type = { .u32, .s32, .u64, .s64 };\nDescription\nred.async\nis a non-blocking instruction which initiates an asynchronous reduction operation\nspecified by\n.op\n, with the operand\nb\nand the value at destination shared memory location\nspecified by operand\na\n.\nOperands\na\nis a destination address, and must be either a register, or of the form\nregister\n+\nimmOff\n, as described in\nAddresses as Operands\n.\nb\nis a source value, of the type indicated by qualifier\n.type\n.\nmbar\nis an mbarrier object address.\nQualifiers\n.mmio\nindicates whether this is an\nmmio Operation\n.\n.sem\nspecifies the memory ordering semantics as described in the\nMemory Consistency Model\n.\n.scope\nspecifies the set of threads with which this instruction can\ndirectly synchronize.\n.ss\nspecifies the state space of the destination operand\na\nand the\nmbarrier operand\nmbar\n.\nIf\n.ss\nis not specified,\nGeneric Addressing\nis used.\n.completion_mechanism\nspecifies the mechanism for observing the\ncompletion of the asynchronous operation.\nWhen\n.completion_mechanism\nis\n.mbarrier::complete_tx::bytes\n: upon\ncompletion of the asynchronous operation, a\ncomplete-tx\noperation will be performed on the mbarrier object specified by the operand\nmbar\n,\nwith\ncompleteCount\nargument equal to the amount of data stored in bytes.\nWhen\n.completion_mechanism\nis not specified: the completion of the store\nsynchronizes with the end of the CTA.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\n.op\nspecifies the reduction operation.\nThe\n.inc\nand\n.dec\noperations return a result in the range\n[0..b]\n.\n.type\nspecifies the type of the source operand\nb\n.\nConditions\nWhen\n.sem\nis\n.relaxed\n:\nThe reduce operation is a relaxed memory operation.\nThe complete-tx operation on the mbarrier has\n.release\nsemantics at\n.cluster\nscope.\nThe shared-memory addresses of the destination operand\na\nand the\nmbarrier operand\nmbar\nmust meet all of the following conditions:\nThey belong to the same CTA.\nThe CTA to which they belong is different from the CTA of the executing thread,\nbut must be within the same cluster.\nOtherwise, the behavior is undefined.\n.mmio\nmust not be specified.\nIf\n.ss\nis specified, it must be\n.shared::cluster\n.\nIf\n.ss\nis not specified, generic addressing is used for operands\na\nand\nmbar\n.\nIf the generic addresses specified do not fall within the address window of\n.shared::cluster\nstate space, the behavior is undefined.\nIf\n.completion_mechanism\nis specified, it must be\n.mbarrier::complete_tx::bytes\n.\nIf\n.completion_mechanism\nis not specified, it defaults to\n.mbarrier::complete_tx::bytes\n.\nWhen\n.sem\nis\n.release\n:\nThe reduce operation is a strong memory operation with\n.release\nsemantics\nat the scope specified by\n.scope\n.\nIf\n.mmio\nis specified,\n.scope\nmust be\n.sys\n.\nIf\n.ss\nis specified, it must be\n.global\n.\nIf\n.ss\nis not specified, generic addressing is used for operand\na\n.\nIf the generic address specified does not fall within the address window of\n.global\nstate space, the behavior is undefined.\n.completion_mechanism\nmust not be specified.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nSupport for\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space,\nand\n.gpu\nand\n.sys\nscopes introduced in PTX ISA version 8.7.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\n.mmio\nqualifier,\n.release\nsemantics,\n.global\nstate space,\nand\n.gpu\nand\n.sys\nscopes require\nsm_100\nor higher.\nExamples\nred.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr];\n\nred.async.release.sys.global.add.u32 [addr], b;\n9.7.13.8.\nParallel Synchronization and Communication Instructions:\nvote\n(deprecated)\n\nvote\n(deprecated)\nVote across thread group.\nSyntax\nvote.mode.pred  d, {!}a;\nvote.ballot.b32 d, {!}a;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni };\nDeprecation Note\nThe\nvote\ninstruction without a\n.sync\nqualifier is deprecated in PTX ISA version 6.0.\nSupport for this instruction with\n.target\nlower than\nsm_70\nmay be removed in a future PTX\nISA version.\nRemoval Note\nSupport for\nvote\ninstruction without a\n.sync\nqualifier is removed in PTX ISA version 6.4 for\n.target\nsm_70\nor higher.\nDescription\nPerforms a reduction of the source predicate across all active threads in a warp. The destination\npredicate value is the same across all threads in the warp.\nThe reduction modes are:\n.all\nTrue\nif source predicate is\nTrue\nfor all active threads in warp. Negate the source\npredicate to compute\n.none\n.\n.any\nTrue\nif source predicate is\nTrue\nfor some active thread in warp. Negate the source\npredicate to compute\n.not_all\n.\n.uni\nTrue\nif source predicate has the same value in all active threads in warp. Negating the\nsource predicate also computes\n.uni\n.\nIn the\nballot\nform,\nvote.ballot.b32\nsimply copies the predicate from each thread in a warp\ninto the corresponding bit position of destination register\nd\n, where the bit position\ncorresponds to the thread’s lane id.\nAn inactive thread in warp will contribute a 0 for its entry when participating in\nvote.ballot.b32\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2.\nDeprecated in PTX ISA version 6.0 in favor of\nvote.sync\n.\nNot supported in PTX ISA version 6.4 for .target\nsm_70\nor higher.\nTarget ISA Notes\nvote\nrequires\nsm_12\nor higher.\nvote.ballot.b32\nrequires\nsm_20\nor higher.\nvote\nis not supported on\nsm_70\nor higher starting PTX ISA version 6.4.\nRelease Notes\nNote that\nvote\napplies to threads in a single warp, not across an entire CTA.\nExamples\nvote.all.pred    p,q;\nvote.uni.pred    p,q;\nvote.ballot.b32  r1,p;  // get 'ballot' across warp\n9.7.13.9.\nParallel Synchronization and Communication Instructions:\nvote.sync\n\nvote.sync\nVote across thread group.\nSyntax\nvote.sync.mode.pred  d, {!}a, membermask;\nvote.sync.ballot.b32 d, {!}a, membermask;  // 'ballot' form, returns bitmask\n\n.mode = { .all, .any, .uni };\nDescription\nvote.sync\nwill cause executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nvote.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s\nlaneid\n. Operand\na\nis a\npredicate register.\nIn the\nmode\nform,\nvote.sync\nperforms a reduction of the source predicate across all non-exited\nthreads in\nmembermask\n. The destination operand\nd\nis a predicate register and its value is\nthe same across all threads in\nmembermask\n.\nThe reduction modes are:\n.all\nTrue\nif source predicate is\nTrue\nfor all non-exited threads in\nmembermask\n. Negate the\nsource predicate to compute\n.none\n.\n.any\nTrue\nif source predicate is\nTrue\nfor some thread in\nmembermask\n. Negate the source\npredicate to compute\n.not_all\n.\n.uni\nTrue\nif source predicate has the same value in all non-exited threads in\nmembermask\n. Negating the source predicate also computes\n.uni\n.\nIn the\nballot\nform, the destination operand\nd\nis a\n.b32\nregister. In this form,\nvote.sync.ballot.b32\nsimply copies the predicate from each thread in\nmembermask\ninto the\ncorresponding bit position of destination register\nd\n, where the bit position corresponds to the\nthread’s lane id.\nA thread not specified in\nmembermask\nwill contribute a 0 for its entry in\nvote.sync.ballot.b32\n.\nThe behavior of\nvote.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nNote\nFor .target\nsm_6x\nor below, all threads in\nmembermask\nmust execute the same\nvote.sync\ninstruction in convergence, and only threads belonging to some\nmembermask\ncan be active when\nthe\nvote.sync\ninstruction is executed. Otherwise, the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nvote.sync.all.pred    p,q,0xffffffff;\nvote.sync.ballot.b32  r1,p,0xffffffff;  // get 'ballot' across warp\n9.7.13.10.\nParallel Synchronization and Communication Instructions:\nmatch.sync\n\nmatch.sync\nBroadcast and compare a value across threads in warp.\nSyntax\nmatch.any.sync.type  d, a, membermask;\nmatch.all.sync.type  d[|p], a, membermask;\n\n.type = { .b32, .b64 };\nDescription\nmatch.sync\nwill cause executing thread to wait until all non-exited threads from\nmembermask\nhave executed\nmatch.sync\nwith the same qualifiers and same\nmembermask\nvalue before resuming\nexecution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s laneid.\nmatch.sync\nperforms broadcast and compare of operand\na\nacross all non-exited threads in\nmembermask\nand sets destination\nd\nand optional predicate\np\nbased on mode.\nOperand\na\nhas instruction type and\nd\nhas\n.b32\ntype.\nDestination\nd\nis a 32-bit mask where bit position in mask corresponds to thread’s laneid.\nThe matching operation modes are:\n.all\nd\nis set to mask corresponding to non-exited threads in\nmembermask\nif all non-exited\nthreads in\nmembermask\nhave same value of operand\na\n; otherwise\nd\nis set\nto 0. Optionally predicate\np\nis set to true if all non-exited threads in\nmembermask\nhave\nsame value of operand\na\n; otherwise\np\nis set to false. The sink symbol ‘_’ may be used in\nplace of any one of the destination operands.\n.any\nd\nis set to mask of non-exited threads in\nmembermask\nthat have same value of operand\na\n.\nThe behavior of\nmatch.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\nTarget ISA Notes\nRequires\nsm_70\nor higher.\nRelease Notes\nNote that\nmatch.sync\napplies to threads in a single warp, not across an entire CTA.\nExamples\nmatch.any.sync.b32    d, a, 0xffffffff;\nmatch.all.sync.b64    d|p, a, mask;\n9.7.13.11.\nParallel Synchronization and Communication Instructions:\nactivemask\n\nactivemask\nQueries the active threads within a warp.\nSyntax\nactivemask.b32 d;\nDescription\nactivemask\nqueries predicated-on active threads from the executing warp and sets the destination\nd\nwith 32-bit integer mask where bit position in the mask corresponds to the thread’s\nlaneid\n.\nDestination\nd\nis a 32-bit destination register.\nAn active thread will contribute 1 for its entry in the result and exited or inactive or\npredicated-off thread will contribute 0 for its entry in the result.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.2.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\nactivemask.b32  %r1;\n9.7.13.12.\nParallel Synchronization and Communication Instructions:\nredux.sync\n\nredux.sync\nPerform reduction operation on the data from each predicated active thread in the thread group.\nSyntax\nredux.sync.op.type dst, src, membermask;\n.op   = {.add, .min, .max}\n.type = {.u32, .s32}\n\nredux.sync.op.b32 dst, src, membermask;\n.op   = {.and, .or, .xor}\n\nredux.sync.op{.abs.}{.NaN}.f32 dst, src, membermask;\n.op   = { .min, .max }\nDescription\nredux.sync\nwill cause the executing thread to wait until all non-exited threads corresponding to\nmembermask\nhave executed\nredux.sync\nwith the same qualifiers and same\nmembermask\nvalue\nbefore resuming execution.\nOperand\nmembermask\nspecifies a 32-bit integer which is a mask indicating threads participating\nin this instruction where the bit position corresponds to thread’s\nlaneid\n.\nredux.sync\nperforms a reduction operation\n.op\nof the 32 bit source register\nsrc\nacross\nall non-exited threads in the\nmembermask\n. The result of the reduction operation is written to\nthe 32 bit destination register\ndst\n.\nReduction operation can be one of the bitwise operation in\n.and\n,\n.or\n,\n.xor\nor arithmetic\noperation in\n.add\n,\n.min\n,\n.max\n.\nFor the\n.add\noperation result is truncated to 32 bits.\nFor\n.f32\ninstruction type, if the input value is 0.0 then +0.0 > -0.0.\nIf\n.abs\nqualifier is specified, then the absolute value of the input is considered for the\nreduction operation.\nIf the\n.NaN\nqualifier is specified, then the result of the reduction operation is canonical NaN\nif the input to the reduction operation from any participating thread is NaN.\nIn the absence of\n.NaN\nqualifier, only non-NaN values are considered for the reduction operation\nand the result will be canonical NaN when all inputs are NaNs.\nThe behavior of\nredux.sync\nis undefined if the executing thread is not in the\nmembermask\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for\n.f32\ntype is introduced in PTX ISA version 8.6.\nSupport for\n.abs\nand\n.NaN\nqualifiers is introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\n.f32\ntype requires\nsm_100a\nand is supported on\nsm_100f\nfrom PTX ISA version 8.8.\nQualifiers\n.abs\nand\n.NaN\nrequire\nsm_100a\nand are supported on\nsm_100f\nor\nhigher in the same family from PTX ISA version 8.8.\nRelease Notes\nNote that\nredux.sync\napplies to threads in a single warp, not across an entire CTA.\nExamples\n.reg .b32 dst, src, init, mask;\nredux.sync.add.s32 dst, src, 0xff;\nredux.sync.xor.b32 dst, src, mask;\n\nredux.sync.min.abs.NaN.f32 dst, src, mask;\n9.7.13.13.\nParallel Synchronization and Communication Instructions:\ngriddepcontrol\n\ngriddepcontrol\nControl execution of dependent grids.\nSyntax\ngriddepcontrol.action;\n\n.action   = { .launch_dependents, .wait }\nDescription\nThe\ngriddepcontrol\ninstruction allows the dependent grids and prerequisite grids as defined by\nthe runtime, to control execution in the following way:\n.launch_dependents\nmodifier signals that specific dependents the runtime system designated to\nreact to this instruction can be scheduled as soon as all other CTAs in the grid issue the same\ninstruction or have completed. The dependent may launch before the completion of the current\ngrid. There is no guarantee that the dependent will launch before the completion of the current\ngrid. Repeated invocations of this instruction by threads in the current CTA will have no additional\nside effects past that of the first invocation.\n.wait\nmodifier causes the executing thread to wait until all prerequisite grids in flight have\ncompleted and all the memory operations from the prerequisite grids are performed and made visible\nto the current grid.\nNote\nIf the prerequisite grid is using\ngriddepcontrol.launch_dependents\n, then the dependent grid\nmust use\ngriddepcontrol.wait\nto ensure correct functional execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\ngriddepcontrol.launch_dependents;\ngriddepcontrol.wait;\n9.7.13.14.\nParallel Synchronization and Communication Instructions:\nelect.sync\n\nelect.sync\nElect a leader thread from a set of threads.\nSyntax\nelect.sync d|p, membermask;\nDescription\nelect.sync\nelects one predicated active leader thread from among a set of threads specified by\nmembermask\n.\nlaneid\nof the elected thread is returned in the 32-bit destination operand\nd\n. The sink symbol ‘_’ can be used for destination operand\nd\n. The predicate destination\np\nis set to\nTrue\nfor the leader thread, and\nFalse\nfor all other threads.\nOperand\nmembermask\nspecifies a 32-bit integer indicating the set of threads from which a leader\nis to be elected. The behavior is undefined if the executing thread is not in\nmembermask\n.\nElection of a leader thread happens deterministically, i.e. the same leader thread is elected for\nthe same\nmembermask\nevery time.\nThe mandatory\n.sync\nqualifier indicates that\nelect\ncauses the executing thread to wait until\nall threads in the\nmembermask\nexecute the\nelect\ninstruction before resuming execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nelect.sync    %r0|%p0, 0xffffffff;\n9.7.13.15.\nParallel Synchronization and Communication Instructions:\nmbarrier\n\nmbarrier\nis a barrier created in shared memory that supports :\nSynchronizing any subset of threads within a CTA\nOne-way synchronization of threads across CTAs of a cluster. As noted in\nmbarrier support with shared memory\n, threads can\nperform only\narrive\noperations but not\n*_wait\non an mbarrier located in\nshared::cluster\nspace.\nWaiting for completion of asynchronous memory operations initiated by a thread and making them\nvisible to other threads.\nAn\nmbarrier object\nis an opaque object in memory which can be initialized and invalidated using :\nmbarrier.init\nmbarrier.inval\nOperations supported on\nmbarrier object\ns are :\nmbarrier.expect_tx\nmbarrier.complete_tx\nmbarrier.arrive\nmbarrier.arrive_drop\nmbarrier.test_wait\nmbarrier.try_wait\nmbarrier.pending_count\ncp.async.mbarrier.arrive\nPerforming any\nmbarrier\noperation except\nmbarrier.init\non an uninitialized\nmbarrier object\nresults in undefined behavior.\nPerforming any\nnon-mbarrier\nor\nmbarrier.init\noperations on an initialized\nmbarrier object\nresults in undefined behavior.\nUnlike\nbar{.cta}\n/\nbarrier{.cta}\ninstructions which can access a limited number of barriers\nper CTA,\nmbarrier objects\nare user defined and are only limited by the total shared memory size\navailable.\nmbarrier\noperations enable threads to perform useful work after the arrival at the\nmbarrier\nand\nbefore waiting for the\nmbarrier\nto complete.\n9.7.13.15.1.\nSize and alignment of mbarrier object\n\nAn mbarrier object is an opaque object with the following type and alignment requirements :\nType\nAlignment (bytes)\nMemory space\n.b64\n8\n.shared\n9.7.13.15.2.\nContents of the mbarrier object\n\nAn opaque\nmbarrier object\nkeeps track of the following information :\nCurrent phase of the\nmbarrier object\nCount of pending arrivals for the current phase of the\nmbarrier object\nCount of expected arrivals for the next phase of the\nmbarrier object\nCount of pending asynchronous memory operations (or transactions) tracked by the current phase of\nthe\nmbarrier object\n. This is also referred to as\ntx-count\n.\nAn\nmbarrier object\nprogresses through a sequence of phases where each phase is defined by threads\nperforming an expected number of\narrive-on\noperations.\nThe valid range of each of the counts is as shown below:\nCount name\nMinimum value\nMaximum value\nExpected arrival count\n1\n2\n20\n- 1\nPending arrival count\n0\n2\n20\n- 1\ntx-count\n-(2\n20\n- 1)\n2\n20\n- 1\n9.7.13.15.3.\nLifecycle of the mbarrier object\n\nThe\nmbarrier object\nmust be initialized prior to use.\nAn\nmbarrier object\nis used to synchronize threads and asynchronous memory operations.\nAn\nmbarrier object\nmay be used to perform a sequence of such synchronizations.\nAn\nmbarrier object\nmust be invalidated to repurpose its memory for any purpose,\nincluding repurposing it for another mbarrier object.\n9.7.13.15.4.\nPhase of the mbarrier object\n\nThe phase of an\nmbarrier object\nis the number of times the\nmbarrier object\nhas been used to\nsynchronize threads and\nasynchronous\noperations. In each phase {0, 1, 2, …}, threads perform in program order :\narrive-on\noperations to complete the current phase and\ntest_wait\n/\ntry_wait\noperations to check for the completion of the current phase.\nAn\nmbarrier object\nis automatically reinitialized upon completion of the current phase for\nimmediate use in the next phase. The current phase is incomplete and all prior phases are complete.\nFor each phase of the mbarrier object, at least one\ntest_wait\nor\ntry_wait\noperation must be\nperformed which returns\nTrue\nfor\nwaitComplete\nbefore an\narrive-on\noperation\nin the subsequent phase.\n9.7.13.15.5.\nTracking asynchronous operations by the mbarrier object\n\nStarting with the Hopper architecture (\nsm_9x\n),\nmbarrier object\nsupports a new count, called\ntx-count\n, which is used for tracking the completion of asynchronous memory operations or\ntransactions.\ntx-count\ntracks the number of asynchronous transactions, in units specified by the\nasynchronous memory operation, that are outstanding and yet to be complete.\nThe\ntx-count\nof an\nmbarrier object\nmust be set to the total amount of asynchronous memory\noperations, in units as specified by the asynchronous operations, to be tracked by the current\nphase. Upon completion of each of the asynchronous operations, the\ncomplete-tx\noperation will be performed on the\nmbarrier object\nand thus progress the mbarrier towards the\ncompletion of the current phase.\n9.7.13.15.5.1.\nexpect-tx operation\n\nThe\nexpect-tx\noperation, with an\nexpectCount\nargument, increases the\ntx-count\nof an\nmbarrier object\nby the value specified by\nexpectCount\n. This sets the current phase of the\nmbarrier object\nto expect and track the completion of additional asynchronous transactions.\n9.7.13.15.5.2.\ncomplete-tx operation\n\nThe\ncomplete-tx\noperation, with an\ncompleteCount\nargument, on an\nmbarrier object\nconsists of the following:\nmbarrier signaling\nSignals the completion of asynchronous transactions that were tracked by the current phase. As a\nresult of this,\ntx-count\nis decremented by\ncompleteCount\n.\nmbarrier potentially completing the current phase\nIf the current phase has been completed then the mbarrier transitions to the next phase. Refer to\nPhase Completion of the mbarrier object\nfor details on phase completion requirements and phase transition process.\n9.7.13.15.6.\nPhase Completion of the mbarrier object\n\nThe requirements for completion of the current phase are described below. Upon completion of the\ncurrent phase, the phase transitions to the subsequent phase as described below.\nCurrent phase completion requirements\nAn\nmbarrier object\ncompletes the current phase when all of the following conditions are met:\nThe count of the pending arrivals has reached zero.\nThe\ntx-count\nhas reached zero.\nPhase transition\nWhen an\nmbarrier\nobject completes the current phase, the following actions are performed\natomically:\nThe\nmbarrier object\ntransitions to the next phase.\nThe pending arrival count is reinitialized to the expected arrival count.\n9.7.13.15.7.\nArrive-on operation on mbarrier object\n\nAn\narrive-on\noperation, with an optional\ncount\nargument, on an\nmbarrier object\nconsists of the\nfollowing 2 steps :\nmbarrier signalling:\nSignals the arrival of the executing thread OR completion of the asynchronous instruction which\nsignals the arrive-on operation initiated by the executing thread on the\nmbarrier object\n. As a\nresult of this, the pending arrival count is decremented by\ncount\n. If the\ncount\nargument is\nnot specified, then it defaults to 1.\nmbarrier potentially completing the current phase:\nIf the current phase has been completed then the mbarrier transitions to the next phase. Refer to\nPhase Completion of the mbarrier object\nfor details on phase completion requirements and phase transition process.\n9.7.13.15.8.\nmbarrier support with shared memory\n\nThe following table summarizes the support of various mbarrier operations on\nmbarrier objects\nlocated at different shared memory locations:\nmbarrier operations\n.shared::cta\n.shared::cluster\nmbarrier.arrive\nSupported\nSupported, cannot return result\nmbarrier.expect_tx\nSupported\nSupported\nmbarrier.complete_tx\nSupported\nSupported\nOther mbarrier operations\nSupported\nNot supported\n9.7.13.15.9.\nParallel Synchronization and Communication Instructions:\nmbarrier.init\n\nmbarrier.init\nInitialize the\nmbarrier object\n.\nSyntax\nmbarrier.init{.shared{::cta}}.b64 [addr], count;\nDescription\nmbarrier.init\ninitializes the\nmbarrier object\nat the location specified by the address operand\naddr\nwith the unsigned 32-bit integer\ncount\n. The value of operand count must be in the range\nas specified in\nContents of the mbarrier object\n.\nInitialization of the\nmbarrier object\ninvolves :\nInitializing the current phase to 0.\nInitializing the expected arrival count to\ncount\n.\nInitializing the pending arrival count to\ncount\n.\nInitializing the\ntx-count\nto 0.\nThe valid range of values for the operand\ncount\nis [1, …, 2\n20\n- 1].\nRefer\nContents of the mbarrier object\nfor the\nvalid range of values for the various constituents of the mbarrier.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe behavior of performing an\nmbarrier.init\noperation on a memory location containing a\nvalid\nmbarrier object\nis undefined; invalidate the\nmbarrier object\nusing\nmbarrier.inval\nfirst, before repurposing the memory location for any other purpose, including another\nmbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.shared .b64 shMem, shMem2;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n\ncvta.shared.u64          addr, shMem2;\nmbarrier.init.b64        [addr],   %r1;\nbar.cta.sync             0;\n// ... other mbarrier operations on addr\n\nmbarrier.init.shared::cta.b64 [shMem], 12;\nbar.sync                 0;\n// ... other mbarrier operations on shMem\n9.7.13.15.10.\nParallel Synchronization and Communication Instructions:\nmbarrier.inval\n\nmbarrier.inval\nInvalidates the\nmbarrier object\n.\nSyntax\nmbarrier.inval{.shared{::cta}}.b64 [addr];\nDescription\nmbarrier.inval\ninvalidates the\nmbarrier object\nat the location specified by the address\noperand\naddr\n.\nAn\nmbarrier object\nmust be invalidated before using its memory location for any other purpose.\nPerforming any\nmbarrier\noperation except\nmbarrier.init\non a memory location that does not\ncontain a valid\nmbarrier object\n, results in undefined behaviour.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.shared .b64 shmem;\n.reg    .b64 addr;\n.reg    .b32 %r1;\n.reg    .pred t0;\n\n// Example 1 :\nbar.sync                      0;\n@t0 mbarrier.init.b64     [addr], %r1;\n// ... other mbarrier operations on addr\nbar.sync                      0;\n@t0 mbarrier.inval.b64    [addr];\n\n\n// Example 2 :\nbar.cta.sync                  0;\nmbarrier.init.shared.b64           [shmem], 12;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared.b64      [shmem];\n\n// shmem can be reused here for unrelated use :\nbar.cta.sync                  0;\nst.shared.b64                      [shmem], ...;\n\n// shmem can be re-initialized as mbarrier object :\nbar.cta.sync                  0;\n@t0 mbarrier.init.shared.b64       [shmem], 24;\n// ... other mbarrier operations on shmem\nbar.cta.sync                  0;\n@t0 mbarrier.inval.shared::cta.b64 [shmem];\n9.7.13.15.11.\nParallel Synchronization and Communication Instructions:\nmbarrier.expect_tx\n\nmbarrier.expect_tx\nPerfoms\nexpect-tx\noperation on the\nmbarrier object\n.\nSyntax\nmbarrier.expect_tx{.sem.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster }\nDescription\nA thread executing\nmbarrier.expect_tx\nperforms an\nexpect-tx\noperation on the\nmbarrier object\nat the location specified by the address operand\naddr\n. The\n32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to the\nexpect-tx\noperation.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nare as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmbarrier.expect_tx.b64                       [addr], 32;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj1], 512;\nmbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj2], 512;\n9.7.13.15.12.\nParallel Synchronization and Communication Instructions:\nmbarrier.complete_tx\n\nmbarrier.complete_tx\nPerfoms\ncomplete-tx\noperation on the\nmbarrier object\n.\nSyntax\nmbarrier.complete_tx{.sem.scope}{.space}.b64 [addr], txCount;\n\n.sem   = { .relaxed }\n.scope = { .cta, .cluster }\n.space = { .shared{::cta}, .shared::cluster }\nDescription\nA thread executing\nmbarrier.complete_tx\nperforms a\ncomplete-tx\noperation on the\nmbarrier object\nat the location specified by the address operand\naddr\n. The\n32-bit unsigned integer operand\ntxCount\nspecifies the\ncompleteCount\nargument to the\ncomplete-tx\noperation.\nmbarrier.complete_tx\ndoes not involve any asynchronous memory operations and only simulates the\ncompletion of an asynchronous memory operation and its side effect of signaling to the\nmbarrier\nobject\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nare as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmbarrier.complete_tx.b64             [addr],     32;\nmbarrier.complete_tx.shared.b64      [mbarObj1], 512;\nmbarrier.complete_tx.relaxed.cta.b64 [addr2],    32;\n9.7.13.15.13.\nParallel Synchronization and Communication Instructions:\nmbarrier.arrive\n\nmbarrier.arrive\nPerforms\narrive-on operation\non the\nmbarrier object\n.\nSyntax\nmbarrier.arrive{.sem.scope}{.shared{::cta}}.b64           state, [addr]{, count};\nmbarrier.arrive{.sem.scope}{.shared::cluster}.b64         _, [addr] {,count}\nmbarrier.arrive.expect_tx{.sem.scope}{.shared{::cta}}.b64 state, [addr], txCount;\nmbarrier.arrive.expect_tx{.sem.scope}{.shared::cluster}.b64   _, [addr], txCount;\nmbarrier.arrive.noComplete{.release.cta}{.shared{::cta}}.b64  state, [addr], count;\n\n.sem   = { .release, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nA thread executing\nmbarrier.arrive\nperforms an\narrive-on\noperation\non the\nmbarrier object\nat the location specified by the address operand\naddr\n. The 32-bit\nunsigned integer operand\ncount\nspecifies the\ncount\nargument to the\narrive-on\noperation.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional qualifier\n.expect_tx\nspecifies that an\nexpect-tx\noperation is performed prior to the\narrive-on\noperation. The 32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to\nthe\nexpect-tx\noperation. When both qualifiers\n.arrive\nand\n.expect_tx\nare specified, then\nthe count argument of the\narrive-on\noperation is assumed to be 1.\nA\nmbarrier.arrive\noperation with\n.noComplete\nqualifier must not cause the\nmbarrier\nto\ncomplete its current phase, otherwise the behavior is undefined.\nThe value of the operand\ncount\nmust be in the range as specified in\nContents of the mbarrier object\n.\nNote: for\nsm_8x\n, when the argument\ncount\nis specified, the modifier\n.noComplete\nis\nrequired.\nmbarrier.arrive\noperation on an\nmbarrier object\nlocated in\n.shared::cta\nreturns an opaque\n64-bit register capturing the phase of the\nmbarrier object\nprior to the\narrive-on operation\nin the\ndestination operand\nstate.\nContents of the\nstate\noperand are implementation\nspecific. Optionally, sink symbol\n'_'\ncan be used for the\nstate\nargument.\nmbarrier.arrive\noperation on an\nmbarrier object\nlocated in\n.shared::cluster\nbut not in\n.shared::cta\ncannot return a value. Sink symbol ‘_’ is mandatory for the destination operand for\nsuch cases.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.release\nis assumed by default.\nThe\n.relaxed\nqualifier does not provide any memory ordering semantics and visibility\nguarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that directly observe the memory\nsynchronizing effect of this operation, as described in the\nMemory Consistency Model\n.\nIf the\n.scope\nqualifier is not specified then it\ndefaults to\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state space where the\nmbarrier resides.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sink symbol ‘_’ as the destination operand is introduced in PTX ISA version 7.1.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\ncount\nargument without the modifier\n.noComplete\nintroduced in PTX ISA version\n7.8.\nSupport for sub-qualifier\n::cluster\nintroduced in PTX ISA version 8.0.\nSupport for qualifier\n.expect_tx\nis introduced in PTX ISA version 8.0.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSupport for\ncount\nargument without the modifier\n.noComplete\nrequires\nsm_90\nor higher.\nQualifier\n.expect_tx\nrequires\nsm_90\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n.reg .b32 cnt, remoteAddr32, remoteCTAId, addr32;\n.reg .b64 %r<5>, addr, remoteAddr64;\n.shared .b64 shMem, shMem2;\n\ncvta.shared.u64            addr, shMem2;\nmov.b32                    addr32, shMem2;\nmapa.shared::cluster.u32   remoteAddr32, addr32, remoteCTAId;\nmapa.u64                   remoteAddr64, addr,   remoteCTAId;\n\ncvta.shared.u64          addr, shMem2;\n\nmbarrier.arrive.shared.b64                       %r0, [shMem];\nmbarrier.arrive.shared::cta.b64                  %r0, [shMem2];\nmbarrier.arrive.release.cta.shared::cluster.b64  _, [remoteAddr32];\nmbarrier.arrive.release.cluster.b64              _, [remoteAddr64], cnt;\nmbarrier.arrive.expect_tx.release.cluster.b64    _, [remoteAddr64], tx_count;\nmbarrier.arrive.noComplete.b64                   %r1, [addr], 2;\nmbarrier.arrive.relaxed.cta.b64                  %r2, [addr], 4;\nmbarrier.arrive.b64                              %r2, [addr], cnt;\n9.7.13.15.14.\nParallel Synchronization and Communication Instructions:\nmbarrier.arrive_drop\n\nmbarrier.arrive_drop\nDecrements the expected count of the\nmbarrier object\nand performs\narrive-on operation\n.\nSyntax\nmbarrier.arrive_drop{.sem.scope}{.shared{::cta}}.b64 state,           [addr]{, count};\nmbarrier.arrive_drop{.sem.scope}{.shared::cluster}.b64           _,   [addr] {,count};\nmbarrier.arrive_drop.expect_tx{.sem.scope}{.shared{::cta}}.b64 state, [addr], tx_count;\nmbarrier.arrive_drop.expect_tx{.sem.scope}{.shared::cluster}.b64   _, [addr], tx_count;\nmbarrier.arrive_drop.noComplete{.release.cta}{.shared{::cta}}.b64 state,  [addr], count;\n\n.sem   = { .release, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nA thread executing\nmbarrier.arrive_drop\non the\nmbarrier object\nat the location specified by\nthe address operand\naddr\nperforms the following steps:\nDecrements the expected arrival count of the\nmbarrier object\nby the value specified by the\n32-bit integer operand\ncount\n. If\ncount\noperand is not specified, it defaults to 1.\nPerforms an\narrive-on operation\non the\nmbarrier object\n. The operand\ncount\nspecifies the\ncount\nargument to the\narrive-on operation\n.\nThe decrement done in the expected arrivals count of the\nmbarrier object\nwill be for all the\nsubsequent phases of the\nmbarrier object\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nor\n.shared::cluster\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nThe optional qualifier\n.expect_tx\nspecifies that an\nexpect-tx\noperation is performed prior to the\narrive_drop\noperation, i.e. the decrement of arrival count and\narrive-on\noperation. The 32-bit unsigned integer operand\ntxCount\nspecifies the\nexpectCount\nargument to\nthe\nexpect-tx\noperation. When both qualifiers\n.arrive_drop\nand\n.expect_tx\nare specified, then\nthe count argument of the\narrive-on\noperation is assumed to be 1.\nmbarrier.arrive_drop\noperation with\n.release\nqualifier forms the\nrelease\npattern as\ndescribed in the Memory Consistency Model and synchronizes with the\nacquire\npatterns.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.release\nis assumed by default.  The\n.relaxed\nqualifier does not provide any memory\nordering semantics and visibility guarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that an\nmbarrier.arrive_drop\ninstruction can directly synchronize. If the\n.scope\nqualifier is not specified then it defaults\nto\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state space where the mbarrier\nresides.\nA\nmbarrier.arrive_drop\nwith\n.noComplete\nqualifier must not complete the\nmbarrier,\notherwise the behavior is undefined.\nThe value of the operand\ncount\nmust be in the range as specified in\nContents of the mbarrier object\n.\nNote: for\nsm_8x\n, when the argument\ncount\nis specified, the modifier\n.noComplete\nis\nrequired.\nA thread that wants to either exit or opt out of participating in the\narrive-on operation\ncan use\nmbarrier.arrive_drop\nto drop itself from the\nmbarrier\n.\nmbarrier.arrive_drop\noperation on an\nmbarrier object\nlocated in\n.shared::cta\nreturns an\nopaque 64-bit register capturing the phase of the\nmbarrier object\nprior to the\narrive-on\noperation\nin the destination operand\nstate\n. Contents of the returned state are implementation\nspecific. Optionally, sink symbol\n'_'\ncan be used for the\nstate\nargument.\nmbarrier.arrive_drop\noperation on an\nmbarrier\nobject located in\n.shared::cluster\nbut not\nin\n.shared::cta\ncannot return a value. Sink symbol ‘_’ is mandatory for the destination operand\nfor such cases.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\ncount\nargument without the modifier\n.noComplete\nintroduced in PTX ISA version\n7.8.\nSupport for qualifier\n.expect_tx\nis introduced in PTX ISA version 8.0.\nSupport for sub-qualifier\n::cluster\nintroduced in PTX ISA version 8.0.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nSupport for\ncount\nargument without the modifier\n.noComplete\nrequires\nsm_90\nor higher.\nQualifier\n.expect_tx\nrequires\nsm_90\nor higher.\nSub-qualifier\n::cluster\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n.reg .b32 cnt;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\n// Example 1\n@p mbarrier.arrive_drop.shared.b64 _, [shMem];\n@p exit;\n@p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a;\n@p2 exit;\n..\n@!p mbarrier.arrive.shared.b64   %r1, [shMem];\n@!p mbarrier.test_wait.shared.b64  q, [shMem], %r1;\n\n// Example 2\nmbarrier.arrive_drop.shared::cluster.b64 _, [addr];\nmbarrier.arrive_drop.shared::cta.release.cluster.b64     _, [addr], cnt;\n\n// Example 3\nmbarrier.arrive_drop.expect_tx.shared::cta.relaxed.cluster.b64 state, [addr], tx_count;\n9.7.13.15.15.\nParallel Synchronization and Communication Instructions:\ncp.async.mbarrier.arrive\n\ncp.async.mbarrier.arrive\nMakes the\nmbarrier object\ntrack all prior\ncp.async\noperations initiated by the\nexecuting thread.\nSyntax\ncp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr];\nDescription\nCauses an\narrive-on operation\nto be\ntriggered by the system on the\nmbarrier object\nupon the completion of all prior\ncp.async\noperations initiated by the\nexecuting thread. The\nmbarrier object\nis at the location specified by the operand\naddr\n. The\narrive-on operation\nis\nasynchronous to execution of\ncp.async.mbarrier.arrive\n.\nWhen\n.noinc\nmodifier is not specified, the pending count of the mbarrier object is incremented\nby 1 prior to the asynchronous\narrive-on operation\n. This\nresults in a zero-net change for the pending count from the asynchronous\narrive-on\noperation\nduring the current phase. The pending count of the\nmbarrier object\nafter the increment should not\nexceed the limit as mentioned in\nContents of the mbarrier object\n. Otherwise,\nthe behavior is undefined.\nWhen the\n.noinc\nmodifier is specified, the increment to the pending count of the\nmbarrier\nobject\nis not performed. Hence the decrement of the pending count done by the asynchronous\narrive-on operation\nmust be\naccounted for in the initialization of the\nmbarrier object\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n// Example 1: no .noinc\nmbarrier.init.shared.b64 [shMem], threadCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n....\n// Absence of .noinc accounts for arrive-on from completion of prior cp.async operations.\n// So mbarrier.init must only account for arrive-on from mbarrier.arrive.\ncp.async.mbarrier.arrive.shared.b64 [shMem];\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n\n\n\n// Example 2: with .noinc\n\n// Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive.\n\n// All threads participating in the mbarrier perform cp.async\nmov.b32 copyOperationCnt, threadCount;\n\n// 3 arrive-on operations will be triggered per-thread\nmul.lo.u32 copyArrivalCnt, copyOperationCnt, 3;\n\nadd.u32 totalCount, threadCount, copyArrivalCnt;\n\nmbarrier.init.shared.b64 [shMem], totalCount;\n....\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n...\n// Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 1st instance\n....\ncp.async.ca.shared.global [shard3], [gbl3], 4;\ncp.async.ca.shared.global [shard4], [gbl4], 16;\ncp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem]; // 2nd instance\n....\ncp.async.ca.shared.global [shard5], [gbl5], 4;\ncp.async.cg.shared.global [shard6], [gbl6], 16;\ncp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 3rd and last instance\n....\nmbarrier.arrive.shared.b64 state, [shMem];\n\nwaitLoop:\nmbarrier.test_wait.shared.b64 p, [shMem], state;\n@!p bra waitLoop;\n9.7.13.15.16.\nParallel Synchronization and Communication Instructions:\nmbarrier.test_wait\n/\nmbarrier.try_wait\n\nmbarrier.test_wait\n,\nmbarrier.try_wait\nChecks whether the\nmbarrier object\nhas completed the phase.\nSyntax\nmbarrier.test_wait{.sem.scope}{.shared{::cta}}.b64        waitComplete, [addr], state;\nmbarrier.test_wait.parity{.sem.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity;\n\nmbarrier.try_wait{.sem.scope}{.shared{::cta}}.b64         waitComplete, [addr], state\n                                                            {, suspendTimeHint};\n\nmbarrier.try_wait.parity{.sem.scope}{.shared{::cta}}.b64  waitComplete, [addr], phaseParity\n                                                            {, suspendTimeHint};\n\n.sem   = { .acquire, .relaxed }\n.scope = { .cta, .cluster }\nDescription\nThe\ntest_wait\nand\ntry_wait\noperations test for the completion of the current or the immediately\npreceding phase of an\nmbarrier object\nat the location specified by the operand\naddr\n.\nmbarrier.test_wait\nis a non-blocking instruction which tests for the completion of the phase.\nmbarrier.try_wait\nis a potentially blocking instruction which tests for the completion of the\nphase. If the phase is not complete, the executing thread may be suspended. Suspended thread resumes\nexecution when the specified phase completes OR before the phase completes following a\nsystem-dependent time limit. The optional 32-bit unsigned integer operand\nsuspendTimeHint\nspecifies the time limit, in nanoseconds, that may be used for the time limit instead of the\nsystem-dependent limit.\nmbarrier.test_wait\nand\nmbarrier.try_wait\ntest for completion of the phase :\nSpecified by the 64-bit unsigned integer operand\nstate\n, which was returned by an\nmbarrier.arrive\ninstruction on the same\nmbarrier object\nduring the current or the\nimmediately preceding phase. Or\nIndicated by the 32-bit unsigned integer operand\nphaseParity\n, which is the integer parity\nof either the current phase or the immediately preceding phase of the\nmbarrier object\n.\nThe\n.parity\nvariant of the instructions test for the completion of the phase indicated by the\noperand\nphaseParity\n, which is the integer parity of either the current phase or the immediately\npreceding phase of the\nmbarrier object\n. An even phase has integer parity 0 and an odd phase has\ninteger parity of 1. So the valid values of\nphaseParity\noperand are 0 and 1.\nNote: the use of the\n.parity\nvariants of the instructions requires tracking the phase of an\nmbarrier object\nthroughout its lifetime.\nThe\ntest_wait\nand\ntry_wait\noperations are valid only for :\nthe current incomplete phase, for which\nwaitComplete\nreturns\nFalse\n.\nthe immediately preceding phase, for which\nwaitComplete\nreturns\nTrue\n.\nIf no state space is specified then\nGeneric Addressing\nis\nused. If the address specified by\naddr\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nSupported addressing modes for operand\naddr\nis as described in\nAddresses as Operands\n.\nAlignment for operand\naddr\nis as described in the\nSize and alignment of mbarrier object\n.\nWhen\nmbarrier.test_wait\nand\nmbarrier.try_wait\noperations with\n.acquire\nqualifier\nreturns\nTrue\n, they form the\nacquire\npattern as described in the\nMemory Consistency Model\n.\nThe optional\n.sem\nqualifier specifies a memory synchronizing effect as described in the\nMemory Consistency Model\n. If the\n.sem\nqualifier is absent,\n.acquire\nis assumed by default.  The\n.relaxed\nqualifier does not provide any memory\nordering semantics and visibility guarantees.\nThe optional\n.scope\nqualifier indicates the set of threads that the\nmbarrier.test_wait\nand\nmbarrier.try_wait\ninstructions can directly synchronize. If the\n.scope\nqualifier is not\nspecified then it defaults to\n.cta\n. In contrast, the\n.shared::<scope>\nindicates the state\nspace where the mbarrier resides.\nQualifiers\n.sem\nand\n.scope\nmust be specified together.\nThe following ordering of memory operations hold for the executing thread when\nmbarrier.test_wait\nor\nmbarrier.try_wait\nhaving acquire semantics returns\nTrue\n:\nAll memory accesses (except\nasync operations\n) requested prior, in program\norder, to\nmbarrier.arrive\nhaving release semantics during the completed phase by\nthe participating threads of the CTA are performed and are visible to the executing thread.\nAll\ncp.async\noperations\nrequested prior, in program order, to\ncp.async.mbarrier.arrive\nduring the completed phase by\nthe participating threads of the CTA are performed and made visible to the executing thread.\nAll\ncp.async.bulk\nasynchronous operations using the same\nmbarrier object\nrequested prior,\nin program order, to\nmbarrier.arrive\nhaving release semantics during the completed\nphase by the participating threads of the CTA are performed and made visible to the executing thread.\nAll memory accesses requested after the\nmbarrier.test_wait\nor\nmbarrier.try_wait\n, in\nprogram order, are not performed and not visible to memory accesses performed prior to\nmbarrier.arrive\nhaving release semantics, in program order, by other threads\nparticipating in the\nmbarrier\n.\nThere is no ordering and visibility guarantee for memory accesses requested by the thread after\nmbarrier.arrive\nhaving release semantics and prior to\nmbarrier.test_wait\n,\nin program order.\nPTX ISA Notes\nmbarrier.test_wait\nintroduced in PTX ISA version 7.0.\nModifier\n.parity\nis introduced in PTX ISA version 7.1.\nmbarrier.try_wait\nintroduced in PTX ISA version 7.8.\nSupport for sub-qualifier\n::cta\non\n.shared\nintroduced in PTX ISA version 7.8.\nSupport for\n.scope\nand\n.sem\nqualifiers introduced in PTX ISA version 8.0\nSupport for\n.relaxed\nqualifier introduced in PTX ISA version 8.6.\nTarget ISA Notes\nmbarrier.test_wait\nrequires\nsm_80\nor higher.\nmbarrier.try_wait\nrequires\nsm_90\nor higher.\nSupport for\n.cluster\nscope requires\nsm_90\nor higher.\nSupport for\n.relaxed\nqualifier requires\nsm_90\nor higher.\nExamples\n// Example 1a, thread synchronization with test_wait:\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.test_wait.shared.b64    complete, [shMem], %r1;\n@!complete nanosleep.u32 20;\n@!complete bra waitLoop;\n\n// Example 1b, thread synchronization with try_wait :\n\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nmbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive\n\n// computation not requiring mbarrier synchronization...\n\nwaitLoop:\nmbarrier.try_wait.relaxed.cluster.shared.b64    complete, [shMem], %r1;\n@!complete bra waitLoop;\n\n\n// Example 2, thread synchronization using phase parity :\n\n.reg .b32 i, parArg;\n.reg .b64 %r1;\n.shared .b64 shMem;\n\nmov.b32 i, 0;\nmbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.\n...\nloopStart :                           // One phase per loop iteration\n    ...\n    mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads\n    ...\n    and.b32 parArg, i, 1;\n    waitLoop:\n    mbarrier.test_wait.parity.shared.b64  complete, [shMem], parArg;\n    @!complete nanosleep.u32 20;\n    @!complete bra waitLoop;\n    ...\n    add.u32 i, i, 1;\n    setp.lt.u32 p, i, IterMax;\n@p bra loopStart;\n\n\n// Example 3, Asynchronous copy completion waiting :\n\n.reg .b64 state;\n.shared .b64 shMem2;\n.shared .b64 shard1, shard2;\n.global .b64 gbl1, gbl2;\n\nmbarrier.init.shared.b64 [shMem2], threadCount;\n...\ncp.async.ca.shared.global [shard1], [gbl1], 4;\ncp.async.cg.shared.global [shard2], [gbl2], 16;\n\n// Absence of .noinc accounts for arrive-on from prior cp.async operation\ncp.async.mbarrier.arrive.shared.b64 [shMem2];\n...\nmbarrier.arrive.shared.b64 state, [shMem2];\n\nwaitLoop:\nmbarrier.test_wait.shared::cta.b64 p, [shMem2], state;\n@!p bra waitLoop;\n\n// Example 4, Synchronizing the CTA0 threads with cluster threads\n.reg .b64 %r1, addr, remAddr;\n.shared .b64 shMem;\n\ncvta.shared.u64          addr, shMem;\nmapa.u64                 remAddr, addr, 0;     // CTA0's shMem instance\n\n// One thread from CTA0 executing the below initialization operation\n@p0 mbarrier.init.shared::cta.b64 [shMem], N;  // N = no of cluster threads\n\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Entire cluster executing the below arrive operation\nmbarrier.arrive.release.cluster.b64              _, [remAddr];\n\n// computation not requiring mbarrier synchronization ...\n\n// Only CTA0 threads executing the below wait operation\nwaitLoop:\nmbarrier.try_wait.parity.acquire.cluster.shared::cta.b64  complete, [shMem], 0;\n@!complete bra waitLoop;\n9.7.13.15.17.\nParallel Synchronization and Communication Instructions:\nmbarrier.pending_count\n\nmbarrier.pending_count\nQuery the pending arrival count from the opaque mbarrier state.\nSyntax\nmbarrier.pending_count.b64 count, state;\nDescription\nThe pending count can be queried from the opaque mbarrier state using\nmbarrier.pending_count\n.\nThe\nstate\noperand is a 64-bit register that must be the result of a prior\nmbarrier.arrive.noComplete\nor\nmbarrier.arrive_drop.noComplete\ninstruction. Otherwise, the\nbehavior is undefined.\nThe destination register\ncount\nis a 32-bit unsigned integer representing the pending count of\nthe\nmbarrier object\nprior to the\narrive-on operation\nfrom\nwhich the\nstate\nregister was obtained.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.reg .b32 %r1;\n.reg .b64 state;\n.shared .b64 shMem;\n\nmbarrier.arrive.noComplete.b64 state, [shMem], 1;\nmbarrier.pending_count.b64 %r1, state;\n9.7.13.16.\nParallel Synchronization and Communication Instructions:\ntensormap.cp_fenceproxy\n\ntensormap.cp_fenceproxy\nA fused copy and fence operation.\nSyntax\ntensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned  [dst], [src], size;\n\n.cp_qualifiers    = { .global.shared::cta }\n.fence_qualifiers = { .to_proxy::from_proxy.release.scope }\n.to_proxy::from_proxy  = { .tensormap::generic }\n.scope            = { .cta, .cluster, .gpu , .sys }\nDescription\nThe\ntensormap.cp_fenceproxy\ninstructions perform the following operations in order :\nCopies data of size specified by the\nsize\nargument, in bytes, from the location specified\nby the address operand\nsrc\nin shared memory to the location specified by the address operand\ndst\nin the global memory, in the generic proxy.\nEstablishes a\nuni-directional\nproxy release pattern on the ordering from the copy operation\nto the subsequent access performed in the tensormap proxy on the address\ndst\n.\nThe valid value of immediate operand\nsize\nis 128.\nThe operands\nsrc\nand\ndst\nspecify non-generic addresses in\nshared::cta\nand\nglobal\nstate space respectively.\nThe\n.scope\nqualifier specifies the set of threads that can directly observe the proxy\nsynchronizing effect of this operation, as described in\nMemory Consistency Model\n.\nThe mandatory\n.sync\nqualifier indicates that\ntensormap.cp_fenceproxy\ncauses the executing\nthread to wait until all threads in the warp execute the same\ntensormap.cp_fenceproxy\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\ntensormap.cp_fenceproxy\ninstruction. In conditionally executed code, an aligned\ntensormap.cp_fenceproxy\ninstruction should only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n// Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor\n\n.reg .b64 new_addr;\n.global .align 128 .b8 gbl[128];\n.shared .align 128 .b8 sMem[128];\n\ncp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar];\n...\ntry_wait_loop:\nmbarrier.try_wait.shared.b64 p, [mbar], state;\n@!p bra try_wait loop;\n\ntensormap.replace.tile.global_address.shared.b1024.b64   [sMem], new_addr;\ntensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.release.gpu.sync.aligned\n                                                         [gbl], [sMem], 128;\nfence.proxy.tensormap::generic.acquire.gpu [gbl], 128;\ncp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [gbl, {tc0}], [mbar0];\n9.7.13.17.\nParallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.try_cancel\n\nclusterlaunchcontrol.try_cancel\nRequests cancellation of cluster which is not launched yet.\nSyntax\nclusterlaunchcontrol.try_cancel.async{.space}.completion_mechanism{.multicast::cluster::all}.b128 [addr], [mbar];\n\n.completion_mechanism = { .mbarrier::complete_tx::bytes };\n.space = { .shared::cta };\nDescription\nThe\nclusterlaunchcontrol.try_cancel\ninstruction requests atomically cancelling the launch of\na cluster that has not started running yet. It asynchronously writes an opaque response to shared\nmemory indicating whether the operation succeeded or failed. The completion of the asynchronous\noperation is tracked using the mbarrier completion mechanism at\n.cluster\nscope.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nOn success, the opaque response contains the\nctaid\nof the first CTA of the canceled cluster; no\nother successful response from other\nclusterlaunchcontrol.try_cancel\noperations from the same\ngrid will contain that id.\nThe mandatory\n.async\nqualifier indicates that the instruction will initiate the cancellation\noperation asynchronously and control will return to the executing thread before the requested\noperation is complete.\nThe\n.space\nqualifier is specified, both operands\naddr\nand\nmbar\nmust be in the\n.shared::cta\nstate space. Otherwise, generic addressing will be assumed for both. The result\nis undefined if any of address operands do not fall within the address window of\n.shared::cta\n.\nThe qualifier\n.completion_mechanism\nspecifies that upon completion of the asynchronous operation,\ncomplete-tx\noperation, with\ncompleteCount\nargument equal to amount of data stored in bytes, will be performed\non the mbarrier object specified by the operand\nmbar\n.\nThe executing thread can then use\nmbarrier instructions\nto wait for completion\nof the asynchronous operation. No other synchronization mechanisms described in\nMemory Consistency Model\ncan be used to guarantee the completion of the asynchronous copy operations.\nThe\n.multicast::cluster::all\nqualifier indicates that the response is asynchronously written using\nweak async-proxy writes to the corresponding local shared memory\naddr\nof each CTA in the requesting\ncluster. The completion of the writes to\naddr\nof a particular CTA is signaled via a complete-tx operation\nto the mbarrier object on the shared memory of that CTA.\nThe behavior of instruction with\n.multicast::cluster::all\nqualifier is undefined if any CTA in the\ncluster is exited.\nOperand\naddr\nspecifies the naturally aligned address of the 16-byte wide shared memory location where\nthe request’s response is written.\nThe response of\nclusterlaunchcontrol.try_cancel\ninstruction will be 16-byte opaque value and will be\nit available at location specified by operand\naddr\n. After loading this response into 16-byte register,\ninstruction\nclusterlaunchcontrol.query_cancel\ncan be used to check if request was successful and to\nretrieve\nctaid\nof the first CTA of the canceled cluster.\nIf the executing CTA has already observed the completion of a\nclusterlaunchcontrol.try_cancel\ninstruction\nas failed, then the behavior of issuing a subsequent\nclusterlaunchcontrol.try_cancel\ninstruction is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nQualifier\n.multicast::cluster::all\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Assumption: 1D cluster (cluster_ctaid.y/.z == 1) with 1 thread per CTA.\n\n// Current Cluster to be processed: initially the launched cluster:\nmov.b32 xctaid, %ctaid.x;\n\n// Establish full synchronization across all CTAs of the cluster for the first iteration.\n// Weaker synchronization may suffice depending on initialization sequence.\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\n\n// Iteration loop over all cluster CTAs\nprocessCluster:\n  mov.u32  %r0, %tid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  // Elect a leader thread (thread idx 0) for each CTA to arrive and expect_tx at\n  // each CTA local shared memory barrier:\n  mov.u32  %r0, %tid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  // All other threads skip to processing the work of the current cluster:\n  @!p0 bra processCurrentCluster;\n\n  // All CTAs in the cluster arrive at their local SMEM barrier and set 16B handle tx count:\n  mbarrier.arrive.expect_tx.cluster.relaxed.shared::cta.b64 state, [mbar], 16;\n\n  // First CTA in Cluster attempts to cancel a not-yet-started cluster:\n  mov.u32  %r0, %cluster_ctaid.x;\n  setp.u32.eq p0, %r0, 0x0;\n  @p0 clusterlaunchcontrol.try_cancel.async.mbarrier::complete_tx::bytes.multicast::cluster::all.b128 [addr], [mbar];\n\n  processCurrentCluster:\n    // ...process current cluster (\"xctaid\") while cancellation request for next cluster runs asynchronously...\n\n  // After processing current cluster, wait on cancellation request response for next cluster via specified mbarrier:\n  waitLoop:\n    // .acquire prevents weak handle read (\"ld.shared handle, [addr]\") from overtaking this mbarrier.try_wait:\n    mbarrier.try_wait.cluster.acquire.shared::cta.b64   complete, [mbar], state;\n    @!complete bra waitLoop;\n   // Cancellation request has completed.\n\n  // Generic-proxy weak read of cancellation request into 16-byte wide register:\n  ld.shared.b128 handle, [addr];\n\n  // Check whether cancellation succeeded:\n  clusterlaunchcontrol.query_cancel.is_canceled.pred.b128 p, handle;\n  // If cancellation request failed, we couldn't cancel any other cluster, so all current cluster CTAs exit.\n  @!p ret;\n\n  // Otherwise, cancellation request succeeded.\n  // Extract \"ctaid\" of first cancelled-cluster CTA which we'll process in next \"processCluster\" loop iteration:\n  @p clusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xctaid, _, _, _},  handle;\n\n  // Release current iteration generic-proxy weak read of handle (\"ld.shared handle, [addr]\")\n  // before next iteration async-proxy write to handle (\"clusterlaunchcontrol.try_cancel [addr]\")\n  fence.proxy.async::generic.release.sync_restrict::shared::cta.cluster;\n\n  // Arrive and wait at the next iteration cluster barrier with relaxed semantics.\n  barrier.cluster.arrive.relaxed;\n  barrier.cluster.wait;\n\n  // Acquire prior iteration generic-proxy weak read of handle (\"ld.shared handle, [addr]\")\n  // before current iteration async-proxy write to handle (\"clusterlaunchcontrol.try_cancel [addr]\")\n  fence.proxy.async::generic.acquire.sync_restrict::shared::cluster.cluster;\n\n  bra processCluster;\n9.7.13.18.\nParallel Synchronization and Communication Instructions:\nclusterlaunchcontrol.query_cancel\n\nclusterlaunchcontrol.query_cancel\nQueries response of\nclusterlaunchcontrol.try_cancel\noperation.\nSyntax\nclusterlaunchcontrol.query_cancel.is_canceled.pred.b128 pred, try_cancel_response;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xdim, ydim, zdim, _},  try_cancel_response;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid{::dimension}.b32.b128 reg, try_cancel_response;\n\n::dimension = { ::x, ::y, ::z };\nDescription\nInstruction\nclusterlaunchcontrol.query_cancel\ncan be used to decode opaque response\nwritten by instruction\nclusterlaunchcontrol.try_cancel\n.\nAfter loading response from\nclusterlaunchcontrol.try_cancel\ninstruction into 16-byte\nregister it can be further queried using\nclusterlaunchcontrol.query_cancel\ninstruction\nas follows:\nclusterlaunchcontrol.query_cancel.is_canceled.pred.b128\n: If the cluster is canceled\nsuccessfully, predicate\np\nis set to\ntrue\n; otherwise, it is set to\nfalse\n.\nIf the request succeeded, the instruction\nclusterlaunchcontrol.query_cancel.get_first_ctaid\nextracts the CTA id of the first CTA in the canceled cluster. By default, the instruction\nreturns a\n.v4\nvector whose first three elements are the\nx\n,\ny\nand\nz\ncoordinate\nof first CTA in canceled cluster. The contents of the 4th element are unspecified. The\nexplicit\n.get_first_ctaid::x\n,\n.get_first_ctaid::y\n, or\n.get_first_ctaid::z\nqualifiers can be used to extract individual\nx\n,\ny\nor\nz\ncoordinates into a 32-bit\nregister.\nIf the request fails the behavior of\nclusterlaunchcontrol.query_cancel.get_first_ctaid\nis undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_100\nor higher.\nExamples\nclusterlaunchcontrol.query_cancel.is_canceled pred.b128 p, handle;\n\n@p clusterlaunchcontrol.query_cancel.get_first_ctaid.v4.b32.b128 {xdim, ydim, zdim, ignr}  handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::x.b32.b128 reg0, handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::y.b32.b128 reg1, handle;\n\nclusterlaunchcontrol.query_cancel.get_first_ctaid::z.b32.b128 reg2, handle;\n9.7.14.\nWarp Level Matrix Multiply-Accumulate Instructions\n\nThe matrix multiply and accumulate operation has the following form:\nD = A * B + C\nwhere\nD\nand\nC\nare called accumulators and may refer to the same matrix.\nPTX provides two ways to perform matrix multiply-and-accumulate computation:\nUsing\nwmma\ninstructions:\nThis warp-level computation is performed collectively by all threads in the warp as follows:\nLoad matrices A, B and C from memory into registers using the\nwmma.load\noperation. When\nthe operation completes, the destination registers in each thread hold a fragment of the\nloaded matrix.\nPerform the matrix multiply and accumulate operation using the\nwmma.mma\noperation on the\nloaded matrices. When the operation completes, the destination registers in each thread hold\na fragment of the result matrix returned by the\nwmma.mma\noperation.\nStore result Matrix D back to memory using the\nwmma.store\noperation. Alternately, result\nmatrix D can also be used as argument C for a subsequent\nwmma.mma\noperation.\nThe\nwmma.load\nand\nwmma.store\ninstructions implicitly handle the organization of matrix\nelements when loading the input matrices from memory for the\nwmma.mma\noperation and when\nstoring the result back to memory.\nUsing\nmma\ninstruction:\nSimilar to\nwmma\n,\nmma\nalso requires computation to be performed collectively by all\nthreads in the warp however distribution of matrix elements across different threads in warp\nneeds to be done explicitly before invoking the\nmma\noperation. The\nmma\ninstruction\nsupports both dense as well as sparse matrix A. The sparse variant can be used when A is a\nstructured sparse matrix as described in\nSparse matrix storage\n.\n9.7.14.1.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and C. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\n, where A is an\nMxK\nmatrix, B is a\nKxN\nmatrix, while C and D are\nMxN\nmatrices.\nThe following matrix shapes are supported for the specified types:\nInstruction\nScale\nSparsity\nMultiplicand Data-type\nShape\nPTX ISA version\nwmma\nNA\nDense\nFloating-point -\n.f16\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 6.0\nwmma\nDense\nAlternate floating-point format -\n.bf16\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 7.0\nwmma\nDense\nAlternate floating-point format -\n.tf32\n.m16n16k8\nPTX ISA version 7.0\nwmma\nDense\nInteger -\n.u8\n/\n.s8\n.m16n16k16\n,\n.m8n32k16\n,\nand\n.m32n8k16\nPTX ISA version 6.3\nwmma\nDense\nSub-byte integer -\n.u4\n/\n.s4\n.m8n8k32\nPTX ISA version 6.3\nwmma\nDense\nSingle-bit -\n.b1\n.m8n8k128\nPTX ISA version 6.3\nmma\nNA\nDense\nFloating-point -\n.f64\n.m8n8k4\nPTX ISA version 7.0\n.m16n8k4\n,\n.m16n8k8\n,\nand\n.m16n8k16\nPTX ISA version 7.8\nmma\nDense\nFloating-point -\n.f16\n.m8n8k4\nPTX ISA version 6.4\n.m16n8k8\nPTX ISA version 6.5\n.m16n8k16\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.bf16\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.tf32\n.m16n8k4\nand\n.m16n8k8\nPTX ISA version 7.0\nmma\nDense\nInteger -\n.u8\n/\n.s8\n.m8n8k16\nPTX ISA version 6.5\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.0\nmma\nDense\nSub-byte integer -\n.u4\n/\n.s4\n.m8n8k32\nPTX ISA version 6.5\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 7.0\nmma\nDense\nSingle-bit -\n.b1\n.m8n8k128\n,\n.m16n8k128\n,\nand\n.m16n8k256\nPTX ISA version 7.0\nmma\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k32\nPTX ISA version 8.4\nmma\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k16\nPTX ISA version 8.7\nmma\nDense\nAlternate floating-point format -\n.e3m2\n/\n.e2m3\n/\n.e2m1\n.m16n8k32\nPTX ISA version 8.7\nmma\nYes\nDense\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nX\n(Scale)\n.ue8m0\n.m16n8k32\nPTX ISA version 8.7\nmma\nDense\nAlternate floating-point format -\n.e2m1\nX\n(Scale)\n.ue8m0\n/\n.ue4m3\n.m16n8k64\nPTX ISA version 8.7\nmma\nNA\nSparse\nFloating-point -\n.f16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.bf16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.tf32\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 7.1\nmma\nSparse\nInteger -\n.u8\n/\n.s8\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 7.1\nmma\nSparse\nSub-byte integer -\n.u4\n/\n.s4\n.m16n8k64\nand\n.m16n8k128\nPTX ISA version 7.1\nmma\nSparse\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k64\nPTX ISA version 8.4\nmma\nSparse\nwith\nordered\nmetadata\nFloating-point -\n.f16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.bf16\n.m16n8k16\nand\n.m16n8k32\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.tf32\n.m16n8k8\nand\n.m16n8k16\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nInteger -\n.u8\n/\n.s8\n.m16n8k32\nand\n.m16n8k64\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nSub-byte integer -\n.u4\n/\n.s4\n.m16n8k64\nand\n.m16n8k128\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n.m16n8k64\nPTX ISA version 8.5\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e3m2\n/\n.e2m3\n/\n.e2m1\n.m16n8k64\nPTX ISA version 8.7\nmma\nYes\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nX\n(Scale)\n.ue8m0\n.m16n8k64\nPTX ISA version 8.7\nmma\nSparse\nwith\nordered\nmetadata\nAlternate floating-point format -\n.e2m1\nX\n(Scale)\n.ue8m0\n/\n.ue4m3\n.m16n8k128\nPTX ISA version 8.7\n9.7.14.2.\nMatrix Data-types\n\nThe matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point.\nFor floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later.\nData-type\nMultiplicands (A or B)\nAccumulators (C or D)\nInteger\n.u8\n,\n.s8\n.s32\nFloating Point\n.f16\n.f16\n,\n.f32\nAlternate floating Point\n.bf16\n.f32\nAlternate floating Point\n.tf32\n.f32\nAlternate floating Point\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n.f16\n,\n.f32\nAlternate floating Point\nwith scale\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\nX (Scale)\n.ue8m0\n.f32\nAlternate floating Point\nwith scale\n.e2m1\nX (Scale)\n.ue8m0\nor\n.ue4m3\n.f32\nFloating Point\n.f64\n.f64\nSub-byte integer\nboth\n.u4\nor both\n.s4\n.s32\nSingle-bit integer\n.b1\n.s32\n9.7.14.3.\nBlock Scaling for\nmma.sync\n\nThe\nmma\ninstruction with the following\n.kind\nqualifier:\n.kind::mxf8f6f4\n.kind::mxf4\n.kind::mxf4nvf4\nperform matrix multiplication with block scaling. This operation has the following form:\nD\n=\n(A\n*\nscale_A)\n*\n(B\n*\nscale_B)\n+\nC\n.\nFor a\nscale_A\nmatrix of shape\nM x SFA_N\n, each row of matrix\nA\nis divided into\nSFA_N\nnumber of chunks and each chunk of a row is multiplied with the corresponding\nelement (henceforth referred as\nSF_A\n) from the same row of\nscale_A\n.\nSimilarly, for a\nscale_B\nmatrix of shape\nSFB_M x N\n, each column of matrix\nB\nis\ndivided into the\nSFB_M\nnumber of chunks and each chunk of a column is multiplied with\nthe corresponding element (henceforth referred as\nSF_B\n) from the same column of\nscale_B\n.\nFigure 42\nshows an example of\nmma\nwith block scaling of\nscale_vec::2X\n.\nFigure 42\nmma\nwith block scaling of\n.scale_vec::2X\n\nThe shapes for\nscale_A\nand\nscale_B\nmatrices depend upon the qualifier\n.scale_vec_size\nas shown in\nTable 35\n.\nTable 35\nShapes for scale matrices depending upon\n.scale_vec_size\nqualifier\n\n.scale_vec_size\nShape of scale_A\nShape of scale_B\n.scale_vec::1X\nM x 1\n1 x N\n.scale_vec::2X\nM x 2\n2 x N\n.scale_vec::4X\nM x 4\n4 x N\nThe valid combination of the exact element types and the\n.scale_vec_size\nare listed in\nTable 36\n.\nTable 36\nValid combinations of\n.scale_vec_size\nand\n.kind\nqualifier\n\n.kind::*\nElement Data Type\n.atype and .btype\nScale Data Type\n.stype\n.scale_vec_size\n.kind::mxf8f6f4\n.e4m3\n,\n.e5m2\n.e3m2\n,\n.e2m3\n.e2m1\n.ue8m0\n.scale_vec::1X\n.kind::mxf4\n.e2m1\n.ue8m0\n.scale_vec::2X\n.kind::mxf4nvf4\n.e2m1\n.ue8m0\n.scale_vec::2X\n,\n.scale_vec::4X\n.e2m1\n.ue4m3\n.scale_vec::4X\nThe\nscale-a-data\nand\nscale-b-data\nargument provides metadata for\nscale_A\nand\nscale_B\nmatrices respectively. The tuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nprovides the selector information to choose elements\nSF_A\nand\nSF_B\nfrom corresponding metadata arguments\nscale-a-data\nand\nscale-b-data\n.\nThe tuple\n{byte-id-a,\nthread-id-a}\nallows to select the scale matrix element\nSF_A\nfrom\nscale-a-data\n. Similarly, the tuple\n{byte-id-b,\nthread-id-b}\nallows to select\nthe scale matrix element\nSF_B\nfrom\nscale-b-data\n.\nThe components\nthread-id-a\n,\nthread-id-b\ndecides which threads among the quad\ncontribute the\nSF_A\nand\nSF_B\nvalues. The following listing describes the impact\nof thread selector component\nthread-id-a\n,\nthread-id-b\n:\nOne thread-pair within the quad determined by\nthread-id-a\n, contributes the\nSF_A\nvalues. The value of 0 selects lower two threads whereas value of 1 selects upper two\nthreads from the quad. In other words, when\nthread-id-a\nset to 0, thread-pair\nsatisfying:\n%laneid\n% 4 == 0 or 1 provides the\nSF_A\n. In contrast when\nthread-id-a\nset to 1, thread-pair satisfying:\n%laneid\n% 4 == 2 or 3 provides\nthe\nSF_A\n. Refer\nFigure 43\nfor more details.\nFigure 43\nSelection of set of values for\nSF_A\nbased on\nthread-id-a\n\nOne thread within the quad, determined by\nthread-id-b\n, contributes the\nSF_B\nvalue. In other words, each thread satisfying:\n%laneid\n% 4 ==\nthread-id-b\nprovides the\nSF_B\n. Refer\nFigure 44\nfor more details.\nFigure 44\nSelection of set of values for\nSF_B\nbased on\nthread-id-b\n\nThe arguments\nbyte-id-a\n,\nbyte-id-b\nselects which bytes from the\nscale-a-data\n,\nscale-b-data\ncontribute the\nSF_A\nand\nSF_B\nvalues. The following listing describes\nimplications of\n.scale_vec_size\nqualifier on byte selector component\nbyte-id-a\n,\nbyte-id-b\n:\nWhen\n.scale_vec_size\nis\n.scale_vec::1X\nOne byte each within\nscale-a-data\nand\nscale-b-data\ndetermined by\nbyte-id-a\n,\nbyte-id-b\nrespectively contributes the\nSF_A\nand\nSF_B\nvalues.\nWhen\n.scale_vec_size\nis\n.scale_vec::2X\nOne byte-pair (two bytes) within\nscale-a-data\nand\nscale-b-data\ndetermined by\nbyte-id-a\nand\nbyte-id-b\ncontributes the\nSF_A\nand\nSF_B\nvalues. The value\nof 0 selects lower two bytes whereas value of 2 selects upper two bytes from the\ncorresponding metadata value.\nWhen\n.scale_vec_size\nis\n.scale_vec::4X\nAll four bytes within\nscale-a-data\nand\nscale-b-data\ncontribute the values.\nHence,\nbyte-id-a\n,\nbyte-id-b\nmust be zero.\nRefer\nFigure 45\nfor more details.\nFigure 45\nSelection of set of values for\nSF_A\nor\nSF_B\nbased on\nbyte-id-a\nor\nbyte-id-b\n\nTable 37\nenumerates the valid values for\nvarious selector components. Any other value results in an undefined behavior.\nTable 37\nValid values for various selector components\n\n.scale_vec_size\nSelector Components\nbyte-id-a\nthread-id-a\nbyte-id-b\nthread-id-b\nscale_vec::1X\n[0, 1, 2, 3]\n[0, 1]\n[0, 1, 2, 3]\n[0, 1, 2, 3]\nscale_vec::2X\n[0, 2]\n[0, 2]\nscale_vec::4X\n0\n0\n9.7.14.4.\nMatrix multiply-accumulate operation using\nwmma\ninstructions\n\nThis section describes warp level\nwmma.load,\nwmma.mma\nand\nwmma.store\ninstructions and the\norganization of various matrices invovled in these instruction.\n9.7.14.4.1.\nMatrix Fragments for WMMA\n\nEach thread in the warp holds a fragment of the matrix. The distribution of fragments loaded by the\nthreads in a warp is unspecified and is target architecture dependent, and hence the identity of the\nfragment within the matrix is also unspecified and is target architecture dependent. The fragment\nreturned by a\nwmma\noperation can be used as an operand for another\nwmma\noperation if the\nshape, layout and element type of the underlying matrix matches. Since fragment layout is\narchitecture dependent, using the fragment returned by a\nwmma\noperation in one function as an\noperand for a\nwmma\noperation in a different function may not work as expected if the two\nfunctions are linked together but were compiled for different link-compatible SM architectures. Note\npassing\nwmma\nfragment to a function having\n.weak\nlinkage is unsafe since at link time\nreferences to such function may get resolved to a function in different compilation module.\nEach fragment is a vector expression whose contents are determined as follows. The identity of\nindividual matrix elements in the fragment is unspecified.\nInteger fragments\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.u8\nor\n.s8\n.m16n16k16\nA\nA vector expression of two\n.b32\nregisters, with each\nregister containing four elements from the matrix.\nB\nA vector expression of two\n.b32\nregisters, with each\nregister containing four elements from the matrix.\n.m8n32k16\nA\nA vector expression containing a single\n.b32\nregister\ncontaining four elements from the matrix.\nB\nA vector expression of four\n.b32\nregisters, with each\nregister containing four elements from the matrix.\n.m32n8k16\nA\nA vector expression of four\n.b32\nregisters, with each\nregister containing four elements from the matrix.\nB\nA vector expression containing single\n.b32\nregister,\nwith each containing four elements from the matrix.\nAccumulators (C or D):\nData-type\nShape\nFragment\n.s32\n.m16n16k16\nA vector expression of eight\n.s32\nregisters.\n.m8n32k16\n.m32n8k16\nFloating point fragments\nData-type\nMatrix\nFragment\n.f16\nA or B\nA vector expression of eight\n.f16x2\nregisters.\n.f16\nC or D\nA vector expression of four\n.f16x2\nregisters.\n.f32\nA vector expression of eight\n.f32\nregisters.\nFloating point fragments for\n.bf16\ndata format\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.bf16\n.m16n16k16\nA\nA vector expression of four\n.b32\nregisters, with each\nregister containing two elements from the matrix.\nB\n.m8n32k16\nA\nA vector expression containing a two\n.b32\nregisters,\nwith containing two elements from the matrix.\nB\nA vector expression of eight\n.b32\nregisters, with\neach register containing two elements from the matrix.\n.m32n8k16\nA\nA vector expression of eight\n.b32\nregisters, with\neach register containing two elements from the matrix.\nB\nA vector expression containing two\n.b32\nregisters,\nwith each containing two elements from the matrix.\nAccumulators (C or D):\nData-type\nMatrix\nFragment\n.f32\nC or D\nA vector expression containing eight\n.f32\nregisters.\nFloating point fragments for\n.tf32\ndata format\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.tf32\n.m16n16k8\nA\nA vector expression of four\n.b32\nregisters.\nB\nA vector expression of four\n.b32\nregisters.\nAccumulators (C or D):\nData-type\nShape\nMatrix\nFragment\n.f32\n.m16n16k8\nC or D\nA vector expression containing eight\n.f32\nregisters.\nDouble precision floating point fragments\nMultiplicands (A or B):\nData-type\nShape\nMatrix\nFragment\n.f64\n.m8n8k4\nA or B\nA vector expression of single\n.f64\nregister.\nAccumulators (C or D):\nData-type\nShape\nMatrix\nFragment\n.f64\n.m8n8k4\nC or D\nA vector expression containing single\n.f64\nregister.\nSub-byte integer and single-bit fragments\nMultiplicands (A or B):\nData-type\nShape\nFragment\n.u4\nor\n.s4\n.m8n8k32\nA vector expression containing a single\n.b32\nregister, containing eight elements from the matrix.\n.b1\n.m8n8k128\nA vector expression containing a single\n.b32\nregister, containing 32 elements from the matrix.\nAccumulators (C or D):\nData-type\nShape\nFragment\n.s32\n.m8n8k32\nA vector expression of two\n.s32\nregisters.\n.m8n8k128\nA vector expression of two\n.s32\nregisters.\nManipulating fragment contents\nThe contents of a matrix fragment can be manipulated by reading and writing to individual\nregisters in the fragment, provided the following conditions are satisfied:\nAll matrix element in the fragment are operated on uniformly across threads, using the same\nparameters.\nThe order of the matrix elements is not changed.\nFor example, if each register corresponding to a given matrix is multiplied by a uniform constant\nvalue, then the resulting matrix is simply the scaled version of the original matrix.\nNote that type conversion between\n.f16\nand\n.f32\naccumulator fragments is not supported in\neither direction. The result is undefined even if the order of elements in the fragment remains\nunchanged.\n9.7.14.4.2.\nMatrix Storage for WMMA\n\nEach matrix can be stored in memory with a\nrow-major\nor\ncolumn-major\nlayout. In a\nrow-major\nformat, consecutive elements of each row are stored in contiguous memory locations, and the row is\ncalled the\nleading dimension\nof the matrix. In a\ncolumn-major\nformat, consecutive elements of\neach column are stored in contiguous memory locations and the column is called the\nleading\ndimension\nof the matrix.\nConsecutive instances of the\nleading dimension\n(rows or columns) need not be stored contiguously\nin memory. The\nwmma.load\nand\nwmma.store\noperations accept an optional argument\nstride\nthat specifies the offset from the beginning of each row (or column) to the next, in terms of matrix\nelements (and not bytes). For example, the matrix being accessed by a\nwmma\noperation may be a\nsubmatrix from a larger matrix stored in memory. This allows the programmer to compose a\nmultiply-and-accumulate operation on matrices that are larger than the shapes supported by the\nwmma\noperation.\nAddress Alignment\nThe starting address of each instance of the leading dimension (row or column) must be aligned\nwith the size of the corresponding fragment in bytes. Note that the starting address is\ndetermined by the base pointer and the optional\nstride\n.\nConsider the following instruction as an example:\nwmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s;\nFragment size in bytes = 32 (eight elements of type\n.f16x2\n)\nActual\nstride\nin bytes = 2 *\ns\n(since\nstride\nis specified in terms of\n.f16\nelements, not bytes)\nFor each row of this matrix to be aligned at fragment size the following must be true:\np\nis a multiple of 32.\n2*s\nis a multiple of 32.\nDefault value for stride\nThe default value of the\nstride\nis the size of the\nleading dimension\nof the matrix. For\nexample, for an\nMxK\nmatrix, the\nstride\nis\nK\nfor a\nrow-major\nlayout and\nM\nfor a\ncolumn-major\nlayout. In particular, the default strides for the supported matrix shapes are as\nfollows:\nShape\nA (row)\nA (column)\nB (row)\nB (column)\nAccumulator (row)\nAccumulator (column)\n16x16x16\n16\n16\n16\n16\n16\n16\n8x32x16\n16\n8\n32\n16\n32\n8\n32x8x16\n16\n32\n8\n16\n8\n32\n8x8x32\n32\n8\n8\n32\n8\n8\n8x8x128\n128\n8\n8\n128\n8\n8\n16x16x8\n8\n16\n16\n8\n16\n16\n8x8x4\n4\n8\n8\n4\n8\n8\n9.7.14.4.3.\nWarp-level Matrix Load Instruction:\nwmma.load\n\nwmma.load\nCollectively load a matrix from memory for WMMA\nSyntax\nFloating point format\n.f16\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride};\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride};\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f16, .s8, .u8};\n.btype  = {.f16, .s8, .u8};\n.ctype  = {.f16, .f32, .s32};\nAlternate floating point format\n.bf16\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.bf16 };\n.btype  = {.bf16 };\n.ctype  = {.f32 };\nAlternate floating point format\n.tf32\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.tf32 };\n.btype  = {.tf32 };\n.ctype  = {.f32 };\nDouble precision Floating point\n.f64\nloads:\nwmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.atype  = {.f64 };\n.btype  = {.f64 };\n.ctype  = {.f64 };\nSub-byte loads:\nwmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\n.ctype  = {.s32};\nSingle-bit loads:\nwmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}\nwmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}\nwmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.atype  = {.b1};\n.btype  = {.b1};\n.ctype  = {.s32};\nDescription\nCollectively load a matrix across all threads in a warp from the location indicated by address\noperand\np\nin the specified state space into destination register\nr\n.\nIf no state space is given, perform the memory accesses using\nGeneric Addressing\n.\nwmma.load\noperation may be used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\nThe mutually exclusive qualifiers\n.a\n,\n.b\nand\n.c\nindicate whether matrix A, B or C is\nbeing loaded respectively for the\nwmma\ncomputation.\nThe destination operand\nr\nis a brace-enclosed vector expression that can hold the fragment\nreturned by the load operation, as described in\nMatrix Fragments for WMMA\n.\nThe\n.shape\nqualifier indicates the dimensions of all the matrix arguments involved in the\nintended\nwmma\ncomputation.\nThe\n.layout\nqualifier indicates whether the matrix to be loaded is stored in\nrow-major\nor\ncolumn-major\nformat.\nstride\nis an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the\nleading dimension\n(rows or columns). The default\nvalue of\nstride\nis described in\nMatrix Storage for WMMA\nand must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior.\nThe required alignment for address\np\nand\nstride\nis described in the\nMatrix Storage for WMMA\n.\nThe mandatory\n.sync\nqualifier indicates that\nwmma.load\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.load\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.load\ninstruction. In conditionally executed code, a\nwmma.load\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.load\nis undefined if all threads do not use the same qualifiers and the\nsame values of\np\nand\nstride\n, or if any thread in the warp has exited.\nwmma.load\nis treated as a\nweak\nmemory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\n.m8n8k4\nand\n.m16n16k8\non\nwmma\nintroduced in PTX ISA version 7.0.\nDouble precision and alternate floating point precision\nwmma\nintroduced in PTX ISA version 7.0.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision and alternate floating point precision\nwmma\nrequires\nsm_80\nor higher.\nExamples\n// Load elements from f16 row-major matrix B\n.reg .b32 x<8>;\n\nwmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr];\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from f32 column-major matrix C and scale the values:\n.reg .b32 x<8>;\n\nwmma.load.c.sync.aligned.m16n16k16.col.f32\n                 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr];\n\nmul.f32 x0, x0, 0.1;\n// repeat for all registers x<8>;\n...\nmul.f32 x7, x7, 0.1;\n// Now use {x0, ..., x7} for the actual wmma.mma\n\n// Load elements from integer matrix A:\n.reg .b32 x<4>\n// destination registers x<4> contain four packed .u8 values each\nwmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr];\n\n// Load elements from sub-byte integer matrix A:\n.reg .b32 x0;\n// destination register x0 contains eight packed .s4 values\nwmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr];\n\n// Load elements from .bf16 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k16.row.bf16\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .tf32 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m16n16k8.row.tf32\n                {x0,x1,x2,x3}, [ptr];\n\n// Load elements from .f64 matrix A:\n.reg .b32 x<4>;\nwmma.load.a.sync.aligned.m8n8k4.row.f64\n                {x0}, [ptr];\n9.7.14.4.4.\nWarp-level Matrix Store Instruction:\nwmma.store\n\nwmma.store\nCollectively store a matrix into memory for WMMA\nSyntax\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride};\n\n.layout = {.row, .col};\n.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f16, .f32, .s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k32, .m8n8k128};\n.ss     = {.global, .shared{::cta}};\n.type   = {.s32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m16n16k8};\n.ss     = {.global, .shared{::cta}};\n.type   = {.f32};\n\nwmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}\n.layout = {.row, .col};\n.shape  = {.m8n8k4 };\n.ss     = {.global, .shared{::cta}};\n.type   = {.f64};\nDescription\nCollectively store a matrix across all threads in a warp at the location indicated by address\noperand\np\nin the specified state space from source register\nr\n.\nIf no state space is given, perform the memory accesses using\nGeneric Addressing\n.\nwmma.load\noperation may be used only with\n.global\nand\n.shared\nspaces and with generic addressing, where the address points to\n.global\nor\n.shared\nspace.\nThe source operand\nr\nis a brace-enclosed vector expression that matches the shape of the\nfragment expected by the store operation, as described in\nMatrix Fragments for WMMA\n.\nThe\n.shape\nqualifier indicates the dimensions of all the matrix arguments involved in the\nintended\nwmma\ncomputation. It must match the\n.shape\nqualifier specified on the\nwmma.mma\ninstruction that produced the D matrix being stored.\nThe\n.layout\nqualifier indicates whether the matrix to be loaded is stored in\nrow-major\nor\ncolumn-major\nformat.\nstride\nis an optional 32-bit integer operand that provides an offset in terms of matrix elements\nbetween the start of consecutive instances of the\nleading dimension\n(rows or columns). The default\nvalue of\nstride\nis described in\nMatrix Storage for WMMA\nand must be specified if the actual value is larger than\nthe default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride\nis the leading dimension of the larger matrix. Specifying a value lower than the default value\nresults in undefined behavior.\nThe required alignment for address\np\nand\nstride\nis described in the\nMatrix Storage for WMMA\n.\nThe mandatory\n.sync\nqualifier indicates that\nwmma.store\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.store\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.store\ninstruction. In conditionally executed code, a\nwmma.store\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.store\nis undefined if all threads do not use the same qualifiers and the\nsame values of\np\nand\nstride\n, or if any thread in the warp has exited.\nwmma.store\nis treated as a\nweak\nmemory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\n.m16n16k8\nintroduced in PTX ISA version 7.0.\nDouble precision\nwmma\nintroduced in PTX ISA version 7.0.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision\nwmma\nand shape\n.m16n16k8\nrequires\nsm_80\nor higher.\nExamples\n// Storing f32 elements computed by a wmma.mma\n.reg .b32 x<8>;\n\nwmma.mma.sync.m16n16k16.row.col.f32.f32\n              {d0, d1, d2, d3, d4, d5, d6, d7}, ...;\nwmma.store.d.sync.m16n16k16.row.f32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m16n16k16 shape:\n.reg .b32 d<8>;\nwmma.store.d.sync.aligned.m16n16k16.row.s32\n              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Store s32 accumulator for m8n8k128 shape:\n.reg .b32 d<2>\nwmma.store.d.sync.aligned.m8n8k128.row.s32\n[ptr], {d0, d1};\n\n// Store f64 accumulator for m8n8k4 shape:\n.reg .f64 d<2>;\nwmma.store.d.sync.aligned.m8n8k4.row.f64\n              [ptr], {d0, d1};\n9.7.14.4.5.\nWarp-level Matrix Multiply-and-Accumulate Instruction:\nwmma.mma\n\nwmma.mma\nPerform a single matrix multiply-and-accumulate operation across a warp\nSyntax\n// Floating point (.f16 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype d, a, b, c;\n\n// Integer (.u8/.s8 multiplicands) wmma.mma\nwmma.mma.sync.aligned.alayout.blayout.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape  =  {.m16n16k16, .m8n32k16, .m32n8k16};\n.dtype   = {.f16, .f32};\n.atype   = {.s8, .u8};\n.btype   = {.s8, .u8};\n.ctype   = {.f16, .f32};\nFloating point format\n.bf16\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k16, .m8n32k16, .m32n8k16};\n.atype   = {.bf16 };\n.btype   = {.bf16};\nFloating point format\n.tf32\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m16n16k8 };\n.atype   = {.tf32 };\n.btype   = {.tf32};\nFloating point Double precision\nwmma.mma\n:\nwmma.mma.sync.aligned.alayout.blayout.shape{.rnd}.f64.f64.f64.f64 d, a, b, c;\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.shape   = {.m8n8k4 };\n.rnd = { .rn, .rz, .rm, .rp };\nSub-byte (\n.u4\n/\n.s4\nmultiplicands)\nwmma.mma\n:\nwmma.mma.sync.aligned.row.col.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;\n.shape  = {.m8n8k32};\n.atype  = {.s4, .u4};\n.btype  = {.s4, .u4};\nSingle-bit (\n.b1\nmultiplicands)\nwmma.mma\n:\nwmma.mma.op.popc.sync.aligned.row.col.shape.s32.atype.btype.s32 d, a, b, c;\n.shape  = {.m8n8k128};\n.atype  = {.b1};\n.btype  = {.b1};\n.op     = {.xor, .and}\nDescription\nPerform a warp-level matrix multiply-and-accumulate computation\nD\n=\nA\n*\nB\n+\nC\nusing matrices A,\nB and C loaded in registers\na\n,\nb\nand\nc\nrespectively, and store the result matrix in\nregister\nd\n. The register arguments\na\n,\nb\n,\nc\nand\nd\nhold unspecified fragments of\nthe corresponding matrices as described in\nMatrix Fragments for WMMA\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively.\nFor\nwmma.mma\nwithout explicit\n.atype\nand\n.btype\n:\n.atype\nand\n.btype\nare\nimplicitly set to\n.f16\n.\nFor integer\nwmma\n,\n.ctype\nand\n.dtype\nmust be specified as\n.s32\n. Also, the values for\n.atype\nand\n.btype\nmust be the same, i.e., either both are\n.s8\nor both are\n.u8\n.\nFor sub-byte single-bit\nwmma\n,\n.ctype\nand\n.dtype\nmust be specified as\n.s32\n. Also, the\nvalues for\n.atype\nand\n.btype\nmust be the same; i.e., either both are\n.s4\n, both are\n.u4\n, or both are\n.b1\n.\nFor single-bit\nwmma\n, multiplication is replaced by a sequence of logical operations;\nspecifically,\nwmma.xor.popc\nand\nwmma.and.popc\ncomputes the XOR, AND respectively of a\n128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result\n(\npopc\n). This result is added to the corresponding element of C and written into D.\nThe qualifiers\n.alayout\nand\n.blayout\nmust match the layout specified on the\nwmma.load\ninstructions that produce the contents of operands\na\nand\nb\nrespectively. Similarly, the\nqualifiers\n.atype\n,\n.btype\nand\n.ctype\nmust match the corresponding qualifiers on the\nwmma.load\ninstructions that produce the contents of operands\na\n,\nb\nand\nc\nrespectively.\nThe\n.shape\nqualifier must match the\n.shape\nqualifier used on the\nwmma.load\ninstructions\nthat produce the contents of all three input operands\na\n,\nb\nand\nc\nrespectively.\nThe destination operand\nd\nis a brace-enclosed vector expression that matches the\n.shape\nof\nthe fragment computed by the\nwmma.mma\ninstruction.\nSaturation at the output:\nThe optional qualifier\n.satfinite\nindicates that the final values in the destination register\nare saturated as follows:\nThe output is clamped to the minimum or maximum 32-bit signed integer value. Otherwise, if the\naccumulation would overflow, the value wraps.\nPrecision and rounding for\n.f16\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with\nat least single precision. When both\n.ctype\nand\n.dtype\nare specified as\n.f16\n, the\naccumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs is unspecified.\nPrecision and rounding for\n.bf16\n,\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding and handling of subnormal inputs is unspecified.\nRounding modifiers on double precision\nwmma.mma\n(default is\n.rn\n):\n.rn\nmantissa LSB rounds to nearest even\n.rz\nmantissa LSB rounds towards zero\n.rm\nmantissa LSB rounds towards negative infinity\n.rp\nmantissa LSB rounds towards positive infinity\nThe mandatory\n.sync\nqualifier indicates that\nwmma.mma\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nwmma.mma\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nwmma.mma\ninstruction. In conditionally executed code, a\nwmma.mma\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise\nbehavior is undefined.\nThe behavior of\nwmma.mma\nis undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.0.\n.m8n32k16\nand\n.m32n8k16\nintroduced in PTX ISA version 6.1.\nInteger, sub-byte integer and single-bit\nwmma\nintroduced in PTX ISA version 6.3.\nDouble precision and alternate floating point precision\nwmma\nintroduced in PTX ISA version 7.0.\nSupport for\n.and\noperation in single-bit\nwmma\nintroduced in PTX ISA version 7.1.\nModifier\n.aligned\nis required from PTX ISA version 6.3 onwards, and considered implicit in PTX\nISA versions less than 6.3.\nSupport for\n.satfinite\non floating point\nwmma.mma\nis deprecated in PTX ISA version 6.4 and\nis removed from PTX ISA version 6.5.\nTarget ISA Notes\nFloating point\nwmma\nrequires\nsm_70\nor higher.\nInteger\nwmma\nrequires\nsm_72\nor higher.\nSub-byte and single-bit\nwmma\nrequires\nsm_75\nor higher.\nDouble precision, alternate floating point precision\nwmma\nrequire\nsm_80\nor higher.\n.and\noperation in single-bit\nwmma\nrequires\nsm_80\nor higher.\nExamples\n.global .align 32 .f16 A[256], B[256];\n.global .align 32 .f32 C[256], D[256];\n.reg .b32 a<8> b<8> c<8> d<8>;\n\nwmma.load.a.sync.aligned.m16n16k16.global.row.f16\n        {a0, a1, a2, a3, a4, a5, a6, a7}, [A];\nwmma.load.b.sync.aligned.m16n16k16.global.col.f16\n        {b0, b1, b2, b3, b4, b5, b6, b7}, [B];\n\nwmma.load.c.sync.aligned.m16n16k16.global.row.f32\n        {c0, c1, c2, c3, c4, c5, c6, c7}, [C];\n\nwmma.mma.sync.aligned.m16n16k16.row.col.f32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3, a4, a5, a6, a7},\n        {b0, b1, b2, b3, b4, b5, b6, b7},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\nwmma.store.d.sync.aligned.m16n16k16.global.col.f32\n        [D], {d0, d1, d2, d3, d4, d5, d6, d7};\n\n// Compute an integer WMMA:\n.reg .b32  a, b<4>;\n.reg .b32 c<8>, d<8>;\nwmma.mma.sync.aligned.m8n32k16.row.col.s32.s8.s8.s32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a}, {b0, b1, b2,  b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n\n// Compute sub-byte WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute single-bit type WMMA:\n.reg .b32 a, b, c<2> d<2>\nwmma.mma.xor.popc.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute double precision wmma\n.reg .f64 a, b, c<2>, d<2>;\nwmma.mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n        {d0, d1}, {a}, {b}, {c0, c1};\n\n// Compute alternate floating point precision wmma\n.reg .b32 a<2>, b<2>, c<8>, d<8>;\nwmma.mma.sync.aligned.m16n16k8.row.col.f32.tf32.tf32.f32\n        {d0, d1, d2, d3, d4, d5, d6, d7},\n        {a0, a1, a2, a3}, {b0, b1, b2, b3},\n        {c0, c1, c2, c3, c4, c5, c6, c7};\n9.7.14.5.\nMatrix multiply-accumulate operation using\nmma\ninstruction\n\nThis section describes warp-level\nmma\n,\nldmatrix\n,\nstmatrix\n, and\nmovmatrix\ninstructions and the organization of various matrices involved in these instructions.\n9.7.14.5.1.\nMatrix Fragments for\nmma.m8n8k4\nwith\n.f16\nfloating point type\n\nA warp executing\nmma.m8n8k4\nwith\n.f16\nfloating point type will compute 4 MMA operations of shape\n.m8n8k4\n.\nElements of 4 matrices need to be distributed across the threads in a warp. The following table\nshows distribution of matrices for MMA operations.\nMMA Computation\nThreads participating in MMA computation\nMMA computation 1\nThreads with\n%laneid\n0-3 (low group) and 16-19 (high group)\nMMA computation 2\nThreads with\n%laneid\n4-7 (low group) and 20-23 (high group)\nMMA computation 3\nThreads with\n%laneid\n8-11 (low group) and 24-27 (high group)\nMMA computation 4\nThreads with\n%laneid\n12-15 (low group) and 28-31 (high group)\nFor each of the individual MMA computation shown above, each of the required thread holds a fragment\nof the matrix for performing mma operation as follows:\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters,\nwith each register containing two\n.f16\nelements from\nthe matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown below:\nFragment layout for Row Major matrix A is shown in\nFigure 46\n.\nFigure 46\nMMA .m8n8k4 fragment layout for row-major matrix A with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\ncol\n=\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nFragment layout for Column Major matrix A is shown in\nFigure 47\n.\nThe layout of the fragments held by different threads is shown below:\nFigure 47\nMMA .m8n8k4 fragment layout for column-major matrix A with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\ni\n%\n4\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nif\n%\nlaneid\n<\n16\n(\ni\n%\n4\n)\n+\n4\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\notherwise\ncol\n=\n%\nlaneid\n%\n4\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown below:\nFragment layout for Row Major matrix B is shown in\nFigure 48\n.\nFigure 48\nMMA .m8n8k4 fragment layout for row-major matrix B with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\ncol\n=\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nif\n%\nlaneid\n<\n16\ni\n+\n4\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\notherwise\nFragment layout for Column Major matrix B is shown in\nFigure 49\n.\nFigure 49\nMMA .m8n8k4 fragment layout for column-major matrix B with\n.f16\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\nAccumulators C (or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing four\n.f16x2\nregisters, with\neach register containing two\n.f16\nelements from the matrix\nC (or D).\nc0, c1, c2, c3, c4, c5, c6, c7\n.f32\nA vector expression of eight\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown below:\nFragment layout for accumulator matrix when\n.ctype\nis\n.f16\nis shown in\nFigure 50\n.\nFigure 50\nMMA .m8n8k4 fragment layout for matrix C/D with\n.ctype\n=\n.f16\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\nif\n%\nlaneid\n<\n16\n(\n%\nlaneid\n%\n4\n)\n+\n4\notherwise\ncol\n=\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nFragment layout for accumulator matrix when\n.ctype\nis\n.f32\nis shown in\nFigure 51\nand\nFigure 52\n.\nFigure 51\nMMA .m8n8k4 computation 1 and 2 fragment layout for matrix C/D with\n.ctype\n=\n.f32\n\nFigure 52\nMMA .m8n8k4 computation 3 and 4 fragment layout for matrix C/D with\n.ctype\n=\n.f32\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\nX\nif\n%\nlaneid\n<\n16\nX\n+\n4\notherwise\nwhere\nX\n=\n(\n%\nlaneid\n&\n0b1\n)\n+\n(\ni\n&\n0b10\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\n(\ni\n&\n0b100\n)\n+\n(\n%\nlaneid\n&\n0b10\n)\n+\n(\ni\n&\n0b1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n7\n}\n9.7.14.5.2.\nMatrix Fragments for\nmma.m8n8k4\nwith\n.f64\nfloating point type\n\nA warp executing\nmma.m8n8k4\nwith\n.f64\nfloating point type will compute an MMA operation of\nshape\n.m8n8k4\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing a single\n.f64\nregister, containing\nsingle\n.f64\nelement from the matrix A.\na0\nThe layout of the fragments held by different threads is shown in\nFigure 53\n.\nFigure 53\nMMA .m8n8k4 fragment layout for matrix A  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n>>\n2\ncol\n=\n%\nlaneid\n%\n4\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression containing a single\n.f64\nregister, containing a single\n.f64\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 54\n.\nFigure 54\nMMA .m8n8k4 fragment layout for matrix B  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\nrow\n=\n%\nlaneid\n%\n4\ncol\n=\n%\nlaneid\n>>\n2\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing of two\n.f64\nregisters containing two\n.f64\nelements from the matrix C.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 55\n.\nFigure 55\nMMA .m8n8k4 fragment layout for accumulator matrix C/D  with\n.f64\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.3.\nMatrix Fragments for\nmma.m8n8k16\n\nA warp executing\nmma.m8n8k16\nwill compute an MMA operation of shape\n.m8n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.s8\nor\n.u8\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 56\n.\nFigure 56\nMMA .m8n8k16 fragment layout for matrix A with\n.u8\n/\n.s8\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.s8\nor\n.u8\nelements from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 57\n.\nFigure 57\nMMA .m8n8k16 fragment layout for matrix B with\n.u8\n/\n.s8\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing of two\n.s32\nregisters.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 58\n.\nFigure 58\nMMA .m8n8k16 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.4.\nMatrix Fragments for\nmma.m8n8k32\n\nA warp executing\nmma.m8n8k32\nwill compute an MMA operation of shape\n.m8n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister, containing\neight\n.s4\nor\n.u4\nelements from the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 59\n.\nFigure 59\nMMA .m8n8k32 fragment layout for matrix A with\n.u4\n/\n.s4\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister, containing\neight\n.s4\nor\n.u4\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 60\n.\nFigure 60\nMMA .m8n8k32 fragment layout for matrix B with\n.u4\n/\n.s4\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression of two\n.s32\nregisters.\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 61\n:\nFigure 61\nMMA .m8n8k32 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.5.\nMatrix Fragments for\nmma.m8n8k128\n\nA warp executing\nmma.m8n8k128\nwill compute an MMA operation of shape\n.m8n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister, containing thirty\ntwo\n.b1\nelements from the matrix A.\na0, a1, … a30, a31\nThe layout of the fragments held by different threads is shown in\nFigure 62\n.\nFigure 62\nMMA .m8n8k128 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n31\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister, containing thirty\ntwo\n.b1\nelements from the matrix B.\nb0, b1, …, b30, b31\nThe layout of the fragments held by different threads is shown in\nFigure 63\n.\nFigure 63\nMMA .m8n8k128 fragment layout for matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n31\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing two\n.s32\nregisters, containing two\n.s32\nelements from the matrix C (or D).\nc0, c1\nThe layout of the fragments held by different threads is shown in\nFigure 64\n.\nFigure 64\nMMA .m8n8k128 fragment layout for accumulator matrix C/D with\n.s32\ntype\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n}\n9.7.14.5.6.\nMatrix Fragments for\nmma.m16n8k4\n\nA warp executing\nmma.m16n8k4\nwill compute an MMA operation of shape\n.m16n8k4\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.tf32\n:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing two\n.b32\nregisters, containing two\n.tf32\nelements from the matrix A.\na0, a1\nThe layout of the fragments held by different threads is shown in\nFigure 65\n.\nFigure 65\nMMA .m16n8k4 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\nthreadID_in_group\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing two\n.f64\nregisters, containing two\n.f64\nelements from the matrix A.\na0, a1\nThe layout of the fragments held by different threads is shown in\nFigure 66\n.\nFigure 66\nMMA .m16n8k4 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\nthreadID_in_group\nMultiplicand B:\n.tf32\n:\n.btype\nFragment\nElements (low to high)\n.tf32\nA vector expression of a single\n.b32\nregister, containing a single\n.tf32\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 67\n.\nFigure 67\nMMA .m16n8k4 fragment layout for matrix B with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\ncol\n=\ngroupID\n.f64\n:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression of a single\n.f64\nregister, containing a single\n.f64\nelement from the matrix B.\nb0\nThe layout of the fragments held by different threads is shown in\nFigure 68\n.\nFigure 68\nMMA .m16n8k4 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\ncol\n=\ngroupID\nAccumulators (C or D):\n.tf32\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f32\nA vector expression containing four\n.f32\nregisters, containing four\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 69\n.\nFigure 69\nMMA .m16n8k4 fragment layout for accumulator matrix C/D with\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.f64\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, containing four\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 70\n.\nFigure 70\nMMA .m16n8k4 fragment layout for accumulator matrix C/D with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.7.\nMatrix Fragments for\nmma.m16n8k8\n\nA warp executing\nmma.m16n8k8\nwill compute an MMA operation of shape\n.m16n8k8\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.f16\nand\n.bf16\n:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\n/\n.bf16\nelements from the\nmatrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 71\n.\nFigure 71\nMMA .m16n8k8 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na1\ngroupID\n+\n8\nfor\na2\nand\na3\ncol\n=\nthreadID_in_group\n*\n2\n+\n(\ni\n&\n0x1\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.tf32\n:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters, containing four\n.tf32\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 72\n.\nFigure 72\nMMA .m16n8k8 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\nthreadID_in_group\nfor\na0\nand\na1\nthreadID_in_group\n+\n4\nfor\na2\nand\na3\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, containing four\n.f64\nelements from the matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 73\n.\nFigure 73\nMMA .m16n8k8 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\nthreadID_in_group\nfor\na0\nand\na1\nthreadID_in_group\n+\n4\nfor\na2\nand\na3\nMultiplicand B:\n.f16\nand\n.bf16\n:\n.btype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing a single\n.f16x2\nregister, containing\ntwo\n.f16\n/\n.bf16\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 74\n.\nFigure 74\nMMA .m16n8k8 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n2\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,\n1\n}\ncol\n=\ngroupID\n.tf32\n:\n.btype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing two\n.b32\nregisters, containing two\n.tf32\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 75\n.\nFigure 75\nMMA .m16n8k8 fragment layout for matrix B with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\nfor\nb0\nthreadID_in_group\n+\n4\nfor\nb1\ncol\n=\ngroupID\n.f64\n:\n.btype\nFragment\nElements (low to high)\n.f64\nA vector expression containing two\n.f64\nregisters, containing two\n.f64\nelements from the matrix B.\nb0, b1\nThe layout of the fragments held by different threads is shown in\nFigure 76\n.\nFigure 76\nMMA .m16n8k8 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\nfor\nb0\nthreadID_in_group\n+\n4\nfor\nb1\ncol\n=\ngroupID\nAccumulators (C or D):\n.f16\n,\n.bf16\nand\n.tf32\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression of four\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown in\nFigure 77\n.\nFigure 77\nMMA .m16n8k8 fragment layout for accumulator matrix C/D with\n.f16x2\n/\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n.f64\n:\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression of four\n.f64\nregisters containing four\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 78\n.\nFigure 78\nMMA .m16n8k8 fragment layout for accumulator matrix C/D with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nc0\nand\nc1\ngroupID\n+\n8\nfor\nc2\nand\nc3\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.8.\nMatrix Fragments for\nmma.m16n8k16\nwith floating point type\n\nA warp executing\nmma.m16n8k16\nfloating point types will compute an MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.f16\nand\n.bf16\n:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.f16x2\nregisters, with\neach register containing two\n.f16\n/\n.bf16\nelements\nfrom the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 79\n.\nFigure 79\nMMA .m16n8k16 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n2\n||\n4\n<=\ni\n<\n6\ngroupID\n+\n8\nOtherwise\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nai\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\n+\n8\nfor\nai\nwhere\ni\n>=\n4\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing eight\n.f64\nregisters, with each\nregister containing one\n.f64\nelement from the matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 80\n.\nFigure 80\nMMA .m16n8k16 fragment layout for matrix A with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n%\n2\n=\n0\ngroupID\n+\n8\nOtherwise\ncol\n=\n(\ni\n*\n2\n)\n+\nthreadID_in_group\nfor\nai\nwhere\ni\n%\n2\n=\n0\n(\ni\n*\n2\n)\n-\n2\n+\n(\nthreadID_in_group\nOtherwise\nMultiplicand B:\n.f16\nand\n.bf16\n:\n.btype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing two\n.f16x2\nregisters, with\neach register containing two\n.f16\n/\n.bf16\nelements\nfrom the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 81\n.\nFigure 81\nMMA .m16n8k16 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nwhere the row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nbi\nwhere\ni\n<\n2\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\n+\n8\nfor\nbi\nwhere\ni\n>=\n2\ncol\n=\ngroupID\n.f64\n:\n.atype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters, with each\nregister containing one\n.f64\nelement from the matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 82\n.\nFigure 82\nMMA .m16n8k16 fragment layout for matrix B with\n.f64\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\nthreadID_in_group\n+\n(\ni\n*\n4\n)\nfor\nbi\nwhere\ni\n<\n4\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.f64\nA vector expression containing four\n.f64\nregisters containing\n.f64\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters containing\nfour\n.f32\nelements from the matrix C (or D).\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nThe layout of the fragments held by different threads is shown in\nFigure 83\n.\nFigure 83\nMMA .m16n8k16 fragment layout for accumulator matrix matrix C/D.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.9.\nMatrix Fragments for\nmma.m16n8k16\nwith integer type\n\nA warp executing\nmma.m16n8k16\nwill compute an MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.u8\n/\n.s8\nelements from the\nmatrix A.\na0, a1, a2, a3, a4, a5, a6, a7\n.e4m3\n/\n.e5m2\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\nelements from the\nmatrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 84\n.\nFigure 84\nMMA .m16n8k16 fragment layout for matrix A with\n.u8\n/\n.s8\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n4\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n4\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n7\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.u8\n/\n.s8\nelements from the matrix B.\nb0, b1, b2, b3\n.e4m3\n/\n.e5m2\nA vector expression containing a single\n.b32\nregister, containing\nfour\n.e4m3\n/\n.e5m2\nelements from the matrix B.\nb0, b1. b2. b3\nThe layout of the fragments held by different threads is shown in\nFigure 85\n.\nFigure 85\nMMA .m16n8k16 fragment layout for matrix B with\n.u8\n/\n.s8\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n3\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing\nfour\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c1, c2\nThe layout of the fragments held by different threads is shown in\nFigure 86\n.\nFigure 86\nMMA .m16n8k16 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.10.\nMatrix Fragments for\nmma.m16n8k32\n\nA warp executing\nmma.m16n8k32\nwill compute an MMA operation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.s4\nor\n.u4\n:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.u4\n/\n.s4\nelements from the\nmatrix A.\na0, a1, …, a14, a15\nThe layout of the fragments held by different threads is shown in\nFigure 87\n.\nFigure 87\nMMA .m16n8k32 fragment layout for matrix A with\n.u4\n/\n.s4\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n8\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n8\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,..,\n15\n}\n.s8\nor\n.u8\nor\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.s8\n/\n.u8\nelements from the\nmatrix A.\na0, a1, .., a14, a15\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from the matrix A.\na0, a1, …, a14, a15\nThe layout of the fragments held by different threads is shown in\nFigure 88\n.\nFigure 88\nMMA .m16n8k32 fragment layout for matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\n||\n8\n<=\ni\n<\n12\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nai\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\n+\n16\nfor\nai\nwhere\ni\n>=\n8\nMultiplicand B:\n.s4\nor\n.u4\n:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing a single\n.b32\nregister,\ncontaining eight\n.s4\n/\n.u4\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 89\n.\nFigure 89\nMMA .m16n8k32 fragment layout for matrix B with\n.u4\n/\n.s4\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nbi\nwhere\ni\n=\n{\n0\n,..,\n7\n}\ncol\n=\ngroupID\n.s8\nor\n.u8\nor\n.e4m3\nor\n.e5m2\nor\n.e3m2\nor\n.e2m3\nor\n.e2m1\n:\n.btype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.s8\n/\n.u8\nelements from the\nmatrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from the matrix B.\nb0, b1, b2, b3, b4, b5, b6, b7\nThe layout of the fragments held by different threads is shown in\nFigure 90\nand\nFigure 91\n.\nFigure 90\nMMA .m16n8k32 fragment layout for rows 0–15 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 91\nMMA .m16n8k32 fragment layout for rows 16–31 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\nfor\nbi\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n4\n)\n+\n(\ni\n&\n0x3\n)\n+\n16\nfor\nbi\nwhere\ni\n>=\n4\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing\nfour\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f16\nA vector expression containing two\n.f16x2\nregisters, with each\nregister containing two\n.f16\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 92\n.\nFigure 92\nMMA .m16n8k32 fragment layout for accumulator matrix C/D with\n.s32\n/\n.f32\n/\n.f16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.11.\nMatrix Fragments for\nmma.m16n8k64\n\nA warp executing\nmma.m16n8k64\nwill compute an MMA operation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight\n.s4\n/\n.u4\nelements from the\nmatrix A.\na0, a1, …, a30, a31\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight\n.e2m1\nelements from the matrix A.\na0, a1, …, a30, a31\nThe layout of the fragments held by different threads is shown in\nFigure 93\n.\nFigure 93\nMMA .m16n8k64 fragment layout for matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\n||\n16\n<=\ni\n<\n24\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nai\nwhere\ni\n<\n16\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\n+\n32\nfor\nai\nwhere\ni\n>=\n16\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.s4\n/\n.u4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.s4\n/\n.u4\nelements from the\nmatrix B.\nb0, b1, …, b14, b15\n.e2m1\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight\n.e2m1\nelements from the matrix B.\nb0, b1, …, b14, b15\nThe layout of the fragments held by different threads is shown in\nFigure 94\nand\nFigure 95\n.\nFigure 94\nMMA .m16n8k64 fragment layout for rows 0–31 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 95\nMMA .m16n8k64 fragment layout for rows 32–63 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\nfor\nbi\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n8\n)\n+\n(\ni\n&\n0x7\n)\n+\n32\nfor\nbi\nwhere\ni\n>=\n8\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing four\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\n.f32\nA vector expression containing four\n.f32\nregisters, containing four\n.f32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 96\n.\nFigure 96\nMMA .m16n8k64 fragment layout for accumulator matrix C/D with\n.s32\n/\n.f32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,..,\n3\n}\n9.7.14.5.12.\nMatrix Fragments for\nmma.m16n8k128\n\nA warp executing\nmma.m16n8k128\nwill compute an MMA operation of shape\n.m16n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing two\n.b32\nregisters, with each register containing\nthirty two\n.b1\nelements from the matrix A.\na0, a1, …, a62, a63\nThe layout of the fragments held by different threads is shown in\nFigure 97\n.\nFigure 97\nMMA .m16n8k128 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\ni\n<\n32\ngroupID\n+\n8\nfor\nai\nwhere\ni\n>=\n32\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\nfor\nai\nwhere\ni\n=\n{\n0\n,\n...,\n63\n}\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing a single\n.b32\nregister containing thirty\ntwo\n.b1\nelements from the matrix B.\nb0, b1, … , b30, b31\nThe layout of the fragments held by different threads is shown in\nFigure 98\n.\nFigure 98\nMMA .m16n8k128 fragment layout for matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nbi\nwhere\ni\n=\n{\n0\n,...,\n31\n}\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 99\n.\nFigure 99\nMMA .m16n8k128 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n,\n2\n,\n3\n}\n9.7.14.5.13.\nMatrix Fragments for\nmma.m16n8k256\n\nA warp executing\nmma.m16n8k256\nwill compute an MMA operation of shape\n.m16n8k256\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing four\n.b32\nregisters, with each register\ncontaining thirty two\n.b1\nelements from the matrix A.\na0, a1, …, a126, a127\nThe layout of the fragments held by different threads is shown in\nFigure 100\n.\nFigure 100\nMMA .m16n8k256 fragment layout for matrix A with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n32\n||\n64\n<=\ni\n<\n96\ngroupID\n+\n8\notherwise\ncol\n=\n(\nthreadID_in_group\n*\n32\n)\n+\ni\nfor\nai\nwhere\ni\n<\n64\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\n+\n128\nfor\nai\nwhere\ni\n>=\n64\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.b1\nA vector expression containing two\n.b32\nregisters, with each register\ncontaining thirty two\n.b1\nelements from the matrix B.\nb0, b1, …, b62, b63\nThe layout of the fragments held by different threads is shown in\nFigure 101\nand\nFigure 102\n.\nFigure 101\nMMA .m16n8k256 fragment layout for rows 0–127 of matrix B with\n.b1\ntype.\n\nFigure 102\nMMA .m16n8k256 fragment layout for rows 128–255 of matrix B with\n.b1\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\nfor\nbi\nwhere\ni\n<\n32\n(\nthreadID_in_group\n*\n32\n)\n+\n(\ni\n&\n0x1F\n)\n+\n128\nfor\nbi\nwhere\ni\n>=\n32\ncol\n=\ngroupID\nAccumulators (C or D):\n.ctype / .dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing four\n.s32\nregisters, containing\nfour\n.s32\nelements from the matrix C (or D).\nc0, c1, c2, c3\nThe layout of the fragments held by different threads is shown in\nFigure 103\n.\nFigure 103\nMMA .m16n8k256 fragment layout for accumulator matrix C/D with\n.s32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nci\nwhere\ni\n<\n2\ngroupID\n+\n8\nfor\nci\nwhere\ni\n>=\n2\ncol\n=\n(\nthreadID_in_group\n*\n2\n)\n+\n(\ni\n&\n0x1\n)\nfor\nci\nwhere\ni\n=\n{\n0\n,\n1\n,\n2\n,\n3\n}\n9.7.14.5.14.\nMultiply-and-Accumulate Instruction:\nmma\n\nmma\nPerform matrix multiply-and-accumulate operation\nSyntax\nHalf precision floating point type:\nmma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.dtype.f16.f16.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c;\n\n.alayout = {.row, .col};\n.blayout = {.row, .col};\n.ctype   = {.f16, .f32};\n.dtype   = {.f16, .f32};\nAlternate floating point type:\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32        d, a, b, c;\nmma.sync.aligned.m16n8k8.row.col.f32.atype.btype.f32      d, a, b, c;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32       d, a, b, c;\nmma.sync.aligned.shape.row.col.dtype.f8type.f8type.ctype  d, a, b, c;\nmma.sync.aligned.m16n8k32.row.col.kind.dtype.f8f6f4type.f8f6f4type.ctype d, a, b, c;\n\n.atype      = {.bf16, .tf32};\n.btype      = {.bf16, .tf32};\n.f8type     = {.e4m3, .e5m2};\n.f8f6f4type = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.ctype      = {.f16, .f32};\n.dtype      = {.f16, .f32};\n.shape      = {.m16n8k16, .m16n8k32};\n.kind       = {.kind::f8f6f4};\nAlternate floating point type with block scaling:\nmma.sync.aligned.m16n8k64.row.col.kind.block_scale{.scale_vec_size}.f32.e2m1.e2m1.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf4};\n.scale_vec_size = {.scale_vec::2X};\n.stype          = {.ue8m0};\n\nmma.sync.aligned.m16n8k64.row.col.kind.block_scale.scale_vec_size.f32.e2m1.e2m1.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf4nvf4};\n.scale_vec_size = {.scale_vec::2X, .scale_vec::4X};\n.stype          = {.ue8m0, .ue4m3};\n\nmma.sync.aligned.m16n8k32.row.col.kind.block_scale{.scale_vec_size}.f32.f8f6f4type.f8f6f4type.f32.stype d, a, b, c, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.kind           = {.kind::mxf8f6f4};\n.scale_vec_size = {.scale_vec::1X};\n.f8f6f4type     = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.stype          = {.ue8m0};\nDouble precision floating point type:\nmma.sync.aligned.shape.row.col.f64.f64.f64.f64 d, a, b, c;\n\n.shape   = {.m8n84, .m16n8k4, .m16n8k8, .m16n8k16};\nInteger type:\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k16, .m16n8k16, .m16n8k32}\n.atype   = {.u8, .s8};\n.btype   = {.u8, .s8};\n\nmma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;\n\n.shape   = {.m8n8k32, .m16n8k32, .m16n8k64}\n.atype   = {.u4, .s4};\n.btype   = {.u4, .s4};\nSingle bit:\nmma.sync.aligned.shape.row.col.s32.b1.b1.s32.bitOp.popc d, a, b, c;\n\n.bitOp = {.xor, .and}\n.shape = {.m8n8k128, .m16n8k128, .m16n8k256}\nDescription\nPerform a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+C\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the C and D matrices are\nMxN\n.\nQualifier\n.block_scale\nspecifies that the matrices A and B are scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply and accumulate operation\nas specified in the section\nBlock Scaling\n. The data type\ncorresponding to each of the element within\nscale_A\nand\nScale_B\nmatrices is specified\nby\n.stype\n. Qualifier\n.scale_vec_size\nspecifies the number of columns of\nscale_A\nmatrix\nand number of rows in the matrix\nscale_B\n.\nThe valid combinations of\n.kind\n,\n.stype\nand\n.scale_vec_size\nare described in\nTable 36\n. For\nmma\nwith\n.kind::mxf4\nwhen the\nqualifier\n.scale_vec_size\nis not specified, then it defaults to\n2X\n. In contrast, when\n.kind\nis specified as\n.kind::mxf8f6f4\nthen the qualifier\n.scale_vec_size\ndefaults\nto\n1X\n. However, for\n.kind::mxf4nvf4\n, it is mandatory to provide valid\n.scale_vec_size\n.\nA warp executing\nmma.sync.m8n8k4\ninstruction computes 4 matrix multiply and accumulate\noperations. Rest of the\nmma.sync\noperations compute a single matrix mutliply and accumulate\noperation per warp.\nFor single-bit\nmma.sync\n, multiplication is replaced by a sequence of logical operations;\nspecifically,\nmma.xor.popc\nand\nmma.and.popc\ncomputes the XOR, AND respectively of a k-bit\nrow of A with a k-bit column of B, then counts the number of set bits in the result (\npopc\n). This\nresult is added to the corresponding element of C and written into D.\nOperands\na\nand\nb\nrepresent two multiplicand matrices A and B, while\nc\nand\nd\nrepresent the accumulator and destination matrices, distributed across the threads in warp.\nWhen\n.block_scale\nqualifier is specified, operand\nscale-a-data\n,\nscale-b-data\nrepresents\nthe scale matrix metadata corresponding to\nscale_A\nand\nscale_B\nmatrices respectively. The\ntuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nrepresent selectors for matrices\nscale_A\nand\nscale_B\nrespectively from their corresponding metadata arguments\nscale-a-data\n,\nscale-b-data\n. The operands\nscale-a-data\n,\nscale-b-data\nare of type\n.b32\n. The operands\nbyte-id-a\n,\nthread-id-a\n,\nbyte-id-b\n,\nthread-id-b\nare unsigned 16-bit integer values.\nFor more details on selector arguments refer\nBlock Scaling\nsection.\nThe registers in each thread hold a fragment of matrix as described in\nMatrix multiply-accumulate operation using mma instruction\n.\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively. The qualifier\n.stype\nindicate the data-type\nof the elements in the matrices\nscale_A\nand\nscale_B\n. Specific shapes have type restrictions :\n.m8n8k4\n: When\n.ctype\nis\n.f32\n,\n.dtype\nmust also be\n.f32\n.\n.m16n8k8\n:\n.dtype\nmust be the same as\n.ctype\n.\n.atype\nmust be the same as\n.btype\n.\n.m16n8k16\nand\n.m16n8k32\n:\n.dtype\nmust be the same as\n.ctype\n.\nThe qualifiers\n.alayout\nand\n.blayout\nindicate the row-major or column-major layouts of\nmatrices A and B respectively.\nWhen\n.kind\nis either of\n.kind::mxf8f6f4\nor\n.kind::f8f6f4\n, the individual 4-bit and the\n6-bit floating point type elements must be packed in an 8-bit container. The matrix element of type\n.e2m1\nresides in central 4 bits of the 8-bit container with padding in the upper 2 bits and\nlower 2 bits of the container. When the matrix element is of type\n.e3m2\nor\n.e2m3\n, the\nmatrix element resides in the lower 6 bits of the 8-bit container with padding in the upper 2 bits\nof the container. In contrast, note that when using\nmma\nwith\n.kind::mxf4\nor\n.kind::mxf4nvf4\n, no explicit padding is necessary even though matrix elements are of type\n.e2m1\n.\nPrecision and rounding :\n.f16\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single\nprecision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values\nis performed with at least single precision. When both\n.ctype\nand\n.dtype\nare specified\nas\n.f16\n, the accumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.e4m3\n,\n.e5m2\n,\n.e3m2\n,\n.e2m3\n,\n.e2m1\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.f64\nfloating point operations :\nPrecision of the element-wise multiplication and addition operation is identical to that of\n.f64\nprecision fused multiply-add. Supported rounding modifiers are :\n.rn\n: mantissa LSB rounds to nearest even. This is the default.\n.rz\n: mantissa LSB rounds towards zero.\n.rm\n: mantissa LSB rounds towards negative infinity.\n.rp\n: mantissa LSB rounds towards positive infinity.\nInteger operations :\nThe integer\nmma\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the range\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nmma\ninstruction causes the executing thread to\nwait until all threads in the warp execute the same\nmma\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmma\ninstruction. In conditionally executed code, a\nmma\ninstruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined.\nThe behavior of\nmma\ninstruction is undefined if all threads in the same warp do not use the same\nqualifiers, or if any thread in the warp has exited.\nNotes\nPrograms using double precision floating point\nmma\ninstruction with shapes\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nrequire at least 64 registers for compilation.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.4.\n.f16\nfloating point type\nmma\noperation with\n.m8n8k4\nshape introduced in PTX ISA version\n6.4.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k8\nshape introduced in PTX ISA version\n6.5.\n.u8/.s8\ninteger type\nmma\noperation with\n.m8n8k16\nshape introduced in PTX ISA version\n6.5.\n.u4/.s4\ninteger type\nmma\noperation with\n.m8n8k32\nshape introduced in PTX ISA version\n6.5.\n.f64\nfloating point type\nmma\noperation with\n.m8n8k4\nshape introduced in PTX ISA version\n7.0.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k16\nshape introduced in PTX ISA\nversion 7.0.\n.bf16\nalternate floating point type\nmma\noperation with\n.m16n8k8\nand\n.m16n8k16\nshapes\nintroduced in PTX ISA version 7.0.\n.tf32\nalternate floating point type\nmma\noperation with\n.m16n8k4\nand\n.m16n8k8\nshapes\nintroduced in PTX ISA version 7.0.\n.u8/.s8\ninteger type\nmma\noperation with\n.m16n8k16\nand\n.m16n8k32\nshapes introduced in\nPTX ISA version 7.0.\n.u4/.s4\ninteger type\nmma\noperation with\n.m16n8k32\nand\n.m16n8k64\nshapes introduced in\nPTX ISA version 7.0.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m8n8k128\n,\n.m16n8k128\nand\n.m16n8k256\nshapes introduced in PTX ISA version 7.0.\nSupport for\n.and\noperation in single-bit\nmma\nintroduced in PTX ISA version 7.1.\n.f64\nfloating point type\nmma\noperation with\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nshapes introduced in PTX ISA version 7.8.\nSupport for\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation introduced in\nPTX ISA version 8.4.\nSupport for shape\n.m16n8k16\nand\n.f16\ndtype\n/\nctype\nwith\n.e4m3\n/\n.e5m2\nalternate\nfloating point type mma operation introduced in PTX ISA version 8.7.\nSupport for\n.e3m2\n,\n.e2m3\n,\n.e2m1\nalternate floating point type\nmma\noperation introduced\nin PTX ISA version 8.7.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier introduced in PTX ISA version 8.7.\nSupport for\n.scale_vec::4X\non\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nis introduced in\nPTX ISA version 9.1.\nTarget ISA Notes\nRequires\nsm_70\nor higher.\n.f16\nfloating point type\nmma\noperation with\n.m8n8k4\nshape requires\nsm_70\nor higher.\nNote\nmma.sync.m8n8k4\nis optimized for target architecture\nsm_70\nand may have substantially\nreduced performance on other target architectures.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k8\nshape requires\nsm_75\nor higher.\n.u8/.s8\ninteger type\nmma\noperation with\n.m8n8k16\nshape requires\nsm_75\nor higher.\n.u4/.s4\ninteger type\nmma\noperation with\n.m8n8k32\nshape\nsm_75\nor higher.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m8n8k128\nshape\nsm_75\nor higher.\n.f64\nfloating point type\nmma\noperation with\n.m8n8k4\nshape requires\nsm_80\nor higher.\n.f16\nfloating point type\nmma\noperation with\n.m16n8k16\nshape requires\nsm_80\nor\nhigher.\n.bf16\nalternate floating point type\nmma\noperation with\n.m16n8k8\nand\n.m16n8k16\nshapes\nrequires\nsm_80\nor higher.\n.tf32\nalternate floating point type\nmma\noperation with\n.m16n8k4\nand\n.m16n8k8\nshapes\nrequires\nsm_80\nor higher.\n.u8/.s8\ninteger type\nmma\noperation with\n.m16n8k16\nand\n.m16n8k32\nshapes requires\nsm_80\nor higher.\n.u4/.s4\ninteger type\nmma\noperation with\n.m16n8k32\nand\n.m16n8k64\nshapes requires\nsm_80\nor higher.\n.b1\nsingle-bit integer type\nmma\noperation with\n.m16n8k128\nand\n.m16n8k256\nshapes\nrequires\nsm_80\nor higher.\n.and\noperation in single-bit\nmma\nrequires\nsm_80\nor higher.\n.f64\nfloating point type\nmma\noperation with\n.m16n8k4\n,\n.m16n8k8\n, and\n.m16n8k16\nshapes require\nsm_90\nor higher.\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation requires\nsm_89\nor higher.\n.e3m2\n,\n.e2m3\nand\n.e2m1\nalternate floating point type\nmma\noperation requires\nsm_120a\nand is supported on\nsm_120f\nfrom PTX ISA version 8.8.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier requires\nsm_120a\nand are\nsupported on\nsm_120f\nor higher in the same family from PTX ISA version 8.8.\nExamples of half precision floating point type\n// f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4> %Rd<4>\nmma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n\n// f16 elements in C and f32 elements in D\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<4>\n.reg .f32 %Rd<8>\nmma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f16\n{%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7},\n{%Ra0, %Ra1},\n{%Rb0, %Rb1},\n{%Rc0, %Rc1, %Rc2, %Rc3};\n\n // f32 elements in C and D\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<4>, %Rb<2>, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .f16 %Ra<4>, %Rb<2>;\n.reg .f32 %Rc<2>, %Rd<2>;\nmma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of alternate floating point type\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Rb2, %Rb3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .f16x2 %Ra<2>, %Rb<1>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f32.e4m3.e5m2.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.f16.e4m3.e5m2.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.f16.e5m2.e5m2.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.kind::f8f6f4.f32.e3m2.e2m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.kind::f8f6f4.f16.e2m3.e2m1.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1};\nExamples of integer type\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\n\n// s8 elements in A and u8 elements in B\nmma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// u4 elements in A and B matrix\nmma.sync.aligned.m8n8k32.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// s8 elements in A and u8 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k16.row.col.satfinite.s32.s8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u4 elements in A and s4 elements in B\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.u4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// s8 elements in A and s8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n// u8 elements in A and u8 elements in B\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k64.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1 },\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of single bit type\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n// b1 elements in A and B\n.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\n.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.and.popc\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of\n.f64\nfloating point type\n.reg .f64 %Ra, %Rb, %Rc<2>, %Rd<2>;\nmma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64\n  {%Rd0, %Rd1},\n  {%Ra},\n  {%Rb},\n  {%Rc0, %Rc1};\n\n.reg .f64 %Ra<8>, %Rb<4>, %Rc<4>, %Rd<4>;\nmma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k8.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\n\nmma.sync.aligned.m16n8k16.row.col.f64.f64.f64.f64.rn\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3, %Ra4, %Ra5, %Ra6, %Ra7},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3};\nExamples of\nmma\nwith block scale\n.reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n mma.sync.aligned.m16n8k64.row.col.kind::mxf4.block_scale.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {2, 1}, scaleBData, {2, 3};\n\n .reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .u16 bidA, bidB, tidA, tidB;\n mma.sync.aligned.m16n8k64.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue4m3\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .u16 bidA, bidB, tidA, tidB;\nmma.sync.aligned.m16n8k64.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\nmma.sync.aligned.m16n8k32.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e3m2.e2m1.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  scaleAData, {0, 1}, scaleBData, {0, 1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\nmma.sync.aligned.m16n8k32.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e4m3.e5m2.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2,  %Ra3},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  scaleAData, {0, 1}, scaleBData, {0, 0};\n9.7.14.5.15.\nWarp-level matrix load instruction:\nldmatrix\n\nldmatrix\nCollectively load one or more matrices from shared memory for\nmma\ninstruction\nSyntax\nldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p];\n\nldmatrix.sync.aligned.m8n16.num{.ss}.dst_fmt.src_fmt        r, [p];\nldmatrix.sync.aligned.m16n16.num.trans{.ss}.dst_fmt.src_fmt r, [p];\n\n.shape   = {.m8n8, .m16n16};\n.num     = {.x1, .x2, .x4};\n.ss      = {.shared{::cta}};\n.type    = {.b16, .b8};\n.dst_fmt = { .b8x16 };\n.src_fmt = { .b6x16_p32, .b4x16_p64 };\nDescription\nCollectively load one or more matrices across all threads in a warp from the location indicated by\nthe address operand\np\n, from\n.shared\nstate space into destination register\nr\n. If no state\nspace is provided, generic addressing is used, such that the address in\np\npoints into\n.shared\nspace. If the generic address doesn’t fall in\n.shared\nstate space, then the behavior\nis undefined.\nThe\n.shape\nqualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit or 8-bit or 6-bit or 4-bit data.\nFollowing table shows the matrix load case for each\n.shape\n.\n.shape\nMatrix shape\nElement size\n.m8n8\n8x8\n16-bit\n.m16n16\n16x16\n8-bit or 6-bit or 4-bit\n.m8n16\n8x16\n6-bit or 4-bit\nFollowing table shows the valid use of 6-bit or 4-bit data load.\n.src_fmt\n.shape\nSource data\nPadding\n.dst_fmt\n.b6x16_p32\n.m8n16\n16 6-bit elements\n32 bits\n.b8x16\n(16 8-bit\nelements)\n.m16n16\n.b4x16_p64\n.m8n16\n16 4-bit elements\n64 bits\n.m16n16\nFor\n.b6x16_p32\nformat source data is 16 unsigned 6-bit elements with 32 bits padding.\nFor\n.b4x16_p64\nformat source data is 16 unsigned 4-bit elements with 64 bits padding.\nThe values\n.x1\n,\n.x2\nand\n.x4\nfor\n.num\nindicate one, two or four matrices\nrespectively. When\n.shape\nis\n.m16n16\n, only\n.x1\nand\n.x2\nare valid values for\n.num\n.\nThe mandatory\n.sync\nqualifier indicates that\nldmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nldmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nldmatrix\ninstruction. In conditionally executed code, an\nldmatrix\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined.\nThe behavior of\nldmatrix\nis undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited.\nThe destination operand\nr\nis a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of\n.num\n. Each component of the vector expression holds a fragment\nfrom the corresponding matrix.\nSupported addressing modes for\np\nare described in\nAddresses as Operands\n.\nConsecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of\n.num\nas shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on.\n.num\nThreads 0–7\nThreads 8–15\nThreads 16–23\nThreads 24–31\n.x1\naddr0–addr7\n–\n–\n–\n.x2\naddr0–addr7\naddr8–addr15\n–\n–\n.x4\naddr0–addr7\naddr8–addr15\naddr16–addr23\naddr24–addr31\nNote\nFor .target\nsm_75\nor below, all threads must contain valid addresses. Otherwise, the behavior\nis undefined. For\n.num\n=\n.x1\nand\n.num\n=\n.x2\n, addresses contained in lower threads can be\ncopied to higher threads to achieve the expected behavior.\nWhen reading 8x8 matrices, a group of four consecutive threads loads 16 bytes. The matrix addresses\nmust be naturally aligned accordingly.\nEach thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its\nregister\nr\n, and so on. A group of four threads loads an entire row of the matrix as shown in\nFigure 104\n.\nFigure 104\nldmatrix fragment layout for one 8x8 Matrix with 16-bit elements\n\nWhen\n.num\n=\n.x2\n, the elements of the second matrix are loaded in the next destination\nregister in each thread as per the layout in above table. Similarly, when\n.num\n=\n.x4\n,\nelements of the third and fourth matrices are loaded in the subsequent destination registers in each\nthread.\nFor matrix shape 16x16, two destination registers\nr0\nand\nr1\nof type\n.b32\nmust be\nspecified and in each register four 8-bit elements are loaded. For 4-bit or 6-bit data, 8-bit\nelement will have 4 bits or 2 bits of padding respectively.\nRefer\nOptional Decompression\nfor more details\non these formats.\nAn entire row of the matrix can be loaded by a group of four consecutive and aligned threads.\nEach thread in a warp loads 4 consecutive columns across 2 rows as shown in the\nFigure 105\n.\nFigure 105\nldmatrix fragment layout for one 16x16 matrix with 8-bit elements\n\nFor matrix shape 8x16, one destination register\nr0\nof type\n.b32\nmust be specified where four\n8-bit elements are loaded in the register. For 4-bit or 6-bit data, 8-bit element will have 4 bits\nor 2 bits of padding respectively.\nAn entire row of the matrix can be loaded by a group of four consecutive and aligned threads.\nEach thread in a warp loads 4 consecutive columns as shown in\nFigure 106\n.\nFigure 106\nldmatrix fragment layout for one 8x16 matrix with 8-bit elements containing 4-bit/6-bit data\n\nOptional qualifier\n.trans\nindicates that the matrix is loaded in column-major format. However,\nfor 16x16 matrices,\n.trans\nis mandatory.\nThe\nldmatrix\ninstruction is treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.5.\nSupport for\n::cta\nsub-qualifier introduced in PTX ISA version 7.8.\nSupport for\n.m16n16\n,\n.m8n16\nshapes introduced in PTX ISA version 8.6.\nSupport for\n.b8\ntype with\nldmatrix\nis introduced in PTX ISA version 8.6.\nSupport for\n.src_fmt\n,\n.dst_fmt\nqualifiers introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nShapes\n.m16n16\n,\n.m8n16\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nType\n.b8\nwith\nldmatrix\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nQualifiers\n.src_fmt\n,\n.dst_fmt\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd are supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Load a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 d;\nldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr];\n\n// Load two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr];\n\n// Load four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr];\n\n// Load one 16x16 matrices of 64-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<2>;\nldmatrix.sync.aligned.m16n16.x1.trans.shared.b8 {d0, d1}, [addr];\n\n// Load two 16x16 matrices of 64-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m16n16.x2.trans.shared::cta.b8 {d0, d1, d2, d3}, [addr];\n\n// Load two 16x16 matrices of 6-bit elements and transpose them\n.reg .b64 addr;\n.reg .b32 d<4>;\nldmatrix.sync.aligned.m16n16.x2.trans.shared::cta.b8x16.b6x16_p32 {d0, d1, d2, d3}, [addr];\n9.7.14.5.16.\nWarp-level matrix store instruction:\nstmatrix\n\nstmatrix\nCollectively store one or more matrices to shared memory.\nSyntax\nstmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r;\n\n.shape  = {.m8n8, .m16n8};\n.num    = {.x1, .x2, .x4};\n.ss     = {.shared{::cta}};\n.type   = {.b16, .b8};\nDescription\nCollectively store one or more matrices across all threads in a warp to the location indicated by\nthe address operand\np\n, in\n.shared\nstate space. If no state space is provided, generic\naddressing is used, such that the address in\np\npoints into\n.shared\nspace. If the generic\naddress doesn’t fall in\n.shared\nstate space, then the behavior is undefined.\nThe\n.shape\nqualifier indicates the dimensions of the matrices being loaded. Each matrix element\nholds 16-bit or 8-bit data as indicated by the\n.type\nqualifier.\n.m16n8\nshape is valid only for\n.b8\ntype.\nThe values\n.x1\n,\n.x2\nand\n.x4\nfor\n.num\nindicate one, two or four matrices\nrespectively.\nThe mandatory\n.sync\nqualifier indicates that\nstmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nstmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nstmatrix\ninstruction. In conditionally executed code, an\nstmatrix\ninstruction should only be\nused if it is known that all threads in the warp evaluate the condition identically, otherwise the\nbehavior is undefined.\nThe behavior of\nstmatrix\nis undefined if all threads do not use the same qualifiers, or if any\nthread in the warp has exited.\nThe source operand\nr\nis a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit\nregisters as per the value of\n.num\n. Each component of the vector expression holds a fragment\nfrom the corresponding matrix.\nSupported addressing modes for\np\nare described in\nAddresses as Operands\n.\nConsecutive instances of row need not be stored contiguously in memory. The eight addresses required\nfor each matrix are provided by eight threads, depending upon the value of\n.num\nas shown in the\nfollowing table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7\ncorrespond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the\nsecond matrix, and so on.\n.num\nThreads 0–7\nThreads 8–15\nThreads 16–23\nThreads 24–31\n.x1\naddr0–addr7\n–\n–\n–\n.x2\naddr0–addr7\naddr8–addr15\n–\n–\n.x4\naddr0–addr7\naddr8–addr15\naddr16–addr23\naddr24–addr31\nWhen storing 8x8 matrices, a group of four consecutive threads stores 16 bytes. The matrix addresses\nmust be naturally aligned accordingly.\nEach thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its\nregister\nr\n, and so on. A group of four threads stores an entire row of the matrix as shown in\nFigure 107\n.\nFigure 107\nstmatrix fragment layout for one 8x8 matrix with 16-bit elements\n\nWhen\n.num\n=\n.x2\n, the elements of the second matrix are storedd from the next source register\nin each thread as per the layout in above table. Similarly, when\n.num\n=\n.x4\n, elements of the\nthird and fourth matrices are stored from the subsequent source registers in each thread.\nFor 16x8 matrix shape, each of the 32 threads in the warp provides four elements of data per matrix.\nEach element in the source operand\nr\nis of type\n.b32\nand contains four 8 bit elements\ne0\n,\ne1\n,\ne2\n,\ne3\nwith\ne0\nand\ne3\ncontaining the LSB and MSB respectively of register\nr\n.\nFigure 108\nstmatrix fragment layout for one 16x8 matrix with 8 bit elements\n\nOptional qualifier\n.trans\nindicates that the matrix is stored in column-major format. However,\nfor 16x8 matrices,\n.trans\nis mandatory.\nThe\nstmatrix\ninstruction is treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nSupport for\n.m16n8\nshape is introduced in PTX ISA version 8.6.\nSupport for\n.b8\ntype with\nstmatrix\nis introduced in PTX ISA version 8.6.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nShape\n.m16n8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nType\n.b8\nwith\nstmatrix\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\n// Store a single 8x8 matrix using 64-bit addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r};\n\n// Store two 8x8 matrices in column-major format\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1};\n\n// Store four 8x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3};\n\n// Store a single 16x8 matrix using generic addressing\n.reg .b64 addr;\n.reg .b32 r;\nstmatrix.sync.aligned.m16n8.x1.trans.shared.b8 [addr], {r};\n\n// Store two 16x8 matrices\n.reg .b64 addr;\n.reg .b32 r<2>;\nstmatrix.sync.aligned.m16n8.x2.trans.shared::cta.b8 [addr],{r0, r1};\n\n// Store four 16x8 matrices\n.reg .b64 addr;\n.reg .b32 r<4>;\nstmatrix.sync.aligned.m16n8.x4.b8 [addr], {r0, r1, r2, r3};\n9.7.14.5.17.\nWarp-level matrix transpose instruction:\nmovmatrix\n\nmovmatrix\nTranspose a matrix in registers across the warp.\nSyntax\nmovmatrix.sync.aligned.shape.trans.type d, a;\n\n.shape  = {.m8n8};\n.type   = {.b16};\nDescription\nMove a row-major matrix across all threads in a warp, reading elements from source\na\n, and\nwriting the transposed elements to destination\nd\n.\nThe\n.shape\nqualifier indicates the dimensions of the matrix being transposed. Each matrix\nelement holds 16-bit data as indicated by the\n.type\nqualifier.\nThe mandatory\n.sync\nqualifier indicates that\nmovmatrix\ncauses the executing thread to wait\nuntil all threads in the warp execute the same\nmovmatrix\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmovmatrix\ninstruction. In conditionally executed code, a\nmovmatrix\ninstruction should only\nbe used if it is known that all threads in the warp evaluate the condition identically, otherwise\nthe behavior is undefined.\nOperands\na\nand\nd\nare 32-bit registers containing fragments of the input matrix and the\nresulting matrix respectively. The mandatory qualifier\n.trans\nindicates that the resulting\nmatrix in\nd\nis a transpose of the input matrix specified by\na\n.\nEach thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first\nfragment in register\na\n, and so on. A group of four threads holds an entire row of the input\nmatrix as shown in\nFigure 109\n.\nFigure 109\nmovmatrix source matrix fragment layout\n\nEach thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the\nfirst fragment in register\nd\n, and so on. A group of four threads holds an entire column of the\nresult matrix as shown in\nFigure 110\n.\nFigure 110\nmovmatrix result matrix fragment layout\n\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\n.reg .b32 d, a;\nmovmatrix.sync.aligned.m8n8.trans.b16 d, a;\n9.7.14.6.\nMatrix multiply-accumulate operation using\nmma.sp\ninstruction with sparse matrix A\n\nThis section describes warp-level\nmma.sp{::ordered_metadata}\ninstruction with sparse matrix A.\nThis variant of the\nmma\noperation can be used when A is a structured sparse matrix with 50%\nzeros in each row distributed in a shape-specific granularity. For an\nMxNxK\nsparse\nmma.sp{::ordered_metadata}\noperation, the\nMxK\nmatrix A is packed into\nMxK/2\nelements.\nFor each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements\nare packed in the operand representing matrix A. The mapping of these K/2 elements to the\ncorresponding K-wide row is provided explicitly as metadata.\n9.7.14.6.1.\nSparse matrix storage\n\nGranularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a\n16x16\nmatrix A, sparsity is expected to be at 2:4\ngranularity, i.e. each 4-element vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row\ncontains 2 zeros. Index of each non-zero element in a sub-chunk is stored in the metadata\noperand. Values\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nare invalid values for metadata and\nwill result in undefined behavior. In a group of four consecutive threads, one or more threads store\nthe metadata for the whole group depending upon the matrix shape. These threads are specified using\nan additional\nsparsity selector\noperand.\nFigure 111\nshows an example of a 16x16 matrix A represented in sparse format and sparsity\nselector indicating which thread in a group of four consecutive threads stores the metadata.\nFigure 111\nSparse MMA storage example\n\nGranularities for different matrix shapes and data types are described below.\nSparse\nmma.sp{::ordered_metadata}\nwith half-precision and\n.bf16\ntype\nFor the\n.m16n8k16\nand\n.m16n8k32\nmma.sp{::ordered_metadata}\noperations, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements\nin a row of matrix A has two zeros and two non-zero elements. Only the two non-zero elements are\nstored in the operand representing matrix A and their positions in the four-wide chunk in matrix\nA are indicated by two 2-bit indices in the metadata operand. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values\nof indices; any other values result in an undefined behavior.\nFigure 112\nSparse MMA metadata example for\n.f16\n/\n.bf16\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k16\n: One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}.\nm16n8k32\n: A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior.\nSparse\nmma.sp{::ordered_metadata}\nwith\n.tf32\ntype\nWhen matrix A has\n.tf32\nelements, matrix A is structured sparse at a granularity of 1:2. In\nother words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero\nelement. Only the non-zero elements are stored in the operand for matrix A and their positions in a\ntwo-wide chunk in matrix A are indicated by the 4-bit index in the metadata.\n0b1110\nand\n0b0100\nare the only meaningful index values; any other values result in an undefined behavior.\nFigure 113\nSparse MMA metadata example for\n.tf32\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k8\n: One thread within a group of four consecutive threads contributes the metadata for\nthe entire group. This thread is indicated by a value in {0, 1, 2, 3}.\nm16n8k16\n: A thread-pair within a group of four consecutive threads contributes the sparsity\nmetadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);\nany other value results in an undefined behavior.\nSparse\nmma.sp{::ordered_metadata}\nwith integer type\nWhen matrices A and B have\n.u8\n/\n.s8\nelements, matrix A is structured sparse at a granularity\nof 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes\nand two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their\npositions in the four-wide chunk are indicated by two 2-bit indices in the metadata. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 114\nSparse MMA metadata example for\n.u8\n/\n.s8\ntype.\n\nwhen matrices A and B have\n.u4\n/\n.s4\nelements, matrix A is pair-wise structured sparse at a\ngranularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has\nfour zeroes and four non-zero values. Further, the zero and non-zero values are clustered in\nsub-chunks of two elements each within the eight-wide chunk. i.e., each two-wide sub-chunk within\nthe eight-wide chunk must be all zeroes or all non-zeros. Only the four non-zero values are stored\nin sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the\neight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata. For\nmma.sp::ordered_metadata\n,\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 115\nSparse MMA metadata example for\n.u4\n/\n.s4\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k32\nwith\n.u8\n/\n.s8\ntype and\nm16n8k64\nwith\n.u4\n/\n.s4\ntype: A thread-pair\nwithin a group of four consecutive threads contributes the sparsity metadata. Hence, the sparsity\nselector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an\nundefined behavior.\nm16n8k64\nwith\n.u8\n/\n.s8\ntype and\nm16n8k128\nwith\n.u4\n/\n.s4\ntype: All threads\nwithin a group of four consecutive threads contribute the sparsity metadata. Hence, the sparsity\nselector in this case must be 0. Any other value of sparsity selector results in an undefined\nbehavior.\nSparse\nmma.sp{::ordered_metadata}\noperating on\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype with\n.kind::f8f6f4\nor\n.kind::mxf8f6f4\nWhen matrices A and B have\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements, matrix A is\nstructured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a\nrow of matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored\nin sparse matrix and their positions in the four-wide chunk are indicated by two 2-bit indices in the\nmetadata.\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful\nvalues of indices; any other values result in an undefined behavior.\nFigure 116\nSparse MMA metadata example for\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k64\n: All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior.\nSparse\nmma.sp::ordered_metadata\noperating on\n.e2m1\ntype with\n.kind::mxf4\nor\n.kind::mxf4nvf4\nWhen matrices A and B have\n.e2m1\nelements, matrix A is pair-wise structured sparse at a granularity\nof 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has four zeroes and\nfour non-zero values. Further, the zero and non-zero values are clustered in sub-chunks of two elements\neach within the eight-wide chunk. i.e., each two-wide sub-chunk within the eight-wide chunk must be all\nzeroes or all non-zeros. Only the four non-zero values are stored in sparse matrix and the positions of\nthe two two-wide sub-chunks with non-zero values in the eight-wide chunk of a row of matrix A are\nindicated by two 2-bit indices in the metadata.\n0b0100\n,\n0b1000\n,\n0b1001\n,\n0b1100\n,\n0b1101\n,\n0b1110\nare the meaningful values of indices; any other values result in an undefined behavior.\nFigure 117\nSparse MMA metadata example for\n.e2m1\ntype with\n.kind::mxf4\nor\n.kind::mxf4nvf4\n\nThe sparsity selector indicates the threads which contribute metadata as listed below:\nm16n8k128\n: All threads within a group of four consecutive threads contribute the sparsity metadata.\nHence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in\nan undefined behavior.\n9.7.14.6.2.\nMatrix fragments for multiply-accumulate operation with sparse matrix A\n\nIn this section we describe how the contents of thread registers are associated with fragments of\nvarious matrices and the sparsity metadata. The following conventions are used throughout this\nsection:\nFor matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data.\nFor matrix B, when the combination of matrix dimension and the supported data type is not already\ncovered in\nMatrix multiply-accumulate operation using mma instruction\n, a pictorial representation of matrix\nfragments is provided.\nFor matrices C and D, since the matrix dimension - data type combination is the same for all\nsupported shapes, and is already covered in\nMatrix multiply-accumulate operation using mma instruction\n, the pictorial representations\nof matrix fragments are not included in this section.\nFor the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included.\nTk:\n[m..n]\npresent\nin cell\n[x][y..z]\nindicates that bits\nm\nthrough\nn\n(with\nm\nbeing higher) in the\nmetadata operand of thread with\n%laneid=k\ncontains the indices of the non-zero elements from\nthe chunk\n[x][y]..[x][z]\nof matrix A.\n9.7.14.6.2.1.\nMatrix Fragments for sparse\nmma.m16n8k16\nwith\n.f16\nand\n.bf16\ntypes\n\nA warp executing sparse\nmma.m16n8k16\nwith\n.f16\n/\n.bf16\nfloating point type will compute\nan MMA operation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.f16\n/\n.bf16\nA vector expression containing two\n.b32\nregisters,\nwith each register containing two non-zero\n.f16\n/\n.bf16\nelements out of 4 consecutive elements from\nmatrix A.\nMapping of the non-zero elements is as\ndescribed in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 118\n.\nFigure 118\nSparse MMA .m16n8k16 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na1\ngroupID\n+\n8\nfor\na2\nand\na3\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n4\nlastcol\n=\nfirstcol\n+\n3\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\nfor\n.f16\n/\n.b16\nformats.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors each storing the index of a non-zero\nelement of a 4-wide chunk of matrix A as shown in\nFigure 119\n.\nFigure 119\nSparse MMA .m16n8k16 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.14.6.2.2.\nMatrix Fragments for sparse\nmma.m16n8k32\nwith\n.f16\nand\n.bf16\ntypes\n\nA warp executing sparse\nmma.m16n8k32\nwith\n.f16\n/\n.bf16\nfloating point type will compute\nan MMA operation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters,\nwith each register containing two non-zero\n.f16\n/\n.bf16\nelements out of 4 consecutive elements from\nmatrix A.\nMapping of the non-zero elements is as\ndescribed in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 120\n.\nFigure 120\nSparse MMA .m16n8k32 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n2\n||\n4\n<=\ni\n<\n6\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n4\nFor\nai\nwhere\ni\n<\n4\n(\nthreadID_in_group\n*\n4\n)\n+\n16\nfor\nai\nwhere\ni\n>=\n4\nlastcol\n=\nfirstcol\n+\n3\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters, each\ncontaining two\n.f16\n/\n.bf16\nelements from matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 121\n.\nFigure 121\nSparse MMA .m16n8k32 fragment layout for matrix B with\n.f16\n/\n.bf16\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\nfor\n.f16\n/\n.b16\nformats.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero element from a 4-wide chunk of matrix A as shown in\nFigure 122\n.\nFigure 122\nSparse MMA .m16n8k32 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.14.6.2.3.\nMatrix Fragments for sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type\n\nA warp executing sparse\nmma.m16n8k16\nwith\n.tf32\nfloating point type will compute an MMA\noperation of shape\n.m16n8k16\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.tf32\nA vector expression containing four\n.b32\nregisters, with each\nregister containing one non-zero\n.tf32\nelement out of 2\nconsecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 123\n.\nFigure 123\nSparse MMA .m16n8k16 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\nand\na2\ngroupID\n+\n8\nfor\na1\nand\na3\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n2\nfor\na0\nand\na1\n(\nthreadID_in_group\n*\n2\n)\n+\n8\nfor\na2\nand\na3\nlastcol\n=\nfirstcol\n+\n1\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters, each\ncontaining four\n.tf32\nelements from matrix B.\nb0, b1, b2, b3\nThe layout of the fragments held by different threads is shown in\nFigure 124\n.\nFigure 124\nSparse MMA .m16n8k16 fragment layout for matrix B with\n.tf32\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with floating point type\n.\nMetadata: A\n.b32\nregister containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in\nFigure 125\n.\nFigure 125\nSparse MMA .m16n8k16 metadata layout for\n.tf32\ntype.\n\n9.7.14.6.2.4.\nMatrix Fragments for sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type\n\nA warp executing sparse\nmma.m16n8k8\nwith\n.tf32\nfloating point type will compute an MMA\noperation of shape\n.m16n8k8\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.tf32\nA vector expression containing two\n.b32\nregisters, each\ncontaining one non-zero\n.tf32\nelement out of 2\nconsecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 126\n.\nFigure 126\nSparse MMA .m16n8k8 fragment layout for matrix A with\n.tf32\ntype.\n\nThe row and column of a matrix fragment can be computed as:\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\na0\ngroupID\n+\n8\nfor\na1\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n2\nlastcol\n=\nfirstcol\n+\n1\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k8\nfor\n.tf32\nformat.\nMetadata: A\n.b32\nregister containing 8 4-bit vectors each storing the index of a non-zero\nelement of a 2-wide chunk of matrix A as shown in\nFigure 127\n.\nFigure 127\nSparse MMA .m16n8k8 metadata layout for\n.tf32\ntype.\n\n9.7.14.6.2.5.\nMatrix Fragments for sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type\n\nA warp executing sparse\nmma.m16n8k32\nwith\n.u8\n/\n.s8\ninteger type will compute an MMA\noperation of shape\n.m16n8k32\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u8\n/\n.s8\nA vector expression containing two\n.b32\nregisters, with each\nregister containing four non-zero\n.u8\n/\n.s8\nelements out\nof 8 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 128\n.\nFigure 128\nSparse MMA .m16n8k32 fragment layout for matrix A with\n.u8\n/\n.s8\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n8\nlastcol\n=\nfirstcol\n+\n7\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k32\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in\nFigure 129\n.\nFigure 129\nSparse MMA .m16n8k32 metadata layout for\n.u8\n/\n.s8\ntype.\n\n9.7.14.6.2.6.\nMatrix Fragments for sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\ntype\n\nA warp executing sparse\nmma.m16n8k64\nwith\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype will compute an MMA operation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u8\n/\n.s8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four non-zero\n.u8\n/\n.s8\nelements out\nof 8 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four non-zero\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements out of 8 consecutive\nelements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 130\nand\nFigure 131\n.\nFigure 130\nSparse MMA .m16n8k64 fragment layout for columns 0–31 of matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 131\nSparse MMA .m16n8k64 fragment layout for columns 32–63 of matrix A with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n4\n||\n8\n<=\ni\n<\n12\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n8\nFor\nai\nwhere\ni\n<\n8\n(\nthreadID_in_group\n*\n8\n)\n+\n32\nFor\nai\nwhere\ni\n>=\n8\nlastcol\n=\nfirstcol\n+\n7\nMultiplicand B:\n.btype\nFragment\nElements (low to high)\n.u8\n/\n.s8\nA vector expression containing four\n.b32\nregisters,\neach containing four\n.u8\n/\n.s8\nelements from\nmatrix B.\nb0, b1, b2, b3, …, b15\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nA vector expression containing four\n.b32\nregisters,\neach containing four\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\nelements from matrix B.\nThe layout of the fragments held by different threads is shown in\nFigure 132\n,\nFigure 133\n,\nFigure 134\nand\nFigure 135\n.\nFigure 132\nSparse MMA .m16n8k64 fragment layout for rows 0–15 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 133\nSparse MMA .m16n8k64 fragment layout for rows 16–31 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 134\nSparse MMA .m16n8k64 fragment layout for rows 32–47 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 135\nSparse MMA .m16n8k64 fragment layout for rows 48–63 of matrix B with\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k16 with integer type\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of two non-zero elements from a 4-wide chunk of matrix A as shown in\nFigure 136\nand\nFigure 137\n.\nFigure 136\nSparse MMA .m16n8k64 metadata layout for columns 0–31 for\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\nFigure 137\nSparse MMA .m16n8k64 metadata layout for columns 32–63 for\n.u8\n/\n.s8\n/\n.e4m3\n/\n.e5m2\n/\n.e3m2\n/\n.e2m3\n/\n.e2m1\ntype.\n\n9.7.14.6.2.7.\nMatrix Fragments for sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type\n\nA warp executing sparse\nmma.m16n8k64\nwith\n.u4\n/\n.s4\ninteger type will compute an MMA\noperation of shape\n.m16n8k64\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u4\n/\n.s4\nA vector expression containing two\n.b32\nregisters, with each\nregister containing eight non-zero\n.u4\n/\n.s4\nelements\nout of 16 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\nThe layout of the fragments held by different threads is shown in\nFigure 138\n.\nFigure 138\nSparse MMA .m16n8k64 fragment layout for matrix A with\n.u4\n/\n.s4\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n16\nlastcol\n=\nfirstcol\n+\n15\nMatrix fragments for multiplicand B and accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k64\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in\nFigure 139\n.\nFigure 139\nSparse MMA .m16n8k64 metadata layout for\n.u4\n/\n.s4\ntype.\n\n9.7.14.6.2.8.\nMatrix Fragments for sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\ninteger type\n\nA warp executing sparse\nmma.m16n8k128\nwith\n.u4\n/\n.s4\n/\n.e2m1\ninteger type will compute an MMA\noperation of shape\n.m16n8k128\n.\nElements of the matrix are distributed across the threads in a warp so each thread of the warp holds\na fragment of the matrix.\nMultiplicand A:\n.atype\nFragment\nElements\n.u4\n/\n.s4\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight non-zero\n.u4\n/\n.s4\nelements out\nof 16 consecutive elements from matrix A.\nMapping of the non-zero elements is\nas described in\nSparse matrix storage\n.\n.e2m1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing eight non-zero\n.e2m1\nelements out\nof 16 consecutive elements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 140\nand\nFigure 141\n.\nFigure 140\nSparse MMA .m16n8k128 fragment layout for columns 0–63 of matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 141\nSparse MMA .m16n8k128 fragment layout for columns 64–127 of matrix A with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\ngroupID\n=\n%\nlaneid\n>>\n2\nthreadID_in_group\n=\n%\nlaneid\n%\n4\nrow\n=\ngroupID\nfor\nai\nwhere\n0\n<=\ni\n<\n8\n||\n16\n<=\ni\n<\n24\ngroupID\n+\n8\nOtherwise\ncol\n=\n[\nfirstcol\n...\nlastcol\n]\n// As per the mapping of non-zero elements\n// as described in Sparse matrix storage\nWhere\nfirstcol\n=\nthreadID_in_group\n*\n16\nFor\nai\nwhere\ni\n<\n16\n(\nthreadID_in_group\n*\n16\n)\n+\n64\nFor\nai\nwhere\ni\n>=\n16\nlastcol\n=\nfirstcol\n+\n15\nMultiplicand B:\n.atype\nFragment\nElements (low to high)\n.u4\n/\n.s4\nA vector expression containing four\n.b32\nregisters, each containing\neight\n.u4\n/\n.s4\nelements from matrix B.\nb0, b1, b2, b3, …, b31\n.e2m1\nA vector expression containing four\n.b32\nregisters, each containing\neight\n.e2m1\nelements from matrix B.\nThe layout of the fragments held by different threads is shown in\nFigure 142\n,\nFigure 143\n,\nFigure 144\n,\nFigure 145\n.\nFigure 142\nSparse MMA .m16n8k128 fragment layout for rows 0–31 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 143\nSparse MMA .m16n8k128 fragment layout for rows 32–63 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 144\nSparse MMA .m16n8k128 fragment layout for rows 64–95 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 145\nSparse MMA .m16n8k128 fragment layout for rows 96–127 of matrix B with\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nMatrix fragments for accumulators C and D are the same as in case of\nMatrix Fragments for mma.m16n8k64\n.\nMetadata: A\n.b32\nregister containing 16 2-bit vectors with each pair of 2-bit vectors storing\nthe indices of four non-zero elements from a 8-wide chunk of matrix A as shown in\nFigure 146\nand\nFigure 147\n.\nFigure 146\nSparse MMA .m16n8k128 metadata layout for  columns 0–63 for\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\nFigure 147\nSparse MMA .m16n8k128 metadata layout for  columns 64–127 for\n.u4\n/\n.s4\n/\n.e2m1\ntype.\n\n9.7.14.6.3.\nMultiply-and-Accumulate Instruction:\nmma.sp\n/\nmma.sp::ordered_metadata\n\nmma.sp\n,\nmma.sp::ordered_metadata\nPerform matrix multiply-and-accumulate operation with sparse matrix A\nSyntax\nHalf precision floating point type:\nmma.spvariant.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;\n\n.ctype     = {.f16, .f32};\n.dtype     = {.f16, .f32};\n.spvariant = {.sp, .sp::ordered_metadata};\nAlternate floating point type:\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32      d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k16.row.col.f32.tf32.tf32.f32     d, a, b, c, e, f;\nmma.spvariant.sync.aligned.m16n8k64.row.col.f32.f8type.f8type.f32 d, a, b, c, e, f;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind.dtype.f8f6f4type.f8f6f4type.ctype d, a, b, c, e, f;\n\n.f8type     = {.e4m3, .e5m2};\n.spvariant  = {.sp, .sp::ordered_metadata};\n.f8f6f4type = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.kind       = {kind::f8f6f4};\n.ctype      = {.f16, .f32};\n.dtype      = {.f16, .f32};\nAlternate floating point type with block scaling:\nmma.spvariant.sync.aligned.m16n8k128.row.col.kind.block_scale{.scale_vec_size}.f32.e2m1.e2m1.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf4};\n.scale_vec_size = {.scale_vec::2X};\n.stype          = {.ue8m0};\n\nmma.spvariant.sync.aligned.m16n8k128.row.col.kind.block_scale.scale_vec_size.f32.e2m1.e2m1.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf4nvf4};\n.scale_vec_size = {.scale_vec::2X, .scale_vec::4X};\n.stype          = {.ue8m0, .ue4m3};\n\nmma.spvariant.sync.aligned.m16n8k64.row.col.kind.block_scale{.scale_vec_size}.f32.f8f6f4type.f8f6f4type.f32.stype d, a, b, c, e, f, scale-a-data, {byte-id-a, thread-id-a}, scale-b-data, {byte-id-b, thread-id-b};\n\n.spvariant      = {.sp::ordered_metadata};\n.kind           = {.kind::mxf8f6f4};\n.scale_vec_size = {.scale_vec::1X};\n.f8f6f4type     = {.e4m3, .e5m2, .e3m2, .e2m3, .e2m1};\n.stype          = {.ue8m0};\nInteger type:\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k32, .m16n8k64}\n.atype     = {.u8, .s8};\n.btype     = {.u8, .s8};\n.spvariant = {.sp, .sp::ordered_metadata};\n\nmma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;\n\n.shape     = {.m16n8k64, .m16n8k128}\n.atype     = {.u4, .s4};\n.btype     = {.u4, .s4};\n.spvariant = {.sp, .sp::ordered_metadata};\nDescription\nPerform a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+C\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the C and D matrices are\nMxN\n.\nA warp executing\nmma.sp.sync/mma.sp::ordered_metadata.sync\ninstruction compute a single matrix\nmultiply and accumulate operation.\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply and accumulate operation\nas specified in the section\nBlock Scaling\n. The data type corresponding\nto each of the element within\nscale_A\nand\nscale_B\nmatrices is specified by\n.stype\n.\nQualifier\n.scale_vec_size\nspecifies the number of columns of\nscale_A\nmatrix and number of\nrows in the matrix\nscale_B\n.\nThe valid combinations of\n.kind\n,\n.stype\nand\n.scale_vec_size\nare described in\nTable 36\n. For\nmma\nwith\n.kind::mxf4\nwhen the\nqualifier\n.scale_vec_size\nis not specified, then it defaults to\n2X\n. In contrast,\nwhen\n.kind\nis specified as\n.kind::mxf8f6f4\nthen the qualifier\n.scale_vec_size\ndefaults to\n1X\n. However, for\n.kind::mxf4nvf4\n, it is mandatory to provide valid\n.scale_vec_size\n.\nOperands\na\nand\nb\nrepresent two multiplicand matrices A and B, while\nc\nand\nd\nrepresent the accumulator and destination matrices, distributed across the threads in warp. Matrix A\nis structured sparse as described in\nSparse matrix storage\nOperands\ne\nand\nf\nrepresent sparsity\nmetadata and sparsity selector respectively. Operand\ne\nis a 32-bit integer and operand\nf\nis\na 32-bit integer constant with values in the range 0..3.\nWhen\n.block_scale\nqualifier is specified, operand\nscale-a-data\n,\nscale-b-data\nrepresents\nthe scale matrix metadata corresponding to\nscale_A\nand\nscale_B\nmatrices respectively.\nThe tuple\n{byte-id-a,\nthread-id-a}\nand\n{byte-id-b,\nthread-id-b}\nrepresent selectors for\nmatrices\nscale_A\nand\nscale_B\nrespectively from their corresponding metadata arguments\nscale-a-data\n,\nscale-b-data\n. The operands\nscale-a-data\n,\nscale-b-data\nare of type\n.b32\n. The operands\nbyte-id-a\n,\nthread-id-a\n,\nbyte-id-b\n,\nthread-id-b\nare unsigned\n16-bit integer values. For more details on selector arguments refer\nBlock Scaling for mma.sync\nsection.\nInstruction\nmma.sp::ordered_metadata\nrequires the indices in the sparsity metadata to be sorted\nin an increasing order starting from LSB, otherwise behavior is undefined.\nThe registers in each thread hold a fragment of matrix as described in\nMatrix fragments for multiply-accumulate operation with sparse matrix A\n.\nThe qualifiers\n.dtype\n,\n.atype\n,\n.btype\nand\n.ctype\nindicate the data-type of the\nelements in the matrices D, A, B and C respectively. The qualifier\n.stype\nindicate the\ndata-type of the elements in the matrices\nscale_A\nand\nscale_B\n. In case of shapes\n.m16n8k16\n,\n.m16n8k32\nand\n.m16n8k64\n,\n.dtype\nmust be the same as\n.ctype\n.\nWhen\n.kind\nis either of\n.kind::mxf8f6f4\nor\n.kind::f8f6f4\n, the individual 4-bit and\nthe 6-bit floating point type elements must be packed in an 8-bit container. The matrix element\nof type\n.e2m1\nresides in central 4 bits of the 8-bit container with padding in the upper 2\nbits and lower 2 bits of the container. When the matrix element is of type\n.e3m2\nor\n.e2m3\n,\nthe matrix element resides in the lower 6 bits of the 8-bit container with padding in the upper\n2 bits of the container. In contrast, note that when using\nmma\nwith\n.kind::mxf4\nor\n.kind::mxf4nvf4\n, no explicit padding is necessary even though matrix elements are of type\n.e2m1\n.\nPrecision and rounding :\n.f16\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with at least single\nprecision. When\n.ctype\nor\n.dtype\nis\n.f32\n, accumulation of the intermediate values\nis performed with at least single precision. When both\n.ctype\nand\n.dtype\nare specified\nas\n.f16\n, the accumulation is performed with at least half precision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.e4m3\n,\n.e5m2\n,\n.e3m2\n,\n.e2m3\n,\n.e2m1\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified precision. Accumulation\nof the intermediate values is performed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations :\nElement-wise multiplication of matrix A and B is performed with specified\nprecision. Accumulation of the intermediate values is performed with at least single\nprecision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations :\nThe integer\nmma.sp/mma.sp::ordered_metadata\noperation is performed with\n.s32\naccumulators.\nThe\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the range\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed 32-bit\ninteger and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nmma.sp/mma.sp::ordered_metadata\ninstruction causes\nthe executing thread to wait until all threads in the warp execute the same\nmma.sp/mma.sp::ordered_metadata\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the same\nmma.sp/mma.sp::ordered_metadata\ninstruction. In conditionally executed code, a\nmma.sp/mma.sp::ordered_metadata\ninstruction should only be used if it is known that all threads in the warp evaluate the condition identically,\notherwise behavior is undefined.\nThe behavior of\nmma.sp/mma.sp::ordered_metadata\ninstruction is undefined if all threads in the same warp\ndo not use the same qualifiers, or if any thread in the warp has exited.\nNotes\nmma.sp\ninstruction may have substantially reduced performance on some target architectures.\nHence, it is advised to use\nmma.sp::ordered_metadata\ninstruction.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.1.\nSupport for\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation introduced in\nPTX ISA version 8.4.\nmma.sp::ordered_metadata\nintroduced in PTX ISA version 8.5.\nSupport for shape\n.m16n8k32\nand\n.f16\ndtype/ctype with\n.e4m3\n/\n.e5m2\nalternate floating\npoint type\nmma\noperation introduced in PTX ISA version 8.7.\nSupport for\n.e3m2\n,\n.e2m3\n,\n.e2m1\nalternate floating point type\nmma\noperation introduced\nin PTX ISA version 8.7.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier introduced in PTX ISA version 8.7.\nSupport for\n.scale_vec::4X\non\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nis introduced in\nPTX ISA version 9.1\nTarget ISA Notes\nRequires\nsm_80\nor higher.\n.e4m3\nand\n.e5m2\nalternate floating point type\nmma\noperation requires\nsm_89\nor higher.\nmma.sp::ordered_metadata\nrequires\nsm_80\nor higher.\nSupport for shape\n.m16n8k32\nand\n.f16\ndtype/ctype with\n.e4m3\n/\n.e5m2\nalternate floating\npoint type\nmma\noperation requires\nsm_120\n.\n.e3m2\n,\n.e2m3\nand\n.e2m1\nalternate floating point type\nmma\noperation requires\nsm_120a\nand are supported on\nsm_120f\nor higher in the same family from PTX ISA version 8.8.\nSupport for\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nqualifier requires\nsm_120a\nand are\nsupported on\nsm_120f\nand later generation targets in the same family from PTX ISA version 8.8 except for\n.kind::mxf4nvf4\n/\n.kind::mxf4\n.\nQualifiers\n.kind::mxf4nvf4\nand\n.kind::mxf4\nare supported on following architectures:\nsm_120a\nsm_121a\nExamples of half precision floating point type\n// f16 elements in C and D matrix\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\n\n.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>\n.reg .b32 %Re;\n\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1}, %Re, 0x1;\nExamples of alternate floating point type\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp.sync.aligned.m16n8k64.row.col.f32.e5m2.e4m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<2>, %Rb<2>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::f8f6f4.f32.e3m2.e2m3.f32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .b32 %Rc<4>, %Rd<4>;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::f8f6f4.f16.e2m3.e2m1.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1}, %Re, 0;\nExamples of integer type\n.reg .b32 %Ra<4>, %Rb<4>, %Rc<4>, %Rd<4>;\n.reg .u32 %Re;\n\n// u8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k32.row.col.satfinite.s32.u8.u8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// s8 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// s8 elements in A and B matrix with ordered metadata\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k64.row.col.s32.s4.s4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1},\n  {%Rb0, %Rb1},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;\n\n// u4 elements in A and B matrix\nmma.sp.sync.aligned.m16n8k128.row.col.satfinite.s32.u4.u4.s32\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;\nExamples of mma with block scale\n.reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .b32 %Re;\n mma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4.block_scale.f32.e2m1.e2m1.f32.ue8m0\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1, %Rb2, %Rb3},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   %Re, 0,\n   scaleAData, {2, 1}, scaleBData, {2, 3};\n\n .reg .b32 %Ra<4>, %Rb<4>;\n .reg .f32 %Rc<4>, %Rd<4>;\n .reg .b32 scaleAData, scaleBData;\n .reg .u16 bidA, bidB, tidA, tidB;\n .reg .b32 %Re;\n mma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue4m3\n   {%Rd0, %Rd1, %Rd2, %Rd3},\n   {%Ra0, %Ra1, %Ra2, %Ra3},\n   {%Rb0, %Rb1, %Rb2, %Rb3},\n   {%Rc0, %Rc1, %Rc2, %Rc3},\n   %Re, 0,\n   scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .u16 bidA, bidB, tidA, tidB;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k128.row.col.kind::mxf4nvf4.block_scale.scale_vec::4X.f32.e2m1.e2m1.f32.ue8m0\n {%Rd0, %Rd1, %Rd2, %Rd3},\n {%Ra0, %Ra1, %Ra2, %Ra3},\n {%Rb0, %Rb1, %Rb2, %Rb3},\n {%Rc0, %Rc1, %Rc2, %Rc3},\n %Re, 0,\n scaleAData, {bidA, tidA}, scaleBData, {bidB, tidB};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e3m2.e2m1.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2, %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  %Re, 0,\n  scaleAData, {0, 1}, scaleBData, {0, 1};\n\n.reg .b32 %Ra<4>, %Rb<4>;\n.reg .f32 %Rc<4>, %Rd<4>;\n.reg .b32 scaleAData, scaleBData;\n.reg .b32 %Re;\nmma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.kind::mxf8f6f4.block_scale.scale_vec::1X.f32.e4m3.e5m2.f32.ue8m0\n  {%Rd0, %Rd1, %Rd2, %Rd3},\n  {%Ra0, %Ra1, %Ra2,  %Ra3},\n  {%Rb0, %Rb1, %Rb2, %Rb3},\n  {%Rc0, %Rc1, %Rc2, %Rc3},\n  %Re, 0,\n  scaleAData, {0, 1}, scaleBData, {0, 0};\n9.7.15.\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions\n\nThe warpgroup level matrix multiply and accumulate operation has either of the following forms,\nwhere matrix\nD\nis called accumulator:\nD\n=\nA\n*\nB\n+\nD\nD\n=\nA\n*\nB\n, where the input from accumulator D is disabled.\nThe\nwgmma\ninstructions perform warpgroup level matrix multiply-and-accumulate operation by\nhaving all threads in a warpgroup collectively perform the following actions:\nLoad matrices A, B and D into registers or into shared memory.\nPerform the following\nfence\noperations:\nwgmma.fence\noperations to indicate that the register/shared-memory across the warpgroup\nhave been written into.\nfence.proxy.async\noperation to make the generic proxy operations visible to the async\nproxy.\nIssue the asynchronous matrix multiply and accumulate operations using the\nwgmma.mma_async\noperation on the input matrices. The\nwgmma.mma_async\noperation is performed in the async\nproxy.\nCreate a wgmma-group and commit all the prior outstanding\nwgmma.mma_async\noperations into the\ngroup, by using\nwgmma.commit_group\noperation.\nWait for the completion of the required wgmma-group.\nOnce the wgmma-group completes, all the\nwgmma.mma_async\noperations have been performed and\ncompleted.\n9.7.15.1.\nWarpgroup\n\nA warpgroup is a set of four contiguous warps such that the\nwarp-rank\nof the first warp is a\nmultiple of 4.\nwarp-rank of a warp is defined as:\n(%tid.x + %tid.y * %ntid.x  + %tid.z * %ntid.x * %ntid.y) / 32\n9.7.15.2.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand\nmatrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\n, where A is an\nMxK\nmatrix, B is a\nKxN\nmatrix, while D is a\nMxN\nmatrix.\nThe following matrix shapes are supported for the specified types for the\nwgmma.mma_async\noperation:\nMultiplicand Data type\nSparsity\nShape\nFloating-point -\n.f16\nDense\n.m64n8k16\n,\n.m64n16k16\n,\n.m64n24k16\n,\n.m64n32k16\n,\n.m64n40k16\n,\n.m64n48k16\n,\n.m64n56k16\n,\n.m64n64k16\n,\n.m64n72k16\n,\n.m64n80k16\n,\n.m64n88k16\n,\n.m64n96k16\n,\n.m64n104k16\n,\n.m64n112k16\n,\n.m64n120k16\n,\n.m64n128k16\n,\n.m64n136k16\n,\n.m64n144k16\n,\n.m64n152k16\n,\n.m64n160k16\n,\n.m64n168k16\n,\n.m64n176k16\n,\n.m64n184k16\n,\n.m64n192k16\n,\n.m64n200k16\n,\n.m64n208k16\n,\n.m64n216k16\n,\n.m64n224k16\n,\n.m64n232k16\n,\n.m64n240k16\n,\n.m64n248k16\n,\n.m64n256k16\nAlternate floating-point\nformat -\n.bf16\nAlternate floating-point\nformat -\n.tf32\nSparse\nAlternate floating-point\nformat -\n.tf32\nDense\n.m64n8k8\n,\n.m64n16k8\n,\n.m64n24k8\n,\n.m64n32k8\n,\n.m64n40k8\n,\n.m64n48k8\n,\n.m64n56k8\n,\n.m64n64k8\n,\n.m64n72k8\n,\n.m64n80k8\n,\n.m64n88k8\n,\n.m64n96k8\n,\n.m64n104k8\n,\n.m64n112k8\n,\n.m64n120k8\n,\n.m64n128k8\n,\n.m64n136k8\n,\n.m64n144k8\n,\n.m64n152k8\n,\n.m64n160k8\n,\n.m64n168k8\n,\n.m64n176k8\n,\n.m64n184k8\n,\n.m64n192k8\n,\n.m64n200k8\n,\n.m64n208k8\n,\n.m64n216k8\n,\n.m64n224k8\n,\n.m64n232k8\n,\n.m64n240k8\n,\n.m64n248k8\n,\n.m64n256k8\nAlternate floating-point\nformat -\n.e4m3\n/\n.e5m2\nDense\n.m64n8k32\n,\n.m64n16k32\n,\n.m64n24k32\n,\n.m64n32k32\n,\n.m64n40k32\n,\n.m64n48k32\n,\n.m64n56k32\n,\n.m64n64k32\n,\n.m64n72k32\n,\n.m64n80k32\n,\n.m64n88k32\n,\n.m64n96k32\n,\n.m64n104k32\n,\n.m64n112k32\n,\n.m64n120k32\n,\n.m64n128k32\n,\n.m64n136k32\n,\n.m64n144k32\n,\n.m64n152k32\n,\n.m64n160k32\n,\n.m64n168k32\n,\n.m64n176k32\n,\n.m64n184k32\n,\n.m64n192k32\n,\n.m64n200k32\n,\n.m64n208k32\n,\n.m64n216k32\n,\n.m64n224k32\n,\n.m64n232k32\n,\n.m64n240k32\n,\n.m64n248k32\n,\n.m64n256k32\nFloating point -\n.f16\nSparse\nAltername floating-point\nformat -\n.bf16\nInteger -\n.u8\n/\n.s8\nDense\n.m64n8k32\n,\n.m64n16k32\n,\n.m64n24k32\n,\n.m64n32k32\n,\n.m64n48k32\n,\n.m64n64k32\n,\n.m64n80k32\n,\n.m64n96k32\n,\n.m64n112k32\n,\n.m64n128k32\n,\n.m64n144k32\n,\n.m64n160k32\n,\n.m64n176k32\n,\n.m64n192k32\n,\n.m64n208k32\n,\n.m64n224k32\n,\n.m64n240k32\n,\n.m64n256k32\nAlternate floating-point\nformat -\n.e4m3\n/\n.e5m2\nSparse\n.m64n8k64\n,\n.m64n16k64\n,\n.m64n24k64\n,\n.m64n32k64\n,\n.m64n40k64\n,\n.m64n48k64\n,\n.m64n56k64\n,\n.m64n64k64\n,\n.m64n72k64\n,\n.m64n80k64\n,\n.m64n88k64\n,\n.m64n96k64\n,\n.m64n104k64\n,\n.m64n112k64\n,\n.m64n120k64\n,\n.m64n128k64\n,\n.m64n136k64\n,\n.m64n144k64\n,\n.m64n152k64\n,\n.m64n160k64\n,\n.m64n168k64\n,\n.m64n176k64\n,\n.m64n184k64\n,\n.m64n192k64\n,\n.m64n200k64\n,\n.m64n208k64\n,\n.m64n216k64\n,\n.m64n224k64\n,\n.m64n232k64\n,\n.m64n240k64\n,\n.m64n248k64\n,\n.m64n256k64\nInteger -\n.u8\n/\n.s8\nSparse\n.m64n8k64\n,\n.m64n16k64\n,\n.m64n24k64\n,\n.m64n32k64\n,\n.m64n48k64\n,\n.m64n64k64\n,\n.m64n80k64\n,\n.m64n96k64\n,\n.m64n112k64\n,\n.m64n128k64\n,\n.m64n144k64\n,\n.m64n160k64\n,\n.m64n176k64\n,\n.m64n192k64\n,\n.m64n208k64\n,\n.m64n224k64\n,\n.m64n240k64\n,\n.m64n256k64\nSingle-bit -\n.b1\nDense\n.m64n8k256\n,\n.m64n16k256\n,\n.m64n24k256\n,\n.m64n32k256\n,\n.m64n48k256\n,\n.m64n64k256\n,\n.m64n80k256\n,\n.m64n96k256\n,\n.m64n112k256\n,\n.m64n128k256\n,\n.m64n144k256\n,\n.m64n160k256\n,\n.m64n176k256\n,\n.m64n192k256\n,\n.m64n208k256\n,\n.m64n224k256\n,\n.m64n240k256\n,\n.m64n256k256\n9.7.15.3.\nMatrix Data-types\n\nThe matrix multiply and accumulate operation is supported separately on integer, floating-point,\nsub-byte integer and single bit data-types. All operands must contain the same basic type kind,\ni.e., integer or floating-point.\nFor floating-point matrix multiply and accumulate operation, different matrix operands may have\ndifferent precision, as described later.\nFor integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have\nelements of the same data-type, e.g. both signed integer or both unsigned integer.\nData-type\nMultiplicands (A or B)\nAccumulator (D)\nInteger\nboth\n.u8\nor both\n.s8\n.s32\nFloating Point\n.f16\n.f16\n,\n.f32\nAlternate floating Point\n.bf16\n.f32\nAlternate floating Point\n.tf32\n.f32\nAlternate floating Point\n.e4m3\n,\n.e5m2\n.f16\n,\n.f32\nSingle-bit integer\n.b1\n.s32\n9.7.15.4.\nAsync Proxy\n\nThe\nwgmma.mma_async\noperations are performed in the asynchronous proxy (or async proxy).\nAccessing the same memory location across multiple proxies needs a cross-proxy fence. For the async\nproxy,\nfence.proxy.async\nshould be used to synchronize memory between generic proxy and the\nasync proxy.\nThe completion of a\nwgmma.mma_async\noperation is followed by an implicit generic-async proxy\nfence. So the result of the asynchronous operation is made visible to the generic proxy as soon as\nits completion is observed.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used\nto wait for the completion of the\nwgmma.mma_async\ninstructions.\n9.7.15.5.\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using\nwgmma.mma_async\ninstruction\n\nThis section describes warpgroup level\nwgmma.mma_async\ninstruction and the organization of\nvarious matrices involved in this instruction.\n9.7.15.5.1.\nRegister Fragments and Shared Memory Matrix Layouts\n\nThe input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared\nmemory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This\nsection describes the layouts of register fragments and shared memory expected by the warpgroup MMA\ninstructions.\nWhen the matrices are in shared memory, their starting addresses must be aligned to 16 bytes.\n9.7.15.5.1.1.\nRegister Fragments\n\nThis section describes the organization of various matrices located in register operands of the\nwgmma.mma_async\ninstruction.\n9.7.15.5.1.1.1.\nMatrix Fragments for\nwgmma.mma_async.m64nNk16\n\nA warpgroup executing\nwgmma.mma_async.m64nNk16\nwill compute an MMA operation of shape\n.m64nNk16\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.f16\n/\n.bf16\nA vector expression containing four\n.f16x2\nregisters, with each\nregister containing two\n.f16\n/\n.bf16\nelements from matrix A.\na0, a1, a2, a3, a4, a5, a6, a7\nThe layout of the fragments held by different threads is shown in\nFigure 148\n.\nFigure 148\nWGMMA .m64nNk16 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.f16\nA vector expression containing N/4 number of\n.f16x2\nregisters, with each register containing two\n.f16\nelements from matrix D.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\n.f32\nA vector expression containing N/2 number of\n.f32\nregisters.\nThe layout of the fragments held by different threads is shown in\nFigure 149\n.\nFigure 149\nWGMMA .m64nNk16 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.2.\nMatrix Fragments for\nwgmma.mma_async.m64nNk8\n\nA warpgroup executing\nwgmma.mma_async.m64nNk8\nwill compute an MMA operation of shape\n.m64nNk8\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.tf32\nA vector expression containing four\n.b32\nregisters containing\nfour\n.tf32\nelements from matrix A.\na0, a1, a2, a3\nThe layout of the fragments held by different threads is shown in\nFigure 150\n.\nFigure 150\nWGMMA .m64nNk8 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.f32\nA vector expression containing N/2 number of\n.f32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\nThe layout of the fragments held by different threads is shown in\nFigure 151\n.\nFigure 151\nWGMMA .m64nNk8 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.3.\nMatrix Fragments for\nwgmma.mma_async.m64nNk32\n\nA warpgroup executing\nwgmma.mma_async.m64nNk32\nwill compute an MMA operation of shape\n.m64nNk32\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.u8\n/\n.s8\nelements from matrix A.\na0, a1, a2, a3, … , a14, a15\n.e4m3\n/\n.e5m2\nA vector expression containing four\n.b32\nregisters, with each\nregister containing four\n.e4m3\n/\n.e5m2\nelements from\nmatrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 152\n.\nFigure 152\nWGMMA .m64nNk32 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\nMiscellaneous Information\n.s32\nA vector expression containing\nN/2 number of\n.s32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\ndepends on .dtype, as\ndescribed in the next column.\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n3,\n4}\n=\n16*i\nwhere\ni\n=\n{3,\n4,\n...,\n15,\n16}\n.f32\nA vector expression containing\nN/2 number of\n.f32\nregisters.\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n...\n,\n32}\n.f16\nA vector expression containing\nN/4 number of\n.f16x2\nregisters, with each register\ncontaining two\n.f16\nelements from matrix D.\nThe layout of the fragments held by different threads is shown in\nFigure 153\n.\nFigure 153\nWGMMA .m64nNk32 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.1.4.\nMatrix Fragments for\nwgmma.mma_async.m64nNk256\n\nA warpgroup executing\nwgmma.mma_async.m64nNk256\nwill compute an MMA operation of shape\n.m64nNk256\nwhere N is a valid\nn\ndimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A in registers:\n.atype\nFragment\nElements (low to high)\n.b1\nA vector expression containing four\n.b32\nregisters, with each\nregister containing thirty two\n.b1\nelement from matrix A.\na0, a1, a2, …, a127\nThe layout of the fragments held by different threads is shown in\nFigure 154\n.\nFigure 154\nWGMMA .m64nNk256 register fragment layout for matrix A.\n\nAccumulator D:\n.dtype\nFragment\nElements (low to high)\n.s32\nA vector expression containing N/2 number of\n.s32\nregisters.\nd0, d1, d2, d3, …, dX, dY, dZ, dW\nwhere\nX\n=\nN/2\n-\n4\nY\n=\nN/2\n-\n3\nZ\n=\nN/2\n-\n2\nW\n=\nN/2\n-\n1\nN\n=\n8*i\nwhere\ni\n=\n{1,\n2,\n3,\n4}\n=\n16*i\nwhere\ni\n=\n{3,\n4,\n...,\n15,\n16}\nThe layout of the fragments held by different threads is shown in\nFigure 155\n.\nFigure 155\nWGMMA .m64nNk256 register fragment layout for accumulator matrix D.\n\n9.7.15.5.1.2.\nShared Memory Matrix Layout\n\nIf the argument\nimm-trans-a\n/\nimm-trans-b\nof the instruction\nwgmma.mma_async{.sp}\nis 0, then\nK-major\nis used for matrix\nA\n/\nB\nrespectively. If the value of argument\nimm-trans-a\nis 1 then\nM-major\nis used for matrix\nA\n. If the value of the argument\nimm-trans-b\nis 1, then\nN-major\nis used for matrix\nB\n.\nIn a column-major default BLAS library such as cuBLAS, the matrices\nA\nand\nB\nwith and\nwithout transpose can be classified as either\nK-Major\nor\nM-or-N-Major\nas shown in the\nfollowing table:\nNon-Transposed\nTransposed\nA\nK-major\nM-major\nB\nK-major\nN-major\nTo avoid confusion with\nA\n,\nB\n,\nrow-major\n,\ncol-major\n,\ntranspose\n, and\nnon-transpose\n, we will use\nMN-Major\nand\nK-Major\nthroughout this section.\nThe matrices in the shared memory are made up of one or more “swizzle layout atom”.\nThe exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,\nand the leading dimension. The layout of the swizzle are shown in\nTable 38\n.\nTable 38\nVarious combinations of swizzling mode, leading dimension and swizzle-atom layout\n\nSwizzling mode\nLeading Dimension\n/ Major-ness\nSwizzle atom layout\n(128b element)\n128B Swizzling Mode\nM/N\n8x8\nK\n8x8\n64B Swizzling Mode\nM/N\n4x8\nK\n8x4\n32B Swizzling Mode\nM/N\n2x8\nK\n8x2\nNone\nM/N\n1x8\nK\n8x1\nThe above shapes are for elements of size 128 bits. For smaller elements sizes, the same\nshapes would get multiplied along the leading dimension by a factor of\n128/sizeof_bits(Element)\n.\nFor example, 128B MN major swizzle atom would have a shape of\n(8*(128/32))x8\n=\n32x8\nfor\ntf32\ntensor core inputs.\nExamples\nThe following are some example layouts of\nMxK\nor\nKxN\nmatrices with various swizzling modes,\nand are in units of 128b elements as shown\nby each colored cell as shown in\nFigure 156\n,\nFigure 157\n,\nFigure 158\n,\nFigure 159\n,\nFigure 160\n,\nFigure 161\n,\nFigure 162\n,\nFigure 163\n.\nFigure 156\nMN major 128B swizzling\n\nFigure 157\nK major 128B swizzling\n\nFigure 158\nMN major 64B swizzling\n\nFigure 159\nK major 64B swizzling\n\nFigure 160\nMN major 32B swizzling\n\nFigure 161\nK major 32B swizzling\n\nFigure 162\nMN major interleaved\n\nFigure 163\nK major interleaved\n\nFollowing are some of the examples of the 128B swizzling layout for\ntf32\nelement type.\nK-Major:\nFigure 164\nFigure 164\nK major\n\nMN-Major:\nFigure 165\nFigure 165\nMN major\n\n9.7.15.5.1.2.1.\nMajor-ness supported by Strides\n\nThere are two strides involved while accessing a matrix from shared memory:\nLeading dimension byte offset\nStride dimension byte offset\n9.7.15.5.1.2.1.1.\nLeading Dimension Byte Offset\n\nThe leading dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The leading byte offset is defined as follows for matrices whose element types are\nnormalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nNo-Swizzling: the offset from the first column to the second columns\nof the 8x2 tile in the 128-bit element type normalized matrix.\nSwizzled layouts: not used, assumed to be 1.\nMN-Major\nInterleave: offset from the first 8 columns to the next 8 columns.\nSwizzled layouts: offset from the first (swizzle-byte-size/16) rows\nto the next (swizzle-byte-size/16) rows.\n9.7.15.5.1.2.1.2.\nStride Dimension Byte Offset\n\nThe stride dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The stride dimension byte offset is defined as follows for matrices whose element\ntypes are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nThe offset from the first 8 rows to the next 8 rows.\nMN-Major\nInterleave: offset from the first row to the next row.\nSwizzled layout: offset from the first 8 columns to the next 8\ncolumns\n9.7.15.5.1.2.1.3.\nCanonical Layouts\n\nIn terms of\nCuTe layouts\nthe canonical layout can be expressed as follows:\nMajor-\nness\nSwizzling mode\nCanonical Layout without swizzling\nSwizzling\non the previous column\nMN-\nmajor\nNo-swizzling or\nInterleaved\n((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nSwizzle<1, 4, 3>\n64B Swizzling\n((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nSwizzle<2, 4, 3>\n128B Swizzling\n((T,8,m),(8,k)):((1,T,LBO),(8T,SBO))\nSwizzle<3, 4, 3>\nK-\nmajor\nNo-swizzling or\nInterleaved\n((8,m),(T,2k)):((1T,SBO),(1,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((8,m),(T,2k)):((2T,SBO),(1,T))\nSwizzle<1, 4, 3>\n64B Swizzling\n((8,m),(T,2k)):((4T,SBO),(1,T))\nSwizzle<2, 4, 3>\n128B Swizzling\n((8,m),(T,2k)):((8T,SBO),(1,T))\nSwizzle<3, 4, 3>\nwhere\nT = 128 / sizeof-elements-in-bits\nT represents scale factor which normalizes matrix element types to 128-bits.\nm represents the number of repeating patterns across rows.\nk represents the number of repeating patterns across columns.\nExamples\nK-Major, no-swizzling and tf32 type:\nFigure 166\nFigure 166\nK major, no-swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))\nCanonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO\n64*sizeof(tf32)\nSBO\n32*sizeof(tf32)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nK-Major, 32B swizzling and tf32 type:\nFigure 167\nFigure 167\nK major, 32B swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))\nCanonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO\nNA\nSBO\n64*sizeof(tf32)\nEncoding of LBO in descriptor\n1 (assumed)\nEncoding of SBO in descriptor\n(SBO) >> 4 = 16\nMN-Major, no-swizzling and bf16 type:\nFigure 168\nFigure 168\nMN major, no-swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))\nCanonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n128*sizeof(bf16)\nSBO\n64*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nMN-Major, 32B swizzling and bf16 type:\nFigure 169\nFigure 169\nMN major, 32B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))\nCanonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n128*sizeof(bf16)\nSBO\n256*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 32\nMN-Major, 64B swizzling and bf16 type:\nFigure 170\nFigure 170\nMN major, 64B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))\nCanonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO\n256*sizeof(bf16)\nSBO\n512*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 32\nEncoding of SBO in descriptor\n(SBO) >> 4 = 64\n9.7.15.5.1.2.2.\nMatrix Descriptor Format\n\nMatrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in\nthe matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the\nfollowing layout:\nBit-field\nSize in bits\nDescription\n13–0\n14\nmatrix-descriptor-encode(Matrix start address)\n29–16\n14\nmatrix-descriptor-encode\n(\nLeading dimension byte offset\n)\n45–32\n14\nmatrix-descriptor-encode\n(\nStride dimension byte offset\n)\n51–49\n3\nMatrix base offset. This is valid for all swizzling modes except the no-swizzle mode.\n63–62\n2\nSpecifies the swizzling mode to be used:\n0: No swizzle\n1: 128-Byte swizzle\n2: 64-Byte swizzle\n3: 32-Byte swizzle\nwhere\nmatrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4\nThe value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as\nper the below table:\nSwizzling mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the base offset must be a non-zero value, computed using the following formula:\nbase offset = (pattern start addr >> 0x7) & 0x7\n9.7.15.5.2.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async\n\nwgmma.mma_async\nPerform matrix multiply-and-accumulate operation across warpgroup\nSyntax\nHalf precision floating point type:\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype   = {.f16, .f32};\nAlternate floating point type :\n.bf16 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8,\n            .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8,\n            .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8,\n            .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8,\n            .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,\n            .m64n168k8, .m64n176k8, .m64n184k8, .m64n192k8,\n            .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8,\n            .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32};\nInteger type:\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32,\n            .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32,\n            .m64n176k32, .m64n192k32, .m64n208k32, .m64n224k32};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8};\nSingle bit:\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a-desc, b-desc, scale-d;\n\nwgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a, b-desc, scale-d;\n\n.shape   = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256,\n            .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256,\n            .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256,\n            .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256,\n            .m64n240k256, .m64n256k256};\n.op  = {.and};\nDescription\nInstruction\nwgmma.mma_async\nissues a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the D matrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nscale-d\nis\nfalse.\nwgmma.fence\ninstruction must be used to fence the register accesses of\nwgmma.mma_async\ninstruction from their prior accesses. Otherwise, the behavior is undefined.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed.\nRegister operand\nd\nrepresents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand\na\nrepresents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands\na-desc\nand\nb-desc\nare the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the warps\nin the warpgroup. The format of the matrix descriptor is described in\nMatrix Descriptor Format\n.\nMatrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments\nimm-trans-a\nand\nimm-trans-b\nrespectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of\nimm-trans-a\nand\nimm-trans-b\nare 0\nand 1. The transpose operation is only supported for the\nwgmma.mma_async\nvariants with\n.f16\n/\n.bf16\ntypes on matrices accessed from shared memory using matrix descriptors.\nFor the floating point variants of the\nwgmma.mma_async\noperation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands\nimm-scale-a\nand\nimm-scale-b\nrespectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of\nimm-scale-a\nand\nimm-scale-b\nare -1 and 1.\nThe qualifiers\n.dtype\n,\n.atype\nand\n.btype\nindicate the data type of the elements in\nmatrices D, A and B respectively.\n.atype\nand\n.btype\nmust be the same for all floating point\nwgmma.mma_async\nvariants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the\nwgmma.mma_async\noperation are as follows:\nMatrices A and B have 8-bit data elements when\n.atype\n/\n.btype\nis\n.e4m3\n/\n.e5m2\n.\nMatrices A and B have 16-bit data elements when\n.atype\n/\n.btype\nis\n.bf16\n.\nMatrices A and B have 32-bit data elements when\n.atype\n/\n.btype\nis\n.tf32\n.\nPrecision and rounding:\nFloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with at least single\nprecision. When\n.dtype\nis\n.f16\n, the accumulation is performed with at least half\nprecision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified\nprecision.\nwgmma.mma_async\noperation involving type\n.tf32\nwill truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations:\nThe integer\nwgmma.mma_async\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the\nrange\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.mma_async\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.mma_async\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.mma_async\ninstruction. In conditionally executed code, a\nwgmma.mma_async\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nSupport for\n.u8.s8\nand\n.s8.u8\nas .atype.btype introduced in PTX ISA version 8.4.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples of half precision floating point type\n.reg .f16x2 f16a<40>, f16d<40>;\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n.reg .pred  scaleD;\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16\n  {f16d0, f16d1,  f16d2,  f16d3,  f16d4,  f16d5,  f16d6,  f16d7,  f16d8,\n   f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17},\n  descA,\n  descB,\n  scaleD, -1, 1, 1, 0;\nExamples of alternate floating point type\n.reg .f32   f32d<40>;\n.reg .b32   bf16a<40>\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9,\n   f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19,\n   f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29,\n   f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39,\n   f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49,\n   f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59},\n  {bf16a0, bf16a1, bf16a2, bf16a3},\n  descB,\n  scaleD, -1, -1, 0;\n\n.reg .f32   f32d<40>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32\n  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7},\n  descA,\n  descB,\n  0, -1, -1;\n\n.reg .b32 f16d<8>, f16a<8>;\n.reg .f32 f32d<8>;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2\n  {f16d0, f16d1},\n  descA,\n  descB,\n  scaleD, -1, 1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3\n  {f32d0, f32d1, f32d2, f32d3},\n  {f16a0, f16a1, f16a2, f16a3},\n  descB,\n  1, -1, -1;\nExamples of integer type\n.reg .s32 s32d<8>, s32a<8>;\n.reg .u32 u32a<8>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  1;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite\n  {s32d0, s32d1, s32d2, s32d3},\n  {s32a0, s32a1, s32a2, s32a3},\n  descB,\n  scaleD;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8\n  {s32d0, s32d1, s32d2, s32d3},\n  descA,\n  descB,\n  scaleD;\nExamples of single bit type\n.reg .s32 s32d<4>;\n.reg .b32 b32a<4>;\n.reg .pred scaleD;\n.reg .b64   descA, descB;\n\n\nwgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc\n  {s32d0, s32d1, s32d2, s32d3},\n  {b32a0, b32a1, b32a2, b32a3},\n  descB,\n  scaleD;\n9.7.15.6.\nAsynchronous Warpgroup Level Multiply-and-Accumulate Operation using\nwgmma.mma_async.sp\ninstruction\n\nThis section describes warp-level\nwgmma.mma_async.sp\ninstruction with sparse matrix A. This\nvariant of the\nwgmma.mma_async\noperation can be used when A is a structured sparse matrix with\n50% zeros in each row distributed in a shape-specific granularity. For an\nMxNxK\nsparse\nwgmma.mma_async.sp\noperation, the\nMxK\nmatrix A is packed into\nMxK/2\nelements. For each\nK-wide row of matrix A, 50% elements are zeros and the remaining\nK/2\nnon-zero elements are\npacked in the operand representing matrix A. The mapping of these\nK/2\nelements to the\ncorresponding K-wide row is provided explicitly as metadata.\n9.7.15.6.1.\nSparse matrix storage\n\nGranularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a\nsub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the\nsub-chunk is shape-specific. For example, in a\n64x32\nmatrix A used in floating point\nwgmma.mma_async\noperations, sparsity is expected to be at 2:4 granularity, i.e. each 4-element\nvector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each\nnon-zero element in a sub-chunk is stored in the metadata operand. Values\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nare invalid values for metadata and will result in undefined behavior. In a\ngroup of four consecutive threads, one or more threads store the metadata for the whole group\ndepending upon the matrix shape. These threads are specified using an additional sparsity selector operand.\nMatrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in\nFigure 111\n, with an appropriate matrix size.\nGranularities for different matrix shapes and data types are described below.\nSparse\nwgmma.mma_async.sp\nwith half-precision and\n.bf16\ntype\nFor\n.f16\nand\n.bf16\ntypes, for all supported\n64xNx32\nshapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand.\nFigure 171\nSparse WGMMA metadata example for\n.f16\n/\n.bf16\ntype.\n\nThe sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith\n.tf32\ntype\nFor\n.tf32\ntype, for all supported\n64xNx16\nshapes, matrix A is structured sparse at a\ngranularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have\none zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and\nthe 4-bit index in the metadata indicates the position of the non-zero element in the two-wide\nchunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in\nan undefined behavior.\nFigure 172\nSparse WGMMA metadata example for\n.tf32\ntype.\n\nThe sparsity selector indicates a thread-pair within a group of four consecutive threads which\ncontributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or\n1 (threads T2, T3); any other value results in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith\n.e4m3\nand\n.e5m2\nfloating point type\nFor\n.e4m3\nand\n.e5m2\ntypes, for all supported\n64xNx64\nshapes, matrix A is structured\nsparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of\nmatrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in\nmatrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices\nin the metadata operand.\nFigure 173\nSparse WGMMA metadata example for\n.e4m3\n/\n.e5m2\ntype.\n\nAll threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior.\nSparse\nwgmma.mma_async.sp\nwith integer type\nFor the integer type, for all supported\n64xNx64\nshapes, matrix A is structured sparse at a\ngranularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have\ntwo zeroes and two non-zero elements. Only the two non-zero elements are stored in matrix A and two\n2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide\nchunk.\nFigure 174\nSparse WGMMA metadata example for\n.u8\n/\n.s8\ntype.\n\nAll threads contribute the sparsity metadata and the sparsity selector must be 0; any other value\nresults in an undefined behavior.\n9.7.15.6.2.\nMatrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A\n\nIn this section we describe how the contents of thread registers are associated with fragments of A\nmatrix and the sparsity metadata.\nEach warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following\ntable shows the assignment of warps to rows of matrix A:\nWarp\nSparsity information for rows of matrix A\n%warpid\n% 4 = 3\n48-63\n%warpid\n% 4 = 2\n32-47\n%warpid\n% 4 = 1\n16-31\n%warpid\n% 4 = 0\n0-15\nThe following conventions are used throughout this section:\nFor matrix A, only the layout of a fragment is described in terms of register vector sizes and\ntheir association with the matrix data.\nFor matrix D, since the matrix dimension - data type combination is the same for all supported\nshapes, and is already covered in\nAsynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction\n, the pictorial\nrepresentations of matrix fragments are not included in this section.\nFor the metadata operand, pictorial representations of the association between indices of the\nelements of matrix A and the contents of the metadata operand are included.\nTk:\n[m..n]\npresent\nin cell\n[x][y..z]\nindicates that bits\nm\nthrough\nn\n(with\nm\nbeing higher) in the\nmetadata operand of thread with\n%laneid=k\ncontains the indices of the non-zero elements from\nthe chunk\n[x][y]..[x][z]\nof matrix A.\n9.7.15.6.2.1.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk32\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk32\nwill compute an MMA operation of shape\n.m64nNk32\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nShared Memory Matrix Layout\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.f16\n/\n.bf16\nA vector expression containing four\n.b32\nregisters, with each register containing two\nnon-zero\n.f16\n/\n.bf16\nelements out of 4\nconsecutive elements from matrix A.\nNon-zero elements:\na0, a1, a2, a3, a4, a5, a6, a7\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\nThe layout of the fragments held by different threads is shown in\nFigure 175\n.\nFigure 175\nSparse WGMMA .m64nNk32 fragment layout for matrix A with\n.f16\n/\n.bf16\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk32\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nShared Memory Matrix Layout\n.\nMetadata operand is a\n.b32\nregister containing 16 2-bit vectors each storing the index of a\nnon-zero element of a 4-wide chunk of matrix A.\nFigure 176\nshows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable\ni\nrepresents the value of the sparsity\nselector operand.\nFigure 176\nSparse WGMMA .m64nNk32 metadata layout for\n.f16\n/\n.bf16\ntype.\n\n9.7.15.6.2.2.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk16\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk16\nwill compute an MMA operation of shape\n.m64nNk16\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nShared Memory Matrix Layout\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.tf32\nA vector expression containing four\n.b32\nregisters, containing four non-zero\n.tf32\nelements out of eight consecutive elements\nfrom matrix A.\nNon-zero elements:\na0, a1, a2, a3\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\nThe layout of the fragments held by different threads is shown in\nFigure 177\n.\nFigure 177\nSparse WGMMA .m64nNk16 fragment layout for matrix A with\n.tf32\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk8\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nShared Memory Matrix Layout\n.\nMetadata operand is a\n.b32\nregister containing eight 4-bit vectors each storing the index of a\nnon-zero element of a 2-wide chunk of matrix A.\nFigure 178\nshows the mapping of the metadata bits to the elements\nof matrix A for a warp. In this figure, variable\ni\nrepresents the value of the sparsity\nselector operand.\nFigure 178\nSparse WGMMA .m64nNk16 metadata layout for\n.tf32\ntype.\n\n9.7.15.6.2.3.\nMatrix Fragments for sparse\nwgmma.mma_async.m64nNk64\n\nA warpgroup executing sparse\nwgmma.mma_async.m64nNk64\nwill compute an MMA operation of shape\n.m64nNk64\nwhere N is a valid n dimension as listed in\nMatrix Shape\n.\nElements of the matrix are distributed across the threads in a warpgroup so each thread of the\nwarpgroup holds a fragment of the matrix.\nMultiplicand A, from shared memory is documented in\nMatrix Fragments for sparse wgmma.mma_async.m64nNk64\n.\nMultiplicand A, from registers:\n.atype\nFragments\nElements\n.e4m3\n/\n.e5m2\nA vector expression containing four\n.b32\nregisters, with each register containing four\nnon-zero\n.e4m3\n/\n.e5m2\nelements out of\neight consecutive elements from matrix A.\nNon-zero elements:\na0, a1, a2, … , a15\nMapping of the non-zero\nelements is as described in\nSparse matrix storage\n.s8\n/\n.u8\nA vector expression containing four\n.b32\nregisters, with each register containing four\nnon-zero\n.s8\n/\n.u8\nelements out of\neight consecutive elements from matrix A.\nThe layout of the fragments held by different threads is shown in\nFigure 179\n.\nFigure 179\nSparse WGMMA .m64nNk64 fragment layout for matrix A with\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype.\n\nAccumulator D:\nMatrix fragments for accumulator D are the same as in case of\nMatrix Fragments for wgmma.mma_async.m64nNk32\nfor the same\n.dtype\nformat.\nMultiplicand B:\nShared memory layout for Matrix B is documented in\nMatrix Fragments for sparse wgmma.mma_async.m64nNk64\n.\nMetadata operand is a\n.b32\nregister containing 16 4-bit vectors each storing the indices of\ntwo non-zero elements of a 4-wide chunk of matrix A.\nFigure 180\nshows the mapping of the metadata\nbits to the elements of columns 0–31 of matrix A.\nFigure 180\nSparse WGMMA .m64nNk64 metadata layout for\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype for columns 0–31\n\nFigure 181\nshows the mapping of the metadata\nbits to the elements of columns 32–63 of matrix A.\nFigure 181\nSparse WGMMA .m64nNk64 metadata layout for\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\ntype for columns 32–63\n\n9.7.15.6.3.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.mma_async.sp\n\nwgmma.mma_async.sp\nPerform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup\nSyntax\nHalf precision floating point type:\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype   = {.f16, .f32};\nAlternate floating point type :\n.bf16 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;\n\n.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,\n            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,\n            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,\n            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,\n            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,\n            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,\n            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,\n            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};\n.dtype  = {.f32};\n\n.tf32 floating point type:\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,\n            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,\n            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,\n            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,\n            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,\n            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,\n            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,\n            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};\n.dtype  = {.f32};\n\nFP8 floating point type\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\nwgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64,\n            .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64,\n            .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64,\n            .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,\n            .m64n168k64, .m64n176k64, .m64n184k64, .m64n192k64,\n            .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64,\n            .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64};\n.atype  = {.e4m3, .e5m2};\n.btype  = {.e4m3, .e5m2};\n.dtype  = {.f16, .f32};\nInteger type:\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d;\n\nwgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d;\n\n.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,\n            .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64,\n            .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64,\n            .m64n176k64, .m64n192k64, .m64n208k64, .m64n224k64,\n            .m64n240k64, .m64n256k64};\n.atype  = {.s8, .u8};\n.btype  = {.s8, .u8};\nDescription\nInstruction\nwgmma.mma_async\nissues a\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\n, where the A matrix is\nMxK\n, the B matrix is\nKxN\n, and the D matrix is\nMxN\n.\nThe matrix A is stored in the packed format Mx(K/2) as described in\nSparse matrix storage\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nscale-d\nis\nfalse.\nwgmma.fence\ninstruction must be used to fence the register accesses of\nwgmma.mma_async\ninstruction from their prior accesses. Otherwise, the behavior is undefined.\nwgmma.commit_group\nand\nwgmma.wait_group\noperations must be used to wait for the completion\nof the asynchronous matrix multiply and accumulate operations before the results are accessed.\nRegister operand\nd\nrepresents the accumulator matrix as well as the destination matrix,\ndistributed across the participating threads. Register operand\na\nrepresents the multiplicand\nmatrix A in register distributed across the participating threads. The 64-bit register operands\na-desc\nand\nb-desc\nare the matrix descriptors which represent the multiplicand matrices A and\nB in shared memory respectively. The contents of a matrix descriptor must be same across all the\nwarps in the warpgroup. The format of the matrix descriptor is described in\nMatrix Descriptor Format\n. Matrix A is\nstructured sparse as described in\nSparse matrix storage\n.  Operands\nsp-meta\nand\nsp-sel\nrepresent sparsity metadata and sparsity selector respectively. Operand\nsp-meta\nis a 32-bit\ninteger and operand\nsp-sel\nis a 32-bit integer constant with values in the range 0..3.\nThe valid values of\nsp-meta\nand\nsp-sel\nfor each shape is specified in\nSparse matrix storage\nand are summarized here :\nMatrix shape\n.atype\nValid values of\nsp-meta\nValid values of\nsp-sel\n.m64nNk16\n.tf32\n0b1110 , 0b0100\n0 (threads T0, T1) or 1 (threads T2, T3)\n.m64nNk32\n.f16\n/\n.bf16\n0b00, 0b01, 0b10, 0b11\n0 (threads T0, T1) or 1 (threads T2, T3)\n.m64nNk64\n.e4m3\n/\n.e5m2\n/\n.s8\n/\n.u8\n0b00, 0b01, 0b10, 0b11\n0 (all threads contribute)\nMatrices A and B are stored in row-major and column-major format respectively. For certain floating\npoint variants, the input matrices A and B can be transposed by specifying the value 1 for the\nimmediate integer arguments\nimm-trans-a\nand\nimm-trans-b\nrespectively. A value of 0 can be\nused to avoid the transpose operation. The valid values of\nimm-trans-a\nand\nimm-trans-b\nare 0\nand 1. The transpose operation is only supported for the\nwgmma.mma_async\nvariants with\n.f16\n/\n.bf16\ntypes on matrices accessed from shared memory using matrix descriptors.\nFor the floating point variants of the\nwgmma.mma_async\noperation, each element of the input\nmatrices A and B can be negated by specifying the value -1 for operands\nimm-scale-a\nand\nimm-scale-b\nrespectively. A value of 1 can be used to avoid the negate operation. The valid\nvalues of\nimm-scale-a\nand\nimm-scale-b\nare -1 and 1.\nThe qualifiers\n.dtype\n,\n.atype\nand\n.btype\nindicate the data type of the elements in\nmatrices D, A and B respectively.\n.atype\nand\n.btype\nmust be the same for all floating point\nwgmma.mma_async\nvariants except for the FP8 floating point variants. The sizes of individual\ndata elements of matrices A and B in alternate floating point variants of the\nwgmma.mma_async\noperation are as follows:\nMatrices A and B have 8-bit data elements when\n.atype\n/\n.btype\nis\n.e4m3\n/\n.e5m2\n.\nMatrices A and B have 16-bit data elements when\n.atype\n/\n.btype\nis\n.bf16\n.\nMatrices A and B have 32-bit data elements when\n.atype\n/\n.btype\nis\n.tf32\n.\nPrecision and rounding:\nFloating point operations:\nElement-wise multiplication of matrix A and B is performed with at least single precision. When\n.dtype\nis\n.f32\n, accumulation of the intermediate values is performed with at least single\nprecision. When\n.dtype\nis\n.f16\n, the accumulation is performed with at least half\nprecision.\nThe accumulation order, rounding and handling of subnormal inputs are unspecified.\n.bf16\nand\n.tf32\nfloating point operations:\nElement-wise multiplication of matrix A and B is performed with specified\nprecision.\nwgmma.mma_async\noperation involving type\n.tf32\nwill truncate lower 13 bits of\nthe 32-bit input data before multiplication is issued. Accumulation of the intermediate values is\nperformed with at least single precision.\nThe accumulation order, rounding, and handling of subnormal inputs are unspecified.\nInteger operations:\nThe integer\nwgmma.mma_async\noperation is performed with\n.s32\naccumulators. The\n.satfinite\nqualifier indicates that on overflow, the accumulated value is limited to the\nrange\nMIN_INT32\n..\nMAX_INT32\n(where the bounds are defined as the minimum negative signed\n32-bit integer and the maximum positive signed 32-bit integer respectively).\nIf\n.satfinite\nis not specified, the accumulated value is wrapped instead.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.mma_async\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.mma_async\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.mma_async\ninstruction. In conditionally executed code, a\nwgmma.mma_async\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.2.\nSupport for\n.u8.s8\nand\n.s8.u8\nas .atype.btype introduced in PTX ISA version 8.4.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples of integer type\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                    descA, descB, spMeta, 0, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n9.7.15.7.\nAsynchronous\nwgmma\nProxy Operations\n\nThis section describes warpgroup level\nwgmma.fence\n,\nwgmma.commit_group\nand\nwgmma.wait_group\ninstructions.\n9.7.15.7.1.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.fence\n\nwgmma.fence\nEnforce an ordering of register accesses between\nwgmma.mma_async\nand other operations.\nSyntax\nwgmma.fence.sync.aligned;\nDescription\nwgmma.fence\ninstruction establishes an ordering between prior accesses to any warpgroup\nregisters and subsequent accesses to the same registers by a\nwgmma.mma_async\ninstruction. Only\nthe accumulator register and the input registers containing the fragments of matrix A require this\nordering.\nThe\nwgmma.fence\ninstruction must be issued by all warps of the warpgroup at the following\nlocations:\nBefore the first\nwgmma.mma_async\noperation in a warpgroup.\nBetween a register access by a thread in the warpgroup and any\nwgmma.mma_async\ninstruction\nthat accesses the same registers, either as accumulator or input register containing fragments of\nmatrix A, except when these are accumulator register accesses across multiple\nwgmma.mma_async\ninstructions of the same shape. In the latter case, an ordering guarantee is provided by default.\nOtherwise, the behavior is undefined.\nAn async proxy fence must be used to establish an ordering between prior writes to shared memory\nmatrices and subsequent reads of the same matrices in a\nwgmma.mma_async\ninstruction.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.fence\ninstruction causes the executing\nthread to wait until all threads in the warp execute the same\nwgmma.fence\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.fence\ninstruction. In conditionally executed code, an\nwgmma.fence\ninstruction\nshould only be used if it is known that all threads in the warpgroup evaluate the condition\nidentically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\n// Example 1, first use example:\nwgmma.fence.sync.aligned;    // Establishes an ordering w.r.t. prior accesses to the registers s32d<0-3>\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n\n// Example 2, use-case with the input value updated in between:\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\n...\nmov.b32 s32d0, new_val;\nwgmma.fence.sync.aligned;\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d4, s32d5, s32d6, s32d7},\n                                                 {s32d0, s32d1, s32d2, s32d3},\n                                                  descB, scaleD;\nwgmma.commit_group.sync.aligned;\nwgmma.wait_group.sync.aligned 0;\n9.7.15.7.2.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.commit_group\n\nwgmma.commit_group\nCommits all prior uncommitted\nwgmma.mma_async\noperations into a\nwgmma-group\n.\nSyntax\nwgmma.commit_group.sync.aligned;\nDescription\nwgmma.commit_group\ninstruction creates a new wgmma-group per warpgroup and batches all prior\nwgmma.mma_async\ninstructions initiated by the executing warp but not committed to any\nwgmma-group into the new wgmma-group. If there are no uncommitted\nwgmma.mma_async\ninstructions\nthen\nwgmma.commit_group\nresults in an empty wgmma-group.\nAn executing thread can wait for the completion of all\nwgmma.mma_async\noperations in a\nwgmma-group by using\nwgmma.wait_group\n.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.commit_group\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.commit_group\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.commit_group\ninstruction. In conditionally executed code, an\nwgmma.commit_group\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\nwgmma.commit_group.sync.aligned;\n9.7.15.7.3.\nAsynchronous Multiply-and-Accumulate Instruction:\nwgmma.wait_group\n\nwgmma.wait_group\nSignal the completion of a preceding warpgroup operation.\nSyntax\nwgmma.wait_group.sync.aligned N;\nDescription\nwgmma.wait_group\ninstruction will cause the executing thread to wait until only N or fewer of\nthe most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing\nthreads are complete. For example, when N is 0, the executing thread waits on all the prior\nwgmma-groups to complete. Operand N is an integer constant.\nAccessing the accumulator register or the input register containing the fragments of matrix A of a\nwgmma.mma_async\ninstruction without first performing a\nwgmma.wait_group\ninstruction that\nwaits on a\nwgmma-group\nincluding that\nwgmma.mma_async\ninstruction is undefined behavior.\nThe mandatory\n.sync\nqualifier indicates that\nwgmma.wait_group\ninstruction causes the\nexecuting thread to wait until all threads in the warp execute the same\nwgmma.wait_group\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nwgmma.wait_group\ninstruction. In conditionally executed code, an\nwgmma.wait_group\ninstruction should only be used if it is known that all threads in the warpgroup evaluate the\ncondition identically, otherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_90a\n.\nExamples\nwgmma.fence.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},\n                                                  descA, descB, scaleD;\nwgmma.commit_group.sync.aligned;\n\nwgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3},\n                                                  {f16a0, f16a1, f16a2, f16a3},\n                                                   descB, 1, -1, -1, 1;\nwgmma.commit_group.sync.aligned;\n\nwgmma.wait_group.sync.aligned 0;\n9.7.16.\nTensorCore 5th Generation Family Instructions\n\n9.7.16.1.\nTensor Memory\n\nThe 5\nth\ngeneration TensorCore has dedicated on-chip memory that is specialized for use by\nTensorCore operations. This Tensor Memory is organized as a two-dimensional matrix where\nthe horizontal rows are called lanes and the vertical columns are called columns.\nOn architecture\nsm_100a\n/\nsm_100f\n, the 5\nth\ngeneration TensorCore’s Tensor Memory has a\ntwo-dimensional structure of 512 columns and 128 rows per CTA, with each cell being 32-bits in size.\nRestrictions on threads accessing the Tensor Memory via the load and store operations\nare specified in\nAccess restrictions\n.\n9.7.16.1.1.\nTensor Memory Addressing\n\nTensor Memory addresses are 32-bit wide and specify two components.\nLane index\nColumn index\nThe layout is as follows:\n31                      16\n15                       0\nLane index\nColumn index\nFigure 182\nshows the view of the Tensor Memory Layout within CTA.\nFigure 182\nTensor Memory Layout and Addressing\n\n9.7.16.1.2.\nTensor Memory Allocation\n\nThe Tensor Memory is dynamically allocated. The Tensor Memory must be allocated by a single\nwarp in a CTA using the\nTensor Memory Allocation and Management Instructions\n.\nThe allocation and deallocation of\nTensor Memory\nis performed in terms of\ncolumns. The unit of allocation is 32 columns and the number of columns being allocated must be\na power of 2. When a column is allocated, all 128 lanes of the column are allocated.\nAll of the Tensor Memory that was allocated in a kernel, must be explicitly deallocated\nbefore the kernel exits.\n9.7.16.2.\nMatrix and Data Movement Shape\n\nThere are two kinds of shapes involved.\nShapes in the data movement operations\nShapes in the MMA operations\n9.7.16.2.1.\nMatrix Shape\n\nThe matrix multiply and accumulate operations support a limited set of shapes for the operand matrices\nA\n,\nB\nand\nD\n. The shapes of all three matrix operands are collectively described by the tuple\nMxNxK\nwhere\nA\nis\nMxK\nmatrix,\nB\nis a\nKxN\nmatrix, and\nD\nis a\nMxN\nmatrix.\nTable 39\nshows matrix shapes that are supported for the specified types for the\ntcgen05.mma\noperation.\nTable 39\nVarious combinations of .kind and shapes\n\nVarious Combinations\nShapes Supported\n.kind::*\nHas .ws\nCTA Group\nSparsity\ndtype\natype/btype\nkind::f16\nNo\n.ws\n1\nDense\n.f16\n.f16\n64xNxK\n128xNxK\nN = {8, 16, 24, … 256} steps of 8\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nK = 32\n.f32\n.f16\n,\n.bf16\n2\nDense\n.f16\n.f16\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nK = 32\n.f32\n.f16\n,\n.bf16\n.ws\n1\nDense\n.f16\n.f16\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 16\n.f32\n.f16\n,\n.bf16\nSparse\n.f16\n.f16\nN = {64, 128}\nK = 32\n.f32\n.f16\n,\n.bf16\n2\nEither\n.f16\n.f16\nInvalid\n.f32\n.f16\n,\n.bf16\n.kind::tf32\nNo\n.ws\n1\nDense\n.f32\n.tf32\n64xNxK\n128xNxK\nN = {8, 16, 24, … 256} steps of 8\nK = 8\nSparse\nK = 16\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 8\nSparse\nK = 16\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 8\nSparse\nN = {64, 128}\nK = 16\n2\nDense\nInvalid\nSparse\n.kind::f8f6f4\nNo\n.ws\n1\nDense\n.f32\n.f16\n.e4m3\n,\n.e5m2\n,\n.e2m3\n,\n.e3m2\n,\n.e2m1\n64xNxK\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 32\nSparse\nK = 64\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 32\nSparse\nN = {64, 128}\nK = 64\n2\nDense\nInvalid\nSparse\n.kind::mxf8f6f4\nNo\n.ws\n1\nDense\n.f32\n.e4m3\n,\n.e5m2\n,\n.e2m3\n,\n.e3m2\n,\n.e2m1\nX\n(Scale)\n.ue8m0\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {16, 32, … 256} steps of 16\nK = 32\nSparse\n256xNxK\nK = 64\n.ws\n1\nDense\nInvalid\nSparse\n2\nDense\nSparse\n.kind::i8\nNo\n.ws\n1\nDense\n.s32\n.s8\n,\n.u8\n64xNxK\n128xNxK\nN = {8, 16, 24, 32, 48, … 256}\nsteps of 16 after N > 32\nK = 32\nSparse\nK = 64\n2\nDense\n128xNxK\n256xNxK\nN = {32, 64, … 256} steps of 32\nK = 32\nSparse\nK = 64\n.ws\n1\nDense\n32xNxK\n64xNxK\n128xNxK\nN = {64, 128, 256}\nK = 32\nSparse\nN = {64, 128}\nK = 64\n2\nDense\nInvalid\nSparse\n.kind::mxf4\nNo\n.ws\n1\nDense\n.f32\n.e2m1\nX\n(Scale)\n.ue8m0\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 64\nSparse\nK = 128\n2\nDense\n128xNxK\n256xNxK\n256xNxK\n1\nN = {16, 32, … 256} steps of 16\nK = 64\nK\n1\n= 96\nSparse\n256xNxK\nK = 128\n.ws\n1 / 2\nEither\nInvalid\n.kind::mxf4nvf4\nNo\n.ws\n1\nDense\n.f32\n.e2m1\nX\n(Scale)\n.ue8m0\n,\n.ue4m3\n128xNxK\nN = {8, 16, … 256} steps of 8\nK = 64\nSparse\nK = 128\n2\nDense\n128xNxK\n256xNxK\n256xNxK\n1\nN = {16, 32, … 256} steps of 16\nK = 64\nK\n1\n= 96\nSparse\n256xNxK\nK = 128\n.ws\n1 / 2\nEither\nInvalid\n9.7.16.2.1.1.\nTarget ISA Note\n\nK = 96 is only supported for target architecture\nsm_103a\n.\n9.7.16.2.2.\nSpecifying Matrix Shape\n\nM\nand\nN\ncan be specified in the\nInstruction descriptor\n.\nK\ncannot be explicitly specified but is implicitly determined by the MMA-kind\nand the sparsity, as shown in the\nTable 39\n.\n9.7.16.2.3.\nData Movement Shape\n\nThe data movement shape indicates the dimension of the data to be moved to or from the\nTensor Memory\n. These shapes are described as a tuple\nlane\nx\nsize\nwhere:\nlane\nindicates the number of rows in the\nTensor Memory\n; and\nsize\nindicates the amount of data, in units of bits (b), across the columns in the\nTensor Memory\n.\nThe following shapes are supported by various tcgen05 operations:\nShape\ntcgen05.<op>\n.16x64b\n,\n.16x128b\n,\n.16x256b\n,\n.16x32bx2\n,\n.32x32b\n.ld\n/\n.st\n.4x256b\n,\n.32x128b\n,\n.64x128b\n,\n.128x256b\n,\n.128x128b\n.cp\n.31x256b\n(implicit)\n.shift\n9.7.16.2.3.1.\nMemory Layout\n\nThe following shows the layout of the matrix fragments across threads of the warp.\n9.7.16.2.3.1.1.\nMatrix fragments for shape .32x32b\n\nA\ntcgen05{.ld,.st}.32x32b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.32x32b\nwill access 32 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (32 * .num)-bits of data as shown in\nFigure 183\n.\nFigure 183\nMatrix Fragment for shape .32x32b\n\n9.7.16.2.3.1.2.\nMatrix fragments for shape .16x64b\n\nA\ntcgen05{.ld,.st}.16x64b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x64b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (64 * .num)-bits of data as shown in\nFigure 184\n.\nFigure 184\nMatrix Fragment for shape .16x64b\n\n9.7.16.2.3.1.3.\nMatrix fragments for shape .16x128b\n\nA\ntcgen05{.ld,.st}.16x128b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x128b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (128 * .num)-bits of data as shown in\nFigure 185\n.\nFigure 185\nMatrix Fragment for shape .16x128b\n\n9.7.16.2.3.1.4.\nMatrix fragments for shape .16x256b\n\nA\ntcgen05{.ld,.st}.16x256b\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, r2, r3, …\nA warp executing\ntcgen05{.ld,.st}.16x256b\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (256 * .num)-bits of data as shown in\nFigure 186\n.\nFigure 186\nMatrix Fragment for shape .16x256b\n\n9.7.16.2.3.1.5.\nMatrix fragments for shape .16x32bx2\n\nA\ntcgen05{.ld,.st}.16x32bx2\ninstruction has the following data vector register.\nFragment\nElements (low to high)\nA vector expression containing\n.num\nnumber of\n.b32\nregisters as\nmentioned in the\nTable 49\n.\nr0, r1, …\nA warp executing\ntcgen05{.ld,.st}.16x32bx2\nwill access 16 lanes of the Tensor Memory.\nIt loads from or stores to each of the lane (32 * .num)-bits of data as shown in\nFigure 187\n.\nFigure 187\nMatrix Fragment for shape .16x32bx2\n\n9.7.16.3.\nMajor-ness supported by Strides\n\nThere are two strides involved while accessing a matrix from shared memory:\nLeading dimension stride (byte offset or absolute address)\nStride dimension byte offset\n9.7.16.3.1.\nLeading Dimension Stride: relative offset or absolute address\n\nThere are two modes of Leading Dimension Strides as described below.\nBit #52 in the\nShared memory descriptor\nis used to distinguish between two modes.\n9.7.16.3.1.1.\nRelative offset mode\n\nIn this mode, the leading dimension stride is specified as a relative byte offset between the\ncolumns as explained in the below table.\nThe leading dimension stride can either be specified as a relative offset between the columns\nor as an absolute byte address of next buffer. The leading dimension stride is defined\ndifferently for transposed and non-transposed matrices. The leading dimension stride is defined\nas follows for matrices whose element types are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nNo-Swizzling: the stride from the first column to the second column\nof the 8x2 tile in the 128-bit element type normalized matrix.\nSwizzled layouts: not used, assumed to be 1.\nMN-Major\nInterleave: stride from the first 8 columns to the next 8 columns.\nSwizzled layouts: stride from the first (swizzle-byte-size/16) rows\nto the next (swizzle-byte-size/16) rows.\n9.7.16.3.1.2.\nAbsolute address mode for K dimension being 48B\n\nThe\ntcgen05.mma\ninstruction with\nK-dimension\nof 48B would overflow the 128B\nshared memory boundary if the data is packed contiguously.\nIn this case, the absolute address mode can be used to break up the data in the\nshared memory into two chunks such that both these chunks are laid out within\nthe aligned 128-byte address boundary.\nThe leading dimension absolute address can point to the second data chunk in the shared memory.\n9.7.16.3.1.2.1.\nRestrictions on the Leading Dimension Absolute Address Stride\n\nFollowing are the restrictions on the absolute address stride mode:\nOnly 128B swizzle (with 16B atomicity) is supported.\nOnly K-Major mode is supported. That is, the transpose bits(bits #15 and #16) in\nInstruction descriptor\nmust be 0.\nThe matrix base offset must be 0.\n9.7.16.3.2.\nStride Dimension Byte Offset\n\nThe stride dimension byte offset is defined differently for transposed and non-transposed\nmatrices. The stride dimension byte offset is defined as follows for matrices whose element\ntypes are normalized to 128-bits:\nMajor-ness\nDefinition\nK-Major\nThe offset from the first 8 rows to the next 8 rows.\nMN-Major\nInterleave: offset from the first row to the next row.\nSwizzled layout: offset from the first 8 columns to the next 8\ncolumns\n9.7.16.3.3.\nCanonical Layouts\n\nIn terms of\nCuTe layouts\nthe canonical layout can be expressed as follows:\nMajor-\nness\nSwizzling mode\nCanonical Layout without swizzling\nSwizzling\non the previous column\nMN-\nmajor\nNo-swizzling or\nInterleaved\n((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nSwizzle<1, 4, 3>\n64B Swizzling\n((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nSwizzle<2, 4, 3>\n128B Swizzling\n((T,8,m),(8,k)):((1,T,LBO),(8T,SBO))\nSwizzle<3, 4, 3>\nK-\nmajor\nNo-swizzling or\nInterleaved\n((8,m),(T,2k)):((1T,SBO),(1,LBO))\nSwizzle<0, 4, 3>\n32B Swizzling\n((8,m),(T,2k)):((2T,SBO),(1,T))\nSwizzle<1, 4, 3>\n64B Swizzling\n((8,m),(T,2k)):((4T,SBO),(1,T))\nSwizzle<2, 4, 3>\n128B Swizzling\n((8,m),(T,2k)):((8T,SBO),(1,T))\nSwizzle<3, 4, 3>\nwhere\nT = 128 / sizeof-elements-in-bits\nT represents scale factor which normalizes matrix element types to 128-bits.\nm represents the number of repeating patterns across rows.\nk represents the number of repeating patterns across columns.\nExamples\nK-Major, no-swizzling and tf32 type:\nFigure 188\nFigure 188\nK major, no-swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))\nCanonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO (relative offset)\n64*sizeof(tf32)\nSBO\n32*sizeof(tf32)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nK-Major, 32B swizzling and tf32 type:\nFigure 189\nFigure 189\nK major, 32B swizzling and tf32 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))\nCanonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))\nParameters\nValue\nT\n4\nm\n2\nk\n2\nLBO (relative offset)\nNA\nSBO\n64*sizeof(tf32)\nEncoding of LBO in descriptor\n1 (assumed)\nEncoding of SBO in descriptor\n(SBO) >> 4 = 16\nMN-Major, no-swizzling and bf16 type:\nFigure 190\nFigure 190\nMN major, no-swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))\nCanonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n128*sizeof(bf16)\nSBO\n64*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 8\nMN-Major, 32B swizzling and bf16 type:\nFigure 191\nFigure 191\nMN major, 32B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))\nCanonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n128*sizeof(bf16)\nSBO\n256*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 16\nEncoding of SBO in descriptor\n(SBO) >> 4 = 32\nMN-Major, 64B swizzling and bf16 type:\nFigure 192\nFigure 192\nMN major, 64B swizzling and bf16 type\n\nthe strides and related details are as follows:\nExact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))\nCanonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))\nParameters\nValue\nT\n8\nm\n2\nk\n2\nLBO (relative offset)\n256*sizeof(bf16)\nSBO\n512*sizeof(bf16)\nEncoding of LBO in descriptor\n(LBO) >> 4 = 32\nEncoding of SBO in descriptor\n(SBO) >> 4 = 64\n9.7.16.4.\nMatrix Descriptors\n\nThere are three kinds of matrix descriptors used by the\ntcgen05\nfamily of instructions.\n9.7.16.4.1.\nShared memory descriptor\n\nThe shared memory descriptor describes the properties of multiplicand matrix in shared\nmemory including its location in the shared memory of the current\nCTA\n. It is a 64-bit\nvalue contained in a register with the following layout:\nTable 40\nShared memory descriptor layout\n\nBit-field\nSize in bits\nDescription\n0-13\n14\nmatrix-descriptor-encode (Matrix start address)\n16-29\n14\nmatrix-descriptor-encode\n(\nLeading dimension byte offset relative\n)\nOR\nmatrix-descriptor-encode\n(\nLeading dimension byte address absolute\n)\n32-45\n14\nmatrix-descriptor-encode\n(\nStride dimension byte offset\n)\n46-48\n3\nFixed constant value of 0b001\n49-51\n3\nMatrix base offset\n52\n1\nLeading dimension stride mode:\n- 0: byte offset relative\n- 1: byte address absolute\n53-60\n8\nFixed constant value of 0xb00000000\n61-63\n3\nSpecifies the swizzling mode to be used:\n0. No swizzling\n1. 128-Byte with 32B atomic swizzling\n2. 128-Byte swizzling\n4. 64-Byte swizzling\n6. 32-Byte swizzling\nNote: Values 3, 5 and 7 are invalid\nwhere matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4\nThe value of base offset is 0 when the repeating pattern of the specified swizzling mode\nstarts as per shown in\nTable 41\n.\nTable 41\nStarting address of repeating pattern for various swizzling modes\n\nSwizzling mode\nStarting address of the repeating pattern\n128-Byte swizzle\n1024-Byte boundary\n64-Byte swizzle\n512-Byte boundary\n32-Byte swizzle\n256-Byte boundary\nOtherwise, the base offset must be a non-zero value, computed using the following formula:\nbase\noffset\n=\n(pattern\nstart\naddr\n>>\n0x7)\n&\n0x7\nThe following must be 16-byte aligned:\nMatrix start address\nLeading dimension byte offset\nStride dimension byte offset\n9.7.16.4.1.1.\nTarget ISA Note\n\nThe byte address mode for the leading dimension stride is supported on\nsm_103a\n.\n9.7.16.4.2.\nInstruction descriptor\n\nThe instruction descriptor describes the shapes, types and other details of all the matrices\nand the matrix-multiplication-and-accumulation operation. It is a 32-bit value in registers\nand the exact layout is dependent on the MMA-Kind:\nTable 42\nInstruction descriptor format for .kind::tf32, .kind::f16, .kind::f8f6f4 and .kind::i8\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::tf32\n.kind::f16\n.kind::f8f6f4\n.kind::i8\n0-1\n2\nSparsity selector\n,\nif Sparsity is enabled\n0-3\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nSaturate for integer types\n0 (NA)\nNo Saturate = 0\nSaturate = 1\n4-5\n2\ndtype (Matrix D type)\nF32 = 1\nF16 = 0\nF32 = 1\nS32 = 2\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nTF32 = 2\nF16 = 0\nBF16 = 1\nE4M3 = 0\nE5M2 = 1\nE2M3 = 3\nE3M2 = 4\nE2M1 = 5\nUnsigned 8b = 0\nSigned 8b = 1\n10-12\n3\nbtype (Matrix B type)\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\nNo Negate = 0\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\nTranspose = 1\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nReserved\n0\n24-28\n5\nM, Dimension of Matrix A\n(4 LSBs not included)\nM >> 4\n29\n1\nReserved\n0\n30-31\n2\nMaximum shift while attempting\nB matrix -reuse in\n.ws\nno shift = 0\nmaximum shift of 8 = 1\nmaximum shift of 16 = 2\nmaximum shift of 32 = 3\nTable 43\nInstruction descriptor format for .kind::mxf8f6f4\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::mxf8f6f4\n0-1\n2\nReserved\n0\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nReserved\n0\n4-5\n2\nMatrix B Scale Factor Data ID\n0-3\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nE4M3 = 0\nE5M2 = 1\nE2M3 = 3\nE3M2 = 4\nE2M1 = 5\n10-12\n3\nbtype (Matrix B type)\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\nTranspose = 1\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nScale Matrix Type, for both scale_A /\nscale_B\nUE8M0 = 1\n24-26\n3\nReserved\n0\n27-28\n2\nM, Dimension of Matrix A\n(7 LSBs not included)\nM >> 7\n29-30\n2\nMatrix A Scale Factor Data ID\n0-3\n31\n1\nReserved\n0\nTable 44\nInstruction descriptor format for .kind::mxf4 and .kind::mxf4nvf4\n\nBits\nSize\n(bits)\nDescription\nValues\n.kind::mxf4\n.kind::mxf4nvf4\n0-1\n2\nReserved\n0\n2\n1\nSparsity\nDense = 0\nSparse = 1\n3\n1\nReserved\n0\n4-5\n2\nMatrix B Scale Factor Data ID\n0 or 2\n6\n1\nReserved\n0\n7-9\n3\natype (Matrix A type)\nE2M1 = 1\n10-11\n2\nbtype (Matrix B type)\n12\n1\nReserved\n0\n13\n1\nNegate A Matrix\nNo Negate = 0\nNegate = 1\n14\n1\nNegate B Matrix\n15\n1\nTranspose A Matrix\nNo Transpose = 0\n16\n1\nTranspose B Matrix\n17-22\n6\nN, Dimension of Matrix B\n(3 LSBs not included)\nN >> 3\n23\n1\nScale Matrix Type, for both scale_A /\nscale_B\nUE8M0 = 1\nUE4M3 = 0\n24-26\n3\nReserved\n0\n27-28\n2\nM, Dimension of Matrix A\n(7 LSBs not included)\nM >> 7\n29-30\n2\nMatrix A Scale Factor Data ID\n0 or 2\n31\n1\nK Dimension\n(Dense K=64 / Sparse K=128) = 0\n(Dense K=96) = 1\n9.7.16.4.3.\nZero-Column Mask Descriptor\n\nThe zero-column mask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix will have zero value for the MMA operation regardless of the values present in\nthe shared memory. The total size of the generated mask is N-bits.\nA 0-bit in the mask specifies that values of the corresponding column in matrix\nB\nshould\nbe used for the MMA operation. A 1-bit in the mask specifies 0s must be used for the entire\ncolumn for the MMA operation.\nThe zero-column mask descriptor is a 64-bit value in registers with the following layout:\nTable 45\nZero-Column Mask descriptor layout\n\nBits\nSize (bits)\nField Name\nDescription\n0-7\n8\nStart Count 0 (sc0)\nSpecifies the LSBs that must be skipped\nfor sub-mask mask-i\n8-15\n8\nStart Count 1 (sc1)\n16-23\n8\nStart Count 2 (sc2)\n24-31\n8\nStart Count 3 (sc3)\n32\n1\nFirst Span 0 (fs0)\nSpecifies the starting value for\nsub-mask mask-i\n33\n1\nFirst Span 1 (fs1)\n34\n1\nFirst Span 2 (fs2)\n35\n1\nFirst Span 3 (fs3)\n36-38\n3\nReserved\n39\n1\nNon-Zero Mask\nValue 0 indicates generated mask will have all 0s\nValue 1 indicates the mask has to be generated\n40-47\n8\nSkip Span\n(Count of consecutive columns where B matrix is used) - 1\n48-55\n8\nUse Span\n(Count of consecutive columns where 0s ar used) - 1\n56-61\n6\nColumn Shift\nShifts column by specified amount.\nThus allows MMA on non-0 starting column.\nMax shift amount = 16 for M=32\nMax shift amount = 32 otherwise\nThe zero-column mask is made up of one or more sub-mask depending on M, as shown in the table:\nM\nZero-Column Mask breakup\nSub-masks\nFirst Span used\nStart Column used\n128\nSingle sub-mask of size N-bits\nmask0\nfs0\nsc0\n64\nTwo sub-masks, each with size of N/2 bits\nmask0, mask1\nfs0, fs1\nsc0, sc1\n32\nFour sub-masks, each with size of N/4 bits\nmask0, mask1\nmask2, mask3\nfs0, fs1, fs2,\nfs3\nsc0, sc1, sc2,\nsc3\nThe following table shows the coverage of the sub-masks across N-dimension:\nSub-mask\nM\n128\n64\n32\nmask0\nColumns [0, N-1]\nColumns [0, N/2-1]\nColumns [0, N/4-1]\nmask1\n–\nColumns [N/2, N-1]\nColumns [N/4, N/2-1]\nmask2\n–\n–\nColumns [N/2, (N/4*3)-1]\nmask3\n–\n–\nColumns [(N/4*3), N-1]\nThe following examples shows zero-column mask descriptor and their corresponding mask generated:\nExample 1: M = 128\nInput zero-column mask descriptor:\nStart count\nFirst span\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{0, 0, 0, 0}\n{0, 0, 0, 0}\n0\n4\n3\n0\nOutput zero-column mask: 0x0.\nAs Non-Zero Mask field is 0, the mask is 0x0. All the columns of the matrix\nB\nwill be used\nfor the MMA operation.\nExample 2: M = 128\nInput zero-column mask descriptor:\nStart count\nFirst span\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{-, -, -, 0}\n{-, -, -, 0}\n1\n2\n3\n0\nOutput mask0: 0b … 111 0000 111 0000 (size = N)\nExample 3: M = 64\nInput zero-column mask descriptor:\nStart count\n{.., sc1, sc0}\nFirst span\n{.., fs1, fs0}\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{-, -, 0, 0}\n{-, -, 0, 1}\n1\n2\n3\n0\nOutput mask0: 0b …   111 0000 111 0000 111\nOutput masl1: 0b … 0000 111 0000 111 0000\nExample 4: M = 32\nInput zero-column mask descriptor:\nStart count\n{sc3, sc2, sc1, sc0}\nFirst span\n{fs3, fs2, fs1, fs0}\nNon-Zero Mask\nSkip Span\nUse Span\nShift\n{1, 2, 1, 0}\n{0, 0, 1, 1}\n1\n2\n3\n2\nOutput mask0: 0b … 0000 111 0000 111\nOutput mask1: 0b … 0000 111 0000 11\nOutput mask2: 0b … 111 0000 111 00\nOutput mask3: 0b … 111 0000 111 000\nIf N = 128 then\nB\nMatrix with columns from 2 to 129 will be used for the MMA operation,\ndue to the shift of 2.\n9.7.16.5.\nIssue Granularity\n\nEach of the\ntcgen05\noperation has different requirements for the number of\nthreads/warps that needs to issue them.\nThe following table lists the execution granularity requirements of each of the\ntcgen05\noperation:\nTable 46\nExecution granularity requirements for tcgen05 operations\n\ntcgen05 operation\n.cta_group\nIssue Granularity\n.\nmma\n,\n.\ncp\n,\n.\nshift\n,\n.\ncommit\n::1\nAn issue from a single thread in the current\nCTA would initiate the base operation.\n::2\nIssue from a single thread from the\nCTA-Pair\nwould initiate\nthe base operation.\nWhen the current CTA issues the operation, the peer\nCTA should be active and should not have exited.\n.\nalloc\n,\n.\ndealloc\n,\n.\nrelinquish_alloc_permit\n::1\nIssue from a single warp in the current CTA\nwould initiate the allocation management instruction.\n::2\nIssue from two warps, one in each of the current CTA\nand its\nPeer CTA\n, in order to\ncollectively perform the operation, i.e., the first\nwarp to perform the operation could block until the\nthe second warp in the\nPeer CTA\nalso performs the operation (see examples below).\n.\nld\n,\n.\nst\n,\n.\nwait\n::\n{\nld\n,\nst\n}\nN/A\nIssue from a warp in the current CTA can access only\n1/4 of the Tensor Memory of the current CTA. So, a\nwarpgroup is needed to access the entire Tensor Memory\nof the current CTA.\n.\nfence\n::*\nN/A\nA thread needs to fence all its accesses to the tensor\nmemory that it wants to order with other accesses to\nthe tensor memory from other threads.\nThe following example shows that:\nBefore attempting to deallocate Tensor Memory, it suffices to ensure that there are no concurrent\nTensor Memory accesses from the\nPeer CTA\n.\nWarps can immediately exit after deallocating Tensor Memory; no extra synchronization required.\nTable 47\nExample of deallocating Tensor Memory before CTA exit.\n\nCTA0 Warp\nCTA1 Warp\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\nThis example uses a cluster barrier for illustration purposes but in practice other\nsynchronization mechanisms are often used.\nThe following example illustrates a scenario in which the program exhibits non-deterministic\nbehavior due to incorrect synchronizaton because\n.dealloc\nmay or may not block:\nTable 48\nExample of non-determinstic hang due to incorrect synchronization\n\nCTA0 Warp\nCTA1 Warp\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\ntcgen05.dealloc.2cta.sync.aligned;\nexit;\ntcgen05.dealloc.2cta.sync.aligned;\nbarrier.cluster.arrive;\nbarrier.cluster.wait;\nexit;\n9.7.16.5.1.\nCTA Pair\n\nAny 2 CTAs within the cluster whose\n%cluster_ctarank\ndiffers by the last bit only\nis said to form a CTA pair.\nWithin a CTA pair, the CTA whose last bit in the\n%cluster_ctarank\nis:\n0 is termed the even numbered CTA within the CTA pair.\n1 is termed as the odd numbered CTA within the CTA pair.\nMost of the\ntcgen05\noperations can either execute at a single CTA level granularity OR\nat a CTA pair level granularity. When a\ntcgen05\noperation is performed at CTA pair\ngranularity, the Tensor Memory of both the CTAs within the CTA pair are accessed. The set\nof threads that need to issue the\ntcgen05\noperation is listed in the\nIssue Granularity\n.\n9.7.16.5.2.\nPeer CTA\n\nThe peer CTA of the odd CTA within the CTA pair is the even CTA in the same pair.\nSimilarly, the peer CTA of the even CTA within the CTA pair is the odd CTA in the same pair.\n9.7.16.6.\nMemory Consistency Model for 5th generation of TensorCore operations\n\nOrdering of\ntcgen05\ninstructions is described in terms of two key concepts:\nPipelined tcgen05 instructions\nSpecialized tcgen05-specific inter-thread synchronization mechanisms.\nThese concepts combine to form four canonical synchronization patterns, as described further below.\n9.7.16.6.1.\nAsynchronous Operations\n\nThe tcgen05 family of instructions are divided into 2 categories:\nAsynchronous instructions:\nThese\ntcgen05\noperations are not inherently ordered with respect to\nother\ntcgen05\noperations in the same thread (unless pipelined as mentioned below).\nSynchronous instructions:\nThese\ntcgen05\noperations are inherently ordered with respect to other\ntcgen05\noperations in the same order.\nThe Tensor Memory allocation related instructions that access shared memory maintain\nsame-address ordering with respect to non-\ntcgen05\ninstructions.\nThe following table lists the category of each of the\ntcgen05\ninstruction:\ntcgen05.* operation\nCategory\n.alloc\nSynchronous\ninstructions\n.dealloc\n.relinquish_alloc_permit\n.fence::*\n.wait::*\n.commit\n.mma\nAsynchronous\ninstructions\n.cp\n.shift\n.ld\n.st\n9.7.16.6.2.\nPipelined tcgen05 Instructions\n\nThe asynchronous\ntcgen05\noperations may execute and complete in a different order than they\nwere issued. However, some specific pairs of the asynchronous\ntcgen05\ninstructions form\ntcgen05\npipelines, where in the two asynchronous operations are guaranteed to execute in\nthe same order as the instructions that issued them. The specific pairings are as follows:\ntcgen05.mma.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N and accumulator and shape)\ntcgen05.copy.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.mma.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.cp.4x256b.cta_group::N\n(same N)\ntcgen05.mma.cta_group::N\n->\ntcgen05.shift.cta_group::N\n(same N)\n9.7.16.6.2.1.\nImplicitly pipelined tcgen05 Instructions\n\nInstructions\ntcgen05.commit\nand\ntcgen05.wait\nare implicitly pipelined with respect\nto previously issued\ntcgen05.{mma,cp,shift}\nand\ntcgen05.{ld,st}\ninstructions\nrespectively that they track from the same thread.\n9.7.16.6.2.1.1.\nmbarrier based completion mechanism\n\nCompletion of the following instruction’s asynchronous operations is observed\nthrough the mbarrier based waiting mechanism:\ntcgen05.mma\ntcgen05.cp\ntcgen05.shift\ntcgen05.commit\nis used to track the completion of the above asynchronous instructions.\nFollowing are the implicitly pipelined\ntcgen05\ninstruction pairing that uses mbarrier\nbased completion mechanism:\ntcgen05.mma.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\ntcgen05.cp.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\ntcgen05.shift.cta_group::N\n->\ntcgen05.commit.cta_group::N\n(same N)\n9.7.16.6.2.1.2.\ntcgen05.wait\ninstruction based completion mechanism\n\nCompletion of the following instruction’s asynchronous operations is observed through\ntcgen05.wait\nbased waiting mechanism:\ntcgen05.ld\ntcgen05.st\ntcgen05.wait::ld\nand\ntcgen05.wait::st\nis used to track the completion of the\ntcgen05.ld\nand\ntcgen05.st\nasynchronous instructions.\nFollowing are the implicitly pipelined\ntcgen05\ninstruction pairing that uses\ntcgen05.wait\nbased completion mechanism:\ntcgen05.ld\n->\ntcgen05.wait::ld\ntcgen05.st\n->\ntcgen05.wait::st\n9.7.16.6.3.\nSpecialized Inter-thread Synchronization for tcgen05 instructions\n\nThe\ntcgen05\ninstructions support a specialized inter-thread synchronization which are\noptimized for\ntcgen05\nfamily of instructions. The standard memory consistency model\nsynchronization mechanisms also apply to the\ntcgen05\nfamily of instructions.\nThe\nTensorCore 5th Generation Specialized Synchronization Operations\nsection contains the specialized inter-thread\nsynchronization for tcgen05 instructions.\nThe\ntcgen05.fence::before_thread_sync\nand\ntcgen05.fence::after_thread_sync\ncomposes\nwith execution ordering instructions, like morally strong\nld\n/\nst\n/\natom\ninstructions,\nmbarrier\ninstruction,\nbarrier\ninstructions and so on, to establish an ordering between\nthe\ntcgen05\noperations across threads. The asynchronous\ntcgen05\ninstructions that are\nordered across threads also form a\ntcgen05\npipeline.\nAn asynchronous\ntcgen05\noperation prior to a\ntcgen05.fence::before_thread_sync\nis ordered\nbefore all subsequent\ntcgen05\nand the execution ordering operations.\nAn asynchronous\ntcgen05\noperation subsequent to a\ntcgen05.fence::after_thread_sync\nis\nordered after all the prior\ntcgen05\nand the execution ordering operations.\n9.7.16.6.4.\nCanonical synchronization patterns\n\nUsing the above rules, the following are the five canonical synchronization patterns:\n9.7.16.6.4.1.\nPipelined instructions, same thread\n\nIn this pattern, no explicit ordering mechanism is needed and the ordering guarantee is\nprovided by the pipelined instruction pairing.\nExample:\ntcgen05.mma\ntcgen05.mma (same shape and accumulator)\nThe two instructions will be executed in program order.\n9.7.16.6.4.2.\nNon-pipelined instructions, same thread\n\nIn this pattern, explicit waiting mechanisms are used to wait for the completion of the\nasynchronous\ntcgen05\noperations.\nExample 1:\ntcgen05.st\ntcgen05.wait::st\ntcgen05.ld\ntcgen05.wait::st\nis used to wait for the completion of the prior asynchronous\ninstruction\ntcgen05.st\n.\nExample 2:\ntcgen05.mma [d], ...\ntcgen05.commit.mbarrier::arrive::one\nmbarrier.try_wait.relaxed.cluster (loop until successful)\ntcgen05.fence::after_thread_sync\ntcgen05.ld [d], ...\nFor the completion of the asynchronous\ntcgen05.mma\n,\ntcgen05.commit\nis used.\nAs\ntcgen05.ld\nis an asynchronous operation, the instruction\ntcgen05.fence::after_thread_sync\nis needed.\nNo explicit\ntcgen05.fence::before_thread_sync\nis needed as this is implicitly performed by\ntcgen05.commit\n. The combination of\ntcgen05.mma\nand\ntcgen05.commit\nforms a\nconceptual asynchronous pipeline and establishes execution ordering.\ntcgen05.mma [d], ...\ntcgen05.fence::before_thread_sync\nmbarrier::arrive\n9.7.16.6.4.3.\nPipelined instructions, different thread\n\nIn this pattern, no explicit waiting mechanism is needed but proper synchronization between threads is needed.\nExample:\nThread 0\nThread 1\ntcgen05\n.\ncp\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nmma\n9.7.16.6.4.4.\nNon-pipelined instructions, different thread\n\nIn this pattern, the producer threads that issue the asynchronous\ntcgen05\ninstructions\nmust explicitly wait for the instructions’ completion before synchronizing with the consumer threads.\nExample 1:\nThread 0\nThread 1\ntcgen05\n.\nld\ntcgen05\n.\nwait\n::\nld\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nmma\nExample 1:\nThread 0\nThread 1\ntcgen05\n.\nmma\ntcgen05\n.\ncommit\n.\nmbarrier\n::\narrive\n::\none\n[\nmbar\n]\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nmbar\n]\n// loop till success\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nld\nThe synchronization mechanisms can also be composed with each other. For example:\nThread 0\nThread 1\ntcgen05\n.\nmma\ntcgen05\n.\ncommit\n.\nmbarrier\n::\narrive\n::\none\n[\nbar1\n]\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nbar1\n]\n// loop\n...\ntcgen05\n.\nfence\n::\nafter_thread_sync\n...\n// completion is guaranteed\ntcgen05\n.\nfence\n::\nbefore_thread_sync\nmbarrier\n.\narrive\n.\nrelaxed\n.\ncluster\n[\nbar2\n]\n// loop\n...\nmbarrier\n.\ntry_wait\n.\nrelaxed\n.\ncluster\n[\nbar2\n]\n// loop\n...\ntcgen05\n.\nfence\n::\nafter_thread_sync\ntcgen05\n.\nld\n9.7.16.6.4.5.\nRegister dependencies, same thread\n\nFor\ntcgen05.ld\n, an intra-thread ordering through true register dependency will be respected\nregardless of the presence or absence of other forms of synchronization. This form of register\ndependency does not imply any other form of ordering. For example, a register dependency does\nnot imply that a dependee instruction’s memory accesses will be performed before a dependent\ninstruction’s memory accesses. To enforce such memory orderings and avoiding anti-dependency\nhazards around\ntcgen05.ld\n,\ntcgen05.wait::ld\nmust be used.\nExample:\ntcgen05.ld %r1, ...;\ntcgen05.mma ..., %r1, ...;\n9.7.16.6.5.\nShared Memory Accesses\n\nThe shared memory accesses by\ntcgen05.mma\nand\ntcgen05.cp\noperations are performed\nin the asynchronous proxy (async proxy).\nAccessing the same memory location across miltiple proxies needs a cross-proxy fence.\nFor the async proxy,\nfence.proxy.async\nshould be used to synchronize memory between\ngeneric proxy and the async proxy.\n9.7.16.7.\nTensor Memory Allocation and Management Instructions\n\n9.7.16.7.1.\nTensorcore 5th Generation Instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\nDynamic\nTensor Memory\nallocation management instructions\nSyntax\ntcgen05.alloc.cta_group.sync.aligned{.shared::cta}.b32  [dst], nCols;\n\ntcgen05.dealloc.cta_group.sync.aligned.b32              taddr, nCols;\n\ntcgen05.relinquish_alloc_permit.cta_group.sync.aligned;\n\n.cta_group = { .cta_group::1, .cta_group::2 }\nDescription\ntcgen05.alloc\nis a potentially blocking instruction which dynamically allocates\nthe specified number of columns in the\nTensor Memory\nand writes\nthe address of the allocated\nTensor Memory\ninto shared memory\nat the location specified by address operand dst. The\ntcgen05.alloc\nblocks if the\nrequested amount of\nTensor Memory\nis not available and unblocks\nas soon as the requested amount of\nTensor Memory\nbecomes\navailable for allocation.\ntcgen05.dealloc\nis a potentially blocking instruction which deallocates the\nTensor Memory\nspecified by the\nTensor Memory\naddress\ntaddr\n. The operand\ntaddr\nmust point to a previous\nTensor Memory\nallocation.\nIf\n.cta_group::2\nis specified,\nissuing warp and\npeer CTA\nwarp must synchronize\nTensor Memory\naccesses\nbefore attempting to collectively deallocate the\nTensor Memory\n, and\ntcgen05.dealloc\nmay block to collectively performs the deallocation with\nthe other\npeer CTA\n’s warp.\nAll of the Tensor Memory that was allocated using\ntcgen05.alloc\ninstruction in a kernel,\nmust be explicitly deallocated using\ntcgen05.dealloc\nbefore the kernel exits.\nThe unsigned 32-bit operand\nnCols\nspecify the number of columns to be allocated or\nde-allocated. The unit of allocation and de-allocation is 32 columns and all of lanes\nper column. The number of columns must be a power of 2. The operand\nnCols\nmust be\nwithin the range [32, 512]. The number of columns allocated should not increase between\nany two allocations in the execution order within the CTA. Operand\nnCols\nmust be\npower of 2.\nInstruction\ntcgen05.relinquish_alloc_permit\nspecifies that the CTA of the executing\nthread is relinquishing the right to allocate\nTensor Memory\n. So,\nit is illegal for a CTA to perform\ntcgen05.alloc\nafter any of its constituent threads\nexecute\ntcgen05.relinquish_alloc_permit\n.\nIf no state space is specified then\nGeneric Addressing\nis used.\nIf the address specified by\ndst\ndoes not fall within the address window of\n.shared::cta\nstate space then the behavior is undefined.\nQualifier\n.cta_group\nspecifies the number of CTAs involved in the allocation and\nde-allocation operation. When\n.cta_group::1\nis specified, one warp from the CTA must\nperform the allocation and de-allocation. When\n.cta_group::2\nis specified, one warp\nfrom each of the\npeer CTAs\nmust collectively perform the allocation and\nde-allocation. Refer to the\nIssue Granularity\nsection.\nWhen\n.cta_group::2\nis specified, the issuing warp must make sure that peer CTA is launched\nand its warps eventually participate in collective operations.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe mandatory\n.sync\nqualifier indicates that the instruction causes the executing thread\nto wait until all threads in the warp execute the same instruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame instruction. In conditionally executed code, the instruction should only be used if it\nis known that all threads in the warp evaluate the condition identically, otherwise behavior\nis undefined.\nThe behavior of the instruction is undefined if all the threads in the warp do not use the\nsame values of\nnCols\n, or if any thread in the warp has exited.\nThe store operation in\ntcgen05.alloc\nis treated as a weak memory operation in the\nMemory Consistency Model\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n// Example 1:\n\ntcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sMemAddr1], 32;\nld.shared.b32 taddr, [sMemAddr1];\n// use taddr ...\n// more allocations and its usages ...\ntcgen05.dealloc.cta_group::1.sync.aligned.b32  taddr, 32;\n// more deallocations ...\ntcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;\n\n// Example 2:\n\n// Following instructions are performed by current warp and the warp in the peer-CTA:\ntcgen05.alloc.cta_group::2.sync.aligned.shared::cta.b32 [sMemAddr2], 32;\nld.shared.b32 taddr, [sMemAddr2];\n// use taddr ...\n// more allocations and its usages ...\ntcgen05.dealloc.cta_group::2.sync.aligned.b32  taddr, 32;\n// more deallocations ...\ntcgen05.relinquish_alloc_permit.cta_group::2.sync.aligned;\n9.7.16.8.\nTensor Memory and Register Load/Store Instructions\n\nThe threads of the CTA can perform the loads and stores to the\nTensor Memory\nof the CTA and move data between registers and Tensor Memory. The loads and stores of data\ncan be performed in certain shapes as specified in the\nMatrix and Data Movement Shape\nsection.\n9.7.16.8.1.\nAccess restrictions\n\nNot all threads of the CTA can access the entire Tensor Memory via the\ntcgen05.ld\nand\ntcgen05.st\noperations.\nThe Tensor Memory of a CTA is divided into 4 equal chunks such that each warp of a warpgroup\nin the CTA can access a chunk of the Tensor Memory. All the columns of the Tensor Memory can\nbe accessed by all the four warps of a warpgroup. A lane of the Tensor Memory can be accessed\nby a single warp in the warpgroup. The following table describes the access restriction.\nID of the warp within the warpgroup\nAccessible Lanes\n0\n0-31\n1\n32-63\n2\n64-95\n3\n96-127\n9.7.16.8.2.\nPacking and Unpacking\n\nOptionally, the following pack and unpack operations can be performed during the load and store:\nPacking: two 16-bit chunks can be packed into a single 32-bit chunk in the register in\ntcgen05.ld\nUnpacking: a single 32-bit chunk in the register can be unpacked into two 16-bit chunks in\ntcgen05.st\nas shown in the\nFigure 193\n.\nFigure 193\nPack/Unpack operations for tcgen05 ld/st\n\n9.7.16.8.3.\nTensorcore 5th Generation Instructions:\ntcgen05.ld\n\ntcgen05.ld\nAsynchronous collective load from tensor memory into registers.\nSyntax\n// Base load instruction:\n\ntcgen05.ld.sync.aligned.shape1.num{.pack}.b32    r, [taddr];\n\ntcgen05.ld.sync.aligned.shape2.num{.pack}.b32    r, [taddr], immHalfSplitoff;\n\n.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }\n.shape2 = { .16x32bx2 }\n.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }\n.pack   = { .pack::16b }\n\n// Floating point type load along with reduction :\n\ntcgen05.ld.red.sync.aligned.shape3.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr];\n\ntcgen05.ld.red.sync.aligned.shape4.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr], immHalfSplitoff;\n\n// Integer type load along with reduction :\n\ntcgen05.ld.red.sync.aligned.shape3.num.redOp.type r, redval, [taddr];\n\ntcgen05.ld.red.sync.aligned.shape4.num.redOp.type r, redval, [taddr], immHalfSplitoff;\n\n.shape3 = { .32x32b   }\n.shape4 = { .16x32bx2 }\n.redOp  = { .min, .max }\n.type   = { .u32, .s32 }\nDescription\nInstruction\ntcgen05.ld\nasynchronously loads data from the\nTensor Memory\nat the location specified by the 32-bit address operand\ntaddr\ninto the destination\nregister\nr\n, collectively across all threads of the warps.\nAll the threads in the warp must specify the same value of\ntaddr\n, which must be the\nbase address of the collective load operation. Otherwise, the behavior is undefined.\nThe\n.shape\nqualifier and the\n.num\nqualifier together determines the total\ndimension of the data which is loaded from the\nTensor Memory\n. The\n.shape\nqualifier indicates the base dimension of data to be accessed as described in the\nData Movement Shape\n. The\n.num\nqualifier indicates\nthe repeat factor on the base dimension resulting in the total dimension of the data that\nis accessed.\nThe shape\n.16x32bx2\nperforms two accesses into Tensor Memory of the shape\n.16x32b\n.\nThe base address of the first access is specified by taddr and the base address of the\nsecond access is specified by\ntaddr+immHalfSplitoff\n, where\nimmHalfSplitoff\nis an\nimmediate argument.\nThe destination operand\nr\nis a brace-enclosed vector expression consisting of one\nor more 32-bit registers as per the value of\n.shape\nand\n.num\n. The size of the\nvector for various combinations of\n.num\nand\n.shape\nis shown in\nTable 49\n.\nTable 49\nVarious-combinations of .num and .shape\n\n.num\n.shape\n.16x32bx2 / .16x64b / .32x32b\n.16x128b\n.16x256b\n.x1\n1\n2\n4\n.x2\n2\n4\n8\n.x4\n4\n8\n16\n.x8\n8\n16\n32\n.x16\n16\n32\n64\n.x32\n32\n64\n128\n.x64\n64\n128\nNA\n.x128\n128\nNA\nNA\nThe qualifier\n.red\nspecifies that the reduction operation specified by\n.redOp\nis\nperformed on the data that is loaded across columns in each lane. The result of the\nreduction operation is written into the corresponding thread’s 32-bit destination register\noperand\nredVal\n. When\n.red\nqualifier is specified,\n.num\nmodifier must be at least\n.x2\n.\nThe optional qualifier\n.pack::16b\ncan be used to pack two 16-bit elements from adjacent\ncolumns into a single 32-bit element during the load as shown in the section\nPacking and Unpacking\n.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.ld\ncauses the executing thread\nto wait until all threads in the warp execute the same\ntcgen05.ld\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame\ntcgen05.ld\ninstruction. In conditionally executed code, a\ntcgen05.ld\ninstruction\nshould only be used if it is known that all threads in the warp evaluate the condition\nidentically, otherwise behavior is undefined.\nThe behavior of\ntcgen05.ld\nis undefined if all threads do not use the same values of\ntaddr\n,\nor if any thread in the warp has exited.\nThe instruction\ntcgen05.ld\nis performed asynchronously and more details are specified in the\nsection\nMemory Consistency Model for 5th generation of TensorCore operations\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\ntcgen05.ld.red\nis introduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\ntcgen05.ld.red\nis supported on following architectures:\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_103f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\ntcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr1];\n\ntcgen05.ld.sync.aligned.16x128b.x4.b32    {r0, r1, r2, r3, r4, r5, r6, r7}, [taddr2];\n\ntcgen05.ld.red.sync.aligned.16x32bx2.x8.u32.max {r0, r1, r2, r3, r4, r5, r6, r7},\n                                                 redVal, [taddr3], 16;\n9.7.16.8.4.\nTensorcore 5th Generation Instructions:\ntcgen05.st\n\ntcgen05.st\nAsynchronous collective store to tensor memory from registers.\nSyntax\ntcgen05.st.sync.aligned.shape1.num{.unpack}.b32    [taddr], r;\n\ntcgen05.st.sync.aligned.shape2.num{.unpack}.b32    [taddr], immHalfSplitoff, r;\n\n.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }\n.shape2 = { .16x32bx2 }\n.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }\n.unpack = { .unpack::16b }\nDescription\nInstruction\ntcgen05.st\nasynchronously stores data from the source register\nr\ninto\nthe\nTensor Memory\nat the location specified by the 32-bit address operand\ntaddr\n,\ncollectively across all threads of the warps.\nAll the threads in the warp must specify the same value of\ntaddr\n, which must be the base\naddress of the collective store operation. Otherwise, the behavior is undefined.\nThe\n.shape\nqualifier and the\n.num\nqualifier together determines the total dimension\nof the data which is stored to the Tensor Memory. The\n.shape\nqualifier indicates the base\ndimension of data to be accessed as described in the\nData Movement Shape\n. The\n.num\nqualifier indicates the repeat factor on the base dimension resulting in the total dimension of\nthe data that is accessed.\nThe shape\n.16x32bx2\nperforms two accesses into Tensor Memory of the shape\n.16x32b\n.\nThe base address of the first access is specified by\ntaddr\nand the base address of the\nsecond access is specified by\ntaddr+immHalfSplitoff\n, where\nimmHalfSplitoff\nis an\nimmediate argument.\nThe source operand\nr\nis a brace-enclosed vector expression consisting of one or more 32-bit\nregisters as per the value of\n.shape\nand\n.num\n. The size of the vector for various\ncombinations of\n.num\nand\n.shape\nis shown in\nTable 50\n.\nTable 50\nVarious-combinations of .num and .shape\n\n.num\n.shape\n.16x32bx2 / .16x64b / .32x32b\n.16x128b\n.16x256b\n.x1\n1\n2\n4\n.x2\n2\n4\n8\n.x4\n4\n8\n16\n.x8\n8\n16\n32\n.x16\n16\n32\n64\n.x32\n32\n64\n128\n.x64\n64\n128\nNA\n.x128\n128\nNA\nNA\nThe optional qualifier\n.unpack::16b\ncan be used to unpack a 32-bit element in the\nregister into two 16-bit elements and store them in adjacent columns as shown in the\nsection\nPacking and Unpacking\n.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.st\ncauses the executing\nthread to wait until all threads in the warp execute the same\ntcgen05.st\ninstruction\nbefore resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute\nthe same\ntcgen05.st\ninstruction. In conditionally executed code, a\ntcgen05.st\ninstruction should only be used if it is known that all threads in the warp evaluate\nthe condition identically, otherwise behavior is undefined.\nThe behavior of\ntcgen05.st\nis undefined if all threads do not use the same values of\ntaddr\n, or if any thread in the warp has exited.\nThe instruction\ntcgen05.st\nis performed asynchronously and more details are specified\nin the section\nMemory Consistency Model for 5th generation of TensorCore operations\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\ntcgen05.st.sync.aligned.16x64b.x4.b32               [taddr0], {r0,  r1,  r2,  r3};\n\ntcgen05.st.sync.aligned.16x128b.x1.unpack::16b.b32  [taddr1], {r0,  r1};\n9.7.16.8.5.\nTensorcore 5th Generation Instructions:\ntcgen05.wait\n\ntcgen05.wait\nWaits for the completion of all prior asynchronous\ntcgen05.ld\n/\ntcgen05.st\ninstructions.\nSyntax\ntcgen05.wait_operation.sync.aligned;\n\n.wait_operation = { .wait::ld, .wait::st }\nDescription\nInstruction\ntcgen05.wait::st\ncauses the executing thread to block until all prior\ntcgen05.st\noperations issued by the executing thread have completed.\nInstruction\ntcgen05.wait::ld\ncauses the executing thread to block until all prior\ntcgen05.ld\noperations issued by the executing thread have completed.\nThe mandatory\n.sync\nqualifier indicates that\ntcgen05.wait_operation\ncauses the\nexecuting thread to wait until all threads in the warp execute the same\ntcgen05.wait_operation\ninstruction before resuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warp must execute the\nsame\ntcgen05.wait_operation\ninstruction.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nExample 1:\n\ntcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr0];\n\n// Prevents subsequent tcgen05.mma from racing ahead of the tcgen05.ld\n\ntcgen05.wait::ld.sync.aligned;\n\ntcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;\n\nExample 2:\n\ntcgen05.st.sync.aligned.32x32b.x2.b32     [taddr0], {r0, r1};\n\n// Prevents the write to taddr0 in tcgen05.mma from racing ahead of the tcgen05.st\n\ntcgen05.wait::st.sync.aligned;\n\ntcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;\n9.7.16.9.\nTensor Memory Data Movement Instructions\n\nData from the shared memory can be copied asynchronously to the\nTensor Memory\nusing the\nTensorcore 5th Generation Instructions: tcgen05.cp\noperation.\n9.7.16.9.1.\nOptional Decompression\n\nOptionally, during the copy, a vector of 4-bit and 6-bit\ncustom floating point types can be decompressed into 8-bit types.\n9.7.16.9.1.1.\nDecompression of 4-bit floating point to 8-bit type\n\nA contiguous set of 16 elements of 4-bits each followed by 8 bytes of padding can be converted\ninto 16 elements of 8-bits each as shown in\nFigure 194\n.\nFigure 194\nDecompression from 4-bit to 8-bit\n\nThe individual 4-bit to 8-bit decompression would look like as shown in\nFigure 195\n.\nFigure 195\nIndividual decompression from 4-bit to 8-bit\n\n9.7.16.9.1.2.\nDecompression of 6-bit floating point to 8-bit type\n\nA contiguous set of 16 elements of 6-bits each followed by 4 bytes of padding is\ndecompressed into 16 elements of 8-bits each as shown in\nFigure 196\n.\nFigure 196\nDecompression from 6-bit to 8-bit\n\nThe individual 6-bit to 8-bit decompression for types\nE3M2\nand\nE2M3\nis shown in\nFigure 197\nand\nFigure 198\nrespectively.\nFigure 197\nIndividual decompression from 6-bit to 8-bit for E3M2 type\n\nFigure 198\nIndividual decompression from 6-bit to 8-bit for E2M3 type\n\n9.7.16.9.2.\nTensorcore 5th Generation Instructions:\ntcgen05.cp\n\ntcgen05.cp\nInitiates an asynchronous copy operation from shared memory to the\nTensor Memory\n.\nSyntax\ntcgen05.cp.cta_group.shape{.multicast}{.dst_fmt.src_fmt} [taddr], s-desc;\n\n.cta_group = { .cta_group::1, .cta_group::2 }\n.src_fmt   = { .b6x16_p32 , .b4x16_p64 }\n.dst_fmt   = { .b8x16 }\n.shape     = { .128x256b, .4x256b, .128x128b, .64x128b**, .32x128b*** }\n.multicast = { .warpx2::02_13** , .warpx2::01_23**, .warpx4*** }\nDescription\nInstruction\ntcgen05.cp\ninitiates an asynchronous copy operation from shared memory to the\nlocation specified by the address operand\ntaddr\nin the\nTensor Memory\n.\nThe 64-bit register operand\ns-desc\nis the matrix descriptor which represents the source\nmatrix in the shared memory that needs to be copied. The format of the matrix descriptor is\ndescribed in\nMatrix Descriptors\n.\nThe\n.shape\nqualifier indicates the dimension of data to be copied as described in the\nData Movement Shape\n.\nQualifier\n.cta_group\nspecifies the number of CTAs whose\nTensor Memory\nis\naccessed when a single thread of a single CTA executes the\ntcgen05.cp\ninstruction.\nWhen\n.cta_group::1\nis specified, the data is copied into the\nTensor Memory\nof the current CTA. When\n.cta_group::2\nis specified, the data is copied into the\nTensor Memory\nof both the current and the\npeer CTAs\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nWhen the qualifiers\n.dst_fmt\nand\n.src_fmt\nare specified, the data is decompressed\nfrom the source format\n.src_fmt\nin the shared memory to the destination format\n.dst_fmt\nin\nTensor Memory\nby the copy operation. The details of source\nand the destination formats as specified in the section\nOptional Decompression\n.\nSome of the\n.shape\nqualifiers require certain\n.multicast\nqualifiers.\n.64x128b\nrequires\n.warpx2::02_13\nor\n.warpx2::01_23\n.32x128b\nrequires\n.warpx4\nWhen the\n.multicast\nqualifier is specified as either\n.warpx2::02_13\nor\n.warpx2::01_23\nthen the data being copied is multicasted into warp pairs and each\nwarp in the warp pair receive half of the data. Warp pairs are formed as follows:\n.warpx2::02_13\n: warps 0 and 2 form a pair; warps 1 and 3 form a pair.\n.warpx2::01_23\n: warps 0 and 1 form a pair; warps 2 and 3 form a pair.\nWhen the\n.multicast\nmodifier is specified as\n.warpx4\nthen the data being\ncopied is multicasted into all 4 warps.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\ntcgen05.cp.cta_group::1.128x256b                 [taddr0], sdesc0;\ntcgen05.cp.cta_group::2.128x128b.b8x16.b6x16_p32 [taddr1], sdesc1;\ntcgen05.cp.cta_group::1.64x128b.warpx2::02_13    [taddr2], sdesc2;\n9.7.16.9.3.\nTensorcore 5th Generation Instructions:\ntcgen05.shift\n\ntcgen05.shift\nAsynchronously shift down the rows of the matrix in the\nTensor Memory\nfor a warp.\nSyntax\ntcgen05.shift.cta_group.down  [taddr];\n\n.cta_group = { .cta_group::1, .cta_group::2 }\nDescription\nInstruction\ntcgen05.shift\nis an asynchronous instruction which initiates the shifting of 32-byte\nelements downwards across all the rows, except the last, by one row. The address operand\ntaddr\nspecifies the base address of the matrix in the\nTensor Memory\nwhose rows must\nbe down shifted.\nThe lane of the address operand\ntaddr\nmust be aligned to 32.\nQualifier\n.cta_group\nspecifies the number of CTAs whose\nTensor Memory\nis touched when a single thread of a single CTA executes the\ntcgen05.shift\ninstruction.\nWhen\n.cta_group::1\nis specified, the shift operation is performed in the\nTensor Memory\nof the current CTA. When\n.cta_group::2\nis specified,\nthe shift operation is performed in the\nTensor Memory\nof both the current and the\npeer CTAs\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_103a\nsm_110a\nExamples\ntcgen05.shift.down.cta_group::1 [taddr0];\ntcgen05.shift.down.cta_group::2 [taddr1];\n9.7.16.10.\nTensorCore 5th Generation Matrix Multiply and accumulate Operations\n\nThe 5\nth\ngeneration of TensorCore operations of shape\nMxNxK\nperform matrix\nmultiplication and accumulation of the form:\nD\n=\nA*B+D\nwhere:\nthe\nA\nmatrix has shape\nMxK\n, in either Tensor Memory or Shared Memory\nthe\nB\nmatrix has shape\nKxN\n, in Shared Memory of the current CTA and optionally in peer CTA\nthe\nD\nmatrix is of the shape\nMxN\n, in Tensor Memory\nOptionally an input predicate can be used to disable the input from the accumulator\nmatrix and the following operation can be performed as\nD\n=\nA*B\nThe matrix multiplication and accumulation operations are categorized into various kinds\nbased on input types and the throughput of the multiplication operation. The following shows the\ndifferent kinds of MMA operations that are supported:\nf16\n: supports\nf16\nand\nbf16\ninput types.\ntf32\n: supports\ntf32\ninput types.\nf8f6f4\n: supports all input combinations of\nf8\n,\nf6\nand\nf4\ntypes.\ni8\n: supports signed and unsigned 8-bit integer input types.\nmxf8f6f4\n/\nmxf4\n: supports mx-floating points input types.\nmxf4nvf4\n: supports\nmxf4\ntype and a custom NVIDIA floating-point\ntype for inputs where the type of the vector elements is 4 bits and requires a common\nscaling factor to form the complete floating-point type, similar to other mx-types.\nOptionally, the 5\nth\ngeneration of TensorCore MMAs support dense and sparse matrix\nA\n.\nSparse Matrices\ndescribes the details of the sparse matrices.\nSome of the MMA-kinds requires scaling of input matrices from memory to form the matrix\nA\nand matrix\nB\nbefore performing the MMA operation.\nBlock Scaling for tcgen05.mma.sync\ndescribes the details of the scaling of matrices.\nThe following table show the various matrices involved in the MMA operations and the memory in\nwhich they can reside:\nMatrix Type\nMemory\nA\nTensor Memory OR Shared Memory\nB\nShared Memory\nD\nTensor Memory\nSparse\nMeta\nData\nA-Scale\n/\nB-Scale\nA sequence of MMA instructions may reuse the same\nA\nmatrix with a sequence of\nB\nmatrices or may reuse the same\nB\nmatrix with a sequence of\nA\nmatrices.\nIn these patterns the TensorCore may be able to laod the unchanged matrix once and reuse\nit through the sequence without multiple reloads. The\nA\nor\nB\nmatrices are loaded\ninto a TensorCore collector buffer (i.e., special cache).\nAn MMA instruction has an optional\ncollector\nqualifier to specify when an\nA\nor\nB\nmatrix is new to the sequence and should be loaded, unchanged within the sequence\nand should be reused, or the last use in the sequence and should be discarded.\nThe\ncollector\nqualifier is used to give the TensorCore permission to reuse a previously\nloaded\nA\nor\nB\nmatrix; however reuse is opportunistic in that the TensorCore may\nreload a matrix even when it has permission to reuse that matrix. Thus, the source\nmemory of an\nA\nor\nB\nmatrix must not be modified while the MMA instruction using those\nmatrices has not completed - regardless of\ncollector\nqualifier permissions.\nThe 5\nth\ngeneration of TensorCore MMAs can be used for general matrix multiplication OR for\nconvolution operations. In case of convolutions, the activations can be stored in either\nmatrix\nA\nor matrix\nB\nwhile the weights will be stored in the other matrix.\nActivation\nMatrix\nWeights\nMatrix\nName of the\nop\nInstruction\nName\nCollector Buffer Applicability\nA\nB\nActivation\nStationary\n(default\ntcgen05.mma\n)\nCollector buffer is applicable on matrix\nA\nB\nA\nWeights\nStationary\n.ws\nCollector buffer is applicable on matrix\nB\n9.7.16.10.1.\nTranspose and Negate operations\n\nThe matrices\nA\nand\nB\ncan be transposed by specifying the Tranpose\nA\nMatrix\nand Transpose\nB\nMatrix bits in the instruction descriptor respectively.\nThe elements of the matrices\nA\nand\nB\ncan be negated by specifying the Negate\nA\nMatrix and Negate\nB\nMatrix bits in the instruction descriptor respectively.\nThe support for Transpose and Negate operation for various MMA-Kind are shown in\nTable 51\n.\nTable 51\nTranspose and Negate operation for various MMA-Kind\n\nMMA-Kind\nIs Transpose A/B supported\nIs Negate A/B supported\n.kind::tf32\nYes\nYes\n.kind::f16\nYes\nYes\n.kind::f8f6f4\nYes\nYes\n.kind::mxf8f6f4\nYes\nYes\n.kind::i8\nYes\nNo\n.kind::mxf4\nNo\nYes\n.kind::mxf4nvf4\nNo\nYes\nFor\n.kind::tf32\n, the transpose operations on matrices\nA\nand\nB\nare supported\nonly with 128B swizzling mode with 32B swizzle-atomicity.\nFor all other MMA-Kinds, the transpose operations on matrices\nA\nand\nB\nare not supported\non 128B swizzling mode with 32B swizzle-atomicity.\nTable 52\nshows the valid combinations of N shape with\n.cta_group\nqualifier for 8bit transpose B.\nTable 52\nVarious combinations of N shape with .cta_group qualifier for 8bit transpose B\n\n.cta_group\nN shape\n1\n16 <= N <= 256, step 16\n2\n32 <= N <= 256, step 32\n9.7.16.10.2.\nMatrix Layout Organization\n\nTable 53\ndescribes the major-ness used for different matrices.\nTable 53\nMajor-ness for different matrices\n\nMatrix\nResiding in Memory\nDefault Major-ness\nD\nTensor Memory\nRow-Major\nA\nTensor Memory\nShared Memory\nDepends on swizzling mode.\nRefer\nShared Memory Layout and Swizzling\nB\nShared Memory\n9.7.16.10.3.\nValid Combinations of Type-Size, Major-ness and Swizzling\n\nTable 54\nValid Combinations of Type-Size, Major-ness and Swizzling\n\nType-Size\nMajor-ness\nMatrix\nSupported Swizzle\n4-bit,\n6-bit,\n8-bit,\n16-bit,\n32-bit\nRow\nA\nAll swizzling modes\nColumn\nB\n8-bit\n16-bit\nColumn (transpose)\nA\nAll except 128B\nswizzling with 32B\natomicity\nRow (transpose)\nB\n32-bit\nColumn (transpose)\nA\nOnly 128B swizzling\nwith 32B atomicity\nRow (transpose)\nB\n9.7.16.10.4.\nPacking formats of elements in Tensor and Shared memory\n\n9.7.16.10.4.1.\nPacking format for matrix D in Tensor Memory\n\nThe sub-word elements of matrix\nD\nare expected not to be packed within a 32-bit Tensor Memory word.\nFor example, if the type of elements of the matrix\nD\nis 16 bits then a Tensor Memory word\nwould contain a single 16-bit element in its lower 16 bits.\n9.7.16.10.4.2.\nPacking format for matrix A and B\n\nThe 6-bit and 4-bit floating point types have different packing format requirements for\ndifferent MMA kinds in both Tensor memory and Shared memory. The requirements are as follows.\n9.7.16.10.4.3.\nPacking format used for matrix A by\n.kind::mxf8f6f4\nin Tensor Memory\n\nThe individual 4-bit and the 6-bit floating point type elements must be packed in an 8-bit container\nin Tensor memory as shown below. The 8-bit containers must be contiguously packed in a 32-bit Tensor\nMemory word. For example, if the type of elements of the matrix\nA\nis 6 bits then 4 consecutive\nA\nelements should be packed in one 32-bit Tensor Memory word.\n4-bit packing format as shown in\nFigure 199\nFigure 199\n4-bit packing format with type E2M1\n\n6-bit packing format\nType E3M2 as shown in\nFigure 200\nFigure 200\n6-bit packing format with type E3M2\n\nType E2M3 as shown in\nFigure 201\nFigure 201\n6-bit packing format with type E2M3\n\n9.7.16.10.4.4.\nPacking format used for matrix A and B by\n.kind::mxf8f6f4\nin Shared Memory\n\nThe 4-bit and 6-bit floating point elements in shared memory must be contiguously packed along\nwith padding as follows.\n4-bit packing format as shown in\nFigure 202\nFigure 202\n4-bit packing format\n\n6-bit packing format as shown in\nFigure 203\nFigure 203\n6-bit packing format\n\n9.7.16.10.4.5.\nPacking format used for matrix A by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Tensor Memory\n\nTwo 4-bit floating point type elements must be packed in an 8-bit container in Tensor memory as\nshown in\nFigure 204\nfor\nmxf4\n.\nFigure 204\n4-bit packing format with type E2M1\n\n9.7.16.10.4.6.\nPacking format used for matrix A and B by\n.kind::mxf4\nand\n.kind::mxf4nvf4\nin Shared Memory\n\nThe packing format for 4-bit floating point elements in shared memory is to pack two 4-bit\nelements in a 8-bit container, with no padding.\n9.7.16.10.5.\nData Path Layout Organization\n\nDifferent MMA variants access the tensor memory with different layout organization.\nThe following table lists the various layouts:\nM\ncta_group\nA-Sparsity\nIs .ws mode\nDatapath organization\nLayout ID\nTensor Memory Datapath Lane Alignment\n32\n::1\nEither\nYes\n1x4\nLayout G\n0\n64\n::1\nEither\nYes\n2x3\nLayout E\n0\n64\n::1\nEither\nNo\n4x1 (1/2 datapath utilized)\nLayout F\n0 or 16\n128\n::1\nEither\nEither\n4x1\nLayout D\n0\n128\n::2\nDense\nN/A\n2x2\nLayout B\n0\n128\n::2\nSparse\nN/A\n4x1 (1/2 datapath utilized)\nLayout C\n0 or 16\n256\n::2\nEither\nN/A\n4x1\nLayout A\n0\nThe layouts which utilize only half the datapath lanes, i.e.,\nLayout F\nand\nLayout C\n, must use the same Tensor Memory\nlane alignment across matrices\nA\n,\nD\nand the sparsity metadata matrix.\nThe following shows the warps that can access the Tensor Memory regions via\ntcgen05.ld\n/\ntcgen05.st\nalong with the addresses for various Tensor Memory Layouts.\n9.7.16.10.5.1.\nLayout A (M = 256)\n\nLayout organization for M = 256 is shown in\nFigure 205\n.\nFigure 205\nLayout organization for M = 256\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 206\nFigure 206\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.2.\nLayout B (M = 128 + cta-group::2 + Dense A matrix)\n\nLayout organization for M = 128 + .cta_group::2 + Dense A matrix is shown in\nFigure 207\n.\nFigure 207\nLayout organization for M = 128 + .cta_group::2 + Dense A matrix\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 208\nFigure 208\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.3.\nLayout C (M = 128 + cta-group::2 + Sparse A matrix)\n\nLayout organization for M = 128 + .cta_group::2 + Sparse A matrix is shown in\nFigure 209\n.\nFigure 209\nLayout organization for M = 128 + .cta_group::2 + Sparse A matrix\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 210\nFigure 210\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.4.\nLayout D (M = 128 + cta-group::1)\n\nLayout organization for M = 128 + .cta_group::1 is shown in\nFigure 211\n.\nFigure 211\nLayout organization for M = 128 + .cta_group::1\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 212\nFigure 212\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.5.\nLayout E (M = 64 + .ws mode)\n\nLayout organization for M = 64 + .ws mode is shown in\nFigure 213\n.\nFigure 213\nLayout organization for M = 64 + .ws mode\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 214\nFigure 214\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.6.\nLayout F (M = 64 + non .ws mode)\n\nLayout organization for M = 64 + non .ws mode is shown in\nFigure 215\n.\nFigure 215\nLayout organization for M = 64 + non .ws mode\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 216\nFigure 216\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.5.7.\nLayout G (M = 32)\n\nLayout organization for M = 32 is shown in\nFigure 217\n.\nFigure 217\nLayout organization for M = 32\n\nAddresses for the above region to be used in\ntcgen05.ld\n/\ntcgen05.st\nis shown in\nFigure 218\nFigure 218\nAddresses to use in\ntcgen05.ld\n/\ntcgen05.st\n\n9.7.16.10.6.\nShared Memory Layout and Swizzling\n\nIf the bit\nTranspose\nA\nMatrix\n/\nTranspose\nB\nMatrix\nin the\nInstruction descriptor\nis 0, then\nK-major\nis\nused for matrix\nA\n/\nB\nrespectively. If the bit\nTranspose\nA\nMatrix\nin the\nInstruction descriptor\nis 1 then\nM-major\nis\nused for matrix\nA\n. If the bit\nTranspose\nB\nMatrix\nin the\nInstruction descriptor\nis 1, then\nN-major\nis\nused for matrix\nB\n.\nIn a column-major default BLAS library such as cuBLAS, the matrices\nA\nand\nB\nwith and\nwithout transpose can be classified as either\nK-Major\nor\nM-or-N-Major\nas shown in the\nfollowing table:\nNon-Transposed\nTransposed\nA\nK-major\nM-major\nB\nK-major\nN-major\nTo avoid confusion with\nA\n,\nB\n,\nrow-major\n,\ncol-major\n,\ntranspose\n, and\nnon-transpose\n, we will use\nMN-Major\nand\nK-Major\nthroughout this section.\nThe matrices in the shared memory are made up of one or more “swizzle layout atom”.\nThe exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,\nand the leading dimension. The layout of the swizzle are shown in\nTable 55\nTable 55\nLayout for swizzle atoms\n\nSwizzling mode and\nSwizzle-Atomicity\nLeading\nDimension\nSwizzle atom layout\n(128b element)\n128B Swizzling with\n32B atomicity\nM/N\n8x4\n–\n–\n128B Swizzling with\n16B atomicity\nM/N\n8x8\nK\n8x8\n64B Swizzling Mode\nM/N\n4x8\nK\n8x4\n32B Swizzling Mode\nM/N\n2x8\nK\n8x2\nNone\nM/N\n1x8\nK\n8x1\nThe above shapes are for elements of size 128 bits. For smaller element sizes, the same shapes\nwould get multiplied along the leading dimension by a factor of\n128\n/\nsizeof_bits(Element)\n.\nFor example, 128B MN major swizzle atom would have a shape of (8*(128/32))x8 = 32x8 for\ntf32 tensor core inputs.\nSome example Layouts of\nMxK\nor\nKxN\nmatrices with various swizzling modes, and are in units\nof 128b elements as shown by each colored cell as shown in\nFigure 219\n,\nFigure 220\n,\nFigure 221\n,\nFigure 222\n,\nFigure 223\n,\nFigure 224\n,\nFigure 225\n,\nFigure 226\n,\nFigure 227\n.\nFigure 219\nMN major 128B swizzling with 32B atomicity\n\nFigure 220\nMN major 128B swizzling\n\nFigure 221\nK major 128B swizzling\n\nFigure 222\nMN major 64B swizzling\n\nFigure 223\nK major 64B swizzling\n\nFigure 224\nMN major 32B swizzling\n\nFigure 225\nK major 32B swizzling\n\nFigure 226\nMN major no-swizzling mode\n\nFigure 227\nK major no-swizzling mode\n\nFollowing are some of the examples of the 128B swizzling layout for\ntf32\nelement type.\nK-Major:\nFigure 228\nFigure 228\nK major\n\nMN-Major:\nFigure 229\nFigure 229\nMN major\n\n9.7.16.10.7.\nBlock Scaling for\ntcgen05.mma.sync\n\nThe\ntcgen05.mma\ninstructions with the following\n.kind\nqualifier:\n.kind::mxf8f6f4\n.kind::mxf4\n.kind::mxf4nvf4\nperform matrix multiplication with block scaling. This operation has the following form:\n(A\n*\nscale_A)\n*\n(B\n*\nscale_B)\n+\nD\nwhere\nscale_A\nand\nscale_B\nare matrices residing in\nTensor Memory\n.\nFor a\nscale_A\nmatrix of shape\nM x SFA_N\n, each row of matrix\nA\nis divided into\nSFA_N\nnumber of chunks and each chunk of a row is multiplied with the corresponding\nelement in the\nSF_A\nof the same row.\nSimilarly, for a\nscale_B\nmatrix of shape\nSFB_M x N\n, each column of matrix\nB\nis\ndivided into the\nSFB_M\nnumber of chunks and each chunk of a column is multiplied with\nthe corresponding element in the\nSF_B\nof the same column.\nScale factors for\nA\nand\nB\nmatrices need to be duplicated to all 32 lane partitions\nof tensor memory.\nFigure 230\nshows an example of\ntcgen05.mma\nwith block scaling of\nscale_vec::2X\n.\nFigure 230\ntcgen05.mma\nwith block scaling of\nscale_vec::2X\n\n9.7.16.10.7.1.\nValid combinations of scale_vectorsize with types and MMA-Kind\n\nThe shape of\nscale_A\nand\nscale_B\nmatrices depend on the\n.scale_vectorsize\nas shown in\nTable 56\n.\nTable 56\nValid combinations of scale_vectorsize and shapes\n\n.scale_vectorsize\n.kind::*\nK\nShape of scale_A\nShape of scale_B\n.scale_vec::1X\n.kind::mxf8f6f4\nAll supported values of K\nM x 1\n1 x N\n.scale_vec::2X\n.kind::mxf4\n,\n.kind::mxf4nvf4\nAll supported values of K\nM x 2\n2 x N\n.scale_vec::4X\n.kind::mxf4nvf4\nAll supported values of K\nM x 4\n4 x N\n.block16\n.kind::mxf4nvf4\nK = 96\nM x 6\n6 x N\nAll supported values of K\nexcept 96\nM x 4\n4 x N\n.block32\n.kind::mxf4\n,\n.kind::mxf4nvf4\nK = 96\nM x 3\n3 x N\nAll supported values of K\nexcept 96\nM x 2\n2 x N\n.kind::mxf8f6f4\nAll supported values of K\nM x 1\n1 x N\nThe valid combination of the exact element types and the\n.scale_vectorsize\nare listed in\nTable 57\n.\nTable 57\nValid combinations of scale_vectorsize with types and MMA-Kind\n\n.kind::*\nElement Data Type\nScale Data Type\n.scale_vectorsize\n.kind::mxf8f6f4\nE4M3, E5M2, E2M3\nE3M2, E2M1\nUE8M0\n.scale_vec::1X\n/\n.block32\n.kind::mxf4\nE2M1\nUE8M0\n.scale_vec::2X\n/\n.block32\n.kind::mxf4nvf4\nE2M1\nUE8M0\n.scale_vec::2X\n/\n.block32\n,\n.scale_vec::4X\n/\n.block16\nE2M1\nUE4M3\n.scale_vec::4X\n/\n.block16\nNew\n.blockN\nqualifiers are aliases for\n.scale_vec::NX\nqualifiers as:\n.block32\nis alias for\n.scale_vec::1X\nor\n.scale_vec::2X\nbased on\n.kind\nand K dimension\n.block16\nis alias for\n.scale_vec::4X\n9.7.16.10.7.2.\nScale Factor A ID\n\nThe value of the scale factor\nA\nID\nselects the sub-columns in the Tensor Memory to\nform the scale factor\nA\nmatrix, which is used to scale the matrix\nA\n.\nThe following shows the scale factor matrix layout for various scale vector sizes:\n9.7.16.10.7.2.1.\nLayout of the Scale Factor A Matrix for scale_vec::1X/block32 with K=32/K=64\n\nThere is one scale factor per row of the\nA\nmatrix with block size as 32 and the scale factor must be provided in\n1-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies the byte offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 231\nshows which sub-columns get selected for\ndifferent values of\nSFA_ID\n.\nFigure 231\nLayout of scale factor A matrix with scale_vec::1X/block32 with K=32/K=64\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFA_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns,\nrespectively.\n9.7.16.10.7.2.2.\nLayout of the Scale Factor A Matrix for scale_vec::2X/block32 with K=64/K=128\n\nThere are two scale factors per row of the\nA\nmatrix with block size as 32 and the scale factor must be provided in\n2-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies the half word offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 232\nshows which sub-columns gets selected for different\nvalues of\nSFA_ID\n.\nFigure 232\nLayout of scale factor A matrix with scale_vec::2X/block32 with K=64/K=128\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFA_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.2.3.\nLayout of the Scale Factor A Matrix for scale_vec::4X/block16 with K=64/K=128\n\nThere are four scale factors per row of the\nA\nmatrix with block size as 16 and the scale factor must be provided in\n4-byte aligned sub-column of the Tensor Memory. The\nSFA_ID\nvalue must be 0 and this specifies\nthat all of the columns (in green) will be used for the scale factor matrix.\nFigure 233\nshows which sub-columns gets selected for different\nvalues of\nSFA_ID\n.\nFigure 233\nLayout of scale factor A matrix with scale_vec::4X/block16 with K=64/K=128\n\n9.7.16.10.7.2.4.\nLayout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n\nThere are three scale factors per row of the\nA\nmatrix with block size as 32 and the scale\nfactor must be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies\nthe byte offset in the Tensor Memory word that must be used for the scale factor matrix.\nFigure 234\n,\nFigure 235\n,\nFigure 236\nand\nFigure 237\nshow which sub-columns get selected for different values of\nSFA_ID\n.\nFigure 234\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=00\n\nFigure 235\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=01\n\nFigure 236\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=10\n\nFigure 237\nLayout of scale factor A matrix with block32 with K=96 with SFA_ID=11\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFA_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns,\nrespectively.\n9.7.16.10.7.2.5.\nLayout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n\nThere are six scale factors per row of the\nA\nmatrix with block size as 16 and the scale\nfactor must be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFA_ID\nspecifies\nthe byte offset in the Tensor Memory word that must be used for the scale factor matrix.\nFigure 238\nand\nFigure 239\nshow which sub-columns get selected for different values of\nSFA_ID\n.\nFigure 238\nLayout of scale factor A matrix with block16 with K=96 with SFA_ID=00\n\nFigure 239\nLayout of scale factor A matrix with block16 with K=96 with SFA_ID=10\n\nFor example, if\nSFA_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFA_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.3.\nScale Factor B ID\n\nThe value of the scale factor\nB\nID\nselects the sub-columns in the Tensor Memory to\nform the scale factor\nB\nmatrix, which is used to scale the matrix\nB\n.\nThe following shows the scale factor matrix layout for various scale vector sizes:\n9.7.16.10.7.3.1.\nLayout of the Scale Factor B Matrix for scale_vec::1X/block32 with K=32/K=64\n\nThere is one scale factor per row of the\nB\nmatrix with block size as 32 and the scale factor must be provided in\n1-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 240\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 240\nLayout of scale factor B matrix with scale_vec::1X/block32 with K=32/K=64\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly,\nSFB_ID\nvalues of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.\n9.7.16.10.7.3.2.\nLayout of the Scale Factor B Matrix for scale_vec::2X/block32 with K=64/K=128\n\nThere are two scale factors per row of the\nB\nmatrix with block size as 32 and the scale factor must be provided in\n2-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the half word offset in the\nTensor Memory word that must be used for the scale factor matrix.\nFigure 241\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 241\nLayout of scale factor B matrix with scale_vec::2X/block32 with K=64/K=128\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the scale factor\nmatrix. Similarly, if\nSFB_ID\nis 2, then all of the blue columns are selected to form the scale\nfactor matrix.\n9.7.16.10.7.3.3.\nLayout of the Scale Factor B Matrix for scale_vec::4X/block16 with K=64/K=128\n\nThere are four scale factors per row of the\nB\nmatrix with block size as 16 and the scale factor must be provided in\n4-byte aligned sub-column of the Tensor Memory. The\nSFB_ID\nvalue must be 0 and this specifies\nthat all of the columns (in green) will be used for the scale factor matrix.\nFigure 242\nshows which sub-columns get selected for\ndifferent values of\nSFB_ID\n.\nFigure 242\nLayout of scale factor B matrix with scale_vec::4X/block16 with K=64/K=128\n\n9.7.16.10.7.3.4.\nLayout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)\n\nThere are three scale factors per row of the\nB\nmatrix with block size as 32 and the scale factor\nmust be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte\noffset in the Tensor Memory word that must be used for the scale factor matrix.\nFor N<=128,\nFigure 243\n,\nFigure 244\n,\nFigure 245\nand\nFigure 246\nshow which\nsub-columns get selected for different values of\nSFB_ID\n.\nFigure 243\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=00\n\nFigure 244\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=01\n\nFigure 245\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=10\n\nFigure 246\nLayout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=11\n\nFor N>128,\nFigure 247\n,\nFigure 248\n,\nFigure 249\n,\nFigure 250\n,\nFigure 251\nand\nFigure 252\nshow which\nsub-columns get selected for different values of\nSFB_ID\n.\nFigure 247\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=00\n\nFigure 248\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=01\n\nFigure 249\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10\n\nFigure 250\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10\n\nFigure 251\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11\n\nFigure 252\nLayout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the\nscale factor matrix. Similarly,\nSFB_ID\nvalues of 1, 2 and 3 would select the blue,\nyellow, and red columns, respectively.\n9.7.16.10.7.3.5.\nLayout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)\n\nThere are six scale factors per row of the\nB\nmatrix with block size as 16 and the scale factor\nmust be provided in 4-byte aligned sub-column of the Tensor Memory.\nSFB_ID\nspecifies the byte\noffset in the Tensor Memory word that must be used for the scale factor matrix.\nFor N<=128,\nFigure 253\nand\nFigure 254\nshow which sub-columns\nget selected for different values of\nSFB_ID\n.\nFigure 253\nLayout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=00\n\nFigure 254\nLayout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=10\n\nFor N>128,\nFigure 255\n,\nFigure 256\n,\nFigure 257\nand\nFigure 258\nshow which sub-columns\nget selected for different values of\nSFB_ID\n.\nFigure 255\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00\n\nFigure 256\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00\n\nFigure 257\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10\n\nFigure 258\nLayout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10\n\nFor example, if\nSFB_ID\nis 0, then all the green columns are selected to form the\nscale factor matrix. Similarly, if\nSFB_ID\nis 2, then all of the blue columns are\nselected to form the scale factor matrix.\n9.7.16.10.8.\nSparse Matrices\n\nThis instruction\ntcgen05.mma.sp\ncan be used when the matrix\nA\nis a structured\nsparse matrix with 50% zeros in each row distributed as per its sparse granularity.\nIn a\nMxNxK\nsparse\ntcgen05.mma.sp\noperation, the matrix\nA\nof shape\nMxK\nis\nstored in a packed form as\nMx(K/2)\nin memory. For each\nK-wide\nrow of matrix\nA\n,\n50% of elements are zeros and the remaining\nK/2\nnon-zero elements are stored in\nmemory. The metadata specifies the mapping of the\nK/2\nnon-zero elements to the\nK\nelements before performing the MMA operation.\nGranularity of sparse matrix\nA\nis defined as the ratio of the number of non-zero\nelements in a sub-chunk of the matrix row to the total number of elements in that\nsub-chunk where the size of the sub-chunk is shape-specific. The following table lists\nthe granularity of different\ntcgen05.mma.sp\nvariants:\n.kind of tcgen05.mma\nSparse Granularity\n.kind::tf32\n1:2\n.kind::f16\n2:4\n.kind::f8f6f4\n.kind::mxf8f6f4\n.kind::i8\n.kind::mxf4\n4:8 (in pairs)\n9.7.16.10.8.1.\nSparse\ntcgen05.mma.sp\nwith\n.kind::tf32\n\nFor\n.kind::tf32\n, matrix\nA\nis structured sparse at a granularity of\n1:2\n.\nIn other words, each chunk of two adjacent elements in a row of matrix\nA\nhas one\nzero and one non-zero element. Only the non-zero element is stored in memory and the\n4-bit index in the metadata indicates the position of the non-zero element in the\ntwo-wide chunk. The only meaningful values of the index are:\n0b1110\n0b0100\nRest of the values result in undefined behavior.\nFigure 259\nSparse tcgen05.mma metadata example for tf32 kind\n\n9.7.16.10.8.2.\nSparse\ntcgen05.mma.sp\nwith\n.kind::f16\n,\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n\nFor the following\n.kind\nvariants of\ntcgen05.mma\n:\n.kind::f16\n.kind::f8f8f4\n.kind::mxf8f6f4\n.kind::i8\nmatrix\nA\nis structured sparse at a granularity of\n2:4\n. In other words, each chunk\nof four adjacent elements in a row of matrix\nA\nhas two zero and two non-zero elements.\nOnly the non-zero elements are stored in memory and the two 2-bit indices in the metadata\nindicates the position of the two non-zero elements in the four-wide chunk. The only\nmeaningful values of the index are:\n0b0100\n0b1000\n0b1100\n0b1001\n0b1101\n0b0110\n0b1110\nFigure 260\nSparse tcgen05.mma metadata example for f16/f8f6f4/mxf8f6f4 kind\n\n9.7.16.10.8.3.\nSparse\ntcgen05.mma.sp\nwith\n.kind::mxf4\nand\n.kind::mxf4nvf4\n\nFor\n.kind::mxf4\nand\n.kind::mxf4nvf4\n, matrix\nA\nis pair-wise structured\nsparse at a granularity of\n4:8\n. In other words, each chunk of eight adjacent\nelements in a row of matrix\nA\nhas four zero and four non-zero elements. The\nzero and non-zero elements are clustered in sub-chunks of two elements each within\nthe eight-wide chunk, so each two-wide sub-chunk within the eight-wide chunk must be\nall zeros or all non-zeros. Only the four non-zero elements are stored in memory and\nthe two 2-bit indices in the metadata indicates the position of the two two-wide\nsub-chunks with non-zero values in the eight-wide chunk of a row of matrix\nA\n.\nThe only meaningful values of the index are:\n0b0100\n0b1000\n0b1100\n0b1001\n0b1101\n0b0110\n0b1110\nRest of the values result in undefined behavior.\nFigure 261\nSparse tcgen05.mma metadata example for mxf4 kind\n\n9.7.16.10.8.4.\nSparsity selector\n\nThe value of the sparsity selector selects the sub-columns in the Tensor Memory\nto form the sparsity metadata matrix, which is used with matrix\nA\nto form the\nmultiplicand matrix.\nThe following shows the sparse metadata matrix layout in Tensor Memory for various MMA variants:\n9.7.16.10.8.4.1.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f16\n\nFigure 262\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 262\nSparsity Metadata Layout for M = 64 for\n.kind::f16\n\n9.7.16.10.8.4.2.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f16\n\nFigure 263\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 263\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::f16\n\n9.7.16.10.8.4.3.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::tf32\n\nFigure 264\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 264\nSparsity Metadata Layout for M = 64 for\n.kind::tf32\n\n9.7.16.10.8.4.4.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::tf32\n\nFigure 265\nshows which sub-columns gets\nselected for different values of Sparsity Selector.\nFigure 265\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::tf32\n\n9.7.16.10.8.4.5.\nLayout of the Sparsity Metadata Matrix for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\nThe value of the sparsity selector:\nmust be 0 for\n.kind::i8\nand\n.kind::f8f6f4\nis assumed to be 0 for\n.kind::mxf8f6f4\n,\n.kind::mxf4\nand\n.kind::mxf4nvf4\nand all of the columns are selected as\nshown in\nFigure 266\nFigure 266\nSparsity Metadata Layout for M = 64 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\n9.7.16.10.8.4.6.\nLayout of the Sparsity Metadata Matrix for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\nThe value of the sparsity selector:\nmust be 0 for\n.kind::i8\nand\n.kind::f8f6f4\nis assumed to be 0 for\n.kind::mxf8f6f4\n,\n.kind::mxf4\nand\n.kind::mxf4nvf4\nand all of the columns are selected as\nshown in\nFigure 267\nFigure 267\nSparsity Metadata Layout for M = 128 / M = 256 for\n.kind::f8f6f4\n,\n.kind::mxf8f6f4\n,\n.kind::i8\n,\n.kind::mxf4\n,\n.kind::mxf4nvf4\n\n9.7.16.10.8.5.\nAlignment restriction\n\nThe layouts which utilize only half the datapath lanes as specified in\nData Path Layout Organization\n,\ni.e.\nLayout F\nand\nLayout C\n, must use the same alignment\nacross matrices A, D and the sparsity metadata matrix.\n9.7.16.10.9.\nTensorCore 5th Generation of MMA Instructions\n\n9.7.16.10.9.1.\nTensorCore 5th Generation Instructions:\ntcgen05.mma\n\ntcgen05.mma\nPerform the 5\nth\ngeneration of matrix multiply and accumulate operation.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.cta_group.kind   [d-tmem],  a-desc,  b-desc, idesc,\n                             { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind   [d-tmem], [a-tmem], b-desc, idesc,\n                             { disable-output-lane }, enable-input-d {, scale-input-d};\n\n.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.cta_group = { .cta_group::1, .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 2. Floating-point type with block scaling:\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}\n                                        [d-tmem],  a-desc,  b-desc, idesc,\n                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}\n                                        [d-tmem], [a-tmem], b-desc, idesc,\n                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.cta_group      = { .cta_group::1,   .cta_group::2 }\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n\n----------------------------------------------------------------------------------\n\n// 3. Convolution MMA for floating-point type without block scaling:\n\ntcgen05.mma.cta_group.kind.collector_usage [d-tmem],  a-desc,  b-desc, idesc,\n                                           { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,\n                                                    { disable-output-lane }, enable-input-d {, scale-input-d};\n\ntcgen05.mma.cta_group.kind.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                    { disable-output-lane }, enable-input-d {, scale-input-d};\n\n.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.cta_group = { .cta_group::1,   .cta_group::2 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 4. Activation Stationary MMA for floating-point type with block scaling:\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                            [d-tmem],  a-desc,  b-desc, idesc,\n                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                            [d-tmem], [a-tmem], b-desc, idesc,\n                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.cta_group       = { .cta_group::1,   .cta_group::2 }\n.scale_vectorsize  = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.kind            = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 5. Integer type:\n\ntcgen05.mma.cta_group.kind::i8  [d-tmem],  a-desc,  b-desc, idesc,\n                                { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8  [d-tmem], [a-tmem], b-desc, idesc,\n                                { disable-output-lane }, enable-input-d;\n\n.cta_group = { .cta_group::1,   .cta_group::2  }\n\n----------------------------------------------------------------------------------\n\n// 6. Convolution MMA for integer type:\n\ntcgen05.mma.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,\n                                                        { disable-output-lane }, enable-input-d;\n\n.cta_group       = { .cta_group::1,   .cta_group::2  }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\nDescription\nInstruction\ntcgen05.mma\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix\nmultiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMxK\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe optional immediate argument\nscale-input-d\ncan be specified to scale the input\nmatrix\nD\nas follows:\nD\n=\nA*B+D\n*\n(2\n^\n-\nscale-input-d)\nThe valid range of values for argument\nscale-input-d\nis [0, 15]. The argument\nscale-input-d\nis only valid for\n.kind::tf32\nand\n.kind::f16\n.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described\nin\nInstruction descriptor\n, specifies\nthe shapes, exact types, sparsity and other details of the input matrices,\noutput matrix and the matrix multiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and\naccumulate operation is performed on the\nTensor Memory\nof the\nexecuting thread’s CTA only. The qualifier\n.cta_group::2\nspecifies that the matrix\nmultiply and accumulate operation is performed on the\nTensor Memory\nof the executing thread’s CTA and its\npeer CTA\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n.\nThe 64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which\nrepresent the matrices\nA\nand\nB\nin shared memory respectively. The format of the\nmatrix descriptor is described in\nMatrix Descriptors\n.\nThe vector operand\ndisable-output-lane\nspecifies the lane(s) in the\nTensor Memory\nthat should be not be updated with the resultant\nmatrix\nD\n. Elements of the vector operand\ndisable-output-lane\nforms a mask where\neach bit corresponds to a lane of the\nTensor Memory\n, with least\nsignificant bit of the first element of the vector (leftmost in syntax) corresponding\nto the lane 0 of the\nTensor Memory\n. If a bit in the mask is 1,\nthen the corresponding lane in the Tensor Memory for the resultant matrix\nD\nwill not\nbe updated. The size of the vector is as follows:\n.cta_group\nSize of the vector disable-output-lane\n::1\n4\n::2\n8\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply\nand accumulate operation as specified in the section\nBlock Scaling for tcgen05.mma.sync\n.\nThe address operand\nscale-A-tmem\nand\nscale-B-tmem\nspecify the base address the\nmatrices\nscale_A\nand\nscale_B\nrespectively in the\nTensor Memory\n.\nFor qualifier\n.scale_vectorsize\n,\nIf\n.scale_vec::NX\nis specified: N specifies the number of columns in\nscale_A\nmatrix and number of rows in\nscale_B\nmatrix.\nIf\n.blockN\nis specified: N specifies the block size for which single scale factor\nwill be applied. In this form, value of N is same as the K-dimension / (N of\n.scale_vec::NX\n).\nAliased\n.scale_vectorsize\nvariants:\n.block16\nis aliased with:\n.scale_vec::4X\nwhen\n.kind\n=\n.kind::mxf4nvf4\nand K = 64 or 128\n.block32\nis aliased with:\n.scale_vec::1X\nwhen\n.kind\n=\n.kind::mxf8f6f4\nfor all supported values of K\n.scale_vec::2X\nwhen\n.kind\n=\n.kind::mxf4\nor\n.kind::mxf4nvf4\nand K = 64 or 128\nThe valid combinations of MMA-kind and\n.scale_vectorsize\nare\ndescribed in\nTable 56\n. For\n.kind::mxf4\nwhen the qualifier\n.scale_vectorsize\nis not specified, then it defaults to\n.block32\n. For\n.kind::mxf4nvf4\n,\nthe qualifier\n.scale_vectorsize\nmust be explicitly specified.\nThe qualifier\n.ashift\nshifts the rows of the\nA\nmatrix down by one row, except for\nthe last row in the\nTensor Memory\n. Qualifier\n.ashift\nis only allowed\nwith\nM\n= 128 or\nM\n= 256.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for matrix\nA\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::a::fill\nSpecifies that the\nA\nmatrix read from the memory\nshould be filled in collector buffer.\n.collector::a::use\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer. This requires a previous fill to\nthe collector buffer to be still valid.\n.collector::a::lastuse\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer and the contents of the collector\nbuffer can be discarded. This requires a previous\nfill to the collector buffer to be valid till the\ncollector buffer is read.\n.collector::a::discard\nSpecifies that the contents of the collector buffer\nfor\nA\ncan be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::a::discard\n.\nIt is illegal to specify either of\n.collector::a::use\nor\n.collector::a::fill\nalong with\n.ashift\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nQualifier\n.kind::mxf4nvf4\nintroduced in PTX ISA version 8.7.\nQualifiers\n.block16\nand\n.block32\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nArgument\nscale-input-d\nrequires\nsm_100a\nand is supported on\nsm_100f\nor higher in the same family from PTX ISA version 8.8.\nFor\n.scale_vectorsize\n,\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\nrequires\nsm_100a\n.\n.block16\n,\n.block32\nrequires\nsm_100f\nor\nsm_110f\n.\nFor Target ISA details on matrix shape, check\nTarget ISA Note\n.\nFor Target ISA details on shared memory descriptor, check\nTarget ISA Note\n.\nExamples\ntcgen05.mma.cta_group::1.kind::tf32      [taddr0],  adesc,  bdesc, idesc, {m0, m1, m2, m3}, p;\ntcgen05.mma.cta_group::1.kind::mxf8f6f4  [taddr2],  [taddr1],  bdesc, idesc,\n                                         [tmem_scaleA], [tmem_scaleB], p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.2.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.sp\n\ntcgen05.mma.sp\nPerform the 5\nth\ngeneration of matrix multiply and accumulate operation with sparse\nA\nmatrix.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.sp.cta_group.kind  [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] ,  idesc,\n                               { disable-output-lane }, enable-input-d{, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind  [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,\n                               { disable-output-lane }, enable-input-d{, scale-input-d};\n\n.kind       = { .kind::f16, , .kind::tf32, .kind::f8f6f4 }\n.cta_group  = { .cta_group::1,  .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 2. Floating-point type with block scaling:\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}\n                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}\n                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.cta_group      = { .cta_group::1,  .cta_group::2 }\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n\n----------------------------------------------------------------------------------\n\n// 3. Convolution MMA with floating-point type without block scaling:\n\ntcgen05.mma.sp.cta_group.kind.collector_usage           [d-tmem],  a-desc,  b-desc,\n                                                        [sp-meta-tmem] ,  idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind.ashift{.collector_usage}  [d-tmem], [a-tmem], b-desc,\n                                                        [sp-meta-tmem] , idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\ntcgen05.mma.sp.cta_group.kind{.ashift}.collector_usage  [d-tmem], [a-tmem], b-desc,\n                                                        [sp-meta-tmem] , idesc,\n                                                        { disable-output-lane }, enable-input-d\n                                                        {, scale-input-d};\n\n.kind            = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 4. Activation Stationary MMA with floating-point type with block scaling:\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage\n                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,\n                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;\n\n.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }\n.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\n\n----------------------------------------------------------------------------------\n\n// 5. Integer type:\n\ntcgen05.mma.sp.cta_group.kind::i8 [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] , idesc,\n                                  { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8 [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,\n                                  { disable-output-lane }, enable-input-d;\n\n.cta_group      = { .cta_group::1,  .cta_group::2 }\n\n----------------------------------------------------------------------------------\n\n// 6. Convolution MMA with Integer type:\n\ntcgen05.mma.sp.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc,\n                                                           [sp-meta-tmem] , idesc,\n                                                           { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                           [sp-meta-tmem], idesc ,\n                                                           { disable-output-lane }, enable-input-d;\n\ntcgen05.mma.sp.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc,\n                                                           [sp-meta-tmem], idesc ,\n                                                           { disable-output-lane }, enable-input-d;\n\n.collector_usage = { .collector::buffer::op }\n::buffer         = { ::a }\n::op             = { ::fill, ::use, ::lastuse, ::discard* }\nDescription\nInstruction\ntcgen05.mma.sp\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix multiply and accumulate operation of the form\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMx(K/2)\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nSparse Matrices\ndescribes the details of the sparsity.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe optional immediate argument\nscale-input-d\ncan be specified to scale the\ninput matrix\nD\nas follows:\nD\n=\nA*B+D\n*\n(2\n^\n-\nscale-input-d)\nThe valid range of values for argument\nscale-input-d\nis [0, 15]. The argument\nscale-input-d\nis only valid for\n.kind::tf32\nand\n.kind::f16\n.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes,\nexact types, sparsity and other details of the input matrices, output matrix and the\nmatrix multiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate\noperation is performed on the\nTensor Memory\nof the executing\nthread’s CTA only. The qualifier\n.cta_group::2\nspecifies that the matrix\nmultiply and accumulate operation is performed on the\nTensor Memory\nof the executing thread’s CTA and its\npeer CTA\n.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.sp\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.sp\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n. The\n64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which represent\nthe matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor\nis described in\nMatrix Descriptors\n.\nThe vector operand\ndisable-output-lane\nspecifies the lane(s) in the\nTensor Memory\nthat should be not be updated with the resultant matrix\nD\n. Elements of the vector operand\ndisable-output-lane\nforms a mask where each bit corresponds to a lane of the\nTensor Memory\n. with least significant bit of the first element of\nthe vector (leftmost in syntax) corresponding to the lane 0 of the Tensor Memory. If a bit in\nthe mask is 1, then the corresponding lane in the Tensor Memory for the resultant matrix\nD\nwill not be updated. The size of the vector is as follows:\n.cta_group\nSize of the vector disable-output-lane\n::1\n4\n::2\n8\nQualifier\n.block_scale\nspecifies that the matrices\nA\nand\nB\nare scaled with\nscale_A\nand\nscale_B\nmatrices respectively before performing the matrix multiply\nand accumulate operation as specified in the section\nBlock Scaling for tcgen05.mma.sync\n.\nThe address operand\nscale-A-tmem\nand\nscale-B-tmem\nspecify the base address the\nmatrices\nscale_A\nand\nscale_B\nrespectively in the\nTensor Memory\n.\nFor qualifier\n.scale_vectorsize\n,\nIf\n.scale_vec::NX\nis specified: N specifies the number of columns in\nscale_A\nmatrix and number of rows in\nscale_B\nmatrix.\nIf\n.blockN\nis specified: N specifies the block size for which single scale factor\nwill be applied. In this form, value of N is same as the K-dimension / (N of\n.scale_vec::NX\n).\nAliased\n.scale_vectorsize\nvariants:\n.block16\nis aliased with:\n.scale_vec::4X\nwhen\n.kind\n=\n.kind::mxf4nvf4\nand K = 64 or 128\n.block32\nis aliased with:\n.scale_vec::1X\nwhen\n.kind\n=\n.kind::mxf8f6f4\nfor all supported values of K\n.scale_vec::2X\nwhen\n.kind\n=\n.kind::mxf4\nor\n.kind::mxf4nvf4\nand K = 64 or 128\nThe valid combinations of MMA-kind and\n.scale_vectorsize\nare\ndescribed in\nTable 56\n. For\n.kind::mxf4\nwhen the qualifier\n.scale_vectorsize\nis not specified, then it defaults to\n.block32\n. For\n.kind::mxf4nvf4\n,\nthe qualifier\n.scale_vectorsize\nmust be explicitly specified.\nThe qualifier\n.ashift\nshifts the rows of the\nA\nmatrix down by one row, except for\nthe last row in the\nTensor Memory\n. Qualifier\n.ashift\nis only allowed\nwith\nM\n= 128 or\nM\n= 256.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for matrix\nA\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::a::fill\nSpecifies that the\nA\nmatrix read from the memory\nshould be filled in collector buffer.\n.collector::a::use\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer. This requires a previous fill to\nthe collector buffer to be still valid.\n.collector::a::lastuse\nSpecifies that the\nA\nmatrix can be read from the\ncollector buffer and the contents of the collector\nbuffer can be discarded. This requires a previous\nfill to the collector buffer to be valid till the\ncollector buffer is read.\n.collector::a::discard\nSpecifies that the contents of the collector buffer\nfor\nA\ncan be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::a::discard\n.\nIt is illegal to specify either of\n.collector::a::use\nor\n.collector::a::fill\nalong with\n.ashift\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nQualifier\n.kind::mxf4nvf4\nintroduced in PTX ISA version 8.7.\nQualifiers\n.block16\nand\n.block32\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n/\n.kind::mxf4nvf4\n/\n.kind::mxf4\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nQualifiers\n.kind::mxf4nvf4\nand\n.kind::mxf4\nare supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_103a\nsm_110a\nArgument\nscale-input-d\nrequires\nsm_100a\nand is supported on\nsm_100f\nor higher in the same family from PTX ISA version 8.8.\nFor\n.scale_vectorsize\n,\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\nrequires\nsm_100a\n.\n.block16\n,\n.block32\nrequires\nsm_100f\nor\nsm_110f\n.\nFor Target ISA details on matrix shape, check\nTarget ISA Note\n.\nFor Target ISA details on shared memory descriptor, check\nTarget ISA Note\n.\nExamples\ntcgen05.mma.sp.cta_group::1.kind::f16      [taddr0],  adesc,  bdesc, [tmem_spmeta0], idesc, p;\n\ntcgen05.mma.sp.cta_group::1.kind::mxf8f6f4.collector::a:fill\n                                           [taddr2],  [taddr1],  bdesc, [tmem_spmeta1], idesc,\n                                           [tmem_scaleA], [tmem_scaleB], p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.3.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.ws\n\ntcgen05.mma.ws\nPerform the 5\nth\ngeneration of weight stationary convolution matrix multiply and accumulate\noperation.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem],  a-desc,  b-desc,  idesc,\n                                                      enable-input-d {, zero-column-mask-desc };\n\ntcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem], [a-tmem], b-desc, idesc,\n                                                      enable-input-d {, zero-column-mask-desc };\n\n.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n\n----------------------------------------------------------------------------------\n\n// 2. Integer type:\n\ntcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem],  a-desc,  b-desc, idesc,\n                                                       enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,\n                                                       enable-input-d {, zero-column-mask-desc};\n\n.collector_usage = { .collector::buffer::op }\n::buffer = { ::b0, ::b1, ::b2, ::b3 }\n::op   = { ::fill, ::use, ::lastuse, ::discard}\nDescription\nInstruction\ntcgen05.mma.ws\nis an asynchronous instruction which initiates an\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMxK\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix is\nMxN\n.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes, exact\ntypes, sparsity and other details of the input matrices, output matrix and the matrix\nmultiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate operation\nis performed on the\nTensor Memory\nof the executing thread’s CTA only.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.ws\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.ws\nwill result in the initiation of the whole matrix multiply and accumulate\noperation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind\nare specified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies the address of the matrix\nA\nin the\nTensor Memory\n. The\n64-bit register operand\na-desc\nand\nb-desc\nare the matrix descriptors which represent\nthe matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor\nis described in\nMatrix Descriptors\n.\nThe optional operand\nzero-column-mask-desc\nis a 64-bit register which specifies the\nZero-Column Mask Descriptor\n. The zero-column\nmask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix\nwill have zero value for the matrix multiply and accumulate operation regardless of the\nvalues present in the shared memory.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for Matrix\nB\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::bN::fill\nSpecifies that the\nB\nmatrix read from the memory\nshould be filled in collector buffer #N.\n.collector::bN::use\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N. This requires a previous fill\nto the collector buffer #N to be still valid.\n.collector::bN::lastuse\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N after which the contents of the\ncollector buffer #N can be discarded. This requires\na previous fill to the collector buffer #N to be\nvalid till the collector buffer #N is read.\n.collector::bN::discard\nSpecifies that the contents of the collector buffer\n#N can be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::b0::discard\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nExamples\ntcgen05.mma.ws.cta_group::1.kind::i8.collector::b2:use [taddr2], [taddr1], bdesc, idesc, p;\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.10.9.4.\nTensorCore 5th Generation Instructions:\ntcgen05.mma.ws.sp\n\ntcgen05.mma.ws.sp\nPerform the 5\nth\ngeneration of weight stationary convolution matrix multiply and accumulate\noperation with sparse\nA\nmatrix.\nSyntax\n// 1. Floating-point type without block scaling:\n\ntcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem],  a-desc,  b-desc,\n                                                      [sp-meta-tmem] ,  idesc,\n                                                      enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                      [sp-meta-tmem] , idesc,\n                                                      enable-input-d {, zero-column-mask-desc};\n\n.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }\n\n----------------------------------------------------------------------------------\n\n// 2. Integer type:\n\ntcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], a-desc, b-desc,\n                                                          [sp-meta-tmem] , idesc,\n                                                          enable-input-d {, zero-column-mask-desc};\n\ntcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc,\n                                                          [sp-meta-tmem] , idesc,\n                                                          enable-input-d {, zero-column-mask-desc};\n\n.collector_usage = { .collector::buffer::op }\n::buffer = { ::b0, ::b1, ::b2, ::b3 }\n::op   = { ::fill, ::use, ::lastuse, ::discard}\nDescription\nInstruction\ntcgen05.mma.ws.sp\nis an asynchronous instruction which initiates\nan\nMxNxK\nmatrix multiply and accumulate operation,\nD\n=\nA*B+D\nwhere the\nA\nmatrix is\nMx(K/2)\n, the\nB\nmatrix is\nKxN\n, and the\nD\nmatrix\nis\nMxN\n.\nSparse Matrices\ndescribes the details of the\nsparsity.\nThe operation of the form\nD\n=\nA*B\nis issued when the input predicate argument\nenable-input-d\nis false.\nThe 32-bit register operand\nidesc\nis the instruction descriptor as described in\nInstruction descriptor\n, specifies the shapes, exact\ntypes, sparsity and other details of the input matrices, output matrix and the matrix\nmultiply and accumulate operation.\nThe qualifier\n.cta_group::1\nspecifies that the matrix multiply and accumulate\noperation is performed on the Tensor Memory of the executing thread’s CTA only.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe instruction\ntcgen05.mma.ws.sp\nhas single thread semantics, unlike the collective\ninstructions\nmma.sync\nor\nwgmma.mma_async\n. So, a single thread issuing the\ntcgen05.mma.ws.sp\nwill result in the initiation of the whole matrix multiply and\naccumulate operation. Refer to the section\nIssue Granularity\n.\nThe qualifier\n.kind\nspecifies the general kind of the element types of the multiplicand\nmatrices. The exact types of the elements of the input and output matrices for each MMA-kind are\nspecified in the\nInstruction descriptor\n.\nThe address operand\nd-tmem\nspecifies the address of the destination and the accumulation\nmatrix\nD\nin the\nTensor Memory\n. The address operand\na-tmem\nspecifies\nthe address of the matrix\nA\nin the\nTensor Memory\n. The 64-bit register\noperand\na-desc\nand\nb-desc\nare the matrix descriptors which represent the matrices\nA\nand\nB\nin shared memory respectively. The format of the matrix descriptor is described in\nMatrix Descriptors\n.\nThe optional operand\nzero-column-mask-desc\nis a 64-bit register which specifies the\nZero-Column Mask Descriptor\n. The zero-column\nmask descriptor is used to generate a mask that specifies which columns of\nB\nmatrix\nwill have zero value for the matrix multiply and accumulate operation regardless of the\nvalues present in the shared memory.\nThe qualifier\n.collector_usage\nspecifies the usage of collector buffer for Matrix\nB\n.\nFollowing collector buffer operations can be specified:\n.collector_usage\nSemantics\n.collector::bN::fill\nSpecifies that the\nB\nmatrix read from the memory\nshould be filled in collector buffer #N.\n.collector::bN::use\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N. This requires a previous fill\nto the collector buffer #N to be still valid.\n.collector::bN::lastuse\nSpecifies that the\nB\nmatrix can be read from the\ncollector buffer #N after which the contents of the\ncollector buffer #N can be discarded. This requires\na previous fill to the collector buffer #N to be\nvalid till the collector buffer #N is read.\n.collector::bN::discard\nSpecifies that the contents of the collector buffer\n#N can be discarded.\nIf no\n.collector_usage\nqualifier is specified, then it defaults to\n.collector::b0::discard\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8 except\n.kind::i8\n:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nQualifier\n.kind::i8\nis supported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_110a\nExamples\ntcgen05.mma.ws.sp.cta_group::1.kind::tf32.collector::b1::fill  [taddr1], [taddr0], bdesc,\n                                                               [tmem_spmeta0], idesc, p;\n\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj0], 0;\n@!p bra loop;\n9.7.16.11.\nTensorCore 5th Generation Specialized Synchronization Operations\n\n9.7.16.11.1.\nTensorCore 5th Generation Instructions:\ntcgen05.fence\n\ntcgen05.fence\nSpecialized fence for the asynchronous tcgen05 operations.\nSyntax\ntcgen05.fence::before_thread_sync ;\ntcgen05.fence::after_thread_sync  ;\nDescription\nThe instruction\ntcgen05.fence::before_thread_sync\norders all the prior asynchronous\ntcgen05\noperations with respect to the subsequent\ntcgen05\nand the execution\nordering operations.\nThe instruction\ntcgen05.fence::after_thread_sync\norders all the subsequent asynchronous\ntcgen05\noperations with respect to the prior\ntcgen05\nand the execution ordering\noperations.\nThe\ntcgen05.fence::*\ninstructions compose with execution ordering instructions across\na thread scope and provide ordering between\ntcgen05\ninstructions across the same scope.\nThe\ntcgen05.fence::before_thread_sync\ninstructions behave as code motion fence for prior\ntcgen05\ninstructions as they cannot be hoisted across. The\ntcgen05.fence::after_thread_sync\ninstructions behave as code motion fence for subsequent\ntcgen05\ninstructions as they cannot\nbe hoisted across.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\n// Producer thread:\n\ntcgen05.cp.cta_group::1.128x256b  [taddr0], sdesc0;\n\ntcgen05.fence::before_thread_sync;\nst.relaxed.b32 [flag], 1;\n\n// Consumer thread:\n\nloop:\nld.relaxed.b32 r, [flag];\nsetp.eq.u32 p, r, 1;\n@!p bra loop;\n\ntcgen05.fence::after_thread_sync;\ntcgen05.mma.cta_group.kind   [taddr0], adesc, bdesc, idesc, p;\n9.7.16.12.\nTensorCore 5th Generation Async Synchronization Operations\n\n9.7.16.12.1.\nTensorCore 5th Generation Instructions:\ntcgen05.commit\n\ntcgen05.commit\nMakes the mbarrier object track the completion of all prior async-tcgen05 operations initiated\nby the executing thread.\nSyntax\ntcgen05.commit.cta_group.completion_mechanism{.shared::cluster}{.multicast}.b64\n                                                            [mbar] {, ctaMask};\n\n.completion_mechanism = { .mbarrier::arrive::one }\n.cta_group            = { .cta_group::1, .cta_group::2 }\n.multicast            = { .multicast::cluster }\nDescription\nThe instruction\ntcgen05.commit\nis an asynchronous instruction which makes the mbarrier object,\nspecified by the address operand\nmbar\n, track the completion of all the prior asynchronous\ntcgen05\noperations, as listed in\nmbarrier based completion mechanism\n,\ninitiated by the executing thread. Upon the completion of the tracked asynchronous\ntcgen05\noperations, the signal specified by the\n.completion_mechanism\nis triggered by the system\non the mbarrier object.\nThis instruction accesses its\nmbarrier\noperand using generic-proxy.\nThe instruction\ntcgen05.commit.cta_group::1\ntracks for the completion of all prior\nasynchronous\ntcgen05\noperations with\n.cta_group::1\nissued by the current thread.\nSimilarly, the instruction\ntcgen05.commit.cta_group::2\ntracks for the completion of all\nprior asynchronous\ntcgen05\noperations with\n.cta_group::2\nissued by the current thread.\nAll\ntcgen05\ninstructions within a kernel must specify the same value for the\n.cta_group\nqualifier.\nThe qualifier\n.mbarrier::arrive::one\nindicates that upon the completion of the prior\nasynchronous\ntcgen05\noperation issued by the current thread, an arrive-on operation, with\nthe count argument of 1, is signaled on the mbarrier object. The scope of the arrive-on operation\nis the cluster scope.\nThe optional qualifier\n.multicast::cluster\nallows signaling on the mbarrier objects of multiple\nCTAs in the cluster. Operand\nctaMask\nspecifies the CTAs in the cluster such that each bit\nposition in the 16-bit\nctaMask\noperand corresponds to the\n%cluster_ctarank\nof the destination\nCTA. The mbarrier signal is multicast to the same offset as\nmbar\nin the shared memory of each\ndestination CTA.\nIf no state space is specified then\nGeneric Addressing\nis used. If the\naddress specified by\nmbar\ndoes not fall within the address window of\n.shared::cluster\nstate\nspace then the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.6.\nTarget ISA Notes\nSupported on following architectures:\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_110f\nor higher in the same family\nExamples\nExample 1:\ntcgen05.cp.cta_group::1.128x256b                      [taddr0], sdesc0;\ntcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj1];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj1], 0;\n@!p bra loop;\n\nExample 2:\ntcgen05.mma.cta_group::2.kind::tf32    [taddr0],  adesc,  bdesc, idesc, p;\ntcgen05.commit.cta_group::2.mbarrier::arrive::one.b64 [mbarObj2];\n\nloop:\nmbarrier.try_wait.parity.b64 p, [mbarObj2], 0;\n@!p bra loop;\n9.7.17.\nStack Manipulation Instructions\n\nThe stack manipulation instructions can be used to dynamically allocate and deallocate memory on the\nstack frame of the current function.\nThe stack manipulation instrucitons are:\nstacksave\nstackrestore\nalloca\n9.7.17.1.\nStack Manipulation Instructions:\nstacksave\n\nstacksave\nSave the value of stack pointer into a register.\nSyntax\nstacksave.type  d;\n\n.type = { .u32, .u64 };\nDescription\nCopies the current value of stack pointer into the destination register\nd\n. Pointer returned by\nstacksave\ncan be used in a subsequent\nstackrestore\ninstruction to restore the stack\npointer. If\nd\nis modified prior to use in\nstackrestore\ninstruction, it may corrupt data in\nthe stack.\nDestination operand\nd\nhas the same type as the instruction type.\nSemantics\nd = stackptr;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nstacksave\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 rd;\nstacksave.u32 rd;\n\n.reg .u64 rd1;\nstacksave.u64 rd1;\n9.7.17.2.\nStack Manipulation Instructions:\nstackrestore\n\nstackrestore\nUpdate the stack pointer with a new value.\nSyntax\nstackrestore.type  a;\n\n.type = { .u32, .u64 };\nDescription\nSets the current stack pointer to source register\na\n.\nWhen\nstackrestore\nis used with operand\na\nwritten by a prior\nstacksave\ninstruction, it\nwill effectively restore the state of stack as it was before\nstacksave\nwas executed. Note that\nif\nstackrestore\nis used with an arbitrary value of\na\n, it may cause corruption of stack\npointer. This implies that the correct use of this feature requires that\nstackrestore.type\na\nis\nused after\nstacksave.type\na\nwithout redefining the value of\na\nbetween them.\nOperand\na\nhas the same type as the instruction type.\nSemantics\nstackptr = a;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nstackrestore\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 ra;\nstacksave.u32 ra;\n// Code that may modify stack pointer\n...\nstackrestore.u32 ra;\n9.7.17.3.\nStack Manipulation Instructions:\nalloca\n\nalloca\nDynamically allocate memory on stack.\nSyntax\nalloca.type  ptr, size{, immAlign};\n\n.type = { .u32, .u64 };\nDescription\nThe\nalloca\ninstruction dynamically allocates memory on the stack frame of the current function\nand updates the stack pointer accordingly. The returned pointer\nptr\npoints to local memory and\ncan be used in the address operand of\nld.local\nand\nst.local\ninstructions.\nIf sufficient memory is unavailable for allocation on the stack, then execution of\nalloca\nmay\nresult in stack overflow. In such cases, attempting to access the allocated memory with\nptr\nwill\nresult in undefined program behavior.\nThe memory allocated by\nalloca\nis deallocated in the following ways:\nIt is automatically deallocated when the function exits.\nIt can be explicitly deallocated using\nstacksave\nand\nstackrestore\ninstructions:\nstacksave\ncan be used to save the value of stack pointer before executing\nalloca\n, and\nstackrestore\ncan be used after\nalloca\nto restore stack pointer to the original value which\nwas previously saved with\nstacksave\n. Note that accessing deallocated memory after executing\nstackrestore\nresults in undefined behavior.\nsize\nis an unsigned value which specifies the amount of memory in number of bytes to be\nallocated on stack.\nsize\n=\n0\nmay not lead to a valid memory allocation.\nBoth\nptr\nand\nsize\nhave the same type as the instruction type.\nimmAlign\nis a 32-bit value which specifies the alignment requirement in number of bytes for the\nmemory allocated by\nalloca\n. It is an integer constant, must be a power of 2 and must not exceed\n2^23.\nimmAlign\nis an optional argument with default value being 8 which is the minimum\nguaranteed alignment.\nSemantics\nalloca.type ptr, size, immAlign:\n\na = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment\n\n// Allocate size bytes of stack memory with alignment a and update the stack pointer.\n// Since the stack grows down, the updated stack pointer contains a lower address.\nstackptr = alloc_stack_mem(size, a);\n\n// Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory\n// allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).\nstacksave ptr;\nPTX ISA Notes\nIntroduced in PTX ISA version 7.3.\nTarget ISA Notes\nalloca\nrequires\nsm_52\nor higher.\nExamples\n.reg .u32 ra, stackptr, ptr, size;\n\nstacksave.u32 stackptr;     // Save the current stack pointer\nalloca ptr, size, 8;        // Allocate stack memory\nst.local.u32 [ptr], ra;     // Use the allocated stack memory\nstackrestore.u32 stackptr;  // Deallocate memory by restoring the stack pointer\n9.7.18.\nVideo Instructions\n\nAll video instructions operate on 32-bit register operands. However, the video instructions may be\nclassified as either scalar or SIMD based on whether their core operation applies to one or multiple\nvalues.\nThe video instructions are:\nvadd\n,\nvadd2\n,\nvadd4\nvsub\n,\nvsub2\n,\nvsub4\nvmad\nvavrg2\n,\nvavrg4\nvabsdiff\n,\nvabsdiff2\n,\nvabsdiff4\nvmin\n,\nvmin2\n,\nvmin4\nvmax\n,\nvmax2\n,\nvmax4\nvshl\nvshr\nvset\n,\nvset2\n,\nvset4\n9.7.18.1.\nScalar Video Instructions\n\nAll scalar video instructions operate on 32-bit register operands. The scalar video instructions\nare:\nvadd\nvsub\nvabsdiff\nvmin\nvmax\nvshl\nvshr\nvmad\nvset\nThe scalar video instructions execute the following stages:\nExtract and sign- or zero-extend byte, half-word, or word values from its source operands, to\nproduce signed 33-bit input values.\nPerform a scalar arithmetic operation to produce a signed 34-bit result.\nOptionally clamp the result to the range of the destination type.\nOptionally perform one of the following:\napply a second operation to the intermediate result and a third operand, or\ntruncate the intermediate result to a byte or half-word value and merge into a specified\nposition in the third operand to produce the final result.\nThe general format of scalar video instructions is as follows:\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}        d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.secop  d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}   d.dsel, a{.asel}, b{.bsel}, c;\n\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.secop = { .add, .min, .max };\nThe source and destination operands are all 32-bit registers. The type of each operand (\n.u32\nor\n.s32\n) is specified in the instruction type; all combinations of\ndtype\n,\natype\n, and\nbtype\nare valid. Using the\natype/btype\nand\nasel/bsel\nspecifiers, the input values are\nextracted and sign- or zero-extended internally to\n.s33\nvalues. The primary operation is then\nperformed to produce an\n.s34\nintermediate result. The sign of the intermediate result depends on\ndtype.\nThe intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging.\n.s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) {\n    if ( !sat )  return tmp;\n\n    switch ( dsel ) {\n        case .b0, .b1, .b2, .b3:\n            if ( sign )  return CLAMP( tmp, S8_MAX, S8_MIN );\n            else         return CLAMP( tmp, U8_MAX, U8_MIN );\n        case .h0, .h1:\n            if ( sign )  return CLAMP( tmp, S16_MAX, S16_MIN );\n            else         return CLAMP( tmp, U16_MAX, U16_MIN );\n        default:\n            if ( sign )  return CLAMP( tmp, S32_MAX, S32_MIN );\n            else         return CLAMP( tmp, U32_MAX, U32_MIN );\n    }\n}\nThis intermediate result is then optionally combined with the third source operand using a secondary\narithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the\nthird operand is based on\ndtype\n.\n.s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) {\n    switch ( secop ) {\n        .add:     return tmp + c;\n        .min:     return MIN(tmp, c);\n        .max      return MAX(tmp, c);\n        default:  return tmp;\n    }\n}\n.s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) {\n    switch ( dsel ) {\n        case .h0:  return ((tmp & 0xffff)        | (0xffff0000 & c);\n        case .h1:  return ((tmp & 0xffff) << 16) | (0x0000ffff & c);\n        case .b0:  return ((tmp & 0xff)          | (0xffffff00 & c);\n        case .b1:  return ((tmp & 0xff) <<  8)   | (0xffff00ff & c);\n        case .b2:  return ((tmp & 0xff) << 16)   | (0xff00ffff & c);\n        case .b3:  return ((tmp & 0xff) << 24)   | (0x00ffffff & c);\n        default:   return tmp;\n    }\n}\nThe lower 32-bits are then written to the destination operand.\n9.7.18.1.1.\nScalar Video Instructions:\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\n\nvadd\n,\nvsub\nInteger byte/half-word/word addition/subtraction.\nvabsdiff\nInteger byte/half-word/word absolute value of difference.\nvmin\n,\nvmax\nInteger byte/half-word/word minimum/maximum.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.btype{.sat}       d, a{.asel}, b{.bsel};\nvop.dtype.atype.btype{.sat}.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.btype{.sat}  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vadd, vsub, vabsdiff, vmin, vmax };\n.dtype = .atype = .btype = { .u32, .s32 };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nPerform scalar arithmetic operation with optional saturate, and optional secondary arithmetic operation or subword data merge.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\n\nswitch ( vop ) {\n    case vadd:     tmp = ta + tb;\n    case vsub:     tmp = ta - tb;\n    case vabsdiff: tmp = | ta - tb |;\n    case vmin:     tmp = MIN( ta, tb );\n    case vmax:     tmp = MAX( ta, tb );\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvadd\n,\nvsub\n,\nvabsdiff\n,\nvmin\n,\nvmax\nrequire\nsm_20\nor higher.\nExamples\nvadd.s32.u32.s32.sat      r1, r2.b0, r3.h0;\nvsub.s32.s32.u32.sat      r1, r2.h1, r3.h1;\nvabsdiff.s32.s32.s32.sat  r1.h0, r2.b0, r3.b2, c;\nvmin.s32.s32.s32.sat.add  r1, r2, r3, c;\n9.7.18.1.2.\nScalar Video Instructions:\nvshl\n,\nvshr\n\nvshl\n,\nvshr\nInteger byte/half-word/word left/right shift.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvop.dtype.atype.u32{.sat}.mode       d, a{.asel}, b{.bsel};\nvop.dtype.atype.u32{.sat}.mode.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvop.dtype.atype.u32{.sat}.mode  d.dsel, a{.asel}, b{.bsel}, c;\n\n vop   = { vshl, vshr };\n.dtype = .atype = { .u32, .s32 };\n.mode  = { .clamp, .wrap };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nvshl\nShift\na\nleft by unsigned amount in\nb\nwith optional saturate, and optional secondary\narithmetic operation or subword data merge. Left shift fills with zero.\nvshr\nShift\na\nright by unsigned amount in\nb\nwith optional saturate, and optional secondary\narithmetic operation or subword data merge. Signed shift fills with the sign bit, unsigned shift\nfills with zero.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a,atype, asel );\ntb = partSelectSignExtend( b, .u32, bsel );\nif ( mode == .clamp  && tb > 32 )  tb = 32;\nif ( mode == .wrap )                       tb = tb & 0x1f;\nswitch ( vop ){\n   case vshl:  tmp = ta << tb;\n   case vshr:  tmp = ta >> tb;\n}\n// saturate, taking into account destination type and merge operations\ntmp = optSaturate( tmp, sat, isSigned(dtype), dsel );\nd = optSecondaryOp( op2, tmp, c );  // optional secondary operation\nd = optMerge( dsel, tmp, c );       // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvshl\n,\nvshr\nrequire\nsm_20\nor higher.\nExamples\nvshl.s32.u32.u32.clamp  r1, r2, r3;\nvshr.u32.u32.u32.wrap   r1, r2, r3.h1;\n9.7.18.1.3.\nScalar Video Instructions:\nvmad\n\nvmad\nInteger byte/half-word/word multiply-accumulate.\nSyntax\n// 32-bit scalar operation\nvmad.dtype.atype.btype{.sat}{.scale}     d, {-}a{.asel}, {-}b{.bsel},\n                                         {-}c;\nvmad.dtype.atype.btype.po{.sat}{.scale}  d, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.scale = { .shr7, .shr15 };\nDescription\nCalculate\n(a*b)\n+\nc\n, with optional operand negates,\nplus one\nmode, and scaling.\nThe source operands support optional negation with some restrictions. Although PTX syntax allows\nseparate negation of the\na\nand\nb\noperands, internally this is represented as negation of the\nproduct\n(a*b)\n. That is,\n(a*b)\nis negated if and only if exactly one of\na\nor\nb\nis\nnegated. PTX allows negation of either\n(a*b)\nor\nc\n.\nThe plus one mode (\n.po\n) computes\n(a*b)\n+\nc\n+\n1\n, which is used in computing averages. Source\noperands may not be negated in\n.po\nmode.\nThe intermediate result of\n(a*b)\nis unsigned if atype and btype are unsigned and the product\n(a*b)\nis not negated; otherwise, the intermediate result is signed. Input\nc\nhas the same\nsign as the intermediate result.\nThe final result is unsigned if the intermediate result is unsigned and\nc\nis not negated.\nDepending on the sign of the\na\nand\nb\noperands, and the operand negates, the following\ncombinations of operands are supported for VMAD:\n(u32 * u32) + u32  // intermediate unsigned; final unsigned\n-(u32 * u32) + s32  // intermediate   signed; final   signed\n (u32 * u32) - u32  // intermediate unsigned; final   signed\n (u32 * s32) + s32  // intermediate   signed; final   signed\n-(u32 * s32) + s32  // intermediate   signed; final   signed\n (u32 * s32) - s32  // intermediate   signed; final   signed\n (s32 * u32) + s32  // intermediate   signed; final   signed\n-(s32 * u32) + s32  // intermediate   signed; final   signed\n (s32 * u32) - s32  // intermediate   signed; final   signed\n (s32 * s32) + s32  // intermediate   signed; final   signed\n-(s32 * s32) + s32  // intermediate   signed; final   signed\n (s32 * s32) - s32  // intermediate   signed; final   signed\nThe intermediate result is optionally scaled via right-shift; this result is sign-extended if the\nfinal result is signed, and zero-extended otherwise.\nThe final result is optionally saturated to the appropriate 32-bit range based on the type (signed\nor unsigned) of the final result.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\nsignedFinal = isSigned(atype) || isSigned(btype) ||\n                                 (a.negate ^ b.negate) || c.negate;\ntmp[127:0] = ta * tb;\n\nlsb = 0;\nif ( .po )                  {              lsb = 1; } else\nif ( a.negate ^ b.negate )  { tmp = ~tmp;  lsb = 1; } else\nif ( c.negate )             { c   = ~c;    lsb = 1; }\n\nc128[127:0] = (signedFinal) sext32( c ) : zext ( c );\ntmp = tmp + c128 + lsb;\nswitch( scale ) {\n   case .shr7:   result = (tmp >>  7) & 0xffffffffffffffff;\n   case .shr15:  result = (tmp >> 15) & 0xffffffffffffffff;\n}\nif ( .sat ) {\n     if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN);\n     else             result = CLAMP(result, U32_MAX, U32_MIN);\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvmad\nrequires\nsm_20\nor higher.\nExamples\nvmad.s32.s32.u32.sat    r0, r1, r2, -r3;\nvmad.u32.u32.u32.shr15  r0, r1.h0, r2.h0, r3;\n9.7.18.1.4.\nScalar Video Instructions:\nvset\n\nvset\nInteger byte/half-word/word comparison.\nSyntax\n// 32-bit scalar operation, with optional secondary operation\nvset.atype.btype.cmp       d, a{.asel}, b{.bsel};\nvset.atype.btype.cmp.op2   d, a{.asel}, b{.bsel}, c;\n\n// 32-bit scalar operation, with optional data merge\nvset.atype.btype.cmp  d.dsel, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };\n.op2   = { .add, .min, .max };\nDescription\nCompare input values using specified comparison, with optional secondary arithmetic operation or\nsubword data merge.\nThe intermediate result of the comparison is always unsigned, and therefore destination\nd\nand\noperand\nc\nare also unsigned.\nSemantics\n// extract byte/half-word/word and sign- or zero-extend\n// based on source operand type\nta = partSelectSignExtend( a, atype, asel );\ntb = partSelectSignExtend( b, btype, bsel );\ntmp = compare( ta, tb, cmp ) ? 1 : 0;\nd = optSecondaryOp( op2, tmp, c );    // optional secondary operation\nd = optMerge( dsel, tmp, c );         // optional merge with c operand\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nvset\nrequires\nsm_20\nor higher.\nExamples\nvset.s32.u32.lt    r1, r2, r3;\nvset.u32.u32.ne    r1, r2, r3.h1;\n9.7.18.2.\nSIMD Video Instructions\n\nThe SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values.\nThe SIMD video instructions are:\nvadd2\n,\nvadd4\nvsub2\n,\nvsub4\nvavrg2\n,\nvavrg4\nvabsdiff2\n,\nvabsdiff4\nvmin2\n,\nvmin4\nvmax2\n,\nvmax4\nvset2\n,\nvset4\nPTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit\nvalues. The SIMD video instructions execute the following stages:\nForm input vectors by extracting and sign- or zero-extending byte or half-word values from the\nsource operands, to form pairs of signed 17-bit values.\nPerform a SIMD arithmetic operation on the input pairs.\nOptionally clamp the result to the appropriate signed or unsigned range, as determinted by the\ndestination type.\nOptionally perform one of the following:\nperform a second SIMD merge operation, or\napply a scalar accumulate operation to reduce the intermediate SIMD results to a single\nscalar.\nThe general format of dual half-word SIMD video instructions is as follows:\n// 2-way SIMD operation, with second SIMD merge or accumulate\nvop2.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };\n.asel  = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } };\nThe general format of quad byte SIMD video instructions is as follows:\n// 4-way SIMD operation, with second SIMD merge or accumulate\nvop4.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\nThe source and destination operands are all 32-bit registers. The type of each operand (\n.u32\nor\n.s32\n) is specified in the instruction type; all combinations of\ndtype\n,\natype\n, and\nbtype\nare valid. Using the\natype/btype\nand\nasel/bsel\nspecifiers, the input values are\nextracted and sign- or zero-extended internally to\n.s33\nvalues. The primary operation is then\nperformed to produce an\n.s34\nintermediate result. The sign of the intermediate result depends on\ndtype\n.\nThe intermediate result is optionally clamped to the range of the destination type (signed or\nunsigned), taking into account the subword destination size in the case of optional data merging.\n9.7.18.2.1.\nSIMD Video Instructions:\nvadd2\n,\nvsub2\n,\nvavrg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\n\nvadd2\n,\nvsub2\nInteger dual half-word SIMD addition/subtraction.\nvavrg2\nInteger dual half-word SIMD average.\nvabsdiff2\nInteger dual half-word SIMD absolute value of difference.\nvmin2\n,\nvmax2\nInteger dual half-word SIMD minimum/maximum.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvop2.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop2.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n vop2  = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 };\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32\nDescription\nTwo-way SIMD parallel arithmetic operation with secondary operation.\nElements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected half-words are then operated on in parallel.\nThe results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.\nFor instructions with a secondary SIMD merge operation:\nFor half-word positions indicated in mask, the selected half-word results are copied into\ndestination\nd\n. For all other positions, the corresponding half-word from source operand\nc\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor half-word positions indicated in mask, the selected half-word results are added to operand\nc\n, producing a result in\nd\n.\nSemantics\n// extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\n\nfor (i=0; i<2; i++) {\n    switch ( vop2 ) {\n       case vadd2:             t[i] = Va[i] + Vb[i];\n       case vsub2:             t[i] = Va[i] - Vb[i];\n       case vavrg2:            if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n       case vabsdiff2:         t[i] = | Va[i] - Vb[i] |;\n       case vmin2:             t[i] = MIN( Va[i], Vb[i] );\n       case vmax2:             t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S16_MAX, S16_MIN );\n        else                   t[i] = CLAMP( t[i], U16_MAX, U16_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvadd2\n,\nvsub2\n,\nvarvg2\n,\nvabsdiff2\n,\nvmin2\n,\nvmax2\nrequire\nsm_30\nor higher.\nExamples\nvadd2.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub2.s32.s32.s32.sat  r1.h0, r2.h10, r3.h32, r1;\nvmin2.s32.u32.u32.add  r1.h10, r2.h00, r3.h22, r1;\n9.7.18.2.2.\nSIMD Video Instructions:\nvset2\n\nvset2\nInteger dual half-word SIMD comparison.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvset2.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset2.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .h0, .h1, .h10 };  // defaults to .h10\n.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };\n   .asel defaults to .h10\n   .bsel defaults to .h32\nDescription\nTwo-way SIMD parallel comparison with secondary operation.\nElements of each dual half-word source to the operation are selected from any of the four half-words\nin the two source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected half-words are then compared in parallel.\nThe intermediate result of the comparison is always unsigned, and therefore the half-words of\ndestination\nd\nand operand\nc\nare also unsigned.\nFor instructions with a secondary SIMD merge operation:\nFor half-word positions indicated in mask, the selected half-word results are copied into\ndestination\nd\n. For all other positions, the corresponding half-word from source operand\nb\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor half-word positions indicated in mask, the selected half-word results are added to operand\nc\n, producing\na\nresult in\nd\n.\nSemantics\n// extract pairs of half-words and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_2( a, b, .asel, .atype );\nVb = extractAndSignExt_2( a, b, .bsel, .btype );\nVc = extractAndSignExt_2( c );\nfor (i=0; i<2; i++) {\n    t[i] = compare( Va[i], Vb[i], .cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvset2\nrequires\nsm_30\nor higher.\nExamples\nvset2.s32.u32.lt      r1, r2, r3, r0;\nvset2.u32.u32.ne.add  r1, r2, r3, r0;\n9.7.18.2.3.\nSIMD Video Instructions:\nvadd4\n,\nvsub4\n,\nvavrg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\n\nvadd4\n,\nvsub4\nInteger quad byte SIMD addition/subtraction.\nvavrg4\nInteger quad byte SIMD average.\nvabsdiff4\nInteger quad byte SIMD absolute value of difference.\nvmin4\n,\nvmax4\nInteger quad byte SIMD minimum/maximum.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvop4.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvop4.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;\nvop4  = { vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 };\n\n.dtype = .atype = .btype = { .u32, .s32 };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654\nDescription\nFour-way SIMD parallel arithmetic operation with secondary operation.\nElements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected bytes are then operated on in parallel.\nThe results are optionally clamped to the appropriate range determined by the destination type\n(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.\nFor instructions with a secondary SIMD merge operation:\nFor byte positions indicated in mask, the selected byte results are copied into destination\nd\n. For all other positions, the corresponding byte from source operand\nc\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor byte positions indicated in mask, the selected byte results are added to operand\nc\n,\nproducing a result in\nd\n.\nSemantics\n// extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    switch ( vop4 ) {\n        case vadd4:            t[i] = Va[i] + Vb[i];\n        case vsub4:            t[i] = Va[i] - Vb[i];\n        case vavrg4:           if ( ( Va[i] + Vb[i] ) >= 0 ) {\n                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;\n                               } else {\n                                   t[i] = ( Va[i] + Vb[i] ) >> 1;\n                               }\n        case vabsdiff4:        t[i] = | Va[i] - Vb[i] |;\n        case vmin4:            t[i] = MIN( Va[i], Vb[i] );\n        case vmax4:            t[i] = MAX( Va[i], Vb[i] );\n    }\n    if (.sat) {\n        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S8_MAX, S8_MIN );\n        else                   t[i] = CLAMP( t[i], U8_MAX, U8_MIN );\n    }\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvadd4\n,\nvsub4\n,\nvarvg4\n,\nvabsdiff4\n,\nvmin4\n,\nvmax4\nrequire\nsm_30\nor higher.\nExamples\nvadd4.s32.s32.u32.sat  r1, r2, r3, r1;\nvsub4.s32.s32.s32.sat  r1.b0, r2.b3210, r3.b7654, r1;\nvmin4.s32.u32.u32.add  r1.b00, r2.b0000, r3.b2222, r1;\n9.7.18.2.4.\nSIMD Video Instructions:\nvset4\n\nvset4\nInteger quad byte SIMD comparison.\nSyntax\n// SIMD instruction with secondary SIMD merge operation\nvset4.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;\n\n// SIMD instruction with secondary accumulate operation\nvset4.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;\n\n.atype = .btype = { .u32, .s32 };\n.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };\n.mask  = { .b0,\n           .b1, .b10\n           .b2, .b20, .b21, .b210,\n           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };\n    defaults to .b3210\n.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };\n   .asel defaults to .b3210\n   .bsel defaults to .b7654\nDescription\nFour-way SIMD parallel comparison with secondary operation.\nElements of each quad byte source to the operation are selected from any of the eight bytes in the\ntwo source operands\na\nand\nb\nusing the\nasel\nand\nbsel\nmodifiers.\nThe selected bytes are then compared in parallel.\nThe intermediate result of the comparison is always unsigned, and therefore the bytes of destination\nd\nand operand\nc\nare also unsigned.\nFor instructions with a secondary SIMD merge operation:\nFor byte positions indicated in mask, the selected byte results are copied into destination\nd\n. For all other positions, the corresponding byte from source operand\nb\nis copied to\nd\n.\nFor instructions with a secondary accumulate operation:\nFor byte positions indicated in mask, the selected byte results are added to operand\nc\n,\nproducing a result in\nd\n.\nSemantics\n// extract quads of bytes and sign- or zero-extend\n// based on operand type\nVa = extractAndSignExt_4( a, b, .asel, .atype );\nVb = extractAndSignExt_4( a, b, .bsel, .btype );\nVc = extractAndSignExt_4( c );\nfor (i=0; i<4; i++) {\n    t[i] = compare( Va[i], Vb[i], cmp ) ? 1 : 0;\n}\n// secondary accumulate or SIMD merge\nmask = extractMaskBits( .mask );\nif (.add) {\n    d = c;\n    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }\n} else {\n    d = 0;\n    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }\n}\nPTX ISA Notes\nIntroduced in PTX ISA version 3.0.\nTarget ISA Notes\nvset4\nrequires\nsm_30\nor higher.\nExamples\nvset4.s32.u32.lt      r1, r2, r3, r0;\nvset4.u32.u32.ne.max  r1, r2, r3, r0;\n9.7.19.\nMiscellaneous Instructions\n\nThe Miscellaneous instructions are:\nbrkpt\nnanosleep\npmevent\ntrap\nsetmaxnreg\n9.7.19.1.\nMiscellaneous Instructions:\nbrkpt\n\nbrkpt\nBreakpoint.\nSyntax\nbrkpt;\nDescription\nSuspends execution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nbrkpt\nrequires\nsm_11\nor higher.\nExamples\nbrkpt;\n@p  brkpt;\n9.7.19.2.\nMiscellaneous Instructions:\nnanosleep\n\nnanosleep\nSuspend the thread for an approximate delay given in nanoseconds.\nSyntax\nnanosleep.u32 t;\nDescription\nSuspends the thread for a sleep duration approximately close to the delay\nt\n, specified in\nnanoseconds.\nt\nmay be a register or an immediate value.\nThe sleep duration is approximated, but guaranteed to be in the interval\n[0,\n2*t]\n. The maximum\nsleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual\nthreads within a warp such that all sleeping threads in the warp wake up together.\nPTX ISA Notes\nnanosleep\nintroduced in PTX ISA 6.3.\nTarget ISA Notes\nnanosleep\nrequires\nsm_70\nor higher.\nExamples\n.reg .b32 r;\n.reg .pred p;\n\nnanosleep.u32 r;\nnanosleep.u32 42;\n@p nanosleep.u32 r;\n9.7.19.3.\nMiscellaneous Instructions:\npmevent\n\npmevent\nTrigger one or more Performance Monitor events.\nSyntax\npmevent       a;    // trigger a single performance monitor event\npmevent.mask  a;    // trigger one or more performance monitor events\nDescription\nTriggers one or more of a fixed number of performance monitor events, with event index or mask\nspecified by immediate operand\na\n.\npmevent\n(without modifier\n.mask\n) triggers a single performance monitor event indexed by\nimmediate operand\na\n, in the range\n0..15\n.\npmevent.mask\ntriggers one or more of the performance monitor events. Each bit in the 16-bit\nimmediate operand\na\ncontrols an event.\nProgrammatic performance moniter events may be combined with other hardware events using Boolean\nfunctions to increment one of the four performance counters. The relationship between events and\ncounters is programmed via API calls from the host.\nNotes\nCurrently, there are sixteen performance monitor events, numbered 0 through 15.\nPTX ISA Notes\npmevent\nintroduced in PTX ISA version 1.4.\npmevent.mask\nintroduced in PTX ISA version 3.0.\nTarget ISA Notes\npmevent supported on all target architectures.\npmevent.mask\nrequires\nsm_20\nor higher.\nExamples\npmevent      1;\n@p  pmevent      7;\n@q  pmevent.mask 0xff;\n9.7.19.4.\nMiscellaneous Instructions:\ntrap\n\ntrap\nPerform trap operation.\nSyntax\ntrap;\nDescription\nAbort execution and generate an interrupt to the host CPU.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\ntrap;\n@p  trap;\n9.7.19.5.\nMiscellaneous Instructions:\nsetmaxnreg\n\nsetmaxnreg\nHint to change the number of registers owned by the warp.\nSyntax\nsetmaxnreg.action.sync.aligned.u32 imm-reg-count;\n\n.action = { .inc, .dec };\nDescription\nsetmaxnreg\nprovides a hint to the system to update the maximum number of per-thread registers\nowned by the executing warp to the value specified by the\nimm-reg-count\noperand.\nQualifier\n.dec\nis used to release extra registers such that the absolute per-thread maximum\nregister count is reduced from its current value to\nimm-reg-count\n. Qualifier\n.inc\nis used to\nrequest additional registers such that the absolute per-thread maximum register count is increased\nfrom its current value to\nimm-reg-count\n.\nA pool of available registers is maintained per-CTA. Register adjustments requested by the\nsetmaxnreg\ninstructions are handled by supplying extra registers from this pool to the\nrequesting warp or by releasing extra registers from the requesting warp to this pool, depending\nupon the value of the\n.action\nqualifier.\nThe\nsetmaxnreg.inc\ninstruction blocks the execution until enough registers are available in the\nCTA’s register pool. After the instruction\nsetmaxnreg.inc\nobtains new registers from the CTA\npool, the initial contents of the new registers are undefined. The new registers must be initialized\nbefore they are used.\nThe same\nsetmaxnreg\ninstruction must be executed by all warps in a\nwarpgroup\n. After executing a\nsetmaxnreg\ninstruction, all warps in the\nwarpgroup\nmust synchronize explicitly before\nexecuting subsequent setmaxnreg instructions. If a\nsetmaxnreg\ninstruction is not executed by all\nwarps in the\nwarpgroup\n, then the behavior is undefined.\nOperand\nimm-reg-count\nis an integer constant. The value of\nimm-reg-count\nmust be in the\nrange 24 to 256 (both inclusive) and must be a multiple of 8.\nChanges to the register file of the warp always happen at the tail-end of the register file.\nThe\nsetmaxnreg\ninstruction requires that the kernel has been launched with a valid value of\nmaximum number of per-thread registers specified via the appropriate compilation via the appropriate\ncompile-time option or the appropriate performance tuning directive. Otherwise, the\nsetmaxnreg\ninstruction may have no effect.\nWhen qualifier\n.dec\nis specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of\nsetmaxnreg\ninstruction should be greater than or equal to the\nimm-reg-count\n. Otherwise, the behaviour is undefined.\nWhen qualifier\n.inc\nis specified, the maximum number of per-thread registers owned by the warp\nprior to the execution of\nsetmaxnreg\ninstruction should be less than or equal to the\nimm-reg-count\n. Otherwise, the behaviour is undefined.\nThe mandatory\n.sync\nqualifier indicates that\nsetmaxnreg\ninstruction causes the executing\nthread to wait until all threads in the warp execute the same\nsetmaxnreg\ninstruction before\nresuming execution.\nThe mandatory\n.aligned\nqualifier indicates that all threads in the warpgroup must execute the\nsame\nsetmaxnreg\ninstruction. In conditionally executed code,\nsetmaxnreg\ninstruction should\nonly be used if it is known that all threads in warpgroup evaluate the condition identically,\notherwise the behavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nSupported on following architectures:\nsm_90a\nsm_100a\nsm_101a\n(Renamed to\nsm_110a\nfrom PTX ISA version 9.0)\nsm_120a\nAnd is supported on following family-specific architectures from PTX ISA version 8.8:\nsm_100f\nor higher in the same family\nsm_101f\nor higher in the same family (Renamed to\nsm_110f\nfrom PTX ISA version 9.0)\nsm_120f\nor higher in the same family\nsm_110f\nor higher in the same family\nExamples\nsetmaxnreg.dec.sync.aligned.u32 64;\nsetmaxnreg.inc.sync.aligned.u32 192;\n10.\nSpecial Registers\n\nPTX includes a number of predefined, read-only variables, which are\nvisible as special registers and accessed through\nmov\nor\ncvt\ninstructions.\nThe special registers are:\n%tid\n%ntid\n%laneid\n%warpid\n%nwarpid\n%ctaid\n%nctaid\n%smid\n%nsmid\n%gridid\n%is_explicit_cluster\n%clusterid\n%nclusterid\n%cluster_ctaid\n%cluster_nctaid\n%cluster_ctarank\n%cluster_nctarank\n%lanemask_eq\n,\n%lanemask_le\n,\n%lanemask_lt\n,\n%lanemask_ge\n,\n%lanemask_gt\n%clock\n,\n%clock_hi\n,\n%clock64\n%pm0,\n...,\n%pm7\n%pm0_64,\n...,\n%pm7_64\n%envreg0,\n...,\n%envreg31\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset<2>\n%total_smem_size\n%aggr_smem_size\n%dynamic_smem_size\n%current_graph_exec\n10.1.\nSpecial Registers:\n%tid\n\n%tid\nThread identifier within a CTA.\nSyntax (predefined)\n.sreg .v4 .u32 %tid;                  // thread id vector\n.sreg .u32 %tid.x, %tid.y, %tid.z;    // thread id components\nDescription\nA predefined, read-only, per-thread special register initialized with the thread identifier within\nthe CTA. The\n%tid\nspecial register contains a 1D, 2D, or 3D vector to match the CTA shape; the\n%tid\nvalue in unused dimensions is\n0\n. The fourth element is unused and always returns\nzero. The number of threads in each dimension are specified by the predefined special register\n%ntid\n.\nEvery thread in the CTA has a unique\n%tid\n.\n%tid\ncomponent values range from\n0\nthrough\n%ntid-1\nin each CTA dimension.\n%tid.y\n==\n%tid.z\n==\n0\nin 1D CTAs.\n%tid.z\n==\n0\nin 2D CTAs.\nIt is guaranteed that:\n0  <=  %tid.x <  %ntid.x\n0  <=  %tid.y <  %ntid.y\n0  <=  %tid.z <  %ntid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%tid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32      %r1,%tid.x;  // move tid.x to %rh\n\n// legacy code accessing 16-bit components of %tid\nmov.u16      %rh,%tid.x;\ncvt.u32.u16  %r2,%tid.z;  // zero-extend tid.z to %r2\n10.2.\nSpecial Registers:\n%ntid\n\n%ntid\nNumber of thread IDs per CTA.\nSyntax (predefined)\n.sreg .v4 .u32 %ntid;                   // CTA shape vector\n.sreg .u32 %ntid.x, %ntid.y, %ntid.z;   // CTA dimensions\nDescription\nA predefined, read-only special register initialized with the number of thread ids in each CTA\ndimension. The\n%ntid\nspecial register contains a 3D CTA shape vector that holds the CTA\ndimensions. CTA dimensions are non-zero; the fourth element is unused and always returns zero. The\ntotal number of threads in a CTA is\n(%ntid.x\n*\n%ntid.y\n*\n%ntid.z)\n.\n%ntid.y == %ntid.z == 1 in 1D CTAs.\n%ntid.z ==1 in 2D CTAs.\nMaximum values of %ntid.{x,y,z} are as follows:\n.target architecture\n%ntid.x\n%ntid.y\n%ntid.z\nsm_1x\n512\n512\n64\nsm_20\n,\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n,\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n1024\n1024\n64\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%ntid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n// compute unified thread id for 2D CTA\nmov.u32  %r0,%tid.x;\nmov.u32  %h1,%tid.y;\nmov.u32  %h2,%ntid.x;\nmad.u32  %r0,%h1,%h2,%r0;\n\nmov.u16  %rh,%ntid.x;      // legacy code\n10.3.\nSpecial Registers:\n%laneid\n\n%laneid\nLane Identifier.\nSyntax (predefined)\n.sreg .u32 %laneid;\nDescription\nA predefined, read-only special register that returns the thread’s lane within the warp. The lane\nidentifier ranges from zero to\nWARP_SZ-1\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %laneid;\n10.4.\nSpecial Registers:\n%warpid\n\n%warpid\nWarp identifier.\nSyntax (predefined)\n.sreg .u32 %warpid;\nDescription\nA predefined, read-only special register that returns the thread’s warp identifier. The warp\nidentifier provides a unique warp number within a CTA but not across CTAs within a grid. The warp\nidentifier will be the same for all threads within a single warp.\nNote that\n%warpid\nreturns the location of a thread at the moment when read, but\nits value may change during execution, e.g., due to rescheduling of threads following\npreemption. For this reason,\n%ctaid\nand\n%tid\nshould be used to compute a virtual warp index\nif such a value is needed in kernel code;\n%warpid\nis intended mainly to enable profiling and\ndiagnostic code to sample and log information such as work place mapping and load distribution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %warpid;\n10.5.\nSpecial Registers:\n%nwarpid\n\n%nwarpid\nNumber of warp identifiers.\nSyntax (predefined)\n.sreg .u32 %nwarpid;\nDescription\nA predefined, read-only special register that returns the maximum number of warp identifiers.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%nwarpid\nrequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %nwarpid;\n10.6.\nSpecial Registers:\n%ctaid\n\n%ctaid\nCTA identifier within a grid.\nSyntax (predefined)\n.sreg .v4 .u32 %ctaid;                      // CTA id vector\n.sreg .u32 %ctaid.x, %ctaid.y, %ctaid.z;    // CTA id components\nDescription\nA predefined, read-only special register initialized with the CTA identifier within the CTA\ngrid. The\n%ctaid\nspecial register contains a 1D, 2D, or 3D vector, depending on the shape and\nrank of the CTA grid. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %ctaid.x <  %nctaid.x\n0  <=  %ctaid.y <  %nctaid.y\n0  <=  %ctaid.z <  %nctaid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%ctaid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r0,%ctaid.x;\nmov.u16  %rh,%ctaid.y;   // legacy code\n10.7.\nSpecial Registers:\n%nctaid\n\n%nctaid\nNumber of CTA ids per grid.\nSyntax (predefined)\n.sreg .v4 .u32 %nctaid                      // Grid shape vector\n.sreg .u32 %nctaid.x,%nctaid.y,%nctaid.z;   // Grid dimensions\nDescription\nA predefined, read-only special register initialized with the number of CTAs in each grid\ndimension. The\n%nctaid\nspecial register contains a 3D grid shape vector, with each element\nhaving a value of at least\n1\n. The fourth element is unused and always returns zero.\nMaximum values of %nctaid.{x,y,z} are as follows:\n.target architecture\n%nctaid.x\n%nctaid.y\n%nctaid.z\nsm_1x\n,\nsm_20\n65535\n65535\n65535\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n,\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n2\n31\n-1\n65535\n65535\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 with type\n.v4.u16\n.\nRedefined as type\n.v4.u32\nin PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit\nmov\nand\ncvt\ninstructions may be used to read the lower 16-bits of each component of\n%nctaid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r0,%nctaid.x;\nmov.u16  %rh,%nctaid.x;     // legacy code\n10.8.\nSpecial Registers:\n%smid\n\n%smid\nSM identifier.\nSyntax (predefined)\n.sreg .u32 %smid;\nDescription\nA predefined, read-only special register that returns the processor (SM) identifier on which a\nparticular thread is executing. The SM identifier ranges from\n0\nto\n%nsmid-1\n. The SM\nidentifier numbering is not guaranteed to be contiguous.\nNotes\nNote that\n%smid\nreturns the location of a thread at the moment when read, but\nits value may change during execution, e.g. due to rescheduling of threads following\npreemption.\n%smid\nis intended mainly to enable profiling and diagnostic code to sample and log\ninformation such as work place mapping and load distribution.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u32  %r, %smid;\n10.9.\nSpecial Registers:\n%nsmid\n\n%nsmid\nNumber of SM identifiers.\nSyntax (predefined)\n.sreg .u32 %nsmid;\nDescription\nA predefined, read-only special register that returns the maximum number of SM identifiers. The SM\nidentifier numbering is not guaranteed to be contiguous, so\n%nsmid\nmay be larger than the\nphysical number of SMs in the device.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%nsmid\nrequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %nsmid;\n10.10.\nSpecial Registers:\n%gridid\n\n%gridid\nGrid identifier.\nSyntax (predefined)\n.sreg .u64 %gridid;\nDescription\nA predefined, read-only special register initialized with the per-grid temporal grid identifier. The\n%gridid\nis used by debuggers to distinguish CTAs and clusters within concurrent (small) grids.\nDuring execution, repeated launches of programs may occur, where each launch starts a\ngrid-of-CTAs. This variable provides the temporal grid launch number for this context.\nFor\nsm_1x\ntargets,\n%gridid\nis limited to the range [0..2\n16\n-1]. For\nsm_20\n,\n%gridid\nis limited to the range [0..2\n32\n-1].\nsm_30\nsupports the entire 64-bit range.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0 as type\n.u16\n.\nRedefined as type\n.u32\nin PTX ISA version 1.3.\nRedefined as type\n.u64\nin PTX ISA version 3.0.\nFor compatibility with legacy PTX code, 16-bit and 32-bit\nmov\nand\ncvt\ninstructions may be\nused to read the lower 16-bits or 32-bits of each component of\n%gridid\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.u64  %s, %gridid;  // 64-bit read of %gridid\nmov.u32  %r, %gridid;  // legacy code with 32-bit %gridid\n10.11.\nSpecial Registers:\n%is_explicit_cluster\n\n%is_explicit_cluster\nChecks if user has explicitly specified cluster launch.\nSyntax (predefined)\n.sreg .pred %is_explicit_cluster;\nDescription\nA predefined, read-only special register initialized with the predicate value of whether the cluster\nlaunch is explicitly specified by user.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .pred p;\n\nmov.pred  p, %is_explicit_cluster;\n10.12.\nSpecial Registers:\n%clusterid\n\n%clusterid\nCluster identifier within a grid.\nSyntax (predefined)\n.sreg .v4 .u32 %clusterid;\n.sreg .u32 %clusterid.x, %clusterid.y, %clusterid.z;\nDescription\nA predefined, read-only special register initialized with the cluster identifier in a grid in each\ndimension. Each cluster in a grid has a unique identifier.\nThe\n%clusterid\nspecial register contains a 1D, 2D, or 3D vector, depending upon the shape and\nrank of the cluster. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %clusterid.x <  %nclusterid.x\n0  <=  %clusterid.y <  %nclusterid.y\n0  <=  %clusterid.z <  %nclusterid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %clusterid.x;\nmov.u32     %r1, %clusterid.z;\nmov.v4.u32  %rx, %clusterid;\n10.13.\nSpecial Registers:\n%nclusterid\n\n%nclusterid\nNumber of cluster identifiers per grid.\nSyntax (predefined)\n.sreg .v4 .u32 %nclusterid;\n.sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z;\nDescription\nA predefined, read-only special register initialized with the number of clusters in each grid\ndimension.\nThe\n%nclusterid\nspecial register contains a 3D grid shape vector that holds the grid dimensions\nin terms of clusters. The fourth element is unused and always returns zero.\nRefer to the\nCuda Programming Guide\nfor details on the maximum values of\n%nclusterid.{x,y,z}\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %nclusterid.x;\nmov.u32     %r1, %nclusterid.z;\nmov.v4.u32  %rx, %nclusterid;\n10.14.\nSpecial Registers:\n%cluster_ctaid\n\n%cluster_ctaid\nCTA identifier within a cluster.\nSyntax (predefined)\n.sreg .v4 .u32 %cluster_ctaid;\n.sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z;\nDescription\nA predefined, read-only special register initialized with the CTA identifier in a cluster in each\ndimension. Each CTA in a cluster has a unique CTA identifier.\nThe\n%cluster_ctaid\nspecial register contains a 1D, 2D, or 3D vector, depending upon the shape of\nthe cluster. The fourth element is unused and always returns zero.\nIt is guaranteed that:\n0  <=  %cluster_ctaid.x <  %cluster_nctaid.x\n0  <=  %cluster_ctaid.y <  %cluster_nctaid.y\n0  <=  %cluster_ctaid.z <  %cluster_nctaid.z\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_ctaid.x;\nmov.u32     %r1, %cluster_ctaid.z;\nmov.v4.u32  %rx, %cluster_ctaid;\n10.15.\nSpecial Registers:\n%cluster_nctaid\n\n%cluster_nctaid\nNumber of CTA identifiers per cluster.\nSyntax (predefined)\n.sreg .v4 .u32 %cluster_nctaid;\n.sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z;\nDescription\nA predefined, read-only special register initialized with the number of CTAs in a cluster in each\ndimension.\nThe\n%cluster_nctaid\nspecial register contains a 3D grid shape vector that holds the cluster\ndimensions in terms of CTAs. The fourth element is unused and always returns zero.\nRefer to the\nCuda Programming Guide\nfor details on the maximum values of\n%cluster_nctaid.{x,y,z}\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r<2>;\n.reg .v4 .b32 %rx;\n\nmov.u32     %r0, %cluster_nctaid.x;\nmov.u32     %r1, %cluster_nctaid.z;\nmov.v4.u32  %rx, %cluster_nctaid;\n10.16.\nSpecial Registers:\n%cluster_ctarank\n\n%cluster_ctarank\nCTA identifier in a cluster across all dimensions.\nSyntax (predefined)\n.sreg .u32 %cluster_ctarank;\nDescription\nA predefined, read-only special register initialized with the CTA rank within a cluster across all\ndimensions.\nIt is guaranteed that:\n0  <=  %cluster_ctarank <  %cluster_nctarank\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r;\n\nmov.u32  %r, %cluster_ctarank;\n10.17.\nSpecial Registers:\n%cluster_nctarank\n\n%cluster_nctarank\nNumber of CTA identifiers in a cluster across all dimensions.\nSyntax (predefined)\n.sreg .u32 %cluster_nctarank;\nDescription\nA predefined, read-only special register initialized with the nunber of CTAs within a cluster across\nall dimensions.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.reg .b32 %r;\n\nmov.u32  %r, %cluster_nctarank;\n10.18.\nSpecial Registers:\n%lanemask_eq\n\n%lanemask_eq\n32-bit mask with bit set in position equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_eq;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with a bit set in the\nposition equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_eq\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_eq;\n10.19.\nSpecial Registers:\n%lanemask_le\n\n%lanemask_le\n32-bit mask with bits set in positions less than or equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_le;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than or equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_le\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_le\n10.20.\nSpecial Registers:\n%lanemask_lt\n\n%lanemask_lt\n32-bit mask with bits set in positions less than the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_lt;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\nless than the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_lt\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_lt;\n10.21.\nSpecial Registers:\n%lanemask_ge\n\n%lanemask_ge\n32-bit mask with bits set in positions greater than or equal to the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_ge;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than or equal to the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_ge\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_ge;\n10.22.\nSpecial Registers:\n%lanemask_gt\n\n%lanemask_gt\n32-bit mask with bits set in positions greater than the thread’s lane number in the warp.\nSyntax (predefined)\n.sreg .u32 %lanemask_gt;\nDescription\nA predefined, read-only special register initialized with a 32-bit mask with bits set in positions\ngreater than the thread’s lane number in the warp.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%lanemask_gt\nrequires\nsm_20\nor higher.\nExamples\nmov.u32     %r, %lanemask_gt;\n10.23.\nSpecial Registers:\n%clock\n,\n%clock_hi\n\n%clock\n,\n%clock_hi\n%clock\nA predefined, read-only 32-bit unsigned cycle counter.\n%clock_hi\nThe upper 32-bits of\n%clock64\nspecial register.\nSyntax (predefined)\n.sreg .u32 %clock;\n.sreg .u32 %clock_hi;\nDescription\nSpecial register\n%clock\nand\n%clock_hi\nare unsigned 32-bit read-only cycle counters that wrap\nsilently.\nPTX ISA Notes\n%clock\nintroduced in PTX ISA version 1.0.\n%clock_hi\nintroduced in PTX ISA version 5.0.\nTarget ISA Notes\n%clock\nsupported on all target architectures.\n%clock_hi\nrequires\nsm_20\nor higher.\nExamples\nmov.u32 r1,%clock;\nmov.u32 r2, %clock_hi;\n10.24.\nSpecial Registers:\n%clock64\n\n%clock64\nA predefined, read-only 64-bit unsigned cycle counter.\nSyntax (predefined)\n.sreg .u64 %clock64;\nDescription\nSpecial register\n%clock64\nis an unsigned 64-bit read-only cycle counter that wraps silently.\nNotes\nThe lower 32-bits of\n%clock64\nare identical to\n%clock\n.\nThe upper 32-bits of\n%clock64\nare identical to\n%clock_hi\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\n%clock64\nrequires\nsm_20\nor higher.\nExamples\nmov.u64  r1,%clock64;\n10.25.\nSpecial Registers:\n%pm0\n…\n%pm7\n\n%pm0\n…\n%pm7\nPerformance monitoring counters.\nSyntax (predefined)\n.sreg .u32 %pm<8>;\nDescription\nSpecial registers\n%pm0\n…\n%pm7\nare unsigned 32-bit read-only performance monitor counters. Their\nbehavior is currently undefined.\nPTX ISA Notes\n%pm0\n…\n%pm3\nintroduced in PTX ISA version 1.3.\n%pm4\n…\n%pm7\nintroduced in PTX ISA version 3.0.\nTarget ISA Notes\n%pm0\n…\n%pm3\nsupported on all target architectures.\n%pm4\n…\n%pm7\nrequire\nsm_20\nor higher.\nExamples\nmov.u32  r1,%pm0;\nmov.u32  r1,%pm7;\n10.26.\nSpecial Registers:\n%pm0_64\n…\n%pm7_64\n\n%pm0_64\n…\n%pm7_64\n64 bit Performance monitoring counters.\nSyntax (predefined)\n.sreg .u64 %pm0_64;\n.sreg .u64 %pm1_64;\n.sreg .u64 %pm2_64;\n.sreg .u64 %pm3_64;\n.sreg .u64 %pm4_64;\n.sreg .u64 %pm5_64;\n.sreg .u64 %pm6_64;\n.sreg .u64 %pm7_64;\nDescription\nSpecial registers\n%pm0_64\n…\n%pm7_64\nare unsigned 64-bit read-only performance monitor\ncounters. Their behavior is currently undefined.\nNotes\nThe lower 32bits of\n%pm0_64\n…\n%pm7_64\nare identical to\n%pm0\n…\n%pm7\n.\nPTX ISA Notes\n%pm0_64\n…\n%pm7_64\nintroduced in PTX ISA version 4.0.\nTarget ISA Notes\n%pm0_64\n…\n%pm7_64\nrequire\nsm_50\nor higher.\nExamples\nmov.u32  r1,%pm0_64;\nmov.u32  r1,%pm7_64;\n10.27.\nSpecial Registers:\n%envreg<32>\n\n%envreg<32>\nDriver-defined read-only registers.\nSyntax (predefined)\n.sreg .b32 %envreg<32>;\nDescription\nA set of 32 pre-defined read-only registers used to capture execution environment of PTX program\noutside of PTX virtual machine. These registers are initialized by the driver prior to kernel launch\nand can contain cta-wide or grid-wide values.\nPrecise semantics of these registers is defined in the driver documentation.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\nmov.b32      %r1,%envreg0;  // move envreg0 to %r1\n10.28.\nSpecial Registers:\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n\n%globaltimer\n,\n%globaltimer_lo\n,\n%globaltimer_hi\n%globaltimer\nA predefined, 64-bit global nanosecond timer.\n%globaltimer_lo\nThe lower 32-bits of %globaltimer.\n%globaltimer_hi\nThe upper 32-bits of %globaltimer.\nSyntax (predefined)\n.sreg .u64 %globaltimer;\n.sreg .u32 %globaltimer_lo, %globaltimer_hi;\nDescription\nSpecial registers intended for use by NVIDIA tools. The behavior is target-specific and may change\nor be removed in future GPUs. When JIT-compiled to other targets, the value of these registers is\nunspecified.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nRequires target\nsm_30\nor higher.\nExamples\nmov.u64  r1,%globaltimer;\n10.29.\nSpecial Registers:\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset_<2>\n%reserved_smem_offset_begin\nStart of the reserved shared memory region.\n%reserved_smem_offset_end\nEnd of the reserved shared memory region.\n%reserved_smem_offset_cap\nTotal size of the reserved shared memory region.\n%reserved_smem_offset_<2>\nOffsets in the reserved shared memory region.\nSyntax (predefined)\n.sreg .b32 %reserved_smem_offset_begin;\n.sreg .b32 %reserved_smem_offset_end;\n.sreg .b32 %reserved_smem_offset_cap;\n.sreg .b32 %reserved_smem_offset_<2>;\nDescription\nThese are predefined, read-only special registers containing information about the shared memory\nregion which is reserved for the NVIDIA system software use. This region of shared memory is not\navailable to users, and accessing this region from user code results in undefined behavior. Refer to\nCUDA Programming Guide\nfor details.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.6.\nTarget ISA Notes\nRequire\nsm_80\nor higher.\nExamples\n.reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1;\n\nmov.b32 %reg_begin,   %reserved_smem_offset_begin;\nmov.b32 %reg_end,     %reserved_smem_offset_end;\nmov.b32 %reg_cap,     %reserved_smem_offset_cap;\nmov.b32 %reg_offset0, %reserved_smem_offset_0;\nmov.b32 %reg_offset1, %reserved_smem_offset_1;\n10.30.\nSpecial Registers:\n%total_smem_size\n\n%total_smem_size\nTotal size of shared memory used by a CTA of a kernel.\nSyntax (predefined)\n.sreg .u32 %total_smem_size;\nDescription\nA predefined, read-only special register initialized with total size of shared memory allocated\n(statically and dynamically, excluding the shared memory reserved for the NVIDIA system software\nuse) for the CTA of a kernel at launch time.\nSize is returned in multiples of shared memory allocation unit size supported by target\narchitecture.\nAllocation unit values are as follows:\nTarget architecture\nShared memory allocation unit size\nsm_2x\n128 bytes\nsm_3x\n,\nsm_5x\n,\nsm_6x\n,\nsm_7x\n256 bytes\nsm_8x\n,\nsm_9x\n,\nsm_10x\n,\nsm_12x\n128 bytes\nPTX ISA Notes\nIntroduced in PTX ISA version 4.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %total_smem_size;\n10.31.\nSpecial Registers:\n%aggr_smem_size\n\n%aggr_smem_size\nTotal size of shared memory used by a CTA of a kernel.\nSyntax (predefined)\n.sreg .u32 %aggr_smem_size;\nDescription\nA predefined, read-only special register initialized with total aggregated size of shared memory\nconsisting of the size of user shared memory allocated (statically and dynamically) at launch time\nand the size of shared memory region which is reserved for the NVIDIA system software use.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.1.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\nmov.u32  %r, %aggr_smem_size;\n10.32.\nSpecial Registers:\n%dynamic_smem_size\n\n%dynamic_smem_size\nSize of shared memory allocated dynamically at kernel launch.\nSyntax (predefined)\n.sreg .u32 %dynamic_smem_size;\nDescription\nSize of shared memory allocated dynamically at kernel launch.\nA predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time.\nPTX ISA Notes\nIntroduced in PTX ISA version 4.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\nmov.u32  %r, %dynamic_smem_size;\n10.33.\nSpecial Registers:\n%current_graph_exec\n\n%current_graph_exec\nAn Identifier for currently executing CUDA device graph.\nSyntax (predefined)\n.sreg .u64 %current_graph_exec;\nDescription\nA predefined, read-only special register initialized with the identifier referring to the CUDA\ndevice graph being currently executed. This register is 0 if the executing kernel is not part of a\nCUDA device graph.\nRefer to the\nCUDA Programming Guide\nfor more details on CUDA device graphs.\nPTX ISA Notes\nIntroduced in PTX ISA version 8.0.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nExamples\nmov.u64  r1, %current_graph_exec;\n11.\nDirectives\n\n11.1.\nPTX Module Directives\n\nThe following directives declare the PTX ISA version of the code in the module, the target\narchitecture for which the code was generated, and the size of addresses within the PTX module.\n.version\n.target\n.address_size\n11.1.1.\nPTX Module Directives:\n.version\n\n.version\nPTX ISA version number.\nSyntax\n.version  major.minor    // major, minor are integers\nDescription\nSpecifies the PTX language version number.\nThe\nmajor\nnumber is incremented when there are incompatible changes to the PTX language, such as\nchanges to the syntax or semantics. The version major number is used by the PTX compiler to ensure\ncorrect execution of legacy PTX code.\nThe\nminor\nnumber is incremented when new features are added to PTX.\nSemantics\nIndicates that this module must be compiled with tools that support an equal or greater version\nnumber.\nEach PTX module must begin with a\n.version\ndirective, and no other\n.version\ndirective is\nallowed anywhere else within the module.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.version 3.1\n.version 3.0\n.version 2.3\n11.1.2.\nPTX Module Directives:\n.target\n\n.target\nArchitecture and Platform target.\nSyntax\n.target stringlist         // comma separated list of target specifiers\nstring = { sm_120a, sm_120f, sm_120,          // sm_12x target architectures\n           sm_121a, sm_121f, sm_121,          // sm_12x target architectures\n           sm_110a, sm_110f, sm_110,          // sm_11x target architectures\n           sm_100a, sm_100f, sm_100,          // sm_10x target architectures\n           sm_101a, sm_101f, sm_101,          // sm_10x target architectures\n           sm_103a, sm_103f, sm_103           // sm_10x target architectures\n           sm_90a, sm_90,                     // sm_9x target architectures\n           sm_80, sm_86, sm_87, sm_88, sm_89, // sm_8x target architectures\n           sm_70, sm_72, sm_75,               // sm_7x target architectures\n           sm_60, sm_61, sm_62,               // sm_6x target architectures\n           sm_50, sm_52, sm_53,               // sm_5x target architectures\n           sm_30, sm_32, sm_35, sm_37,        // sm_3x target architectures\n           sm_20,                             // sm_2x target architectures\n           sm_10, sm_11, sm_12, sm_13,        // sm_1x target architectures\n           texmode_unified, texmode_independent,   // texturing mode\n           debug,                                  // platform option\n           map_f64_to_f32 };                       // platform option\nDescription\nSpecifies the set of features in the target architecture for which the current PTX code was\ngenerated. In general, generations of SM architectures follow an\nonion layer\nmodel, where each\ngeneration adds new features and retains all features of previous generations. The onion layer model\nallows the PTX code generated for a given target to be run on later generation devices.\nTarget architectures with suffix “\na\n”, such as\nsm_90a\n, include architecture-specific\nfeatures that are supported on the specified architecture only, hence such targets do not follow the\nonion layer model. Therefore, PTX code generated for such targets cannot be run on later generation\ndevices. Architecture-specific features can only be used with targets that support these\nfeatures.\nTarget architectures with suffix “\nf\n”, such as\nsm_100f\n, include family-specific features that\nare supported only within the same architecture family. Therefore, PTX code generated for such\ntargets can run only on later generation devices in the same family. Family-specific features can be\nused with f-targets as well as a-targets of later generation devices in the same family.\nTable 58\ndefines the architecture families.\nTable 58\nArchitecture Families\n\nFamily\nTarget SM architectures included\nsm_10x family\nsm_100f, sm_103f, future targets\nin sm_10x family\nsm_11x family\nsm_110f, sm_101f, future targets\nin sm_11x family\nsm_12x family\nsm_120f, sm_121f, future targets\nin sm_12x family\nSemantics\nEach PTX module must begin with a\n.version\ndirective, immediately followed by a\n.target\ndirective containing a target architecture and optional platform options. A\n.target\ndirective\nspecifies a single target architecture, but subsequent\n.target\ndirectives can be used to change\nthe set of target features allowed during parsing. A program with multiple\n.target\ndirectives\nwill compile and run only on devices that support all features of the highest-numbered architecture\nlisted in the program.\nPTX features are checked against the specified target architecture, and an error is generated if an\nunsupported feature is used. The following table summarizes the features in PTX that vary according\nto target architecture.\nTarget\nDescription\nsm_120\nBaseline feature set for\nsm_120\narchitecture.\nsm_120f\nAdds support for\nsm_120f\nfamily specific features.\nsm_120a\nAdds support for\nsm_120a\narchitecture-specific features.\nsm_121\nBaseline feature set for\nsm_121\narchitecture.\nsm_121f\nAdds support for\nsm_121f\nfamily specific features.\nsm_121a\nAdds support for\nsm_121a\narchitecture-specific features.\nTarget\nDescription\nsm_110\nBaseline feature set for\nsm_110\narchitecture.\nsm_110f\nAdds support for\nsm_110f\nfamily specific features.\nsm_110a\nAdds support for\nsm_110a\narchitecture-specific features.\nTarget\nDescription\nsm_100\nBaseline feature set for\nsm_100\narchitecture.\nsm_100f\nAdds support for\nsm_100f\nfamily specific features.\nsm_100a\nAdds support for\nsm_100a\narchitecture-specific features.\nsm_101\nBaseline feature set for\nsm_101\narchitecture. (Renamed to\nsm_110\n)\nsm_101f\nAdds support for\nsm_101f\nfamily specific features. (Renamed to\nsm_110f\n)\nsm_101a\nAdds support for\nsm_101a\narchitecture-specific features. (Renamed to\nsm_110a\n)\nsm_103\nBaseline feature set for\nsm_103\narchitecture.\nsm_103f\nAdds support for\nsm_103f\nfamily specific features.\nsm_103a\nAdds support for\nsm_103a\narchitecture-specific features.\nTarget\nDescription\nsm_90\nBaseline feature set for\nsm_90\narchitecture.\nsm_90a\nAdds support for\nsm_90a\narchitecture-specific features.\nTarget\nDescription\nsm_80\nBaseline feature set for\nsm_80\narchitecture.\nsm_86\nAdds support for\n.xorsign\nmodifier on\nmin\nand\nmax\ninstructions.\nsm_87\nBaseline feature set for\nsm_87\narchitecture.\nsm_88\nBaseline feature set for\nsm_88\narchitecture.\nsm_89\nBaseline feature set for\nsm_89\narchitecture.\nTarget\nDescription\nsm_70\nBaseline feature set for\nsm_70\narchitecture.\nsm_72\nAdds support for integer multiplicand and accumulator matrices in\nwmma\ninstructions.\nAdds support for\ncvt.pack\ninstruction.\nsm_75\nAdds support for sub-byte integer and single-bit multiplicant matrices in\nwmma\ninstructions.\nAdds support for\nldmatrix\ninstruction.\nAdds support for\nmovmatrix\ninstruction.\nAdds support for\ntanh\ninstruction.\nTarget\nDescription\nsm_60\nBaseline feature set for\nsm_60\narchitecture.\nsm_61\nAdds support for\ndp2a\nand\ndp4a\ninstructions.\nsm_62\nBaseline feature set for\nsm_61\narchitecture.\nTarget\nDescription\nsm_50\nBaseline feature set for\nsm_50\narchitecture.\nsm_52\nBaseline feature set for\nsm_50\narchitecture.\nsm_53\nAdds support for arithmetic, comparsion and texture instructions for\n.f16\nand\n.f16x2\ntypes.\nTarget\nDescription\nsm_30\nBaseline feature set for\nsm_30\narchitecture.\nsm_32\nAdds 64-bit\n{atom,red}.{and,or,xor,min,max}\ninstructions.\nAdds\nshf\ninstruction.\nAdds\nld.global.nc\ninstruction.\nsm_35\nAdds support for CUDA Dynamic Parallelism.\nsm_37\nBaseline feature set for\nsm_35\narchitecture.\nTarget\nDescription\nsm_20\nBaseline feature set for\nsm_20\narchitecture.\nTarget\nDescription\nsm_10\nBaseline feature set for\nsm_10\narchitecture.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_11\nAdds 64-bit\n{atom,red}.{and,or,xor,min,max}\ninstructions.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_12\nAdds\n{atom,red}.shared\n, 64-bit\n{atom,red}.global\n,\nvote\ninstructions.\nRequires\nmap_f64_to_f32\nif any\n.f64\ninstructions used.\nsm_13\nAdds double-precision support, including expanded rounding modifiers.\nDisallows use of\nmap_f64_to_f32\n.\nThe texturing mode is specified for an entire module and cannot be changed within the module.\nThe\n.target\ndebug option declares that the PTX file contains DWARF debug information, and\nsubsequent compilation of PTX will retain information needed for source-level debugging. If the\ndebug option is declared, an error message is generated if no DWARF information is found in the\nfile. The debug option requires PTX ISA version 3.0 or later.\nmap_f64_to_f32\nindicates that all double-precision instructions map to single-precision\nregardless of the target architecture. This enables high-level language compilers to compile\nprograms containing type double to target device that do not support double-precision\noperations. Note that\n.f64\nstorage remains as 64-bits, with only half being used by instructions\nconverted from\n.f64\nto\n.f32\n.\nNotes\nTargets of the form\ncompute_xx\nare also accepted as synonyms for\nsm_xx\ntargets.\nTargets\nsm_{101,101f,101a}\nare renamed to targets\nsm_{110,110f,110a}\nfrom PTX ISA version 9.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget strings\nsm_10\nand\nsm_11\nintroduced in PTX ISA version 1.0.\nTarget strings\nsm_12\nand\nsm_13\nintroduced in PTX ISA version 1.2.\nTexturing mode introduced in PTX ISA version 1.5.\nTarget string\nsm_20\nintroduced in PTX ISA version 2.0.\nTarget string\nsm_30\nintroduced in PTX ISA version 3.0.\nPlatform option\ndebug\nintroduced in PTX ISA version 3.0.\nTarget string\nsm_35\nintroduced in PTX ISA version 3.1.\nTarget strings\nsm_32\nand\nsm_50\nintroduced in PTX ISA version 4.0.\nTarget strings\nsm_37\nand\nsm_52\nintroduced in PTX ISA version 4.1.\nTarget string\nsm_53\nintroduced in PTX ISA version 4.2.\nTarget string\nsm_60\n,\nsm_61\n,\nsm_62\nintroduced in PTX ISA version 5.0.\nTarget string\nsm_70\nintroduced in PTX ISA version 6.0.\nTarget string\nsm_72\nintroduced in PTX ISA version 6.1.\nTarget string\nsm_75\nintroduced in PTX ISA version 6.3.\nTarget string\nsm_80\nintroduced in PTX ISA version 7.0.\nTarget string\nsm_86\nintroduced in PTX ISA version 7.1.\nTarget string\nsm_87\nintroduced in PTX ISA version 7.4.\nTarget string\nsm_88\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_89\nintroduced in PTX ISA version 7.8.\nTarget string\nsm_90\nintroduced in PTX ISA version 7.8.\nTarget string\nsm_90a\nintroduced in PTX ISA version 8.0.\nTarget string\nsm_100\nintroduced in PTX ISA version 8.6.\nTarget string\nsm_100f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_100a\nintroduced in PTX ISA version 8.6.\nTarget string\nsm_101\nintroduced in PTX ISA version 8.6. (Renamed to\nsm_110\n)\nTarget string\nsm_101f\nintroduced in PTX ISA version 8.8. (Renamed to\nsm_110f\n)\nTarget string\nsm_101a\nintroduced in PTX ISA version 8.6. (Renamed to\nsm_110a\n)\nTarget string\nsm_103\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_103f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_103a\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_110\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_110f\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_110a\nintroduced in PTX ISA version 9.0.\nTarget string\nsm_120\nintroduced in PTX ISA version 8.7.\nTarget string\nsm_120f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_120a\nintroduced in PTX ISA version 8.7.\nTarget string\nsm_121\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_121f\nintroduced in PTX ISA version 8.8.\nTarget string\nsm_121a\nintroduced in PTX ISA version 8.8.\nTarget ISA Notes\nThe\n.target\ndirective is supported on all target architectures.\nExamples\n.target sm_10       // baseline target architecture\n.target sm_13       // supports double-precision\n.target sm_20, texmode_independent\n.target sm_90       // baseline target architecture\n.target sm_90a      // PTX using architecture-specific features\n.target sm_100f     // PTX using family-specific features\n11.1.3.\nPTX Module Directives:\n.address_size\n\n.address_size\nAddress size used throughout PTX module.\nSyntax\n.address_size  address-size\naddress-size = { 32, 64 };\nDescription\nSpecifies the address size assumed throughout the module by the PTX code and the binary DWARF\ninformation in PTX.\nRedefinition of this directive within a module is not allowed. In the presence of separate\ncompilation all modules must specify (or default to) the same address size.\nThe\n.address_size\ndirective is optional, but it must immediately follow the\n.target\ndirective if present within a module.\nSemantics\nIf the\n.address_size\ndirective is omitted, the address size defaults to 32.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n// example directives\n   .address_size 32       // addresses are 32 bit\n   .address_size 64       // addresses are 64 bit\n\n// example of directive placement within a module\n   .version 2.3\n   .target sm_20\n   .address_size 64\n...\n.entry foo () {\n...\n}\n11.2.\nSpecifying Kernel Entry Points and Functions\n\nThe following directives specify kernel entry points and functions.\n.entry\n.func\n11.2.1.\nKernel and Function Directives:\n.entry\n\n.entry\nKernel entry point and body, with optional parameters.\nSyntax\n.entry kernel-name ( param-list )  kernel-body\n.entry kernel-name  kernel-body\nDescription\nDefines a kernel entry point name, parameters, and body for the kernel function.\nParameters are passed via\n.param\nspace memory and are listed within an optional parenthesized\nparameter list. Parameters may be referenced by name within the kernel body and loaded into\nregisters using\nld.param{::entry}\ninstructions.\nIn addition to normal parameters, opaque\n.texref\n,\n.samplerref\n, and\n.surfref\nvariables\nmay be passed as parameters. These parameters can only be referenced by name within texture and\nsurface load, store, and query instructions and cannot be accessed via\nld.param\ninstructions.\nThe shape and size of the CTA executing the kernel are available in special registers.\nSemantics\nSpecify the entry point for a kernel program.\nAt kernel launch, the kernel dimensions and properties are established and made available via\nspecial registers, e.g.,\n%ntid\n,\n%nctaid\n, etc.\nPTX ISA Notes\nFor PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter\nlist. For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body.\nThe maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764\nbytes. Depending upon the PTX ISA version, the parameter size limit varies. The following table\nshows the allowed parameter size for a PTX ISA version:\nPTX ISA Version\nMaximum parameter size (In bytes)\nPTX ISA version 8.1 and above\n32764\nPTX ISA version 1.5 and above\n4352\nPTX ISA version 1.4 and above\n256\nThe CUDA and OpenCL drivers support the following limits for parameter memory:\nDriver\nParameter memory size\nCUDA\n256 bytes for\nsm_1x\n, 4096 bytes for\nsm_2x\nand\nhigher\n,\n32764 bytes fo\nsm_70\nand higher\nOpenCL\n32764 bytes for\nsm_70\nand higher, 4352 bytes on\nsm_6x\nand lower\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry cta_fft\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r<99>;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n\n.entry prefix_sum ( .param .align 4 .s32 pitch[8000] )\n{\n    .reg .s32 %t;\n    ld.param::entry.s32  %t, [pitch];\n    ...\n}\n11.2.2.\nKernel and Function Directives:\n.func\n\n.func\nFunction definition.\nSyntax\n.func {.attribute(attr-list)} fname {.noreturn} {.abi_preserve N} {.abi_preserve_control N} function-body\n.func {.attribute(attr-list)} fname (param-list) {.noreturn} {.abi_preserve N} {.abi_preserve_control N} function-body\n.func {.attribute(attr-list)} (ret-param) fname (param-list) {.abi_preserve N} {.abi_preserve_control N} function-body\nDescription\nDefines a function, including input and return parameters and optional function body.\nAn optional\n.noreturn\ndirective indicates that the function does not return to the caller\nfunction.\n.noreturn\ndirective cannot be specified on functions which have return parameters. See\nthe description of\n.noreturn\ndirective in\nPerformance-Tuning Directives: .noreturn\n.\nAn optional\n.attribute\ndirective specifies additional information associated with the\nfunction. See the description of\nVariable and Function Attribute Directive: .attribute\nfor allowed attributes.\nOptional\n.abi_preserve\nand\n.abi_preserve_control\ndirectives are used to specify the number\nof general purpose registers and control registers. See description of\nPerformance-Tuning Directives: .abi_preserve\nand\nPerformance-Tuning Directives: .abi_preserve_control\nfor more details.\nDirectives, if any specified, for a function, e.g.\n.noreturn\n, must be specified consistently\nbetween function declaration and definition.\nA\n.func\ndefinition with no body provides a function prototype.\nThe parameter lists define locally-scoped variables in the function body. Parameters must be base\ntypes in either the register or parameter state space. Parameters in register state space may be\nreferenced directly within instructions in the function body. Parameters in\n.param\nspace are\naccessed using\nld.param{::func}\nand\nst.param{::func}\ninstructions in the body. Parameter\npassing is call-by-value.\nThe last parameter in the parameter list may be a\n.param\narray of type\n.b8\nwith no size\nspecified. It is used to pass an arbitrary number of parameters to the function packed into a single\narray object.\nWhen calling a function with such an unsized last argument, the last argument may be omitted from\nthe\ncall\ninstruction if no parameter is passed through it. Accesses to this array parameter must\nbe within the bounds of the array. The result of an access is undefined if no array was passed, or\nif the access was outside the bounds of the actual array being passed.\nSemantics\nThe PTX syntax hides all details of the underlying calling convention and ABI.\nThe implementation of parameter passing is left to the optimizing translator, which may use a\ncombination of registers and stack locations to pass parameters.\nRelease Notes\nFor PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and\nrecursion is illegal.\nPTX ISA versions 2.0 and later with target\nsm_20\nor higher allow parameters in the\n.param\nstate space, implements an ABI with stack, and supports recursion.\nPTX ISA versions 2.0 and later with target\nsm_20\nor higher support at most one return value.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nSupport for unsized array parameter introduced in PTX ISA version 6.0.\nSupport for\n.noreturn\ndirective introduced in PTX ISA version 6.4.\nSupport for\n.attribute\ndirective introduced in PTX ISA version 8.0.\nSupport for\n.abi_preserve\nand\n.abi_preserve_control\ndirectives introduced in PTX ISA version 9.0.\nTarget ISA Notes\nFunctions without unsized array parameter supported on all target architectures.\nUnsized array parameter requires\nsm_30\nor higher.\n.noreturn\ndirective requires\nsm_30\nor higher.\n.attribute\ndirective requires\nsm_90\nor higher.\n.abi_preserve\nand\n.abi_preserve_control\ndirectives require\nsm_80\nor higher.\nExamples\n.func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl)\n{\n.reg .b32 localVar;\n\n... use N, dbl;\nother code;\n\nmov.b32 rval,result;\nret;\n}\n\n...\ncall (fooval), foo, (val0, val1);  // return value in fooval\n...\n\n.func foo (.reg .b32 N, .reg .f64 dbl) .noreturn\n{\n.reg .b32 localVar;\n... use N, dbl;\nother code;\nmov.b32 rval, result;\nret;\n}\n...\ncall foo, (val0, val1);\n...\n\n.func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[])\n{\n    .reg .b32 input0, input1;\n    ld.param.b32   input0, [numbers + 0];\n    ld.param.b32   input1, [numbers + 4];\n    ...\n    other code;\n    ret;\n}\n...\n\n.param .u32 N;\n.param .align 4 .b8 numbers[8];\nst.param.u32    [N], 2;\nst.param.b32    [numbers + 0], 5;\nst.param.b32    [numbers + 4], 10;\ncall (rval), bar, (N, numbers);\n...\n11.2.3.\nKernel and Function Directives:\n.alias\n\n.alias\nDefine an alias to existing function symbol.\nSyntax\n.alias fAlias, fAliasee;\nDescription\n.alias\nis a module scope directive that defines identifier\nfAlias\nto be an alias to function\nspecified by\nfAliasee\n.\nBoth\nfAlias\nand\nfAliasee\nare non-entry function symbols.\nIdentifier\nfAlias\nis a function declaration without body.\nIdentifier\nfAliasee\nis a function symbol which must be defined in the same module as\n.alias\ndeclaration. Function\nfAliasee\ncannot have\n.weak\nlinkage.\nPrototype of\nfAlias\nand\nfAliasee\nmust match.\nProgram can use either\nfAlias\nor\nfAlisee\nidentifiers to reference function defined with\nfAliasee\n.\nPTX ISA Notes\n.alias\ndirective introduced in PTX ISA 6.3.\nTarget ISA Notes\n.alias\ndirective requires\nsm_30\nor higher.\nExamples\n.visible .func foo(.param .u32 p) {\n   ...\n}\n.visible .func bar(.param .u32 p);\n.alias bar, foo;\n.entry test()\n{\n      .param .u32 p;\n      ...\n      call foo, (p);       // call foo directly\n       ...\n       .param .u32 p;\n       call bar, (p);        // call foo through alias\n}\n.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )\n{\n    .reg .b32 %r1, %r2, %r3;\n    ld.param.b32  %r1, [x];\n    ld.param.b32  %r2, [y];\n    ld.param.b32  %r3, [z];\n    ...\n}\n11.3.\nControl Flow Directives\n\nPTX provides directives for specifying potential targets for\nbrx.idx\nand\ncall\ninstructions. See the descriptions of\nbrx.idx\nand\ncall\nfor more information.\n.branchtargets\n.calltargets\n.callprototype\n11.3.1.\nControl Flow Directives:\n.branchtargets\n\n.branchtargets\nDeclare a list of potential branch targets.\nSyntax\nLabel:   .branchtargets  list-of-labels ;\nDescription\nDeclares a list of potential branch targets for a subsequent\nbrx.idx\n, and associates the list\nwith the label at the start of the line.\nAll control flow labels in the list must occur within the same function as the declaration.\nThe list of labels may use the compact, shorthand syntax for enumerating a range of labels having a\ncommon prefix, similar to the syntax described in\nParameterized Variable Names\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\n.function foo () {\n      .reg .u32 %r0;\n      ...\n      L1:\n      ...\n      L2:\n      ...\n      L3:\n      ...\n      ts: .branchtargets L1, L2, L3;\n      @p brx.idx %r0, ts;\n      ...\n\n.function bar() {\n      .reg .u32 %r0;\n      ...\n      N0:\n      ...\n      N1:\n      ...\n      N2:\n      ...\n      N3:\n      ...\n      N4:\n      ...\n      ts: .branchtargets N<5>;\n      @p brx.idx %r0, ts;\n      ...\n11.3.2.\nControl Flow Directives:\n.calltargets\n\n.calltargets\nDeclare a list of potential call targets.\nSyntax\nLabel:   .calltargets  list-of-functions ;\nDescription\nDeclares a list of potential call targets for a subsequent indirect call, and associates the list\nwith the label at the start of the line.\nAll functions named in the list must be declared prior to the\n.calltargets\ndirective, and all\nfunctions must have the same type signature.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\nExamples\ncalltgt:  .calltargets  fastsin, fastcos;\n...\n@p   call  (%f1), %r0, (%x), calltgt;\n...\n11.3.3.\nControl Flow Directives:\n.callprototype\n\n.callprototype\nDeclare a prototype for use in an indirect call.\nSyntax\n// no input or return parameters\nlabel: .callprototype _ .noreturn {.abi_preserve N} {.abi_preserve_control N};\n// input params, no return params\nlabel: .callprototype _ (param-list) .noreturn {.abi_preserve N} {.abi_preserve_control N};\n// no input params, // return params\nlabel: .callprototype (ret-param) _ {.abi_preserve N} {.abi_preserve_control N};\n// input, return parameters\nlabel: .callprototype (ret-param) _ (param-list) {.abi_preserve N} {.abi_preserve_control N};\nDescription\nDefines a prototype with no specific function name, and associates the prototype with a label. The\nprototype may then be used in indirect call instructions where there is incomplete knowledge of the\npossible call targets.\nParameters may have either base types in the register or parameter state spaces, or array types in\nparameter state space. The sink symbol\n'_'\nmay be used to avoid dummy parameter names.\nAn optional\n.noreturn\ndirective indicates that the function does not return to the caller\nfunction.\n.noreturn\ndirective cannot be specified on functions which have return parameters. See\nthe description of .noreturn directive in\nPerformance-Tuning Directives: .noreturn\n.\nOptional\n.abi_preserve\nand\n.abi_preserve_control\ndirectives are used to specify the number\nof general purpose registers and control registers. See description of\nPerformance-Tuning Directives: .abi_preserve\nand\nPerformance-Tuning Directives: .abi_preserve_control\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nSupport for\n.noreturn\ndirective introduced in PTX ISA version 6.4.\nSupport for\n.abi_preserve\nand\n.abi_preserve_control\ndirectives introduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher.\n.noreturn\ndirective requires\nsm_30\nor higher.\n.abi_preserve\nand\n.abi_preserve_control\ndirectives require\nsm_80\nor higher.\nExamples\nFproto1: .callprototype  _ ;\nFproto2: .callprototype  _ (.param .f32 _);\nFproto3: .callprototype  (.param .u32 _) _ ;\nFproto4: .callprototype  (.param .u32 _) _ (.param .f32 _);\n...\n@p   call  (%val), %r0, (%f1), Fproto4;\n...\n\n// example of array parameter\nFproto5: .callprototype _ (.param .b8 _[12]);\n\nFproto6: .callprototype  _ (.param .f32 _) .noreturn;\n...\n@p   call  %r0, (%f1), Fproto6;\n...\n\n// example of .abi_preserve\nFproto7: .callprototype _ (.param .b32 _) .abi_preserve 10;\n...\n@p   call %r0, (%r1), Fproto7;\n...\n11.4.\nPerformance-Tuning Directives\n\nTo provide a mechanism for low-level performance tuning, PTX supports the following directives,\nwhich pass information to the optimizing backend compiler.\n.maxnreg\n.maxntid\n.reqntid\n.minnctapersm\n.maxnctapersm\n(deprecated)\n.pragma\n.abi_preserve\n.abi_preserve_control\nThe\n.maxnreg\ndirective specifies the maximum number of registers to be allocated to a single\nthread;\nthe\n.maxntid\ndirective specifies the maximum number of threads\nin a thread block (CTA); the\n.reqntid\ndirective specifies the required number of threads in a\nthread block (CTA); and the\n.minnctapersm\ndirective specifies a minimum number of thread blocks\nto be scheduled on a single multiprocessor (SM). These can be used, for example, to throttle the\nresource requirements (e.g., registers) to increase total thread count and provide a greater\nopportunity to hide memory latency. The\n.minnctapersm\ndirective can be used together with either\nthe\n.maxntid\nor\n.reqntid\ndirective to trade-off registers-per-thread against multiprocessor\nutilization without needed to directly specify a maximum number of registers. This may achieve better\nperformance when compiling PTX for multiple devices having different numbers of registers per SM.\nDevice function directives\n.abi_preserve\nand\n.abi_preserve_control\nspecify number of data\nand control registers from callee save registers that a function must preserve for its caller. This\ncan be considered to be the number of general purpose and control registers live in the caller when function\nis called. Control registers refer to the number of divergent program points that happen in the calltree\nleading to current function call.\nCurrently, the\n.maxnreg\n,\n.maxntid\n,\n.reqntid\n, and\n.minnctapersm\ndirectives may be applied per-entry and must appear between an\n.entry\ndirective and its body.\nThe directives take precedence over any module-level constraints passed to the optimizing backend.\nA warning message is generated if the directives’ constraints are inconsistent or cannot be met for\nthe specified target device.\nA general\n.pragma\ndirective is supported for passing information to the PTX backend. The\ndirective passes a list of strings to the backend, and the strings have no semantics within the PTX\nvirtual machine model. The interpretation of\n.pragma\nvalues is determined by the backend\nimplementation and is beyond the scope of the PTX ISA. Note that\n.pragma\ndirectives may appear\nat module (file) scope, at entry-scope, or as statements within a kernel or device function body.\n11.4.1.\nPerformance-Tuning Directives:\n.maxnreg\n\n.maxnreg\nMaximum number of registers that can be allocated per thread.\nSyntax\n.maxnreg n\nDescription\nDeclare the maximum number of registers per thread in a CTA.\nSemantics\nThe compiler guarantees that this limit will not be exceeded. The actual number of registers used\nmay be less; for example, the backend may be able to compile to fewer registers, or the maximum\nnumber of registers may be further constrained by\n.maxntid\nand\n.maxctapersm\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxnreg 16 { ... }  // max regs per thread = 16\n11.4.2.\nPerformance-Tuning Directives:\n.maxntid\n\n.maxntid\nMaximum number of threads in the thread block (CTA).\nSyntax\n.maxntid nx\n.maxntid nx, ny\n.maxntid nx, ny, nz\nDescription\nDeclare the maximum number of threads in the thread block (CTA). This maximum is specified by giving\nthe maximum extent of each dimension of the 1D, 2D, or 3D CTA. The maximum number of threads is the\nproduct of the maximum extent in each dimension.\nSemantics\nThe maximum number of threads in the thread block, computed as the product of the maximum extent\nspecified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in\nwhich this directive appears. Exceeding the maximum number of threads results in a runtime error or\nkernel launch failure.\nNote that this directive guarantees that the\ntotal\nnumber of threads does not exceed the maximum,\nbut does not guarantee that the limit in any particular dimension is not exceeded.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256       { ... }  // max threads = 256\n.entry bar .maxntid 16,16,4   { ... }  // max threads = 1024\n11.4.3.\nPerformance-Tuning Directives:\n.reqntid\n\n.reqntid\nNumber of threads in the thread block (CTA).\nSyntax\n.reqntid nx\n.reqntid nx, ny\n.reqntid nx, ny, nz\nDescription\nDeclare the number of threads in the thread block (CTA) by specifying the extent of each dimension\nof the 1D, 2D, or 3D CTA. The total number of threads is the product of the number of threads in\neach dimension.\nSemantics\nThe size of each CTA dimension specified in any invocation of the kernel is required to be equal to\nthat specified in this directive. Specifying a different CTA dimension at launch will result in a\nruntime error or kernel launch failure.\nNotes\nThe\n.reqntid\ndirective cannot be used in conjunction with the\n.maxntid\ndirective.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .reqntid 256       { ... }  // num threads = 256\n.entry bar .reqntid 16,16,4   { ... }  // num threads = 1024\n11.4.4.\nPerformance-Tuning Directives:\n.minnctapersm\n\n.minnctapersm\nMinimum number of CTAs per SM.\nSyntax\n.minnctapersm ncta\nDescription\nDeclare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor\n(SM).\nNotes\nOptimizations based on\n.minnctapersm\nneed either\n.maxntid\nor\n.reqntid\nto be specified as\nwell.\nIf the total number of threads on a single SM resulting from\n.minnctapersm\nand\n.maxntid\n/\n.reqntid\nexceed maximum number of threads supported by an SM then directive\n.minnctapersm\nwill be ignored.\nIn PTX ISA version 2.1 or higher, a warning is generated if\n.minnctapersm\nis specified without\nspecifying either\n.maxntid\nor\n.reqntid\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0 as a replacement for\n.maxnctapersm\n.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256 .minnctapersm 4 { ... }\n11.4.5.\nPerformance-Tuning Directives:\n.maxnctapersm\n(deprecated)\n\n.maxnctapersm\nMaximum number of CTAs per SM.\nSyntax\n.maxnctapersm ncta\nDescription\nDeclare the maximum number of CTAs from the kernel’s grid that may be mapped to a single\nmultiprocessor (SM).\nNotes\nOptimizations based on .maxnctapersm generally need\n.maxntid\nto be specified as well. The\noptimizing backend compiler uses\n.maxntid\nand\n.maxnctapersm\nto compute an upper-bound on\nper-thread register usage so that the specified number of CTAs can be mapped to a single\nmultiprocessor. However, if the number of registers used by the backend is sufficiently lower than\nthis bound, additional CTAs may be mapped to a single multiprocessor. For this reason,\n.maxnctapersm\nhas been renamed to .minnctapersm in PTX ISA version 2.0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.3. Deprecated in PTX ISA version 2.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo .maxntid 256 .maxnctapersm 4 { ... }\n11.4.6.\nPerformance-Tuning Directives:\n.noreturn\n\n.noreturn\nIndicate that the function does not return to its caller function.\nSyntax\n.noreturn\nDescription\nIndicate that the function does not return to its caller function.\nSemantics\nAn optional\n.noreturn\ndirective indicates that the function does not return to caller\nfunction.\n.noreturn\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nThe directive cannot be specified on functions which have return parameters.\nIf a function with\n.noreturn\ndirective returns to the caller function at runtime, then the\nbehavior is undefined.\nPTX ISA Notes\nIntroduced in PTX ISA version 6.4.\nTarget ISA Notes\nRequires\nsm_30\nor higher.\nExamples\n.func foo .noreturn { ... }\n11.4.7.\nPerformance-Tuning Directives:\n.pragma\n\n.pragma\nPass directives to PTX backend compiler.\nSyntax\n.pragma list-of-strings ;\nDescription\nPass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler.\nThe\n.pragma\ndirective may occur at module-scope, at entry-scope, or at statement-level.\nSemantics\nThe interpretation of\n.pragma\ndirective strings is implementation-specific and has no impact on\nPTX semantics. See\nDescriptions of .pragma Strings\nfor\ndescriptions of the pragma strings defined in\nptxas\n.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.pragma \"nounroll\";    // disable unrolling in backend\n\n// disable unrolling for current kernel\n.entry foo .pragma \"nounroll\"; { ... }\n11.4.8.\nPerformance-Tuning Directives:\n.abi_preserve\n\n.abi_preserve\nSpecify number of general purpose registers that should be preserved by the callers of this function.\nSyntax\n.abi_preserve N\nDescription\nIt is an architecture agnostic value specifying actual number of general purpose registers.\nInternally ABI defines some general purpose registers as preserved (callee save) registers.\nInteger N specifies the actual number of general purpose registers that should be preserved by\nthe function.\n.abi_preserve\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nSemantics\nWhen this directive is specified compiler backend modifies low level ABI components to ensure that\nnumber of live data variables in the callers of this function that are stored in the callee save\nregisters are less than specified value.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.func bar() .abi_preserve 8\n\n// Indirect call via call prototype\n.func (.param .b32 out[30]) foo (.param .b32 in[30]) .abi_preserve 10 { ... }\n...\nmov.b64 lpfoo, foo;\nprot: .callprototype (.param .b32 out[30]) _ (.param .b32 in[30]) .abi_preserve 10;\ncall (out), lpfoo, (in), prot;\n11.4.9.\nPerformance-Tuning Directives:\n.abi_preserve_control\n\n.abi_preserve_control\nSpecify number of control registers that should be preserved by the callers of this function.\nSyntax\n.abi_preserve_control N\nDescription\nIt is an architecture agnostic value specifying the number of divergent program points that happen\nin the calltree leading to current function call.\nInternally ABI defines some control registers as preserved (callee save) registers.\nInteger N specifies the actual number of control registers that should be preserved by the function.\n.abi_preserve_control\ndirective can only be specified on device functions and must appear between\na\n.func\ndirective and its body.\nSemantics\nWhen this directive is specified compiler backend modifies low level ABI components to ensure that\nnumber of live control variables in the callers of this function that are stored in the callee save\ncontrol registers are less than specified value.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_80\nor higher.\nExamples\n.func foo() .abi_preserve_control 14\n\n// Indirect call via call prototype\n.func (.param .b32 out[30]) bar (.param .b32 in[30]) .abi_preserve_control 10 { ... }\n...\nmov.b64 lpbar, bar;\nprot: .callprototype (.param .b32 out[30]) _ (.param .b32 in[30]) .abi_preserve_control 10;\ncall (out), lpbar, (in), prot;\n11.5.\nDebugging Directives\n\nDWARF-format debug information is passed through PTX modules using the following directives:\n@@DWARF\n.section\n.file\n.loc\nThe\n.section\ndirective was introduced in PTX ISA version 2.0 and replaces the\n@@DWARF\nsyntax. The\n@@DWARF\nsyntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX\nISA version 1.x code.\nBeginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the\n.target\ndebug\nplatform option. This forward declaration directs PTX compilation to retain\nmappings for source-level debugging.\n11.5.1.\nDebugging Directives:\n@@dwarf\n\n@@dwarf\nDWARF-format information.\nSyntax\n@@DWARF dwarf-string\n\ndwarf-string may have one of the\n.byte   byte-list   // comma-separated hexadecimal byte values\n.4byte  int32-list  // comma-separated hexadecimal integers in range [0..2^32-1]\n.quad   int64-list  // comma-separated hexadecimal integers in range [0..2^64-1]\n.4byte  label\n.quad   label\nPTX ISA Notes\nIntroduced in PTX ISA version 1.2. Deprecated as of PTX ISA version 2.0, replaced by\n.section\ndirective.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n@@DWARF .section .debug_pubnames, \"\", @progbits\n@@DWARF .byte   0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n@@DWARF .4byte  .debug_info\n@@DWARF .4byte  0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n@@DWARF .4byte  0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n@@DWARF .byte   0x00, 0x00, 0x00, 0x00, 0x00\n11.5.2.\nDebugging Directives:\n.section\n\n.section\nPTX section definition.\nSyntax\n.section section_name { dwarf-lines }\n\ndwarf-lines have the following formats:\n  .b8    byte-list       // comma-separated list of integers\n                         // in range [-128..255]\n  .b16   int16-list      // comma-separated list of integers\n                         // in range [-2^15..2^16-1]\n  .b32   int32-list      // comma-separated list of integers\n                         // in range [-2^31..2^32-1]\n  label:                 // Define label inside the debug section\n  .b64   int64-list      // comma-separated list of integers\n                         // in range [-2^63..2^64-1]\n  .b32   label\n  .b64   label\n  .b32   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 32bit)\n  .b64   label+imm       // a sum of label address plus a constant integer byte\n                         // offset(signed, 64bit)\n  .b32   label1-label2   // a difference in label addresses between labels in\n                         // the same dwarf section (32bit)\n  .b64   label3-label4   // a difference in label addresses between labels in\n                         // the same dwarf section (64bit)\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0, replaces\n@@DWARF\nsyntax.\nlabel+imm expression introduced in PTX ISA version 3.2.\nSupport for\n.b16\nintegers in dwarf-lines introduced in PTX ISA version 6.0.\nSupport for defining\nlabel\ninside the DWARF section is introduced in PTX ISA version 7.2.\nlabel1-label2\nexpression introduced in PTX ISA version 7.5.\nNegative numbers in dwarf lines introduced in PTX ISA version 7.5.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.section .debug_pubnames\n{\n    .b32    LpubNames_end0-LpubNames_begin0\n  LpubNames_begin0:\n    .b8     0x2b, 0x00, 0x00, 0x00, 0x02, 0x00\n    .b32    .debug_info\n  info_label1:\n    .b32    0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63\n    .b32    0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172\n    .b8     0x00, 0x00, 0x00, 0x00, 0x00\n  LpubNames_end0:\n}\n\n.section .debug_info\n{\n    .b32 11430\n    .b8 2, 0\n    .b32 .debug_abbrev\n    .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49\n    .b8 0\n    .b32 3, 37, 176, -99\n    .b32 info_label1\n    .b32 .debug_loc+0x4\n    .b8 -11, 11, 112, 97\n    .b32 info_label1+12\n    .b64 -1\n    .b16 -5, -65535\n}\n11.5.3.\nDebugging Directives:\n.file\n\n.file\nSource file name.\nSyntax\n.file file_index \"filename\" {, timestamp, file_size}\nDescription\nAssociates a source filename with an integer index.\n.loc\ndirectives reference source files by\nindex.\n.file\ndirective allows optionally specifying an unsigned number representing time of last\nmodification and an unsigned integer representing size in bytes of source file.\ntimestamp\nand\nfile_size\nvalue can be 0 to indicate this information is not available.\ntimestamp\nvalue is in format of C and C++ data type\ntime_t\n.\nfile_size\nis an unsigned 64-bit integer.\nThe\n.file\ndirective is allowed only in the outermost scope, i.e., at the same level as kernel\nand device function declarations.\nSemantics\nIf timestamp and file size are not specified, they default to 0.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTimestamp and file size introduced in PTX ISA version 3.2.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.file 1 \"example.cu\"\n.file 2 \"kernel.cu\"\n.file 1 \"kernel.cu\", 1339013327, 64118\n11.5.4.\nDebugging Directives:\n.loc\n\n.loc\nSource file location.\nSyntax\n.loc file_index line_number column_position\n.loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2\nDescription\nDeclares the source file location (source file, line number, and column position) to be associated\nwith lexically subsequent PTX instructions.\n.loc\nrefers to\nfile_index\nwhich is defined by a\n.file\ndirective.\nTo indicate PTX instructions that are generated from a function that got inlined, additional\nattribute\n.inlined_at\ncan be specified as part of the\n.loc\ndirective.\n.inlined_at\nattribute specifies source location at which the specified function is inlined.\nfile_index2\n,\nline_number2\n, and\ncolumn_position2\nspecify the location at which function is inlined. Source\nlocation specified as part of\n.inlined_at\ndirective must lexically precede as source location in\n.loc\ndirective.\nThe\nfunction_name\nattribute specifies an offset in the DWARF section named\n.debug_str\n. Offset is specified as\nlabel\nexpression or\nlabel\n+\nimmediate\nexpression\nwhere\nlabel\nis defined in\n.debug_str\nsection. DWARF section\n.debug_str\ncontains ASCII\nnull-terminated strings that specify the name of the function that is inlined.\nNote that a PTX instruction may have a single associated source location, determined by the nearest\nlexically preceding .loc directive, or no associated source location if there is no preceding .loc\ndirective. Labels in PTX inherit the location of the closest lexically following instruction. A\nlabel with no following PTX instruction has no associated source location.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nfunction_name\nand\ninlined_at\nattributes are introduced in PTX ISA version 7.2.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.loc 2 4237 0\nL1:                        // line 4237, col 0 of file #2,\n                           // inherited from mov\n    mov.u32  %r1,%r2;      // line 4237, col 0 of file #2\n    add.u32  %r2,%r1,%r3;  // line 4237, col 0 of file #2\n...\nL2:                        // line 4239, col 5 of file #2,\n                           // inherited from sub\n    .loc 2 4239 5\n    sub.u32  %r2,%r1,%r3;  // line 4239, col 5 of file #2\n    .loc 1 21 3\n    .loc 1 9 3, function_name info_string0, inlined_at 1 21 3\n    ld.global.u32   %r1, [gg]; // Function at line 9\n    setp.lt.s32 %p1, %r1, 8;   // inlined at line 21\n    .loc 1 27 3\n    .loc 1 10 5, function_name info_string1, inlined_at 1 27 3\n    .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5\n    setp.ne.s32 %p2, %r1, 18;\n    @%p2 bra    BB2_3;\n\n    .section .debug_str {\n    info_string0:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 102 // f\n     .b8 111 // o\n     .b8 111 // o\n     .b8 118 // v\n     .b8 0\n\n    info_string1:\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 98  // b\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n     .b8 95  // _\n     .b8 90  // z\n     .b8 51  // 3\n     .b8 99  // c\n     .b8 97  // a\n     .b8 114 // r\n     .b8 118 // v\n     .b8 0\n    }\n11.6.\nLinking Directives\n\n.extern\n.visible\n.weak\n11.6.1.\nLinking Directives:\n.extern\n\n.extern\nExternal symbol declaration.\nSyntax\n.extern identifier\nDescription\nDeclares identifier to be defined external to the current module. The module defining such\nidentifier must define it as\n.weak\nor\n.visible\nonly once in a single object file. Extern\ndeclaration of symbol may appear multiple times and references to that get resolved against the\nsingle definition of that symbol.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.extern .global .b32 foo;  // foo is defined in another module\n11.6.2.\nLinking Directives:\n.visible\n\n.visible\nVisible (externally) symbol declaration.\nSyntax\n.visible identifier\nDescription\nDeclares identifier to be globally visible. Unlike C, where identifiers are globally visible unless\ndeclared static, PTX identifiers are visible only within the current module unless declared\n.visible\noutside the current.\nPTX ISA Notes\nIntroduced in PTX ISA version 1.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.visible .global .b32 foo;  // foo will be externally visible\n11.6.3.\nLinking Directives:\n.weak\n\n.weak\nVisible (externally) symbol declaration.\nSyntax\n.weak identifier\nDescription\nDeclares identifier to be globally visible but\nweak\n. Weak symbols are similar to globally visible\nsymbols, except during linking, weak symbols are only chosen after globally visible symbols during\nsymbol resolution. Unlike globally visible symbols, multiple object files may declare the same weak\nsymbol, and references to a symbol get resolved against a weak symbol only if no global symbols have\nthe same name.\nPTX ISA Notes\nIntroduced in PTX ISA version 3.1.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.weak .func (.reg .b32 val) foo;  // foo will be externally visible\n11.6.4.\nLinking Directives:\n.common\n\n.common\nVisible (externally) symbol declaration.\nSyntax\n.common identifier\nDescription\nDeclares identifier to be globally visible but “common”.\nCommon symbols are similar to globally visible symbols. However multiple object files may declare\nthe same common symbol and they may have different types and sizes and references to a symbol get\nresolved against a common symbol with the largest size.\nOnly one object file can initialize a common symbol and that must have the largest size among all\nother definitions of that common symbol from different object files.\n.common\nlinking directive can be used only on variables with\n.global\nstorage. It cannot be\nused on function symbols or on symbols with opaque type.\nPTX ISA Notes\nIntroduced in PTX ISA version 5.0.\nTarget ISA Notes\n.common\ndirective requires\nsm_20\nor higher.\nExamples\n.common .global .u32 gbl;\n11.7.\nCluster Dimension Directives\n\nThe following directives specify information about clusters:\n.reqnctapercluster\n.explicitcluster\n.maxclusterrank\nThe\n.reqnctapercluster\ndirective specifies the number of CTAs in the cluster. The\n.explicitcluster\ndirective specifies that the kernel should be launched with explicit cluster\ndetails. The\n.maxclusterrank\ndirective specifies the maximum number of CTAs in the cluster.\nThe cluster dimension directives can be applied only on kernel functions.\n11.7.1.\nCluster Dimension Directives:\n.reqnctapercluster\n\n.reqnctapercluster\nDeclare the number of CTAs in the cluster.\nSyntax\n.reqnctapercluster nx\n.reqnctapercluster nx, ny\n.reqnctapercluster nx, ny, nz\nDescription\nSet the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of\nthe 1D, 2D, or 3D cluster. The total number of CTAs is the product of the number of CTAs in each\ndimension. For kernels with\n.reqnctapercluster\ndirective specified, runtime will use the\nspecified values for configuring the launch if the same are not specified at launch time.\nSemantics\nIf cluster dimension is explicitly specified at launch time, it should be equal to the values\nspecified in this directive. Specifying a different cluster dimension at launch will result in a\nruntime error or kernel launch failure.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .reqnctapercluster 2         { . . . }\n.entry bar .reqnctapercluster 2, 2, 1   { . . . }\n.entry ker .reqnctapercluster 3, 2      { . . . }\n11.7.2.\nCluster Dimension Directives:\n.explicitcluster\n\n.explicitcluster\nDeclare that Kernel must be launched with cluster dimensions explicitly specified.\nSyntax\n.explicitcluster\nDescription\nDeclares that this Kernel should be launched with cluster dimension explicitly specified.\nSemantics\nKernels with\n.explicitcluster\ndirective must be launched with cluster dimension explicitly\nspecified (either at launch time or via\n.reqnctapercluster\n), otherwise program will fail with\nruntime error or kernel launch failure.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .explicitcluster         { . . . }\n11.7.3.\nCluster Dimension Directives:\n.maxclusterrank\n\n.maxclusterrank\nDeclare the maximum number of CTAs that can be part of the cluster.\nSyntax\n.maxclusterrank n\nDescription\nDeclare the maximum number of thread blocks (CTAs) allowed to be part of the cluster.\nSemantics\nProduct of the number of CTAs in each cluster dimension specified in any invocation of the kernel is\nrequired to be less or equal to that specified in this directive. Otherwise invocation will result\nin a runtime error or kernel launch failure.\nThe\n.maxclusterrank\ndirective cannot be used in conjunction with the\n.reqnctapercluster\ndirective.\nPTX ISA Notes\nIntroduced in PTX ISA version 7.8.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo ..maxclusterrank 8         { . . . }\n11.8.\nMiscellaneous Directives\n\nPTX provides the following miscellaneous directives:\n.blocksareclusters\n11.8.1.\nMiscellaneous Directives:\n.blocksareclusters\n\n.blocksareclusters\nSpecify that CUDA thread blocks are mapped to clusters.\nSyntax\n.blocksareclusters\nDescription\nDefault behavior of CUDA API is to specify the grid launch configuration by specifying the number of\nthread blocks and the number of threads per block.\nWhen\n.blocksareclusters\ndirective is specified, it implies that the grid launch configuration\nfor the corresponding\n.entry\nfunction is specifying the number of clusters, i.e. the launch\nconfiguration is specifying number of clusters instead of the number of thread blocks. In this case,\nthe number of thread blocks per cluster is specified by\n.reqnctapercluster\ndirective and the\nthread block size is specified with the\n.reqntid\ndirective.\n.blocksareclusters\ndirective is only allowed for\n.entry\nfunctions and also needs\n.reqntid\nand\n.reqnctapercluster\ndirectives to be specified.\nRefer to\nCUDA Programming Guide\nfor more details.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_90\nor higher.\nExamples\n.entry foo .reqntid 32, 32, 1 .reqnctapercluster 32, 32, 1 .blocksareclusters { ... }\n12.\nDescriptions of\n.pragma\nStrings\n\nThis section describes the\n.pragma\nstrings defined by ptxas.\n12.1.\nPragma Strings:\n\"nounroll\"\n\n\"nounroll\"\nDisable loop unrolling in optimizing the backend compiler.\nSyntax\n.pragma \"nounroll\";\nDescription\nThe\n\"nounroll\"\npragma\nis a directive to disable loop unrolling in the optimizing backend\ncompiler.\nThe\n\"nounroll\"\npragma\nis allowed at module, entry-function, and statement levels, with the\nfollowing meanings:\nmodule scope\ndisables unrolling for all loops in module, including loops preceding the\n.pragma\n.\nentry-function scope\ndisables unrolling for all loops in the entry function body.\nstatement-level pragma\ndisables unrolling of the loop for which the current block is the loop header.\nNote that in order to have the desired effect at statement level, the\n\"nounroll\"\ndirective must\nappear before any instruction statements in the loop header basic block for the desired loop. The\nloop header block is defined as the block that dominates all blocks in the loop body and is the\ntarget of the loop backedge. Statement-level\n\"nounroll\"\ndirectives appearing outside of loop\nheader blocks are silently ignored.\nPTX ISA Notes\nIntroduced in PTX ISA version 2.0.\nTarget ISA Notes\nRequires\nsm_20\nor higher. Ignored for\nsm_1x\ntargets.\nExamples\n.entry foo (...)\n.pragma \"nounroll\";  // do not unroll any loop in this function\n{\n...\n}\n\n.func bar (...)\n{\n...\nL1_head:\n     .pragma \"nounroll\";  // do not unroll this loop\n     ...\n@p   bra L1_end;\nL1_body:\n     ...\nL1_continue:\n     bra L1_head;\nL1_end:\n     ...\n}\n12.2.\nPragma Strings:\n\"used_bytes_mask\"\n\n\"used_bytes_mask\"\nMask for indicating used bytes in data of ld operation.\nSyntax\n.pragma \"used_bytes_mask mask\";\nDescription\nThe\n\"used_bytes_mask\"\npragma\nis a directive that specifies used bytes in a load\noperation based on the mask provided.\n\"used_bytes_mask\"\npragma\nneeds to be specified prior to a load instruction for which\ninformation about bytes used from the load operation is needed.\nPragma is ignored if instruction following it is not a load instruction.\nFor a load instruction without this pragma, all bytes from the load operation are assumed\nto be used.\nOperand\nmask\nis a 32-bit integer with set bits indicating the used bytes in data of\nload operation.\nSemantics\nEach bit in mask operand corresponds to a byte data where each set bit represents the used byte.\nMost-significant bit corresponds to most-significant byte of data.\n\n// For 4 bytes load with only lower 3 bytes used\n.pragma \"used_bytes_mask 0x7\";\nld.global.u32 %r0, [gbl];     // Higher 1 byte from %r0 is unused\n\n// For vector load of 16 bytes with lower 12 bytes used\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];  // %r3 unused\nPTX ISA Notes\nIntroduced in PTX ISA version 8.3.\nTarget ISA Notes\nRequires\nsm_50\nor higher.\nExamples\n.pragma \"used_bytes_mask 0xfff\";\nld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // Only lower 12 bytes used\n12.3.\nPragma Strings:\n\"enable_smem_spilling\"\n\n\"enable_smem_spilling\"\nEnable shared memory spilling for CUDA kernels.\nSyntax\n.pragma \"enable_smem_spilling\";\nDescription\nThe\n\"enable_smem_spilling\"\npragma\nis a directive that enables register spilling into shared memory.\nDuring the spilling process, registers are first spilled into shared memory, and once the allocated\nshared memory is full, any additional spills are redirected to local memory. This can enhance\nperformance by reducing memory access latency since shared memory accesses are faster than local memory.\nThe\n\"enable_smem_spilling\"\npragma\nis only allowed within the function scope. When applied, it enables\nshared memory spilling for the specified function.\nThe usage of pragma is valid only in certain scenarios and specific compilation modes. The usage of\npragma is disallowed under following cases and may result in an error:\nPer-function compilation mode: e.g., Separate Compilation, Device-debug, Whole program with recursive\nfunction calls, Extensible-whole-program\nKernels utilizing dynamically allocated shared memory\nKernels using\nsetmaxnreg\ninstruction\nNote\nIf launch bounds are not explicitly specified, the compiler assumes the maximum possible number of\nthreads per CTA to estimate shared memory allocated per CTA and corresponding spill size. However,\nif the kernel is launched with fewer threads per CTA than estimated, the shared memory allocated\nper CTA may exceed the compiler estimated size, thereby potentially limiting the number of CTAs\nthat can be launched on an SM. Due to this, using the pragma without launch bounds may lead to\nperformance regressions. Hence it is recommended to use this pragma only when launch bounds are\nexplicitly specified.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nRequires\nsm_75\nor higher.\nExamples\n.entry foo (...)\n{\n    ...\n    .pragma \"enable_smem_spilling\";   // Enable shared memory spilling for this function\n    ...\n}\n12.4.\nPragma Strings:\n\"frequency\"\n\n\"frequency\"\nSpecify frequency for basic block execution.\nSyntax\n.pragma \"frequency n\";\nDescription\nThe\n\"frequency\"\npragma\nis a directive that specifies the number of times a basic block is\nexecuted by an executing thread. The optimizing compiler backend treats this pragma as a hint\nwhich will be used for optimizations.\nOperand\nn\nis a 64-bit non-negative integer constant that specifies the execution frequency.\nNote that in order to have the desired effect of this pragma, it should be specified at the start of\nthe basic block. Basic block is defined as a straight-line sequence of instructions with only one\nentry point and one exit point.\nPTX ISA Notes\nIntroduced in PTX ISA version 9.0.\nTarget ISA Notes\nSupported on all target architectures.\nExamples\n.entry foo (...)\n{\n    .pragma \"frequency 32\";\n    ...\n}\n13.\nRelease Notes\n\nThis section describes the history of change in the PTX ISA and implementation. The first section\ndescribes ISA and implementation changes in the current release of PTX ISA version 9.1, and the\nremaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX\nISA version 2.0.\nTable 59\nshows the PTX release history.\nTable 59\nPTX Release History\n\nPTX ISA Version\nCUDA Release\nSupported Targets\nPTX ISA 1.0\nCUDA 1.0\nsm_{10,11}\nPTX ISA 1.1\nCUDA 1.1\nsm_{10,11}\nPTX ISA 1.2\nCUDA 2.0\nsm_{10,11,12,13}\nPTX ISA 1.3\nCUDA 2.1\nsm_{10,11,12,13}\nPTX ISA 1.4\nCUDA 2.2\nsm_{10,11,12,13}\nPTX ISA 1.5\ndriver r190\nsm_{10,11,12,13}\nPTX ISA 2.0\nCUDA 3.0, driver r195\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.1\nCUDA 3.1, driver r256\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.2\nCUDA 3.2, driver r260\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 2.3\nCUDA 4.0, driver r270\nsm_{10,11,12,13}\n,\nsm_20\nPTX ISA 3.0\nCUDA 4.1, driver r285\nsm_{10,11,12,13}\n,\nsm_20\nCUDA 4.2, driver r295\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_30\nPTX ISA 3.1\nCUDA 5.0, driver r302\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,35}\nPTX ISA 3.2\nCUDA 5.5, driver r319\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,35}\nPTX ISA 4.0\nCUDA 6.0, driver r331\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35}\n,\nsm_50\nPTX ISA 4.1\nCUDA 6.5, driver r340\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52}\nPTX ISA 4.2\nCUDA 7.0, driver r346\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\nPTX ISA 4.3\nCUDA 7.5, driver r352\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\nPTX ISA 5.0\nCUDA 8.0, driver r361\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\nPTX ISA 6.0\nCUDA 9.0, driver r384\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\nPTX ISA 6.1\nCUDA 9.1, driver r387\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\nPTX ISA 6.2\nCUDA 9.2, driver r396\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\nPTX ISA 6.3\nCUDA 10.0, driver r400\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 6.4\nCUDA 10.1, driver r418\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 6.5\nCUDA 10.2, driver r440\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_70\n,\nsm_72\n,\nsm_75\nPTX ISA 7.0\nCUDA 11.0, driver r445\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_80\nPTX ISA 7.1\nCUDA 11.1, driver r455\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.2\nCUDA 11.2, driver r460\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.3\nCUDA 11.3, driver r465\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86}\nPTX ISA 7.4\nCUDA 11.4, driver r470\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.5\nCUDA 11.5, driver r495\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.6\nCUDA 11.6, driver r510\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.7\nCUDA 11.7, driver r515\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87}\nPTX ISA 7.8\nCUDA 11.8, driver r520\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_90\nPTX ISA 8.0\nCUDA 12.0, driver r525\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.1\nCUDA 12.1, driver r530\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.2\nCUDA 12.2, driver r535\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.3\nCUDA 12.3, driver r545\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.4\nCUDA 12.4, driver r550\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nPTX ISA 8.5\nCUDA 12.5, driver r555\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\nCUDA 12.6, driver r560\nPTX ISA 8.6\nCUDA 12.7, driver r565\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100a,101,101a}\nPTX ISA 8.7\nCUDA 12.8, driver r570\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100,101,101a}\n,\nsm_{120,120a}\nPTX ISA 8.8\nCUDA 12.9, driver r575\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,101,101f,101a,103,103f,103a}\n,\nsm_{120,120f,120a,121,121f,121a}\nPTX ISA 9.0\nCUDA 13.0, driver r580\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,88,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,103,103f,103a}\n,\nsm_{110,110f,110a}\n,\nsm_{120,120f,120a,121,121f,121a}\nPTX ISA 9.1\nCUDA 13.1, driver r590\nsm_{10,11,12,13}\n,\nsm_20\n,\nsm_{30,32,35,37}\n,\nsm_{50,52,53}\n,\nsm_{60,61,62}\n,\nsm_{70,72,75}\n,\nsm_{80,86,87,88,89}\n,\nsm_{90,90a}\n,\nsm_{100,100f,100a,103,103f,103a}\n,\nsm_{110,110f,110a}\n,\nsm_{120,120f,120a,121,121f,121a}\nTable 60\nArch-specific/ Family-specific PTX Features Release History\n\nInstruction\nVariant\nPTX ISA Version\nSupported Targets\ntensormap.replace\nBase variant\n8.3\nsm_90a\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ntensormap.replace.swizzle_atomicity\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\n.elemtype\nfor\n.field3\nwith values\n13\n,\n14\n,\n15\nfor\nnew_val\n8.7\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\n.swizzle_mode\nfor\n.field3\nwith value\n4\nfor\nnew_val\n8.8\nsm_103a\nwgmma.mma_async\n,\nwgmma.mma_async.sp\n,\nwgmma.fence\n,\nwgmma.commit_group\n,\nwgmma.wait_group\nBase variant\n8.0\nsm_90a\nsetmaxnreg\nBase variant\n8.0\nsm_90a\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nmultimem.ld_reduce\n,\nmultimem.st\n,\nmultimem.red\nTypes\n.e5m2\n,\n.e4m3\n,\n.e5m2x2\n,\n.e4m3x2\n,\n.e4m3x4\n,\n.e5m2x4\n8.6\nsm_100a\n,\nsm_120a\n,\nsm_121a\n8.8\nsm_100f\n9.0\nsm_110f\n.acc::f16\nqualifier\n8.6\nsm_100a\n,\nsm_120a\n,\nsm_121a\n8.8\nsm_100f\n9.0\nsm_110f\ncvt\ncvt\nfor\n.f32\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n/\n.ue8m0x2\nAND\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\nto\n.f16x2\nAND\n.ue8m0x2\nto\n.bf16x2\nAND\n.bf16x2\nto\n.ue8m0x2\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ncvt\nwith\n.rs\nrounding mode\n8.7\nsm_100a\n8.8\nsm_103a\ncvt\nwith\n.s2f6x2\ntype\n9.1\nsm_100a\n,\nsm_103a\n,\nsm_110a\n,\nsm_120a\n,\nsm_121a\ncvt\nfor\n.f16x2\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n9.1\nsm_100f\n,\nsm_110f\n,\nsm_120f\ncvt\nfor\n.bf16x2\nto\n.e2m1x2\n/\n.e2m3x2\n/\n.e3m2x2\n/\n.e4m3x2\n/\n.e5m2x2\n9.1\nsm_100f\n,\nsm_110f\n,\nsm_120f\ncp.async.bulk.tensor\n.tile::gather4\nand\n.im2col::w\nwith\n.shared::cluster\nas destination state\nspace\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\n.tile::scatter4\nand\n.im2col::w::128\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\n.cta_group\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ncp.async.bulk.prefetch.tensor\n.tile::gather4\n,\n.im2col::w\n,\n.im2col::w::128\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nredux.sync\nType\n.f32\nand\n.abs\n,\n.NaN\nqualifiers\n8.6\nsm_100a\n8.8\nsm_100f\nclusterlaunchcontrol.try_cancel\n.multicast::cluster::all\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nldmatrix\nShapes\n.m16n16\n,\n.m8n16\nAND\nType\n.b8\nAND\nQualifiers\n.src_fmt\n,\n.dst_fmt\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\nstmatrix\nShapes\n.m16n8\nAND\nType\n.b8\nAND\n8.6\nsm_100a\n,\nsm_120a\n8.8\nsm_100f\n,\nsm_120f\n9.0\nsm_110f\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ntcgen05.ld\n,\ntcgen05.st\n,\ntcgen05.wait\n,\ntcgen05.cp\n,\ntcgen05.fence\n,\ntcgen05.commit\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\ntcgen05.ld.red\nBase variant\n8.8\nsm_103f\n9.0\nsm_110f\ntcgen05.shift\nBase variant\n8.6\nsm_100a\n8.8\nsm_103a\n9.0\nsm_110a\ntcgen05.mma\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nArgument\nscale-input-d\n8.6\nsm_100a\n8.8\nsm_100f\nQualifiers\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\n8.6\nsm_100a\nQualifiers\n.block16\n,\n.block32\n8.8\nsm_100f\n,\nsm_110f\nK shape value\n96\n8.8\nsm_103a\ntcgen05.mma.sp\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nKind\n.kind::mxf4nvf4\nand\n.kind::mxf4\n8.6\nsm_100a\n8.8\nsm_103a\n9.0\nsm_110a\nArgument\nscale-input-d\n8.6\nsm_100a\n8.8\nsm_100f\nQualifiers\n.scale_vec::1X\n,\n.scale_vec::2X\n,\n.scale_vec::4X\n8.6\nsm_100a\nQualifiers\n.block16\n,\n.block32\n8.8\nsm_100f\n,\nsm_110f\ntcgen05.mma.ws\n,\ntcgen05.mma.ws.sp\nBase variant\n8.6\nsm_100a\n8.8\nsm_100f\n9.0\nsm_110f\nKind\n.kind::i8\n8.6\nsm_100a\n9.0\nsm_110a\nmma\nTypes\n.e3m2\n,\n.e2m3\n,\n.e2m1\nAND Qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n8.7\nsm_120a\n8.8\nsm_120f\nmma.sp\nTypes\n.e3m2\n,\n.e2m3\n,\n.e2m1\nAND Qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n8.7\nsm_120a\n8.8\nsm_120f\nKind\n.kind::mxf4nvf4\nand\n.kind::mxf4\n8.7\nsm_120a\n,\nsm_121a\n13.1.\nChanges in PTX ISA Version 9.1\n\nNew Features\nPTX ISA version 9.1 introduces the following new features:\nAdds support for\n.volatile\nqualifier with\n.local\nstate space for\nld\nand\nst\ninstructions.\nAdds support for\n.f16x2\nand\n.bf16x2\nsource types for\ncvt\ninstruction\nwith destination types\n.e2m1x2\n,\n.e2m3x2\n,\n.e3m2x2\n,\n.e4m3x2\n,\n.e5m2x2\n.\nAdds support for\n.scale_vec::4X\nwith\n.ue8m0\nas\n.stype\nwith\n.kind::mxf4nvf4\nfor\nmma\n/\nmma.sp\ninstructions.\nAdds support for\n.s2f6x2\ninstruction type for\ncvt\ninstruction.\nAdds support for\nmultimem.cp.async.bulk\nand\nmultimem.cp.reduce.async.bulk\ninstructions.\nSemantic Changes and Clarifications\nNone.\n13.2.\nChanges in PTX ISA Version 9.0\n\nNew Features\nPTX ISA version 9.0 introduces the following new features:\nAdds support for\nsm_88\ntarget architecture.\nAdds support for\nsm_110\ntarget architecture.\nAdds support for target\nsm_110f\nthat supports family-specific features.\nAdds support for target\nsm_110a\nthat supports architecture-specific features.\nAdds support for pragma\nenable_smem_spilling\nthat is used to enable shared\nmemory spilling for a function.\nAdds support for pragma\nfrequency\nthat is used to specify the execution frequency of a basic\nblock.\nAdds support for directive\n.blocksareclusters\nthat is used to specify that CUDA thread blocks\nare mapped to clusters.\nExtends\nsize\noperand of\nst.bulk\ninstruction to support 32-bit length.\nAdds support for performance-tuning directives\n.abi_preserve\nand\n.abi_preserve_control\nthat are used to specify the number of data and control registers that should be preserved by the\ncallers of a function.\nNotes\nTargets\nsm_{101,101f,101a}\nare renamed to targets\nsm_{110,110f,110a}\nfrom PTX ISA version 9.0.\nSemantic Changes and Clarifications\nAll\ntcgen05\ninstructions(\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n,\ntcgen05.cp\n,\ntcgen05.shift\n,\ntcgen05.mma\n,\ntcgen05.mma.sp\n,\ntcgen05.mma.ws,\ntcgen05.mma.ws.sp\n,\ntcgen05.commit\n) within a kernel must specify the same value for the\n.cta_group\nqualifier.\n13.3.\nChanges in PTX ISA Version 8.8\n\nNew Features\nPTX ISA version 8.8 introduces the following new features:\nAdds support for\nsm_103\ntarget architecture.\nAdds support for target\nsm_103a\nthat supports architecture-specific features.\nAdds support for\nsm_121\ntarget architecture.\nAdds support for target\nsm_121a\nthat supports architecture-specific features.\nIntroduces family-specific target architectures that are represented with “f” suffix.\nPTX for family-specific targets is compatible with all subsequent targets in same family.\nAdds support for\nsm_100f\n,\nsm_101f\n,\nsm_103f\n,\nsm_120f\n,\nsm_121f\n.\nExtends\nmin\nand\nmax\ninstructions to support three input arguments.\nExtends\ntcgen05.mma\ninstruction to add support for new\nscale_vectorsize\nqualifiers\n.block16\nand\n.block32\nand K dimension 96.\nExtends\n.field3\nof\ntensormap.replace\ninstruction to support 96B swizzle mode.\nAdds support for\ntcgen05.ld.red\ninstruction.\nExtends\nld\n,\nld.global.nc\nand\nst\ninstructions to support 256b load/store operations.\nTable 61\nshows the list of features that are\nsupported on family-specific targets:\nTable 61\nList of features promoted to family-specific architecture\n\nFeature\nSupported targets\n.m16n8\n,\n.m16n16\n,\n.m8n16\nshapes and\n.b8\ntype for\nldmatrix\n/\nstmatrix\nsm_100f\n,\nsm_101f\n,\nsm_120f\nShapes for\ntcgen05\n.16x64b\n.16x128b\n,\n.16x256b\n,\n.16x32bx2\n,\n.32x32b\n,\n.4x256b\n,\n.32x128b\n,\n.64x128b\n,\n.128x256b\n,\n.128x128b\n,\n.31x256b\nsm_100f\n,\nsm_101f\nsetmaxnreg\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.cta_group\nmodifier\nsm_100f\n,\nsm_101f\ncvt\nwith\n.e2m1x2\n,\n.e3m2x2\n,\n.e2m3x2\n,\n.ue8m0x2\nsm_100f\n,\nsm_101f\n,\nsm_120f\nmultimem\nwith\n.acc::f16\nand\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\n,\n.e4m3x4\ntypes\nsm_100f\n,\nsm_101f\ntensormap.replace\nsm_100f\n,\nsm_101f\n,\nsm_120f\ntcgen05.ld.red\nsm_101f\n,\nsm_103f\ntcgen05.ld\n/\nst\n/\nfence\n/\nwait\n/\ncommit\n/\ncp\n/\nalloc\n/\ndealloc\n/\nrelinquish_alloc_permit\nsm_100f\n,\nsm_101f\ntcgen05.mma{.ws}{.sp}\n(except\nkind::mxf4\n/\nkind::mxf4nvf4\nfor\n.sp\n)\nsm_100f\n,\nsm_101f\ntcgen05\n.kind::mxf4nvf4\n,\n.kind::mxf4\n,\n.kind::mxf8f6f4\n,\n.kind::f16\n,\n.kind::tf32\n,\n.kind::f8f6f4\nsm_100f\n,\nsm_101f\n.ashift\n,\n.collector_usage\nmodifiers for\ntcgen05\nsm_100f\n,\nsm_101f\nModifiers\n.b8x16\n,\n.b6x16_p32\n,\n.b4x16_p64\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.block_scale\nmodifier\nsm_100f\n,\nsm_101f\n,\nsm_120f\nmma{.sp}\nwith\n.e3m2\n,\n.e2m3\n,\n.e2m1\ntypes and\n.kind\n,\n.block_scale\n,\n.scale_vec_size\nmodifiers\n(except\n.sp\nwith\nmxf4\n/\nmxf4nvf4\n)\nsm_120f\n.scale_vec::1X\n/\n2X\n/\n4X\nmodifiers\nsm_120f\n.block16\n/\n.block32\nmodifiers (alias to\nscale_vec\n)\nsm_100f\n,\nsm_101f\n.warpx2::02_13\n,\n.warpx2::01_23\n,\n.warpx4\n,\n.pack::16b\n,\n.unpack::16b\nmodifiers for\ntcgen05\nsm_100f\n,\nsm_101f\nclusterlaunchcontrol.try_cancel\nmulticast::cluster::all\nsm_100f\n,\nsm_101f\n,\nsm_120f\n.tile::scatter4\n,\n.tile::gather4\n,\n.im2col::w\n,\n.im2col::w::128\nsm_100f\n,\nsm_101f\nredux.f32\nsm_100f\nscale-input-d\nfor\ntcgen05\nsm_100f\nSemantic Changes and Clarifications\nClarified the behavior of float-to-integer conversions for\nNaN\ninput.\n13.4.\nChanges in PTX ISA Version 8.7\n\nNew Features\nPTX ISA version 8.7 introduces the following new features:\nAdds support for\nsm_120\ntarget architecture.\nAdds support for target\nsm_120a\nthat supports architecture-specific features.\nExtends\ntcgen05.mma\ninstruction to add support for\n.kind::mxf4nvf4\nand\n.scale_vec::4X\nqualifiers.\nExtends\nmma\ninstructions to support\n.f16\ntype accumulator and shape\n.m16n8k16\nwith\nFP8 types\n.e4m3\nand\n.e5m2\n.\nExtends\ncvt\ninstruction to add support for\n.rs\nrounding mode and destination types\n.e2m1x4\n,\n.e4m3x4\n,\n.e5m2x4\n,\n.e3m2x4\n,\n.e2m3x4\n.\nExtends support for\nst.async\nand\nred.async\ninstructions to add support for\n.mmio\n,\n.release\n,\n.global\nand\n.scope\nqualifiers.\nExtends\ntensormap.replace\ninstruction to add support for values\n13\nto\n15\nfor\n.elemtype\nqualifier.\nExtends\nmma\nand\nmma.sp::ordered_metadata\ninstructions to add support for types\n.e3m2\n/\n.e2m3\n/\n.e2m1\nand qualifiers\n.kind\n,\n.block_scale\n,\n.scale_vec_size\n.\nSemantic Changes and Clarifications\nClarified that in\n.tile::gather4\n,\n.tile::scatter4\nmodes, tensor coordinates need to be\nspecified as {col_idx, row_idx0, row_idx1, row_idx2, row_idx3} i.e. {x, y0, y1, y2, y3} instead\nof {x0, x1, x2, x3, y}.\nUpdated\nInstruction descriptor\nof\ntcgen05.mma\ninstruction\nto clarify the bits that are reserved for future use.\n13.5.\nChanges in PTX ISA Version 8.6\n\nNew Features\nPTX ISA version 8.6 introduces the following new features:\nAdds support for\nsm_100\ntarget architecture.\nAdds support for target\nsm_100a\nthat supports architecture-specific features.\nAdds support for\nsm_101\ntarget architecture.\nAdds support for target\nsm_101a\nthat supports architecture-specific features.\nExtends\ncp.async.bulk\nand\ncp.async.bulk.tensor\ninstructions to add\n.shared::cta\nas destination state space.\nExtends\nfence\ninstruction to add support for\n.acquire\nand\n.release\nqualifiers.\nExtends\nfence\nand\nfence.proxy\ninstructions to add support for\n.sync_restrict\nqualifier.\nExtends\nldmatrix\ninstruction to support\n.m16n16\n,\n.m8n16\nshapes and\n.b8\ntype.\nExtends\nldmatrix\ninstruction to support\n.src_fmt\n,\n.dst_fmt\nqualifiers.\nExtends\nstmatrix\ninstruction to support\n.m16n8\nshape and\n.b8\ntype.\nAdds support for\nclusterlaunchcontrol\ninstruction.\nExtends\nadd\n,\nsub\nand\nfma\ninstructions to support mixed precision floating point\noperations with\n.f32\nas destaination operand type and\n.f16\n/\n.bf16\nas source operand\ntypes.\nExtends\nadd\n,\nsub\n,\nmul\nand\nfma\ninstructions to support\n.f32x2\ntype.\nExtends\ncvt\ninstruction with\n.tf32\ntype to support\n.satfinite\nqualifier\nfor\n.rn\n/\n.rz\nrounding modes.\nExtends\ncp.async.bulk\ninstruction to support\n.cp_mask\nqualifier and\nbyteMask\noperand.\nExtends\nmultimem.ld_reduce\nand\nmultimem.st\ninstructions to support\n.e5m2\n,\n.e5m2x2\n,\n.e5m2x4\n,\n.e4m3\n,\n.e4m3x2\nand\n.e4m3x4\ntypes.\nExtends\ncvt\ninstruction to support conversions to/from\n.e2m1x2\n,\n.e3m2x2\n,\n.e2m3x2\nand\n.ue8m0x2\ntypes.\nExtends\ncp.async.bulk.tensor\nand\ncp.async.bulk.prefetch.tensor\ninstructions to\nsupport new load_mode qualifiers\n.tile::scatter4\nand\n.tile::gather4\n.\nExtends\ntensormap.replace\ninstruction to add support for new qualifier\n.swizzle_atomicity\nfor supporting new swizzle modes.\nExtends\nmbarrier.arrive\n,\nmbarrier.arrive_drop\n,\n.mbarrier.test_wait\nand\n.mbarrier.try_wait\ninstructions to support\n.relaxed\nqualifier.\nExtends\ncp.async.bulk.tensor\nand\ncp.async.bulk.prefetch.tensor\ninstructions to\nsupport new load_mode qualifiers\n.im2col::w\nand\n.im2col::w::128\n.\nExtends\ncp.async.bulk.tensor\ninstruction to support new qualifier\n.cta_group\n.\nAdd support for\nst.bulk\ninstruction.\nAdds support for tcgen05 features and related instructions:\ntcgen05.alloc\n,\ntcgen05.dealloc\n,\ntcgen05.relinquish_alloc_permit\n,\ntcgen05.ld\n,\ntcgen05.st\n,\ntcgen05.wait\n,\ntcgen05.cp\n,\ntcgen05.shift\n,\ntcgen05.mma\n,\ntcgen05.mma.sp\n,\ntcgen05.mma.ws\n,\ntcgen05.mma.ws.sp\n,\ntcgen05.fence\nand\ntcgen05.commit\n.\nExtends\nredux.sync\ninstruction to add support for\n.f32\ntype with qualifiers\n.abs\nand\n.NaN\n.\nSemantic Changes and Clarifications\nNone.\n13.6.\nChanges in PTX ISA Version 8.5\n\nNew Features\nPTX ISA version 8.5 introduces the following new features:\nAdds support for\nmma.sp::ordered_metadata\ninstruction.\nSemantic Changes and Clarifications\nValues\n0b0000\n,\n0b0101\n,\n0b1010\n,\n0b1111\nfor sparsity metadata (operand\ne\n)\nof instruction\nmma.sp\nare invalid and their usage results in undefined behavior.\n13.7.\nChanges in PTX ISA Version 8.4\n\nNew Features\nPTX ISA version 8.4 introduces the following new features:\nExtends\nld\n,\nst\nand\natom\ninstructions with\n.b128\ntype to support\n.sys\nscope.\nExtends integer\nwgmma.mma_async\ninstruction to support\n.u8.s8\nand\n.s8.u8\nas\n.atype\nand\n.btype\nrespectively.\nExtends\nmma\n,\nmma.sp\ninstructions to support FP8 types\n.e4m3\nand\n.e5m2\n.\nSemantic Changes and Clarifications\nNone.\n13.8.\nChanges in PTX ISA Version 8.3\n\nNew Features\nPTX ISA version 8.3 introduces the following new features:\nAdds support for pragma\nused_bytes_mask\nthat is used to specify mask for used bytes for a load operation.\nExtends\nisspacep\n,\ncvta.to\n,\nld\nand\nst\ninstructions to accept\n::entry\nand\n::func\nsub-qualifiers with\n.param\nstate space qualifier.\nAdds support for\n.b128\ntype on instructions\nld\n,\nld.global.nc\n,\nldu\n,\nst\n,\nmov\nand\natom\n.\nAdd support for instructions\ntensormap.replace\n,\ntensormap.cp_fenceproxy\nand support for qualifier\n.to_proxykind::from_proxykind\non instruction\nfence.proxy\nto support modifying\ntensor-map\n.\nSemantic Changes and Clarifications\nNone.\n13.9.\nChanges in PTX ISA Version 8.2\n\nNew Features\nPTX ISA version 8.2 introduces the following new features:\nAdds support for\n.mmio\nqualifier on\nld\nand\nst\ninstructions.\nExtends\nlop3\ninstruction to allow predicate destination.\nExtends\nmultimem.ld_reduce\ninstruction to support\n.acc::f32\nqualifer to allow\n.f32\nprecision of the intermediate accumulation.\nExtends the asynchronous warpgroup-level matrix multiply-and-accumulate operation\nwgmma.mma_async\nto support\n.sp\nmodifier that allows matrix multiply-accumulate operation\nwhen input matrix A is sparse.\nSemantic Changes and Clarifications\nThe\n.multicast::cluster\nqualifier on\ncp.async.bulk\nand\ncp.async.bulk.tensor\ninstructions\nis optimized for target architecture\nsm_90a\nand may have substantially reduced performance on\nother targets and hence\n.multicast::cluster\nis advised to be used with\nsm_90a\n.\n13.10.\nChanges in PTX ISA Version 8.1\n\nNew Features\nPTX ISA version 8.1 introduces the following new features:\nAdds support for\nst.async\nand\nred.async\ninstructions for asynchronous store and\nasynchronous reduction operations respectively on shared memory.\nAdds support for\n.oob\nmodifier on half-precision\nfma\ninstruction.\nAdds support for\n.satfinite\nsaturation modifer on\ncvt\ninstruction for\n.f16\n,\n.bf16\nand\n.tf32\nformats.\nExtends support for\ncvt\nwith\n.e4m3\n/\n.e5m2\nto\nsm_89\n.\nExtends\natom\nand\nred\ninstructions to support vector types.\nAdds support for special register\n%aggr_smem_size\n.\nExtends\nsured\ninstruction with 64-bit\nmin\n/\nmax\noperations.\nAdds support for increased kernel parameter size of 32764 bytes.\nAdds support for multimem addresses in memory consistency model.\nAdds support for\nmultimem.ld_reduce\n,\nmultimem.st\nand\nmultimem.red\ninstructions to\nperform memory operations on multimem addresses.\nSemantic Changes and Clarifications\nNone.\n13.11.\nChanges in PTX ISA Version 8.0\n\nNew Features\nPTX ISA version 8.0 introduces the following new features:\nAdds support for target\nsm_90a\nthat supports architecture-specific features.\nAdds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation\nwgmma\n.\nExtends the asynchronous copy operations with bulk operations that operate on large data,\nincluding tensor data.\nIntroduces packed integer types\n.u16x2\nand\n.s16x2\n.\nExtends integer arithmetic instruction\nadd\nto allow packed integer types\n.u16x2\nand\n.s16x2\n.\nExtends integer arithmetic instructions\nmin\nand\nmax\nto allow packed integer types\n.u16x2\nand\n.s16x2\n, as well as saturation modifier\n.relu\non\n.s16x2\nand\n.s32\ntypes.\nAdds support for special register\n%current_graph_exec\nthat identifies the currently executing\nCUDA device graph.\nAdds support for\nelect.sync\ninstruction.\nAdds support for\n.unified\nattribute on functions and variables.\nAdds support for\nsetmaxnreg\ninstruction.\nAdds support for\n.sem\nqualifier on\nbarrier.cluster\ninstruction.\nExtends the\nfence\ninstruction to allow opcode-specific synchronizaion using\nop_restrict\nqualifier.\nAdds support for\n.cluster\nscope on\nmbarrier.arrive\n,\nmbarrier.arrive_drop\n,\nmbarrier.test_wait\nand\nmbarrier.try_wait\noperations.\nAdds support for transaction count operations on\nmbarrier\nobjects, specified with\n.expect_tx\nand\n.complete_tx\nqualifiers.\nSemantic Changes and Clarifications\nNone.\n13.12.\nChanges in PTX ISA Version 7.8\n\nNew Features\nPTX ISA version 7.8 introduces the following new features:\nAdds support for\nsm_89\ntarget architecture.\nAdds support for\nsm_90\ntarget architecture.\nExtends\nbar\nand\nbarrier\ninstructions to accept optional scope qualifier\n.cta\n.\nExtends\n.shared\nstate space qualifier with optional sub-qualifier\n::cta\n.\nAdds support for\nmovmatrix\ninstruction which transposes a matrix in registers across a warp.\nAdds support for\nstmatrix\ninstruction which stores one or more matrices to shared memory.\nExtends the\n.f64\nfloating point type\nmma\noperation with shapes\n.m16n8k4\n,\n.m16n8k8\n,\nand\n.m16n8k16\n.\nExtends\nadd\n,\nsub\n,\nmul\n,\nset\n,\nsetp\n,\ncvt\n,\ntanh\n,\nex2\n,\natom\nand\nred\ninstructions with\nbf16\nalternate floating point data format.\nAdds support for new alternate floating-point data formats\n.e4m3\nand\n.e5m2\n.\nExtends\ncvt\ninstruction to convert\n.e4m3\nand\n.e5m2\nalternate floating point data formats.\nAdds support for\ngriddepcontrol\ninstruction as a communication mechanism to control the\nexecution of dependent grids.\nExtends\nmbarrier\ninstruction to allow a new phase completion check operation\ntry_wait\n.\nAdds support for new thread scope\n.cluster\nwhich is a set of Cooperative Thread Arrays (CTAs).\nExtends\nfence\n/\nmembar\n,\nld\n,\nst\n,\natom\n, and\nred\ninstructions to accept\n.cluster\nscope.\nAdds support for extended visibility of shared state space to all threads within a cluster.\nExtends\n.shared\nstate space qualifier with\n::cluster\nsub-qualifier for cluster-level\nvisibility of shared memory.\nExtends\nisspacep\n,\ncvta\n,\nld\n,\nst\n,\natom\n, and\nred\ninstructions to accept\n::cluster\nsub-qualifier with\n.shared\nstate space qualifier.\nAdds support for\nmapa\ninstruction to map a shared memory address to the corresponding address\nin a different CTA within the cluster.\nAdds support for\ngetctarank\ninstruction to query the rank of the CTA that contains a given\naddress.\nAdds support for new barrier synchronization instruction\nbarrier.cluster\n.\nExtends the memory consistency model to include the new cluster scope.\nAdds support for special registers related to cluster information:\n%is_explicit_cluster\n,\n%clusterid\n,\n%nclusterid\n,\n%cluster_ctaid\n,\n%cluster_nctaid\n,\n%cluster_ctarank\n,\n%cluster_nctarank\n.\nAdds support for cluster dimension directives\n.reqnctapercluster\n,\n.explicitcluster\n, and\n.maxclusterrank\n.\nSemantic Changes and Clarifications\nNone.\n13.13.\nChanges in PTX ISA Version 7.7\n\nNew Features\nPTX ISA version 7.7 introduces the following new features:\nExtends\nisspacep\nand\ncvta\ninstructions to include the\n.param\nstate space for kernel\nfunction parameters.\nSemantic Changes and Clarifications\nNone.\n13.14.\nChanges in PTX ISA Version 7.6\n\nNew Features\nPTX ISA version 7.6 introduces the following new features:\nSupport for\nszext\ninstruction which performs sign-extension or zero-extension on a specified\nvalue.\nSupport for\nbmsk\ninstruction which creates a bitmask of the specified width starting at the\nspecified bit position.\nSupport for special registers\n%reserved_smem_offset_begin\n,\n%reserved_smem_offset_end\n,\n%reserved_smem_offset_cap\n,\n%reserved_smem_offset<2>\n.\nSemantic Changes and Clarifications\nNone.\n13.15.\nChanges in PTX ISA Version 7.5\n\nNew Features\nPTX ISA version 7.5 introduces the following new features:\nDebug information enhancements to support label difference and negative values in the\n.section\ndebugging directive.\nSupport for\nignore-src\noperand on\ncp.async\ninstruction.\nExtensions to the memory consistency model to introduce the following new concepts:\nA\nmemory proxy\nas an abstract label for different methods of memory access.\nVirtual aliases as distinct memory addresses accessing the same physical memory location.\nSupport for new\nfence.proxy\nand\nmembar.proxy\ninstructions to allow synchronization of\nmemory accesses performed via virtual aliases.\nSemantic Changes and Clarifications\nNone.\n13.16.\nChanges in PTX ISA Version 7.4\n\nNew Features\nPTX ISA version 7.4 introduces the following new features:\nSupport for\nsm_87\ntarget architecture.\nSupport for\n.level::eviction_priority\nqualifier which allows specifying cache eviction\npriority hints on\nld\n,\nld.global.nc\n,\nst\n, and\nprefetch\ninstructions.\nSupport for\n.level::prefetch_size\nqualifier which allows specifying data prefetch hints on\nld\nand\ncp.async\ninstructions.\nSupport for\ncreatepolicy\ninstruction which allows construction of different types of cache\neviction policies.\nSupport for\n.level::cache_hint\nqualifier which allows the use of cache eviction policies with\nld\n,\nld.global.nc\n,\nst\n,\natom\n,\nred\nand\ncp.async\ninstructions.\nSupport for\napplypriority\nand\ndiscard\noperations on cached data.\nSemantic Changes and Clarifications\nNone.\n13.17.\nChanges in PTX ISA Version 7.3\n\nNew Features\nPTX ISA version 7.3 introduces the following new features:\nExtends\nmask()\noperator used in initializers to also support integer constant expression.\nAdds support for stack manpulation instructions that allow manipulating stack using\nstacksave\nand\nstackrestore\ninstructions and allocation of per-thread stack using\nalloca\ninstruction.\nSemantic Changes and Clarifications\nThe unimplemented version of\nalloca\nfrom the older PTX ISA specification has been replaced with\nnew stack manipulation instructions in PTX ISA version 7.3.\n13.18.\nChanges in PTX ISA Version 7.2\n\nNew Features\nPTX ISA version 7.2 introduces the following new features:\nEnhances\n.loc\ndirective to represent inline function information.\nAdds support to define labels inside the debug sections.\nExtends\nmin\nand\nmax\ninstructions to support\n.xorsign\nand\n.abs\nmodifiers.\nSemantic Changes and Clarifications\nNone.\n13.19.\nChanges in PTX ISA Version 7.1\n\nNew Features\nPTX ISA version 7.1 introduces the following new features:\nSupport for\nsm_86\ntarget architecture.\nAdds a new operator,\nmask()\n, to extract a specific byte from variable’s address used in\ninitializers.\nExtends\ntex\nand\ntld4\ninstructions to return an optional predicate that indicates if data\nat specified coordinates is resident in memory.\nExtends single-bit\nwmma\nand\nmma\ninstructions to support\n.and\noperation.\nExtends\nmma\ninstruction to support\n.sp\nmodifier that allows matrix multiply-accumulate\noperation when input matrix A is sparse.\nExtends\nmbarrier.test_wait\ninstruction to test the completion of specific phase parity.\nSemantic Changes and Clarifications\nNone.\n13.20.\nChanges in PTX ISA Version 7.0\n\nNew Features\nPTX ISA version 7.0 introduces the following new features:\nSupport for\nsm_80\ntarget architecture.\nAdds support for asynchronous copy instructions that allow copying of data asynchronously from one\nstate space to another.\nAdds support for\nmbarrier\ninstructions that allow creation of\nmbarrier objects\nin memory and\nuse of these objects to synchronize threads and asynchronous copy operations initiated by threads.\nAdds support for\nredux.sync\ninstruction which allows reduction operation across threads in a\nwarp.\nAdds support for new alternate floating-point data formats\n.bf16\nand\n.tf32\n.\nExtends\nwmma\ninstruction to support\n.f64\ntype with shape\n.m8n8k4\n.\nExtends\nwmma\ninstruction to support\n.bf16\ndata format.\nExtends\nwmma\ninstruction to support\n.tf32\ndata format with shape\n.m16n16k8\n.\nExtends\nmma\ninstruction to support\n.f64\ntype with shape\n.m8n8k4\n.\nExtends\nmma\ninstruction to support\n.bf16\nand\n.tf32\ndata formats with shape\n.m16n8k8\n.\nExtends\nmma\ninstruction to support new shapes\n.m8n8k128\n,\n.m16n8k4\n,\n.m16n8k16\n,\n.m16n8k32\n,\n.m16n8k64\n,\n.m16n8k128\nand\n.m16n8k256\n.\nExtends\nabs\nand\nneg\ninstructions to support\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\nmin\nand\nmax\ninstructions to support\n.NaN\nmodifier and\n.f16\n,\n.f16x2\n,\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\nfma\ninstruction to support\n.relu\nsaturation mode and\n.bf16\nand\n.bf16x2\ndata formats.\nExtends\ncvt\ninstruction to support\n.relu\nsaturation mode and\n.f16\n,\n.f16x2\n,\n.bf16\n,\n.bf16x2\nand\n.tf32\ndestination formats.\nAdds support for\ntanh\ninstruction that computes hyperbolic-tangent.\nExtends\nex2\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nSemantic Changes and Clarifications\nNone.\n13.21.\nChanges in PTX ISA Version 6.5\n\nNew Features\nPTX ISA version 6.5 introduces the following new features:\nAdds support for integer destination types for half precision comparison instruction\nset\n.\nExtends\nabs\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nAdds support for\ncvt.pack\ninstruction which allows converting two integer values and packing\nthe results together.\nAdds new shapes\n.m16n8k8\n,\n.m8n8k16\nand\n.m8n8k32\non the\nmma\ninstruction.\nAdds support for\nldmatrix\ninstruction which loads one or more matrices from shared memory for\nmma\ninstruction.\nRemoved Features\nPTX ISA version 6.5 removes the following features:\nSupport for\n.satfinite\nqualifier on floating point\nwmma.mma\ninstruction has been\nremoved. This support was deprecated since PTX ISA version 6.4.\nSemantic Changes and Clarifications\nNone.\n13.22.\nChanges in PTX ISA Version 6.4\n\nNew Features\nPTX ISA version 6.4 introduces the following new features:\nAdds support for\n.noreturn\ndirective which can be used to indicate a function does not return\nto it’s caller function.\nAdds support for\nmma\ninstruction which allows performing matrix multiply-and-accumulate\noperation.\nDeprecated Features\nPTX ISA version 6.4 deprecates the following features:\nSupport for\n.satfinite\nqualifier on floating point\nwmma.mma\ninstruction.\nRemoved Features\nPTX ISA version 6.4 removes the following features:\nSupport for\nshfl\nand\nvote\ninstructions without the\n.sync\nqualifier has been removed\nfor\n.target\nsm_70\nand higher. This support was deprecated since PTX ISA version 6.0 as\ndocumented in PTX ISA version 6.2.\nSemantic Changes and Clarifications\nClarified that resolving references of a\n.weak\nsymbol considers only\n.weak\nor\n.visible\nsymbols with the same name and does not consider local symbols with the same name.\nClarified that in\ncvt\ninstruction, modifier\n.ftz\ncan only be specified when either\n.atype\nor\n.dtype\nis\n.f32\n.\n13.23.\nChanges in PTX ISA Version 6.3\n\nNew Features\nPTX ISA version 6.3 introduces the following new features:\nSupport for\nsm_75\ntarget architecture.\nAdds support for a new instruction\nnanosleep\nthat suspends a thread for a specified duration.\nAdds support for\n.alias\ndirective which allows definining alias to function symbol.\nExtends\natom\ninstruction to perform\n.f16\naddition operation and\n.cas.b16\noperation.\nExtends\nred\ninstruction to perform\n.f16\naddition operation.\nThe\nwmma\ninstructions are extended to support multiplicand matrices of type\n.s8\n,\n.u8\n,\n.s4\n,\n.u4\n,\n.b1\nand accumulator matrices of type\n.s32\n.\nSemantic Changes and Clarifications\nIntroduced the mandatory\n.aligned\nqualifier for all\nwmma\ninstructions.\nSpecified the alignment required for the base address and stride parameters passed to\nwmma.load\nand\nwmma.store\n.\nClarified that layout of fragment returned by\nwmma\noperation is architecture dependent and\npassing\nwmma\nfragments around functions compiled for different link compatible SM\narchitectures may not work as expected.\nClarified that atomicity for\n{atom/red}.f16x2}\noperations is guranteed separately for each of\nthe two\n.f16\nelements but not guranteed to be atomic as single 32-bit access.\n13.24.\nChanges in PTX ISA Version 6.2\n\nNew Features\nPTX ISA version 6.2 introduces the following new features:\nA new instruction\nactivemask\nfor querying active threads in a warp.\nExtends atomic and reduction instructions to perform\n.f16x2\naddition operation with mandatory\n.noftz\nqualifier.\nDeprecated Features\nPTX ISA version 6.2 deprecates the following features:\nThe use of\nshfl\nand\nvote\ninstructions without the\n.sync\nis deprecated retrospectively\nfrom PTX ISA version 6.0, which introduced the\nsm_70\narchitecture that implements\nIndependent Thread Scheduling\n.\nSemantic Changes and Clarifications\nClarified that\nwmma\ninstructions can be used in conditionally executed code only if it is\nknown that all threads in the warp evaluate the condition identically, otherwise behavior is\nundefined.\nIn the memory consistency model, the definition of\nmorally strong operations\nwas updated to\nexclude fences from the requirement of\ncomplete overlap\nsince fences do not access memory.\n13.25.\nChanges in PTX ISA Version 6.1\n\nNew Features\nPTX ISA version 6.1 introduces the following new features:\nSupport for\nsm_72\ntarget architecture.\nSupport for new matrix shapes\n32x8x16\nand\n8x32x16\nin\nwmma\ninstruction.\nSemantic Changes and Clarifications\nNone.\n13.26.\nChanges in PTX ISA Version 6.0\n\nNew Features\nPTX ISA version 6.0 introduces the following new features:\nSupport for\nsm_70\ntarget architecture.\nSpecifies the memory consistency model for programs running on\nsm_70\nand later architectures.\nVarious extensions to memory instructions to specify memory synchronization semantics and scopes\nat which such synchronization can be observed.\nNew instruction\nwmma\nfor matrix operations which allows loading matrices from memory,\nperforming multiply-and-accumulate on them and storing result in memory.\nSupport for new\nbarrier\ninstruction.\nExtends\nneg\ninstruction to support\n.f16\nand\n.f16x2\ntypes.\nA new instruction\nfns\nwhich allows finding n-th set bit in integer.\nA new instruction\nbar.warp.sync\nwhich allows synchronizing threads in warp.\nExtends\nvote\nand\nshfl\ninstructions with\n.sync\nmodifier which waits for specified\nthreads before executing the\nvote\nand\nshfl\noperation respectively.\nA new instruction\nmatch.sync\nwhich allows broadcasting and comparing a value across threads in\nwarp.\nA new instruction\nbrx.idx\nwhich allows branching to a label indexed from list of potential\ntargets.\nSupport for unsized array parameter for\n.func\nwhich can be used to implement variadic\nfunctions.\nSupport for\n.b16\ninteger type in dwarf-lines.\nSupport for taking address of device function return parameters using\nmov\ninstruction.\nSemantic Changes and Clarifications\nSemantics of\nbar\ninstruction were updated to indicate that executing thread waits for other\nnon-exited threads from it’s warp.\nSupport for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from\nthe spec.\nSupport for taking address of labels, using labels in initializers which was unimplemented has\nbeen removed from the spec.\nSupport for variadic functions which was unimplemented has been removed from the spec.\n13.27.\nChanges in PTX ISA Version 5.0\n\nNew Features\nPTX ISA version 5.0 introduces the following new features:\nSupport for\nsm_60\n,\nsm_61\n,\nsm_62\ntarget architecture.\nExtends atomic and reduction instructions to perform double-precision add operation.\nExtends atomic and reduction instructions to specify\nscope\nmodifier.\nA new\n.common\ndirective to permit linking multiple object files containing declarations of the\nsame symbol with different size.\nA new\ndp4a\ninstruction which allows 4-way dot product with accumulate operation.\nA new\ndp2a\ninstruction which allows 2-way dot product with accumulate operation.\nSupport for special register\n%clock_hi\n.\nSemantic Changes and Clarifications\nSemantics of cache modifiers on\nld\nand\nst\ninstructions were clarified to reflect cache\noperations are treated as performance hint only and do not change memory consistency behavior of the\nprogram.\nSemantics of\nvolatile\noperations on\nld\nand\nst\ninstructions were clarified to reflect how\nvolatile\noperations are handled by optimizing compiler.\n13.28.\nChanges in PTX ISA Version 4.3\n\nNew Features\nPTX ISA version 4.3 introduces the following new features:\nA new\nlop3\ninstruction which allows arbitrary logical operation on 3 inputs.\nAdds support for 64-bit computations in extended precision arithmetic instructions.\nExtends\ntex.grad\ninstruction to support\ncube\nand\nacube\ngeometries.\nExtends\ntld4\ninstruction to support\na2d\n,\ncube\nand\nacube\ngeometries.\nExtends\ntex\nand\ntld4\ninstructions to support optional operands for offset vector and depth\ncompare.\nExtends\ntxq\ninstruction to support querying texture fields from specific LOD.\nSemantic Changes and Clarifications\nNone.\n13.29.\nChanges in PTX ISA Version 4.2\n\nNew Features\nPTX ISA version 4.2 introduces the following new features:\nSupport for\nsm_53\ntarget architecture.\nSupport for arithmetic, comparsion and texture instructions for\n.f16\nand\n.f16x2\ntypes.\nSupport for\nmemory_layout\nfield for surfaces and\nsuq\ninstruction support for querying this\nfield.\nSemantic Changes and Clarifications\nSemantics for parameter passing under ABI were updated to indicate\nld.param\nand\nst.param\ninstructions used for argument passing cannot be predicated.\nSemantics of\n{atom/red}.add.f32\nwere updated to indicate subnormal inputs and results are\nflushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on\nshared memory preserve subnormal inputs and results and don’t flush them to zero.\n13.30.\nChanges in PTX ISA Version 4.1\n\nNew Features\nPTX ISA version 4.1 introduces the following new features:\nSupport for\nsm_37\nand\nsm_52\ntarget architectures.\nSupport for new fields\narray_size\n,\nnum_mipmap_levels\nand\nnum_samples\nfor Textures, and\nthe\ntxq\ninstruction support for querying these fields.\nSupport for new field\narray_size\nfor Surfaces, and the\nsuq\ninstruction support for\nquerying this field.\nSupport for special registers\n%total_smem_size\nand\n%dynamic_smem_size\n.\nSemantic Changes and Clarifications\nNone.\n13.31.\nChanges in PTX ISA Version 4.0\n\nNew Features\nPTX ISA version 4.0 introduces the following new features:\nSupport for\nsm_32\nand\nsm_50\ntarget architectures.\nSupport for 64bit performance counter special registers\n%pm0_64,..,%pm7_64\n.\nA new\nistypep\ninstruction.\nA new instruction,\nrsqrt.approx.ftz.f64\nhas been added to compute a fast approximation of the\nsquare root reciprocal of a value.\nSupport for a new directive\n.attribute\nfor specifying special attributes of a variable.\nSupport for\n.managed\nvariable attribute.\nSemantic Changes and Clarifications\nThe\nvote\ninstruction semantics were updated to clearly indicate that an inactive thread in a\nwarp contributes a 0 for its entry when participating in\nvote.ballot.b32\n.\n13.32.\nChanges in PTX ISA Version 3.2\n\nNew Features\nPTX ISA version 3.2 introduces the following new features:\nThe texture instruction supports reads from multi-sample and multisample array textures.\nExtends\n.section\ndebugging directive to include label + immediate expressions.\nExtends\n.file\ndirective to include timestamp and file size information.\nSemantic Changes and Clarifications\nThe\nvavrg2\nand\nvavrg4\ninstruction semantics were updated to indicate that instruction adds 1\nonly if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than\nbeing divided by 2).\n13.33.\nChanges in PTX ISA Version 3.1\n\nNew Features\nPTX ISA version 3.1 introduces the following new features:\nSupport for\nsm_35\ntarget architecture.\nSupport for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work.\nld.global.nc\nfor loading read-only global data though the non-coherent texture cache.\nA new funnel shift instruction,\nshf\n.\nExtends atomic and reduction instructions to perform 64-bit\n{and,\nor,\nxor}\noperations, and\n64-bit integer\n{min,\nmax}\noperations.\nAdds support for\nmipmaps\n.\nAdds support for indirect access to textures and surfaces.\nExtends support for generic addressing to include the\n.const\nstate space, and adds a new\noperator,\ngeneric()\n, to form a generic address for\n.global\nor\n.const\nvariables used in\ninitializers.\nA new\n.weak\ndirective to permit linking multiple object files containing declarations of the\nsame symbol.\nSemantic Changes and Clarifications\nPTX 3.1 redefines the default addressing for global variables in initializers, from generic\naddresses to offsets in the global state space. Legacy PTX code is treated as having an implicit\ngeneric()\noperator for each global variable used in an initializer. PTX 3.1 code should either\ninclude explicit\ngeneric()\noperators in initializers, use\ncvta.global\nto form generic\naddresses at runtime, or load from the non-generic address using\nld.global\n.\nInstruction\nmad.f32\nrequires a rounding modifier for\nsm_20\nand higher targets. However for\nPTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and\nmad.f32\nsilently\ndefaults to\nmad.rn.f32\n. For PTX ISA version 3.1, ptxas generates a warning and defaults to\nmad.rn.f32\n, and in subsequent releases ptxas will enforce the requirement for PTX ISA version\n3.2 and later.\n13.34.\nChanges in PTX ISA Version 3.0\n\nNew Features\nPTX ISA version 3.0 introduces the following new features:\nSupport for\nsm_30\ntarget architectures.\nSIMD video instructions.\nA new warp shuffle instruction.\nInstructions\nmad.cc\nand\nmadc\nfor efficient, extended-precision integer multiplication.\nSurface instructions with 3D and array geometries.\nThe texture instruction supports reads from cubemap and cubemap array textures.\nPlatform option\n.target\ndebug to declare that a PTX module contains\nDWARF\ndebug information.\npmevent.mask\n, for triggering multiple performance monitor events.\nPerformance monitor counter special registers\n%pm4..%pm7\n.\nSemantic Changes and Clarifications\nSpecial register\n%gridid\nhas been extended from 32-bits to 64-bits.\nPTX ISA version 3.0 deprecates module-scoped\n.reg\nand\n.local\nvariables when compiling to the\nApplication Binary Interface (ABI). When compiling without use of the ABI, module-scoped\n.reg\nand\n.local\nvariables are supported as before. When compiling legacy PTX code (ISA versions prior\nto 3.0) containing module-scoped\n.reg\nor\n.local\nvariables, the compiler silently disables\nuse of the ABI.\nThe\nshfl\ninstruction semantics were updated to clearly indicate that value of source operand\na\nis unpredictable for inactive and predicated-off threads within the warp.\nPTX modules no longer allow duplicate\n.version\ndirectives. This feature was unimplemented, so\nthere is no semantic change.\nUnimplemented instructions\nsuld.p\nand\nsust.p.{u32,s32,f32}\nhave been removed.\n13.35.\nChanges in PTX ISA Version 2.3\n\nNew Features\nPTX 2.3 adds support for texture arrays. The texture array feature supports access to an array of 1D\nor 2D textures, where an integer indexes into the array of textures, and then one or two\nsingle-precision floating point coordinates are used to address within the selected 1D or 2D\ntexture.\nPTX 2.3 adds a new directive,\n.address_size\n, for specifying the size of addresses.\nVariables in\n.const\nand\n.global\nstate spaces are initialized to zero by default.\nSemantic Changes and Clarifications\nThe semantics of the\n.maxntid\ndirective have been updated to match the current\nimplementation. Specifically,\n.maxntid\nonly guarantees that the total number of threads in a\nthread block does not exceed the maximum. Previously, the semantics indicated that the maximum was\nenforced separately in each dimension, which is not the case.\nBit field extract and insert instructions BFE and BFI now indicate that the\nlen\nand\npos\noperands are restricted to the value range\n0..255\n.\nUnimplemented instructions\n{atom,red}.{min,max}.f32\nhave been removed.\n13.36.\nChanges in PTX ISA Version 2.2\n\nNew Features\nPTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a\nnew directives for specifying that a kernel parameter is a pointer, for specifying to which state\nspace the parameter points, and for optionally specifying the alignment of the memory to which the\nparameter points.\nPTX 2.2 adds a new field named\nforce_unnormalized_coords\nto the\n.samplerref\nopaque\ntype. This field is used in the independent texturing mode to override the\nnormalized_coords\nfield in the texture header. This field is needed to support languages such as OpenCL, which\nrepresent the property of normalized/unnormalized coordinates in the sampler header rather than in\nthe texture header.\nPTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the\n.const\nstate space. Legacy PTX that uses explicit constant banks is still supported.\nPTX 2.2 adds a new\ntld4\ninstruction for loading a component (\nr\n,\ng\n,\nb\n, or\na\n) from\nthe four texels compising the bilinear interpolation footprint of a given texture location. This\ninstruction may be used to compute higher-precision bilerp results in software, or for performing\nhigher-bandwidth texture loads.\nSemantic Changes and Clarifications\nNone.\n13.37.\nChanges in PTX ISA Version 2.1\n\nNew Features\nThe underlying, stack-based ABI is supported in PTX ISA version 2.1 for\nsm_2x\ntargets.\nSupport for indirect calls has been implemented for\nsm_2x\ntargets.\nNew directives,\n.branchtargets\nand\n.calltargets\n, have been added for specifying potential\ntargets for indirect branches and indirect function calls. A\n.callprototype\ndirective has been\nadded for declaring the type signatures for indirect function calls.\nThe names of\n.global\nand\n.const\nvariables can now be specified in variable initializers to\nrepresent their addresses.\nA set of thirty-two driver-specific execution environment special registers has been added. These\nare named\n%envreg0..%envreg31\n.\nTextures and surfaces have new fields for channel data type and channel order, and the\ntxq\nand\nsuq\ninstructions support queries for these fields.\nDirective\n.minnctapersm\nhas replaced the\n.maxnctapersm\ndirective.\nDirective\n.reqntid\nhas been added to allow specification of exact CTA dimensions.\nA new instruction,\nrcp.approx.ftz.f64\n, has been added to compute a fast, gross approximate\nreciprocal.\nSemantic Changes and Clarifications\nA warning is emitted if\n.minnctapersm\nis specified without also specifying\n.maxntid\n.\n13.38.\nChanges in PTX ISA Version 2.0\n\nNew Features\nFloating Point Extensions\nThis section describes the floating-point changes in PTX ISA version 2.0 for\nsm_20\ntargets. The\ngoal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility\nwith legacy PTX ISA version 1.x code and\nsm_1x\ntargets.\nThe changes from PTX ISA version 1.x are as follows:\nSingle-precision instructions support subnormal numbers by default for\nsm_20\ntargets. The\n.ftz\nmodifier may be used to enforce backward compatibility with\nsm_1x\n.\nSingle-precision\nadd\n,\nsub\n, and\nmul\nnow support\n.rm\nand\n.rp\nrounding modifiers\nfor\nsm_20\ntargets.\nA single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754\ncompliant rounding modifiers and support for subnormal numbers. The\nfma.f32\ninstruction also\nsupports\n.ftz\nand\n.sat\nmodifiers.\nfma.f32\nrequires\nsm_20\n. The\nmad.f32\ninstruction has been extended with rounding modifiers so that it’s synonymous with\nfma.f32\nfor\nsm_20\ntargets. Both\nfma.f32\nand\nmad.f32\nrequire a rounding modifier for\nsm_20\ntargets.\nThe\nmad.f32\ninstruction\nwithout rounding\nis retained so that compilers can generate code for\nsm_1x\ntargets. When code compiled for\nsm_1x\nis executed on\nsm_20\ndevices,\nmad.f32\nmaps to\nfma.rn.f32\n.\nSingle- and double-precision\ndiv\n,\nrcp\n, and\nsqrt\nwith IEEE 754 compliant rounding have\nbeen added. These are indicated by the use of a rounding modifier and require\nsm_20\n.\nInstructions\ntestp\nand\ncopysign\nhave been added.\nNew Instructions\nA\nload uniform\ninstruction,\nldu\n, has been added.\nSurface instructions support additional\n.clamp\nmodifiers,\n.clamp\nand\n.zero\n.\nInstruction\nsust\nnow supports formatted surface stores.\nA\ncount leading zeros\ninstruction,\nclz\n, has been added.\nA\nfind leading non-sign bit instruction\n,\nbfind\n, has been added.\nA\nbit reversal\ninstruction,\nbrev\n, has been added.\nBit field extract and insert instructions,\nbfe\nand\nbfi\n, have been added.\nA\npopulation count\ninstruction,\npopc\n, has been added.\nA\nvote ballot\ninstruction,\nvote.ballot.b32\n, has been added.\nInstructions\n{atom,red}.add.f32\nhave been implemented.\nInstructions\n{atom,red}\n.shared have been extended to handle 64-bit data types for\nsm_20\ntargets.\nA system-level membar instruction,\nmembar.sys\n, has been added.\nThe\nbar\ninstruction has been extended as follows:\nA\nbar.arrive\ninstruction has been added.\nInstructions\nbar.red.popc.u32\nand\nbar.red.{and,or}.pred\nhave been added.\nbar\nnow supports optional thread count and register operands.\nScalar video instructions (includes\nprmt\n) have been added.\nInstruction\nisspacep\nfor querying whether a generic address falls within a specified state space\nwindow has been added.\nInstruction\ncvta\nfor converting global, local, and shared addresses to generic address and\nvice-versa has been added.\nOther New Features\nInstructions\nld\n,\nldu\n,\nst\n,\nprefetch\n,\nprefetchu\n,\nisspacep\n,\ncvta\n,\natom\n,\nand\nred\nnow support generic addressing.\nNew special registers\n%nwarpid\n,\n%nsmid\n,\n%clock64\n,\n%lanemask_{eq,le,lt,ge,gt}\nhave\nbeen added.\nCache operations have been added to instructions\nld\n,\nst\n,\nsuld\n, and\nsust\n, e.g., for\nprefetching\nto specified level of memory hierarchy. Instructions\nprefetch\nand\nprefetchu\nhave also been added.\nThe\n.maxnctapersm\ndirective was deprecated and replaced with\n.minnctapersm\nto better match\nits behavior and usage.\nA new directive,\n.section\n, has been added to replace the\n@@DWARF\nsyntax for passing\nDWARF-format debugging information through PTX.\nA new directive,\n.pragma\nnounroll\n, has been added to allow users to disable loop unrolling.\nSemantic Changes and Clarifications\nThe errata in\ncvt.ftz\nfor PTX ISA versions 1.4 and earlier, where single-precision subnormal\ninputs and results were not flushed to zero if either source or destination type size was 64-bits,\nhas been fixed. In PTX ISA version 1.5 and later,\ncvt.ftz\n(and\ncvt\nfor\n.target\nsm_1x\n,\nwhere\n.ftz\nis implied) instructions flush single-precision subnormal inputs and results to\nsign-preserving zero for all combinations of floating-point instruction types. To maintain\ncompatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs\nand results are flushed to sign-preserving zero only when neither source nor destination type size\nis 64-bits.\nComponents of special registers\n%tid\n,\n%ntid\n,\n%ctaid\n, and\n%nctaid\nhave been extended\nfrom 16-bits to 32-bits. These registers now have type\n.v4.u32\n.\nThe number of samplers available in independent texturing mode was incorrectly listed as thirty-two\nin PTX ISA version 1.5; the correct number is sixteen.\n14.\nNotices\n\n14.1.\nNotice\n\nThis document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\n14.2.\nOpenCL\n\nOpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.\n14.3.\nTrademarks\n\nNVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.\nPrivacy Policy\n|\nManage My Privacy\n|\nDo Not Sell or Share My Data\n|\nTerms of Service\n|\nAccessibility\n|\nCorporate Policies\n|\nProduct Security\n|\nContact\nCopyright © 2007-2026, NVIDIA Corporation & affiliates. All rights reserved.\nLast updated on Jan 08, 2026."}], [{"role": "system", "content": "You are a synthesis agent. Produce the final deliverable using the mandate, execution history, final notes, accumulated deliverables, and retrieved context. Be accurate and complete, but avoid unnecessary verbosity. Return JSON with keys: 'deliverable' and 'summary'."}, {"role": "user", "content": "MANDATE\n=======\nBenchmark Protocol 2: NVIDIA Tensor Core Evolution\n\nTask Statement\n\"Summarize how NVIDIA Tensor Core instructions evolved from Volta (2017) to Hopper (2022). Keep it short with citations.\"\n\nRequired Deliverables\n1. Instruction Evolution\nFor Volta and Hopper:\n\nPrimary instruction name\n\nOne supported data type per generation\n\n2 citations total: NVIDIA whitepaper or PTX docs\n\n2. PTX Snapshot\nPick one architecture (Volta or Hopper):\n\nProvide one exact PTX instruction syntax line\n\nOne-sentence description of what it does\n\nCite PTX ISA or CUDA programming manual\n\n3. Minimal Implementation Evidence\nFind one official NVIDIA example or library page that mentions Tensor Cores\n\nProvide the page URL and the feature name referenced\n\nShow a short path: \"CUDA docs → example/library page\"\n\nSuccess Criteria\n3 citations total\n\nPath with 2 steps\n\nExact instruction syntax present\n\nEXECUTION HISTORY\n=================\n1. [Tick 1] Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\n2. [Tick 2] Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\n3. [Tick 3] PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\n4. [Tick 4] Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.\n\nNOTES\n=====\n1. Next: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).\n2. Next: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.\n3. Search PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.\n4. Next: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.\n\nRETRIEVED CONTEXT\n=================\n[1] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[2] CVE-2022-40982 public disclosure was August 10, 2023 per NVD. Research by Daniel Moghimi (downfall.page) with PoC code linked from the project site to GitHub. Intel advisory is INTEL-SA-00828 naming it Gather Data Sampling (GDS).\n[3] Downfall (CVE-2022-40982) exploits Intel's vector 'Gather' instruction (vgather* variants) allowing transient/execution leakage of the internal vector register file. Intel advisory INTEL-SA-00828 names this Gather Data Sampling (GDS) issue and credits researcher Daniel Moghimi.\n[4] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[5] Linux kernel added documentation and mitigation support for Gather Data Sampling under Documentation/admin-guide/hw-vuln/gather_data_sampling.rst and related x86 mitigation code; distributions offer kernel/microcode updates and a gather_data_sampling= kernel parameter to control mitigation behavior.\n[6] Intel’s Gather Data Sampling (Downfall) advisory (INTEL-SA-00828) describes a transient execution side-channel where the vector GATHER instruction can leak data from internal buffers/vector registers across security domains. It credits Daniel Moghimi and provides mitigation guidance (microcode + OS).\n[7] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[8] The NVD entry for CVE-2022-40982 (Downfall / Gather Data Sampling) provides the CVE description, dates (published August 2023), and links to Intel’s advisory and the researcher site. Use this as the canonical CVE citation.\n[9] NVD’s CVE-2022-40982 entry (Downfall/Gather Data Sampling) provides the official description and a published date of 2023-08-10, plus references to Intel’s INTEL-SA-00828 advisory and the researcher site downfall.page.\n[10] PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\n[11] OpenSSH release-9.8 / 9.8p1 advisory notes the regreSSHion (CVE-2024-6387) fix: Portable OpenSSH 8.5p1–9.7p1 were affected and the 9.8/9.8p1 release contains the vendor fix; advisory lists affected versions and mitigation details.\n[12] Ubuntu security advisory USN-6859-1 confirms implementation of the CVE-2024-6387 fix within their packages, proving the fix distribution."}]], "llm_outputs": ["{\n  \"history_update\": \"Planned to gather exact PTX syntax and whitepaper citations for Volta and Hopper, plus an official NVIDIA example page mentioning Tensor Cores.\",\n  \"note_update\": \"Next: find PTX WMMA syntax (Volta) and Hopper whitepaper/WGMMA; then an official example page (WMMA sample or cuBLASLt).\",\n  \"cache_update\": [],\n  \"next_action\": \"search, PTX wmma.mma.sync syntax site:nvidia.com PDF\",\n  \"cache_retrieve\": [\n    \"Volta whitepaper Tensor Core FP16 NVIDIA PDF\",\n    \"Hopper whitepaper WGMMA FP8 NVIDIA PDF\",\n    \"PTX ISA WMMA instruction syntax example\",\n    \"NVIDIA CUDA sample or cuBLAS page mentioning Tensor Cores\"\n  ],\n  \"deliverable\": \"\"\n}", "{\n  \"history_update\": \"Reviewed initial search results and identified PTX ISA PDF for exact syntax and PTX pages referencing wmma/mma/wgmma. Planning to capture exact PTX line and whitepaper citations for Volta/Hopper.\",\n  \"note_update\": \"Next: open PTX ISA 8.5 for exact WMMA line; find Volta and Hopper whitepapers; then cuBLASLt or CUDA Samples page mentioning Tensor Cores.\",\n  \"cache_update\": [\n    {\n      \"document\": \"PTX introduces tensor instructions across generations: Volta exposes warp-level WMMA ops (wmma.load, wmma.mma, wmma.store), later PTX adds mma.sync, and Hopper adds warp-group WGMMA (wgmma.mma_async). The PTX ISA index explicitly lists wmma.mma, mma, and wgmma.mma_async as tensor ops.\",\n      \"metadata\": {\n        \"title\": \"PTX tensor instruction families (WMMA, MMA, WGMMA)\",\n        \"source\": \"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\",\n        \"topics\": \"PTX, tensor cores, wmma, mma, wgmma\"\n      }\n    }\n  ],\n  \"next_action\": \"visit, https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf\",\n  \"cache_retrieve\": [\n    \"NVIDIA Volta whitepaper Tensor Cores FP16 accumulate FP32 citation\",\n    \"NVIDIA Hopper H100 whitepaper WGMMA FP8 BF16 TF32 citation\",\n    \"PTX WMMA exact syntax line wmma.mma.sync.aligned example\"\n  ],\n  \"deliverable\": \"\"\n}", "{\n  \"history_update\": \"PTX PDF failed to open; pivoting to search for HTML PTX ISA pages and NVIDIA whitepapers. Will capture exact PTX syntax (wmma or wgmma) and official example/library page mentioning Tensor Cores.\",\n  \"note_update\": \"Search PTX ISA HTML for wmma/wgmma; get Volta and Hopper whitepapers; find CUDA docs → cuBLASLt Tensor Cores page.\",\n  \"cache_update\": [],\n  \"next_action\": \"search, NVIDIA PTX ISA wmma.mma.sync.aligned site:docs.nvidia.com\",\n  \"cache_retrieve\": [\n    \"NVIDIA Volta whitepaper Tensor Cores data types\",\n    \"NVIDIA Hopper whitepaper wgmma warp-group MMA\",\n    \"PTX ISA wgmma.mma_async syntax\",\n    \"PTX ISA wmma.mma.sync.aligned syntax\",\n    \"CUDA cuBLASLt Tensor Cores documentation page\"\n  ],\n  \"deliverable\": \"\"\n}", "{\n  \"history_update\": \"Searched NVIDIA PTX docs to confirm WMMA and WGMMA names and find an exact PTX syntax line plus data-type support. Preparing concise deliverable with three total citations.\",\n  \"note_update\": \"Next: open PTX sections for WMMA and WGMMA to grab exact syntax and FP16/FP8 tokens; then pick an official CUDA sample/library page mentioning Tensor Cores.\",\n  \"cache_update\": [\n    {\n      \"document\": \"PTX tensor instructions evolved from WMMA (warp-level) introduced with Volta to WGMMA (warp-group, async) in Hopper. Volta used wmma.mma for FP16 matrix multiply with FP32 accumulation. Hopper added wgmma.mma_async supporting FP8 (E4M3/E5M2), BF16, and more, enabling larger tile sizes and warp-group execution.\",\n      \"metadata\": {\n        \"title\": \"PTX tensor instruction evolution: WMMA to WGMMA\",\n        \"topics\": \"PTX,WMMA,WGMMA,Volta,Hopper,Tensor Cores,data types\"\n      }\n    }\n  ],\n  \"next_action\": \"visit, https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\",\n  \"cache_retrieve\": [\n    \"PTX WMMA exact syntax line for Volta wmma.mma\",\n    \"PTX WGMMA exact syntax line for Hopper wgmma.mma_async\",\n    \"Volta whitepaper FP16 Tensor Core description\",\n    \"Hopper PTX doc showing FP8 e4m3,e5m2 support\"\n  ],\n  \"deliverable\": \"\"\n}"], "cache_docs_added": 2, "cache_doc_chars": 596, "cache_doc_bytes": 596, "retrieved_chunks": 48, "retrieved_chars": 12729, "retrieved_bytes": 12777}}}
