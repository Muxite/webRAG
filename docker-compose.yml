name: "euglena"
services:
  rabbitmq:
    image: rabbitmq:3-management
    container_name: euglena_rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=guest
      - RABBITMQ_DEFAULT_PASS=guest

  inference:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    volumes:
      - ./inference/models:/models
    environment:
      - LLAMA_ARG_MODEL=/models/Phi-3-mini-4k-instruct-q4.gguf
      - LLAMA_ARG_N_GPU_LAYERS=999
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_API=true
    ports:
      - "8000:8000"
    entrypoint: ["/app/llama-server"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  chroma:
    image: chromadb/chroma:latest
    container_name: chroma_db
    ports:
      - "8001:8001"
    volumes:
      - ./chroma:/chroma-cache_retrieved
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma-data

  agent:
    profiles: ["main"]
    build:
      context: .
      dockerfile: agent/.dockerfile
    depends_on:
      - chroma
      - rabbitmq
    command: [ "python", "app/main.py" ]
    env_file:
      - .env
      - keys.env

  agent-cli:
    profiles: ["cli"]
    build:
      context: .
      dockerfile: agent/.dockerfile
    stdin_open: true
    tty: true
    depends_on:
      - chroma
    command: ["python", "app/basic_cli.py"]
    env_file:
      - .env
      - keys.env

  agent-test:
    profiles: ["test"]
    build:
      context: .
      dockerfile: agent/.dockerfile
    depends_on:
      - chroma
      - rabbitmq
    command: [ "pytest", "-vv", "tests" ]
    env_file:
      - .env
      - keys.env

  gateway:
    profiles: ["main"]
    build:
      context: .
      dockerfile: gateway/.dockerfile
    ports:
      - "8080:8080"
    depends_on:
      - rabbitmq
      - cache
    command: ["python", "-m", "app.main"]

  cache:
    image: redis:7-alpine
    container_name: euglena_cache
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: ["redis-server", "--appendonly", "yes"]

volumes:
  redis_data:
